Note:
Unit tests for differentiable operators library implementation.
Tests operator registration, basic arithmetic operators, transcendental functions,
linear algebra operations, activation functions, and custom operator registration.
:End Note

Import "math/engine/autodiff/operators" as Operators
Import "dev/debug/testing/assertion_engine" as Assert
Import "collections" as Collections

Note: =====================================================================
Note: OPERATOR SIGNATURE AND REGISTRY TESTS
Note: =====================================================================

Process called "test_operator_signature_construction" that takes no parameters returns Boolean:
    Note: Test basic operator signature construction
    Let input_shapes be Collections.CreateList[Collections.CreateList[Integer]]()
    Let shape1 be Collections.CreateList[Integer]()
    Call shape1.append(2)
    Call shape1.append(3)
    Call input_shapes.append(shape1)
    
    Let shape2 be Collections.CreateList[Integer]()
    Call shape2.append(2)
    Call shape2.append(3)
    Call input_shapes.append(shape2)
    
    Let output_shape be Collections.CreateList[Integer]()
    Call output_shape.append(2)
    Call output_shape.append(3)
    
    Let op_sig be Operators.OperatorSignature with:
        name = "element_wise_add"
        input_shapes = input_shapes
        output_shape = output_shape
        forward_function = "add_forward"
        backward_function = "add_backward"
        supports_broadcasting = true
    
    Assert.AreEqual(op_sig.name, "element_wise_add")
    Assert.AreEqual(op_sig.input_shapes.length, 2)
    Assert.AreEqual(op_sig.forward_function, "add_forward")
    Assert.AreEqual(op_sig.backward_function, "add_backward")
    Assert.IsTrue(op_sig.supports_broadcasting)
    
    Return true

Process called "test_gradient_operator_construction" that takes no parameters returns Boolean:
    Note: Test gradient operator construction
    Let local_gradients be Collections.CreateList[String]()
    Call local_gradients.append("grad_input1")
    Call local_gradients.append("grad_input2")
    
    Let grad_op be Operators.GradientOperator with:
        forward_pass = "multiply_forward"
        backward_pass = "multiply_backward"
        local_gradients = local_gradients
        memory_requirements = 1024
    
    Assert.AreEqual(grad_op.forward_pass, "multiply_forward")
    Assert.AreEqual(grad_op.backward_pass, "multiply_backward")
    Assert.AreEqual(grad_op.local_gradients.length, 2)
    Assert.AreEqual(grad_op.memory_requirements, 1024)
    
    Return true

Process called "test_operator_registry_construction" that takes no parameters returns Boolean:
    Note: Test operator registry construction and management
    Let operators be Dictionary[String, Operators.OperatorSignature]
    Let custom_operators be Dictionary[String, Operators.GradientOperator]
    Let operator_categories be Dictionary[String, Collections.CreateList[String]]
    
    Let arithmetic_ops be Collections.CreateList[String]()
    Call arithmetic_ops.append("add")
    Call arithmetic_ops.append("subtract")
    Call arithmetic_ops.append("multiply")
    Call arithmetic_ops.append("divide")
    Call operator_categories.set("arithmetic", arithmetic_ops)
    
    Let registry be Operators.OperatorRegistry with:
        operators = operators
        custom_operators = custom_operators
        operator_categories = operator_categories
    
    Assert.AreEqual(registry.operator_categories.get("arithmetic").length, 4)
    Assert.IsTrue(registry.operator_categories.get("arithmetic").contains("add"))
    
    Return true

Note: =====================================================================
Note: BASIC ARITHMETIC OPERATOR TESTS
Note: =====================================================================

Process called "test_addition_operator" that takes no parameters returns Boolean:
    Note: Test addition operator with gradient computation
    Let a be 5.0
    Let b be 3.0
    Let compute_gradient be true
    
    Let result_dict be Operators.add_operator(a, b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 8.0)
    Assert.AreEqual(result_dict.get("operation"), "add")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a"), 1.0)
    Assert.AreEqual(gradients.get("grad_b"), 1.0)
    
    Return true

Process called "test_addition_operator_no_gradients" that takes no parameters returns Boolean:
    Note: Test addition operator without gradient computation
    Let a be 7.0
    Let b be 2.0
    Let compute_gradient be false
    
    Let result_dict be Operators.add_operator(a, b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 9.0)
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a"), 0.0)
    Assert.AreEqual(gradients.get("grad_b"), 0.0)
    
    Return true

Process called "test_subtraction_operator" that takes no parameters returns Boolean:
    Note: Test subtraction operator with gradient computation
    Let a be 10.0
    Let b be 4.0
    Let compute_gradient be true
    
    Let result_dict be Operators.subtract_operator(a, b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 6.0)
    Assert.AreEqual(result_dict.get("operation"), "subtract")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a"), 1.0)
    Assert.AreEqual(gradients.get("grad_b"), -1.0)
    
    Return true

Process called "test_multiplication_operator" that takes no parameters returns Boolean:
    Note: Test multiplication operator with gradient computation
    Let a be 3.0
    Let b be 4.0
    Let compute_gradient be true
    
    Let result_dict be Operators.multiply_operator(a, b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 12.0)
    Assert.AreEqual(result_dict.get("operation"), "multiply")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a"), 4.0)  Note: gradient wrt a is b
    Assert.AreEqual(gradients.get("grad_b"), 3.0)  Note: gradient wrt b is a
    
    Return true

Process called "test_division_operator" that takes no parameters returns Boolean:
    Note: Test division operator with gradient computation
    Let a be 12.0
    Let b be 3.0
    Let compute_gradient be true
    
    Let result_dict be Operators.divide_operator(a, b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 4.0)
    Assert.AreEqual(result_dict.get("operation"), "divide")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a"), 1.0/3.0)  Note: 1/b
    Assert.AreEqual(gradients.get("grad_b"), -4.0/9.0)  Note: -a/b²
    
    Return true

Process called "test_division_by_zero_operator" that takes no parameters returns Boolean:
    Note: Test division operator error handling for zero denominator
    Let a be 5.0
    Let b be 0.0
    Let compute_gradient be true
    
    Let error_caught be false
    Try:
        Let invalid_result be Operators.divide_operator(a, b, compute_gradient)
    Catch error:
        Set error_caught to true
        Assert.IsTrue(error.message.contains("division by zero"))
    
    Assert.IsTrue(error_caught)
    
    Return true

Process called "test_power_operator" that takes no parameters returns Boolean:
    Note: Test power operator with gradient computation
    Let base be 2.0
    Let exponent be 3.0
    Let compute_gradient be true
    
    Let result_dict be Operators.power_operator(base, exponent, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 8.0)  Note: 2³ = 8
    Assert.AreEqual(result_dict.get("operation"), "power")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_base"), 12.0)  Note: 3 * 2² = 12
    Assert.IsTrue(MathCore.abs(gradients.get("grad_exponent") - 5.545) < 0.01)  Note: 8 * ln(2)
    
    Return true

Note: =====================================================================
Note: TRANSCENDENTAL FUNCTION OPERATOR TESTS
Note: =====================================================================

Process called "test_exponential_operator" that takes no parameters returns Boolean:
    Note: Test exponential operator with gradient computation
    Let x be 1.0
    Let compute_gradient be true
    
    Let result_dict be Operators.exp_operator(x, compute_gradient)
    
    Assert.IsTrue(MathCore.abs(result_dict.get("result") - 2.718) < 0.01)
    Assert.AreEqual(result_dict.get("operation"), "exp")
    
    Let gradients be result_dict.get("gradients")
    Assert.IsTrue(MathCore.abs(gradients.get("grad_x") - 2.718) < 0.01)  Note: d/dx[e^x] = e^x
    
    Return true

Process called "test_logarithm_operator" that takes no parameters returns Boolean:
    Note: Test natural logarithm operator with gradient computation
    Let x be 2.718
    Let compute_gradient be true
    
    Let result_dict be Operators.log_operator(x, compute_gradient)
    
    Assert.IsTrue(MathCore.abs(result_dict.get("result") - 1.0) < 0.01)
    Assert.AreEqual(result_dict.get("operation"), "log")
    
    Let gradients be result_dict.get("gradients")
    Assert.IsTrue(MathCore.abs(gradients.get("grad_x") - 0.368) < 0.01)  Note: d/dx[ln(x)] = 1/x
    
    Return true

Process called "test_sine_operator" that takes no parameters returns Boolean:
    Note: Test sine operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.sin_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.0)
    Assert.AreEqual(result_dict.get("operation"), "sin")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 1.0)  Note: d/dx[sin(x)] = cos(x), cos(0) = 1
    
    Return true

Process called "test_cosine_operator" that takes no parameters returns Boolean:
    Note: Test cosine operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.cos_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 1.0)
    Assert.AreEqual(result_dict.get("operation"), "cos")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 0.0)  Note: d/dx[cos(x)] = -sin(x), -sin(0) = 0
    
    Return true

Process called "test_tangent_operator" that takes no parameters returns Boolean:
    Note: Test tangent operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.tan_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.0)
    Assert.AreEqual(result_dict.get("operation"), "tan")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 1.0)  Note: d/dx[tan(x)] = sec²(x), sec²(0) = 1
    
    Return true

Note: =====================================================================
Note: HYPERBOLIC FUNCTION OPERATOR TESTS
Note: =====================================================================

Process called "test_hyperbolic_sine_operator" that takes no parameters returns Boolean:
    Note: Test hyperbolic sine operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.sinh_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.0)
    Assert.AreEqual(result_dict.get("operation"), "sinh")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 1.0)  Note: d/dx[sinh(x)] = cosh(x), cosh(0) = 1
    
    Return true

Process called "test_hyperbolic_cosine_operator" that takes no parameters returns Boolean:
    Note: Test hyperbolic cosine operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.cosh_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 1.0)
    Assert.AreEqual(result_dict.get("operation"), "cosh")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 0.0)  Note: d/dx[cosh(x)] = sinh(x), sinh(0) = 0
    
    Return true

Process called "test_hyperbolic_tangent_operator" that takes no parameters returns Boolean:
    Note: Test hyperbolic tangent operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.tanh_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.0)
    Assert.AreEqual(result_dict.get("operation"), "tanh")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 1.0)  Note: d/dx[tanh(x)] = sech²(x), sech²(0) = 1
    
    Return true

Note: =====================================================================
Note: LINEAR ALGEBRA OPERATOR TESTS
Note: =====================================================================

Process called "test_matrix_multiply_operator" that takes no parameters returns Boolean:
    Note: Test matrix multiplication operator with gradient computation
    Let matrix_a be Collections.CreateList[Collections.CreateList[Float]]()
    Let row1_a be Collections.CreateList[Float]()
    Call row1_a.append(1.0)
    Call row1_a.append(2.0)
    Call matrix_a.append(row1_a)
    
    Let row2_a be Collections.CreateList[Float]()
    Call row2_a.append(3.0)
    Call row2_a.append(4.0)
    Call matrix_a.append(row2_a)
    
    Let matrix_b be Collections.CreateList[Collections.CreateList[Float]]()
    Let row1_b be Collections.CreateList[Float]()
    Call row1_b.append(5.0)
    Call row1_b.append(6.0)
    Call matrix_b.append(row1_b)
    
    Let row2_b be Collections.CreateList[Float]()
    Call row2_b.append(7.0)
    Call row2_b.append(8.0)
    Call matrix_b.append(row2_b)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.matmul_operator(matrix_a, matrix_b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("operation"), "matmul")
    
    Let result_matrix be result_dict.get("result")
    Note: [1,2] × [5,6] = [1*5+2*7, 1*6+2*8] = [19, 22]
    Note: [3,4]   [7,8]   [3*5+4*7, 3*6+4*8]   [43, 50]
    Assert.AreEqual(result_matrix.get(0).get(0), 19.0)
    Assert.AreEqual(result_matrix.get(0).get(1), 22.0)
    Assert.AreEqual(result_matrix.get(1).get(0), 43.0)
    Assert.AreEqual(result_matrix.get(1).get(1), 50.0)
    
    Return true

Process called "test_transpose_operator" that takes no parameters returns Boolean:
    Note: Test matrix transpose operator with gradient computation
    Let matrix be Collections.CreateList[Collections.CreateList[Float]]()
    Let row1 be Collections.CreateList[Float]()
    Call row1.append(1.0)
    Call row1.append(2.0)
    Call row1.append(3.0)
    Call matrix.append(row1)
    
    Let row2 be Collections.CreateList[Float]()
    Call row2.append(4.0)
    Call row2.append(5.0)
    Call row2.append(6.0)
    Call matrix.append(row2)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.transpose_operator(matrix, compute_gradient)
    
    Assert.AreEqual(result_dict.get("operation"), "transpose")
    
    Let result_matrix be result_dict.get("result")
    Assert.AreEqual(result_matrix.get(0).get(0), 1.0)  Note: [1,4]
    Assert.AreEqual(result_matrix.get(0).get(1), 4.0)  Note: [2,5]
    Assert.AreEqual(result_matrix.get(1).get(0), 2.0)  Note: [3,6]
    Assert.AreEqual(result_matrix.get(1).get(1), 5.0)
    Assert.AreEqual(result_matrix.get(2).get(0), 3.0)
    Assert.AreEqual(result_matrix.get(2).get(1), 6.0)
    
    Return true

Process called "test_dot_product_operator" that takes no parameters returns Boolean:
    Note: Test vector dot product operator with gradient computation
    Let vector_a be Collections.CreateList[Float]()
    Call vector_a.append(1.0)
    Call vector_a.append(2.0)
    Call vector_a.append(3.0)
    
    Let vector_b be Collections.CreateList[Float]()
    Call vector_b.append(4.0)
    Call vector_b.append(5.0)
    Call vector_b.append(6.0)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.dot_product_operator(vector_a, vector_b, compute_gradient)
    
    Note: 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32
    Assert.AreEqual(result_dict.get("result"), 32.0)
    Assert.AreEqual(result_dict.get("operation"), "dot_product")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_a").get(0), 4.0)  Note: gradient wrt a[i] is b[i]
    Assert.AreEqual(gradients.get("grad_b").get(0), 1.0)  Note: gradient wrt b[i] is a[i]
    
    Return true

Note: =====================================================================
Note: REDUCTION OPERATOR TESTS
Note: =====================================================================

Process called "test_sum_reduction_operator" that takes no parameters returns Boolean:
    Note: Test sum reduction operator with gradient computation
    Let values be Collections.CreateList[Float]()
    Call values.append(1.0)
    Call values.append(2.0)
    Call values.append(3.0)
    Call values.append(4.0)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.sum_operator(values, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 10.0)
    Assert.AreEqual(result_dict.get("operation"), "sum")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_inputs").get(0), 1.0)  Note: all gradients are 1
    Assert.AreEqual(gradients.get("grad_inputs").get(1), 1.0)
    Assert.AreEqual(gradients.get("grad_inputs").get(2), 1.0)
    Assert.AreEqual(gradients.get("grad_inputs").get(3), 1.0)
    
    Return true

Process called "test_mean_reduction_operator" that takes no parameters returns Boolean:
    Note: Test mean reduction operator with gradient computation
    Let values be Collections.CreateList[Float]()
    Call values.append(2.0)
    Call values.append(4.0)
    Call values.append(6.0)
    Call values.append(8.0)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.mean_operator(values, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 5.0)  Note: (2+4+6+8)/4 = 5
    Assert.AreEqual(result_dict.get("operation"), "mean")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_inputs").get(0), 0.25)  Note: gradient is 1/n = 1/4
    Assert.AreEqual(gradients.get("grad_inputs").get(1), 0.25)
    Assert.AreEqual(gradients.get("grad_inputs").get(2), 0.25)
    Assert.AreEqual(gradients.get("grad_inputs").get(3), 0.25)
    
    Return true

Process called "test_max_reduction_operator" that takes no parameters returns Boolean:
    Note: Test max reduction operator with gradient computation
    Let values be Collections.CreateList[Float]()
    Call values.append(1.0)
    Call values.append(5.0)
    Call values.append(3.0)
    Call values.append(2.0)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.max_operator(values, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 5.0)
    Assert.AreEqual(result_dict.get("operation"), "max")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_inputs").get(0), 0.0)  Note: gradient is 0 for non-max elements
    Assert.AreEqual(gradients.get("grad_inputs").get(1), 1.0)  Note: gradient is 1 for max element
    Assert.AreEqual(gradients.get("grad_inputs").get(2), 0.0)
    Assert.AreEqual(gradients.get("grad_inputs").get(3), 0.0)
    
    Return true

Note: =====================================================================
Note: ACTIVATION FUNCTION OPERATOR TESTS
Note: =====================================================================

Process called "test_relu_activation_operator" that takes no parameters returns Boolean:
    Note: Test ReLU activation operator with gradient computation
    Let x be 2.0
    Let compute_gradient be true
    
    Let result_dict be Operators.relu_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 2.0)  Note: max(0, 2) = 2
    Assert.AreEqual(result_dict.get("operation"), "relu")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 1.0)  Note: gradient is 1 for x > 0
    
    Return true

Process called "test_relu_negative_input" that takes no parameters returns Boolean:
    Note: Test ReLU with negative input
    Let x be -1.5
    Let compute_gradient be true
    
    Let result_dict be Operators.relu_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.0)  Note: max(0, -1.5) = 0
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 0.0)  Note: gradient is 0 for x < 0
    
    Return true

Process called "test_sigmoid_activation_operator" that takes no parameters returns Boolean:
    Note: Test sigmoid activation operator with gradient computation
    Let x be 0.0
    Let compute_gradient be true
    
    Let result_dict be Operators.sigmoid_operator(x, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), 0.5)  Note: sigmoid(0) = 1/(1+e^0) = 0.5
    Assert.AreEqual(result_dict.get("operation"), "sigmoid")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 0.25)  Note: sigmoid'(0) = 0.5 * (1 - 0.5) = 0.25
    
    Return true

Process called "test_leaky_relu_activation_operator" that takes no parameters returns Boolean:
    Note: Test Leaky ReLU activation operator with gradient computation
    Let x be -2.0
    Let alpha be 0.01
    Let compute_gradient be true
    
    Let result_dict be Operators.leaky_relu_operator(x, alpha, compute_gradient)
    
    Assert.AreEqual(result_dict.get("result"), -0.02)  Note: 0.01 * (-2) = -0.02
    Assert.AreEqual(result_dict.get("operation"), "leaky_relu")
    
    Let gradients be result_dict.get("gradients")
    Assert.AreEqual(gradients.get("grad_x"), 0.01)  Note: gradient is alpha for x < 0
    
    Return true

Process called "test_softmax_activation_operator" that takes no parameters returns Boolean:
    Note: Test softmax activation operator with gradient computation
    Let logits be Collections.CreateList[Float]()
    Call logits.append(1.0)
    Call logits.append(2.0)
    Call logits.append(3.0)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.softmax_operator(logits, compute_gradient)
    
    Assert.AreEqual(result_dict.get("operation"), "softmax")
    
    Let probabilities be result_dict.get("result")
    Let sum_probs be probabilities.get(0) + probabilities.get(1) + probabilities.get(2)
    Assert.IsTrue(MathCore.abs(sum_probs - 1.0) < 0.001)  Note: probabilities should sum to 1
    
    Return true

Note: =====================================================================
Note: BROADCASTING AND RESHAPING TESTS
Note: =====================================================================

Process called "test_broadcasting_add_operator" that takes no parameters returns Boolean:
    Note: Test broadcasting addition for different shaped tensors
    Let tensor_a be Collections.CreateList[Float]()
    Call tensor_a.append(1.0)
    Call tensor_a.append(2.0)
    
    Let scalar_b be 3.0
    Let compute_gradient be true
    
    Let result_dict be Operators.broadcast_add_operator(tensor_a, scalar_b, compute_gradient)
    
    Assert.AreEqual(result_dict.get("operation"), "broadcast_add")
    
    Let result_tensor be result_dict.get("result")
    Assert.AreEqual(result_tensor.get(0), 4.0)  Note: 1 + 3 = 4
    Assert.AreEqual(result_tensor.get(1), 5.0)  Note: 2 + 3 = 5
    
    Return true

Process called "test_reshape_operator" that takes no parameters returns Boolean:
    Note: Test reshape operator with gradient computation
    Let input_tensor be Collections.CreateList[Float]()
    Call input_tensor.append(1.0)
    Call input_tensor.append(2.0)
    Call input_tensor.append(3.0)
    Call input_tensor.append(4.0)
    
    Let new_shape be Collections.CreateList[Integer]()
    Call new_shape.append(2)
    Call new_shape.append(2)
    
    Let compute_gradient be true
    
    Let result_dict be Operators.reshape_operator(input_tensor, new_shape, compute_gradient)
    
    Assert.AreEqual(result_dict.get("operation"), "reshape")
    
    Let result_matrix be result_dict.get("result")
    Assert.AreEqual(result_matrix.get(0).get(0), 1.0)
    Assert.AreEqual(result_matrix.get(0).get(1), 2.0)
    Assert.AreEqual(result_matrix.get(1).get(0), 3.0)
    Assert.AreEqual(result_matrix.get(1).get(1), 4.0)
    
    Return true

Note: =====================================================================
Note: CUSTOM OPERATOR REGISTRATION TESTS
Note: =====================================================================

Process called "test_custom_operator_registration" that takes no parameters returns Boolean:
    Note: Test registration of custom differentiable operators
    Let local_gradients be Collections.CreateList[String]()
    Call local_gradients.append("custom_grad_x")
    Call local_gradients.append("custom_grad_y")
    
    Let custom_op be Operators.GradientOperator with:
        forward_pass = "custom_forward"
        backward_pass = "custom_backward"
        local_gradients = local_gradients
        memory_requirements = 512
    
    Let registry be Operators.OperatorRegistry with:
        operators = Dictionary[String, Operators.OperatorSignature]
        custom_operators = Dictionary[String, Operators.GradientOperator]
        operator_categories = Dictionary[String, Collections.CreateList[String]]
    
    Let registration_success be Operators.register_custom_operator(registry, "my_custom_op", custom_op)
    
    Assert.IsTrue(registration_success)
    Assert.IsTrue(registry.custom_operators.has_key("my_custom_op"))
    
    Return true

Process called "test_custom_operator_execution" that takes no parameters returns Boolean:
    Note: Test execution of registered custom operator
    Let registry be Operators.OperatorRegistry with:
        operators = Dictionary[String, Operators.OperatorSignature]
        custom_operators = Dictionary[String, Operators.GradientOperator]
        operator_categories = Dictionary[String, Collections.CreateList[String]]
    
    Let inputs be Collections.CreateList[Float]()
    Call inputs.append(5.0)
    Call inputs.append(3.0)
    
    Let custom_result be Operators.execute_custom_operator(registry, "my_custom_op", inputs)
    
    Assert.IsTrue(custom_result.has_key("result"))
    Assert.IsTrue(custom_result.has_key("gradients"))
    Assert.IsTrue(custom_result.has_key("operation"))
    
    Return true

Note: =====================================================================
Note: OPERATOR CHAINING AND COMPOSITION TESTS
Note: =====================================================================

Process called "test_operator_chaining" that takes no parameters returns Boolean:
    Note: Test chaining multiple differentiable operators
    Let x be 2.0
    Let y be 3.0
    
    Note: Compute (x + y) * exp(x)
    Let add_result be Operators.add_operator(x, y, true)
    Let sum_value be add_result.get("result")
    
    Let exp_result be Operators.exp_operator(x, true)
    Let exp_value be exp_result.get("result")
    
    Let final_result be Operators.multiply_operator(sum_value, exp_value, true)
    
    Assert.AreEqual(final_result.get("operation"), "multiply")
    Assert.IsTrue(final_result.get("result") > 0.0)
    
    Let final_gradients be final_result.get("gradients")
    Assert.IsTrue(final_gradients.has_key("grad_a"))
    Assert.IsTrue(final_gradients.has_key("grad_b"))
    
    Return true

Process called "test_complex_operator_composition" that takes no parameters returns Boolean:
    Note: Test complex composition of operators: sigmoid(x * w + b)
    Let x be 1.0
    Let w be 0.5
    Let b be 0.2
    
    Let multiply_result be Operators.multiply_operator(x, w, true)
    Let product be multiply_result.get("result")
    
    Let add_result be Operators.add_operator(product, b, true)
    Let linear_combination be add_result.get("result")
    
    Let sigmoid_result be Operators.sigmoid_operator(linear_combination, true)
    
    Assert.AreEqual(sigmoid_result.get("operation"), "sigmoid")
    Assert.IsTrue(sigmoid_result.get("result") > 0.0)
    Assert.IsTrue(sigmoid_result.get("result") < 1.0)
    
    Return true

Note: =====================================================================
Note: PERFORMANCE AND MEMORY TESTS
Note: =====================================================================

Process called "test_operator_memory_usage" that takes no parameters returns Boolean:
    Note: Test memory usage tracking for operators
    Let large_tensor be Collections.CreateList[Float]()
    Let i be 0
    While i < 1000:
        Call large_tensor.append(Float(i))
        Set i to i + 1
    
    Let memory_before be Operators.get_memory_usage()
    
    Let sum_result be Operators.sum_operator(large_tensor, true)
    
    Let memory_after be Operators.get_memory_usage()
    
    Assert.IsTrue(memory_after >= memory_before)
    Assert.IsTrue(sum_result.has_key("result"))
    
    Return true

Process called "test_operator_performance_benchmarking" that takes no parameters returns Boolean:
    Note: Test operator performance benchmarking
    Let iterations be 100
    Let x be 2.0
    Let y be 3.0
    
    Let start_time be DateTime.GetCurrentTimestamp()
    
    Let i be 0
    While i < iterations:
        Let result be Operators.multiply_operator(x, y, true)
        Set i to i + 1
    
    Let end_time be DateTime.GetCurrentTimestamp()
    Let execution_time be end_time - start_time
    
    Assert.IsTrue(execution_time >= 0.0)
    
    Return true

Note: =====================================================================
Note: TEST RUNNER PROCESS
Note: =====================================================================

Process called "run_operators_tests" that takes no parameters returns Boolean:
    Note: Run all differentiable operators tests
    Let test_results be Collections.CreateList[Boolean]()
    Let test_names be Collections.CreateList[String]()
    
    Note: Operator signature and registry tests
    Call test_names.append("Operator Signature Construction")
    Call test_results.append(test_operator_signature_construction())
    
    Call test_names.append("Gradient Operator Construction")
    Call test_results.append(test_gradient_operator_construction())
    
    Call test_names.append("Operator Registry Construction")
    Call test_results.append(test_operator_registry_construction())
    
    Note: Basic arithmetic operator tests
    Call test_names.append("Addition Operator")
    Call test_results.append(test_addition_operator())
    
    Call test_names.append("Addition Operator No Gradients")
    Call test_results.append(test_addition_operator_no_gradients())
    
    Call test_names.append("Subtraction Operator")
    Call test_results.append(test_subtraction_operator())
    
    Call test_names.append("Multiplication Operator")
    Call test_results.append(test_multiplication_operator())
    
    Call test_names.append("Division Operator")
    Call test_results.append(test_division_operator())
    
    Call test_names.append("Division by Zero Operator")
    Call test_results.append(test_division_by_zero_operator())
    
    Call test_names.append("Power Operator")
    Call test_results.append(test_power_operator())
    
    Note: Transcendental function tests
    Call test_names.append("Exponential Operator")
    Call test_results.append(test_exponential_operator())
    
    Call test_names.append("Logarithm Operator")
    Call test_results.append(test_logarithm_operator())
    
    Call test_names.append("Sine Operator")
    Call test_results.append(test_sine_operator())
    
    Call test_names.append("Cosine Operator")
    Call test_results.append(test_cosine_operator())
    
    Call test_names.append("Tangent Operator")
    Call test_results.append(test_tangent_operator())
    
    Note: Hyperbolic function tests
    Call test_names.append("Hyperbolic Sine Operator")
    Call test_results.append(test_hyperbolic_sine_operator())
    
    Call test_names.append("Hyperbolic Cosine Operator")
    Call test_results.append(test_hyperbolic_cosine_operator())
    
    Call test_names.append("Hyperbolic Tangent Operator")
    Call test_results.append(test_hyperbolic_tangent_operator())
    
    Note: Linear algebra tests
    Call test_names.append("Matrix Multiply Operator")
    Call test_results.append(test_matrix_multiply_operator())
    
    Call test_names.append("Transpose Operator")
    Call test_results.append(test_transpose_operator())
    
    Call test_names.append("Dot Product Operator")
    Call test_results.append(test_dot_product_operator())
    
    Note: Reduction operator tests
    Call test_names.append("Sum Reduction Operator")
    Call test_results.append(test_sum_reduction_operator())
    
    Call test_names.append("Mean Reduction Operator")
    Call test_results.append(test_mean_reduction_operator())
    
    Call test_names.append("Max Reduction Operator")
    Call test_results.append(test_max_reduction_operator())
    
    Note: Activation function tests
    Call test_names.append("ReLU Activation Operator")
    Call test_results.append(test_relu_activation_operator())
    
    Call test_names.append("ReLU Negative Input")
    Call test_results.append(test_relu_negative_input())
    
    Call test_names.append("Sigmoid Activation Operator")
    Call test_results.append(test_sigmoid_activation_operator())
    
    Call test_names.append("Leaky ReLU Activation Operator")
    Call test_results.append(test_leaky_relu_activation_operator())
    
    Call test_names.append("Softmax Activation Operator")
    Call test_results.append(test_softmax_activation_operator())
    
    Note: Broadcasting and reshaping tests
    Call test_names.append("Broadcasting Add Operator")
    Call test_results.append(test_broadcasting_add_operator())
    
    Call test_names.append("Reshape Operator")
    Call test_results.append(test_reshape_operator())
    
    Note: Custom operator tests
    Call test_names.append("Custom Operator Registration")
    Call test_results.append(test_custom_operator_registration())
    
    Call test_names.append("Custom Operator Execution")
    Call test_results.append(test_custom_operator_execution())
    
    Note: Operator composition tests
    Call test_names.append("Operator Chaining")
    Call test_results.append(test_operator_chaining())
    
    Call test_names.append("Complex Operator Composition")
    Call test_results.append(test_complex_operator_composition())
    
    Note: Performance tests
    Call test_names.append("Operator Memory Usage")
    Call test_results.append(test_operator_memory_usage())
    
    Call test_names.append("Operator Performance Benchmarking")
    Call test_results.append(test_operator_performance_benchmarking())
    
    Note: Report test results
    Let total_tests be test_results.length
    Let passed_tests be 0
    Let failed_tests be 0
    
    Let i be 0
    While i < total_tests:
        If test_results.get(i):
            Set passed_tests to passed_tests + 1
        Otherwise:
            Set failed_tests to failed_tests + 1
            Assert.LogMessage("FAILED: " + test_names.get(i))
        Set i to i + 1
    
    Assert.LogMessage("Operators Test Results:")
    Assert.LogMessage("Total Tests: " + String(total_tests))
    Assert.LogMessage("Passed: " + String(passed_tests))
    Assert.LogMessage("Failed: " + String(failed_tests))
    
    If failed_tests == 0:
        Assert.LogMessage("All operator tests PASSED!")
        Return true
    Otherwise:
        Assert.LogMessage("Some operator tests FAILED!")
        Return false