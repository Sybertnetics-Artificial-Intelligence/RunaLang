Note:
tests/unit/libraries/math/engine/optimization/gradient_test.runa
Unit Tests for Math Engine Optimization Gradient Module

This test suite provides comprehensive testing for the math engine optimization gradient module including:
- Advanced gradient descent methods (AdaGrad, RMSprop, Adam, AdaDelta, Nadam)
- Momentum-based optimization algorithms
- Adaptive learning rate methods and strategies
- Proximal gradient methods for regularized optimization
- Stochastic gradient descent variants and mini-batch optimization
- Gradient clipping and normalization techniques
- Learning rate scheduling and annealing strategies
- Gradient history tracking and analysis
- Subgradient methods for non-smooth optimization
- Coordinate descent and block coordinate descent
- Accelerated gradient methods (Nesterov momentum, FISTA)
- Variance reduction techniques (SVRG, SAG, SAGA)
:End Note

Import "stdlib/math/engine/optimization/gradient" as GradOpt
Import "dev/debug/test_framework/assertions" as Assert
Import "dev/debug/test_framework/test_runner" as TestRunner
Import "dev/debug/test_framework/data_generators" as DataGen
Import "math.core" as MathCore
Import "collections" as Collections

Note: =====================================================================
Note: HELPER FUNCTIONS AND TEST UTILITIES
Note: =====================================================================

Process called "create_simple_quadratic_objective" that takes scale as Float returns GradientObjective:
    Note: Create simple quadratic objective for testing f(x) = scale * ||x||¬≤
    Return GradOpt.GradientObjective{
        function: "lambda x: " + ToString(scale) + " * sum(xi*xi for xi in x)",
        gradient: "lambda x: [2*" + ToString(scale) + "*xi for xi in x]",
        dimension: 2,
        smooth: True,
        convex: True,
        lipschitz_constant: ToString(2.0 * scale)
    }

Process called "create_rosenbrock_objective" that takes dimension as Integer returns GradientObjective:
    Note: Create Rosenbrock objective for gradient method testing
    Return GradOpt.GradientObjective{
        function: "rosenbrock_" + ToString(dimension) + "d",
        gradient: "rosenbrock_gradient_" + ToString(dimension) + "d",
        dimension: dimension,
        smooth: True,
        convex: False,
        lipschitz_constant: "adaptive"
    }

Process called "create_noisy_quadratic_objective" that takes scale as Float, noise_level as Float returns StochasticGradientObjective:
    Note: Create noisy quadratic objective for stochastic methods
    Return GradOpt.StochasticGradientObjective{
        function: "lambda x: " + ToString(scale) + " * sum(xi*xi for xi in x)",
        gradient: "lambda x: [2*" + ToString(scale) + "*xi + normal(0, " + ToString(noise_level) + ") for xi in x]",
        batch_gradient: "lambda x, batch: [2*" + ToString(scale) + "*xi for xi in x]",
        dimension: 2,
        smooth: True,
        convex: True,
        variance: ToString(noise_level)
    }

Process called "create_test_gradient_config" returns GradientConfig:
    Note: Create default gradient optimization configuration for testing
    Return GradOpt.GradientConfig{
        method: "adam",
        learning_rate: "0.01",
        momentum: "0.9",
        beta1: "0.9",
        beta2: "0.999",
        epsilon: "1e-8",
        gradient_clipping: "none",
        clipping_threshold: "1.0",
        learning_rate_schedule: "constant"
    }

Process called "create_adaptive_config" returns AdaptiveConfig:
    Note: Create adaptive learning rate configuration
    Return GradOpt.AdaptiveConfig{
        initial_rate: "0.1",
        decay_rate: "0.95",
        decay_steps: 100,
        min_learning_rate: "1e-6",
        patience: 10,
        reduction_factor: "0.5",
        adaptation_method: "plateau_reduction"
    }

Process called "assert_gradient_result_valid" that takes result as GradientOptimizationResult returns Boolean:
    Note: Assert that gradient optimization result is valid
    Assert.IsNotNull(result)
    Assert.IsTrue(result.converged == "true" or result.converged == "false")
    Assert.IsTrue(result.iterations >= 0)
    Assert.IsTrue(result.gradient_evaluations >= 0)
    Assert.IsNotEmpty(result.solution)
    Assert.IsNotEmpty(result.final_gradient_norm)
    Assert.IsNotEmpty(result.final_objective_value)
    Return True

Process called "generate_random_starting_points" that takes dimension as Integer, count as Integer returns List[List[String]]:
    Note: Generate random starting points for testing robustness
    Let points be Collections.create_list()
    For i from 0 to count - 1:
        Let point be Collections.create_list()
        For j from 0 to dimension - 1:
            Let random_value be DataGen.generate_random_float(-2.0, 2.0)
            point.append(ToString(random_value))
        points.append(point)
    Return points

Note: =====================================================================
Note: ADAGRAD OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_adagrad_basic_convergence" that takes no parameters returns Boolean:
    Note: Test basic AdaGrad convergence on simple quadratic
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be create_test_gradient_config()
    Set config.method to "adagrad"
    Set config.learning_rate to "0.1"
    
    Let starting_point be ["1.0", "1.0"]
    Let result be GradOpt.adagrad_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Check solution accuracy
    Let solution_x be MathCore.parse_float(result.solution[0])
    Let solution_y be MathCore.parse_float(result.solution[1])
    Assert.IsTrue(AbsoluteValue(solution_x) < 1e-4)
    Assert.IsTrue(AbsoluteValue(solution_y) < 1e-4)
    Return True

Process called "test_adagrad_learning_rate_adaptation" that takes no parameters returns Boolean:
    Note: Test AdaGrad learning rate adaptation over iterations
    Let objective be create_simple_quadratic_objective(2.0)
    Let config be create_test_gradient_config()
    Set config.method to "adagrad"
    Set config.learning_rate to "0.5"
    
    Let starting_point be ["2.0", "-1.5"]
    Let result be GradOpt.adagrad_optimize_with_history(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.history)
    Assert.IsTrue(result.history.effective_learning_rates.length > 5)
    
    Note: Check that effective learning rates decrease over time
    Let first_rate be MathCore.parse_float(result.history.effective_learning_rates[0])
    Let last_rate be MathCore.parse_float(result.history.effective_learning_rates[-1])
    Assert.IsTrue(last_rate < first_rate)
    Return True

Process called "test_adagrad_diagonal_accumulation" that takes no parameters returns Boolean:
    Note: Test AdaGrad diagonal accumulation mechanism
    Let objective be create_rosenbrock_objective(2)
    Let config be create_test_gradient_config()
    Set config.method to "adagrad"
    Set config.learning_rate to "0.01"
    Set config.max_iterations to 200
    
    Let starting_point be ["-1.2", "1.0"]
    Let result be GradOpt.adagrad_optimize_with_state(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.final_state)
    Assert.IsTrue(result.final_state.gradient_accumulator.length == 2)
    
    Note: Accumulated gradients should be positive
    For accumulated_grad in result.final_state.gradient_accumulator:
        Assert.IsTrue(MathCore.parse_float(accumulated_grad) >= 0.0)
    
    Return True

Note: =====================================================================
Note: RMSPROP OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_rmsprop_exponential_averaging" that takes no parameters returns Boolean:
    Note: Test RMSprop exponential moving average of squared gradients
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be create_test_gradient_config()
    Set config.method to "rmsprop"
    Set config.learning_rate to "0.01"
    Set config.rho to "0.9"
    
    Let starting_point be ["1.0", "-1.0"]
    Let result be GradOpt.rmsprop_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_rmsprop_vs_adagrad_comparison" that takes no parameters returns Boolean:
    Note: Compare RMSprop vs AdaGrad on same problem
    Let objective be create_simple_quadratic_objective(0.5)
    Let base_config be create_test_gradient_config()
    Set base_config.learning_rate to "0.1"
    Set base_config.max_iterations to 100
    
    Note: Test AdaGrad
    Let adagrad_config be base_config
    Set adagrad_config.method to "adagrad"
    Let starting_point be ["2.0", "1.5"]
    Let adagrad_result be GradOpt.adagrad_optimize(objective, starting_point, adagrad_config)
    
    Note: Test RMSprop
    Let rmsprop_config be base_config
    Set rmsprop_config.method to "rmsprop"
    Set rmsprop_config.rho to "0.9"
    Let rmsprop_result be GradOpt.rmsprop_optimize(objective, starting_point, rmsprop_config)
    
    Assert.IsTrue(assert_gradient_result_valid(adagrad_result))
    Assert.IsTrue(assert_gradient_result_valid(rmsprop_result))
    
    Note: RMSprop should often converge faster due to exponential averaging
    Assert.IsTrue(rmsprop_result.iterations <= adagrad_result.iterations + 20)
    Return True

Process called "test_rmsprop_decay_parameter_sensitivity" that takes no parameters returns Boolean:
    Note: Test RMSprop sensitivity to decay parameter (rho)
    Let objective be create_rosenbrock_objective(2)
    Let base_config be create_test_gradient_config()
    Set base_config.method to "rmsprop"
    Set base_config.learning_rate to "0.001"
    Set base_config.max_iterations to 300
    
    Let rho_values be ["0.5", "0.9", "0.99"]
    Let starting_point be ["-1.0", "1.0"]
    
    Let results be Collections.create_list()
    For rho in rho_values:
        Let config be base_config
        Set config.rho to rho
        Let result be GradOpt.rmsprop_optimize(objective, starting_point, config)
        Assert.IsTrue(assert_gradient_result_valid(result))
        results.append(result)
    
    Note: All configurations should show progress
    For result in results:
        Assert.IsTrue(result.iterations > 10)
    
    Return True

Note: =====================================================================
Note: ADAM OPTIMIZATION TESTS  
Note: =====================================================================

Process called "test_adam_basic_optimization" that takes no parameters returns Boolean:
    Note: Test basic Adam optimization functionality
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate to "0.01"
    Set config.beta1 to "0.9"
    Set config.beta2 to "0.999"
    
    Let starting_point be ["1.0", "1.0"]
    Let result be GradOpt.adam_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Adam should converge quickly on quadratics
    Assert.IsTrue(result.iterations < 100)
    Return True

Process called "test_adam_bias_correction" that takes no parameters returns Boolean:
    Note: Test Adam bias correction in early iterations
    Let objective be create_simple_quadratic_objective(2.0)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate to "0.1"
    Set config.max_iterations to 20
    
    Let starting_point be ["3.0", "-2.0"]
    Let result be GradOpt.adam_optimize_with_state(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.final_state)
    
    Note: Check bias correction factors
    Let m_bias_correction be result.final_state.m_bias_correction
    Let v_bias_correction be result.final_state.v_bias_correction
    Assert.IsTrue(MathCore.parse_float(m_bias_correction) > 0.9)
    Assert.IsTrue(MathCore.parse_float(v_bias_correction) > 0.9)
    Return True

Process called "test_adam_momentum_parameters" that takes no parameters returns Boolean:
    Note: Test Adam with different momentum parameters
    Let objective be create_rosenbrock_objective(2)
    Let base_config be create_test_gradient_config()
    Set base_config.method to "adam"
    Set base_config.learning_rate to "0.001"
    Set base_config.max_iterations to 200
    
    Let parameter_combinations be [
        ["0.9", "0.999"],
        ["0.95", "0.999"],  
        ["0.9", "0.99"],
        ["0.8", "0.9"]
    ]
    
    Let starting_point be ["-1.2", "1.0"]
    
    For params in parameter_combinations:
        Let config be base_config
        Set config.beta1 to params[0]
        Set config.beta2 to params[1]
        
        Let result be GradOpt.adam_optimize(objective, starting_point, config)
        Assert.IsTrue(assert_gradient_result_valid(result))
    
    Return True

Process called "test_adam_vs_sgd_comparison" that takes no parameters returns Boolean:
    Note: Compare Adam vs standard SGD on noisy problem
    Let objective be create_noisy_quadratic_objective(1.0, 0.1)
    
    Note: Test standard SGD
    Let sgd_config be create_test_gradient_config()
    Set sgd_config.method to "sgd"
    Set sgd_config.learning_rate to "0.01"
    Set sgd_config.max_iterations to 500
    
    Let starting_point be ["2.0", "1.0"]
    Let sgd_result be GradOpt.sgd_optimize(objective, starting_point, sgd_config)
    
    Note: Test Adam
    Let adam_config be create_test_gradient_config()
    Set adam_config.method to "adam"
    Set adam_config.learning_rate to "0.01"
    Set adam_config.max_iterations to 500
    
    Let adam_result be GradOpt.adam_optimize(objective, starting_point, adam_config)
    
    Assert.IsTrue(assert_gradient_result_valid(sgd_result))
    Assert.IsTrue(assert_gradient_result_valid(adam_result))
    
    Note: Adam should typically be more stable on noisy problems
    Let adam_final_objective be MathCore.parse_float(adam_result.final_objective_value)
    Let sgd_final_objective be MathCore.parse_float(sgd_result.final_objective_value)
    Assert.IsTrue(adam_final_objective <= sgd_final_objective * 1.5)
    
    Return True

Note: =====================================================================
Note: ADADELTA OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_adadelta_parameter_free" that takes no parameters returns Boolean:
    Note: Test AdaDelta parameter-free adaptation
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be GradOpt.GradientConfig{
        method: "adadelta",
        rho: "0.95",
        epsilon: "1e-6",
        gradient_clipping: "none",
        max_iterations: 200
    }
    
    Let starting_point be ["1.5", "-1.0"]
    Let result be GradOpt.adadelta_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_adadelta_delta_accumulation" that takes no parameters returns Boolean:
    Note: Test AdaDelta parameter update accumulation
    Let objective be create_rosenbrock_objective(2)
    Let config be GradOpt.GradientConfig{
        method: "adadelta",
        rho: "0.9",
        epsilon: "1e-8",
        max_iterations: 300
    }
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be GradOpt.adadelta_optimize_with_state(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.final_state)
    Assert.IsTrue(result.final_state.gradient_accumulator.length == 2)
    Assert.IsTrue(result.final_state.delta_accumulator.length == 2)
    Return True

Note: =====================================================================
Note: NADAM OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_nadam_nesterov_acceleration" that takes no parameters returns Boolean:
    Note: Test Nadam with Nesterov accelerated adaptive moments
    Let objective be create_simple_quadratic_objective(0.5)
    Let config be create_test_gradient_config()
    Set config.method to "nadam"
    Set config.learning_rate to "0.02"
    Set config.beta1 to "0.9"
    Set config.beta2 to "0.999"
    
    Let starting_point be ["2.0", "1.5"]
    Let result be GradOpt.nadam_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_nadam_vs_adam_acceleration" that takes no parameters returns Boolean:
    Note: Compare Nadam vs Adam on the same problem
    Let objective be create_rosenbrock_objective(2)
    Let base_config be create_test_gradient_config()
    Set base_config.learning_rate to "0.002"
    Set base_config.max_iterations to 400
    
    Let starting_point be ["-1.2", "1.0"]
    
    Note: Test Adam
    Let adam_config be base_config
    Set adam_config.method to "adam"
    Let adam_result be GradOpt.adam_optimize(objective, starting_point, adam_config)
    
    Note: Test Nadam
    Let nadam_config be base_config  
    Set nadam_config.method to "nadam"
    Let nadam_result be GradOpt.nadam_optimize(objective, starting_point, nadam_config)
    
    Assert.IsTrue(assert_gradient_result_valid(adam_result))
    Assert.IsTrue(assert_gradient_result_valid(nadam_result))
    
    Note: Both should make progress but Nadam may converge faster due to Nesterov acceleration
    Assert.IsTrue(adam_result.iterations > 10)
    Assert.IsTrue(nadam_result.iterations > 10)
    Return True

Note: =====================================================================
Note: MOMENTUM-BASED OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_momentum_sgd_basic" that takes no parameters returns Boolean:
    Note: Test basic momentum SGD functionality
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be create_test_gradient_config()
    Set config.method to "momentum_sgd"
    Set config.learning_rate to "0.01"
    Set config.momentum to "0.9"
    
    Let starting_point be ["1.0", "1.0"]
    Let result be GradOpt.momentum_sgd_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_nesterov_momentum" that takes no parameters returns Boolean:
    Note: Test Nesterov accelerated gradient method
    Let objective be create_simple_quadratic_objective(2.0)
    Let config = create_test_gradient_config()
    Set config.method to "nesterov"
    Set config.learning_rate to "0.01"
    Set config.momentum to "0.95"
    
    Let starting_point be ["2.0", "-1.5"]
    Let result be GradOpt.nesterov_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_momentum_parameter_sensitivity" that takes no parameters returns Boolean:
    Note: Test sensitivity to momentum parameter
    Let objective be create_simple_quadratic_objective(1.0)
    Let base_config be create_test_gradient_config()
    Set base_config.method to "momentum_sgd"
    Set base_config.learning_rate to "0.01"
    Set base_config.max_iterations to 100
    
    Let momentum_values be ["0.0", "0.5", "0.9", "0.99"]
    Let starting_point be ["1.5", "1.0"]
    
    For momentum in momentum_values:
        Let config be base_config
        Set config.momentum to momentum
        
        Let result be GradOpt.momentum_sgd_optimize(objective, starting_point, config)
        Assert.IsTrue(assert_gradient_result_valid(result))
    
    Return True

Note: =====================================================================
Note: PROXIMAL GRADIENT METHODS TESTS
Note: =====================================================================

Process called "test_proximal_gradient_l1_regularization" that takes no parameters returns Boolean:
    Note: Test proximal gradient method with L1 regularization
    Let objective be create_simple_quadratic_objective(1.0)
    Let proximal_op be GradOpt.ProximalOperator{
        type: "l1",
        lambda: "0.1",
        function: "soft_threshold"
    }
    
    Let config be create_test_gradient_config()
    Set config.method to "proximal_gradient"
    Set config.learning_rate to "0.01"
    Set config.proximal_operator to proximal_op
    
    Let starting_point be ["1.0", "-0.8"]
    Let result be GradOpt.proximal_gradient_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_fista_acceleration" that takes no parameters returns Boolean:
    Note: Test FISTA (Fast Iterative Shrinkage-Thresholding Algorithm)
    Let objective be create_simple_quadratic_objective(0.5)
    Let proximal_op be GradOpt.ProximalOperator{
        type: "l1",
        lambda: "0.05",
        function: "soft_threshold"
    }
    
    Let config be create_test_gradient_config()
    Set config.method to "fista"
    Set config.learning_rate to "0.02"
    Set config.proximal_operator to proximal_op
    
    Let starting_point be ["2.0", "1.5"]
    Let result be GradOpt.fista_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_proximal_operators_accuracy" that takes no parameters returns Boolean:
    Note: Test accuracy of different proximal operators
    Let test_vector be ["1.5", "-0.8", "0.3", "-2.1"]
    Let lambda_value be "0.5"
    
    Note: Test L1 soft thresholding
    Let l1_result be GradOpt.apply_l1_proximal(test_vector, lambda_value)
    Assert.IsTrue(l1_result.length == test_vector.length)
    
    Note: Test L2 (identity for unconstrained)  
    Let l2_result be GradOpt.apply_l2_proximal(test_vector, lambda_value)
    Assert.IsTrue(l2_result.length == test_vector.length)
    
    Note: Test box constraint projection
    Let lower_bounds be ["-1.0", "-1.0", "-1.0", "-1.0"]
    Let upper_bounds be ["1.0", "1.0", "1.0", "1.0"]
    Let box_result be GradOpt.apply_box_proximal(test_vector, lower_bounds, upper_bounds)
    Assert.IsTrue(box_result.length == test_vector.length)
    
    Note: Check that box constraints are satisfied
    For i from 0 to box_result.length - 1:
        Let value be MathCore.parse_float(box_result[i])
        Assert.IsTrue(value >= -1.0 and value <= 1.0)
    
    Return True

Note: =====================================================================
Note: STOCHASTIC GRADIENT DESCENT TESTS
Note: =====================================================================

Process called "test_stochastic_gradient_descent_basic" that takes no parameters returns Boolean:
    Note: Test basic stochastic gradient descent
    Let objective be create_noisy_quadratic_objective(1.0, 0.05)
    Let config be create_test_gradient_config()
    Set config.method to "sgd"
    Set config.learning_rate to "0.01"
    Set config.batch_size to 1
    
    Let starting_point be ["1.0", "1.0"]
    Let result be GradOpt.sgd_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Process called "test_mini_batch_gradient_descent" that takes no parameters returns Boolean:
    Note: Test mini-batch gradient descent
    Let objective be create_noisy_quadratic_objective(1.0, 0.1)
    Let config be create_test_gradient_config()
    Set config.method to "mini_batch_sgd"
    Set config.learning_rate to "0.02"
    Set config.batch_size to 32
    
    Let starting_point be ["1.5", "-1.0"]
    Let result be GradOpt.mini_batch_sgd_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Process called "test_batch_size_effect" that takes no parameters returns Boolean:
    Note: Test effect of different batch sizes on convergence
    Let objective be create_noisy_quadratic_objective(1.0, 0.2)
    Let base_config be create_test_gradient_config()
    Set base_config.method to "mini_batch_sgd"
    Set base_config.learning_rate to "0.01"
    Set base_config.max_iterations to 200
    
    Let batch_sizes be [1, 8, 32, 128]
    Let starting_point be ["2.0", "1.5"]
    
    Let results be Collections.create_list()
    For batch_size in batch_sizes:
        Let config be base_config
        Set config.batch_size to batch_size
        
        Let result be GradOpt.mini_batch_sgd_optimize(objective, starting_point, config)
        Assert.IsTrue(assert_gradient_result_valid(result))
        results.append(result)
    
    Note: Larger batch sizes should generally give more stable convergence
    Assert.IsTrue(results.length == batch_sizes.length)
    Return True

Note: =====================================================================
Note: GRADIENT CLIPPING TESTS
Note: =====================================================================

Process called "test_gradient_clipping_by_norm" that takes no parameters returns Boolean:
    Note: Test gradient clipping by norm
    Let objective be create_rosenbrock_objective(2)
    Let config be create_test_gradient_config()
    Set config.method to "sgd"
    Set config.learning_rate to "0.001"
    Set config.gradient_clipping to "norm"
    Set config.clipping_threshold to "1.0"
    
    Let starting_point be ["-2.0", "2.0"]
    Let result be GradOpt.sgd_optimize_with_clipping(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.clipping_statistics)
    Return True

Process called "test_gradient_clipping_by_value" that takes no parameters returns Boolean:
    Note: Test gradient clipping by value (element-wise)
    Let objective be create_simple_quadratic_objective(5.0)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate to "0.1"
    Set config.gradient_clipping to "value"
    Set config.clipping_threshold to "0.5"
    
    Let starting_point be ["3.0", "-2.5"]
    Let result be GradOpt.adam_optimize_with_clipping(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_gradient_clipping_effectiveness" that takes no parameters returns Boolean:
    Note: Test that gradient clipping prevents exploding gradients
    Let objective be GradOpt.GradientObjective{
        function: "lambda x: exp(10*x[0]*x[0] + 10*x[1]*x[1])",
        gradient: "lambda x: [20*x[0]*exp(10*x[0]*x[0] + 10*x[1]*x[1]), 20*x[1]*exp(10*x[0]*x[0] + 10*x[1]*x[1])]",
        dimension: 2,
        smooth: True,
        convex: True,
        lipschitz_constant: "adaptive"
    }
    
    Let config be create_test_gradient_config()
    Set config.method to "sgd"
    Set config.learning_rate to "0.01"
    Set config.gradient_clipping to "norm"
    Set config.clipping_threshold to "1.0"
    Set config.max_iterations to 50
    
    Let starting_point be ["0.1", "0.1"]
    Let result be GradOpt.sgd_optimize_with_clipping(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Note: Should not crash or produce NaN values
    Assert.IsFalse(result.final_objective_value == "nan")
    Assert.IsFalse(result.final_objective_value == "inf")
    Return True

Note: =====================================================================
Note: LEARNING RATE SCHEDULING TESTS
Note: =====================================================================

Process called "test_exponential_decay_schedule" that takes no parameters returns Boolean:
    Note: Test exponential decay learning rate schedule
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be create_test_gradient_config()
    Set config.method to "sgd"
    Set config.learning_rate_schedule to "exponential_decay"
    Set config.initial_learning_rate to "0.1"
    Set config.decay_rate to "0.95"
    Set config.decay_steps to 10
    
    Let starting_point be ["2.0", "1.5"]
    Let result be GradOpt.sgd_optimize_with_schedule(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.schedule_history)
    Assert.IsTrue(result.schedule_history.learning_rates.length > 5)
    
    Note: Learning rate should decrease over time
    Let first_rate be MathCore.parse_float(result.schedule_history.learning_rates[0])
    Let last_rate be MathCore.parse_float(result.schedule_history.learning_rates[-1])
    Assert.IsTrue(last_rate < first_rate)
    Return True

Process called "test_step_decay_schedule" that takes no parameters returns Boolean:
    Note: Test step decay learning rate schedule
    Let objective be create_simple_quadratic_objective(0.5)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate_schedule to "step_decay"
    Set config.initial_learning_rate to "0.1"
    Set config.step_size to 50
    Set config.gamma to "0.5"
    
    Let starting_point be ["1.5", "-1.0"]
    Let result be GradOpt.adam_optimize_with_schedule(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_cosine_annealing_schedule" that takes no parameters returns Boolean:
    Note: Test cosine annealing learning rate schedule
    Let objective be create_rosenbrock_objective(2)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate_schedule to "cosine_annealing"
    Set config.initial_learning_rate to "0.01"
    Set config.min_learning_rate to "1e-6"
    Set config.t_max to 100
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be GradOpt.adam_optimize_with_schedule(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.schedule_history)
    Return True

Process called "test_plateau_reduction_schedule" that takes no parameters returns Boolean:
    Note: Test learning rate reduction on plateau
    Let objective be create_rosenbrock_objective(2)
    Let config be create_test_gradient_config()
    Set config.method to "adam"
    Set config.learning_rate_schedule to "plateau_reduction"
    Set config.initial_learning_rate to "0.01"
    Set config.patience to 20
    Set config.reduction_factor to "0.5"
    Set config.min_learning_rate to "1e-6"
    
    Let starting_point be ["-1.2", "1.0"]
    Let result be GradOpt.adam_optimize_with_schedule(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsNotNull(result.schedule_history)
    Return True

Note: =====================================================================
Note: VARIANCE REDUCTION METHODS TESTS
Note: =====================================================================

Process called "test_svrg_variance_reduction" that takes no parameters returns Boolean:
    Note: Test SVRG (Stochastic Variance Reduced Gradient) method
    Let objective be create_noisy_quadratic_objective(1.0, 0.5)
    Let config be GradOpt.VarianceReductionConfig{
        method: "svrg",
        learning_rate: "0.01",
        update_frequency: 50,
        batch_size: 32,
        inner_loop_size: 100
    }
    
    Let starting_point be ["1.5", "1.0"]
    Let result be GradOpt.svrg_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.variance_reduction_stats.variance_before > result.variance_reduction_stats.variance_after)
    Return True

Process called "test_saga_method" that takes no parameters returns Boolean:
    Note: Test SAGA (Stochastic Average Gradient Augmented) method
    Let objective be create_noisy_quadratic_objective(0.5, 0.3)
    Let config be GradOpt.VarianceReductionConfig{
        method: "saga",
        learning_rate: "0.02", 
        memory_size: 100,
        batch_size: 1
    }
    
    Let starting_point be ["2.0", "-1.5"]
    Let result be GradOpt.saga_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Process called "test_sag_method" that takes no parameters returns Boolean:
    Note: Test SAG (Stochastic Average Gradient) method
    Let objective be create_noisy_quadratic_objective(1.5, 0.2)
    Let config be GradOpt.VarianceReductionConfig{
        method: "sag",
        learning_rate: "0.01",
        memory_size: 200,
        batch_size: 16
    }
    
    Let starting_point be ["1.0", "1.5"]
    Let result be GradOpt.sag_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Note: =====================================================================
Note: COORDINATE DESCENT TESTS
Note: =====================================================================

Process called "test_coordinate_descent_basic" that takes no parameters returns Boolean:
    Note: Test basic coordinate descent method
    Let objective be create_simple_quadratic_objective(1.0)
    Let config be GradOpt.CoordinateDescentConfig{
        selection_rule: "cyclic",
        learning_rate: "0.1",
        max_iterations: 200,
        coordinate_wise_tolerance: "1e-6"
    }
    
    Let starting_point be ["1.0", "1.0"]
    Let result be GradOpt.coordinate_descent_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_randomized_coordinate_descent" that takes no parameters returns Boolean:
    Note: Test randomized coordinate descent
    Let objective be create_simple_quadratic_objective(2.0)
    Let config be GradOpt.CoordinateDescentConfig{
        selection_rule: "random",
        learning_rate: "0.05",
        max_iterations: 300,
        coordinate_wise_tolerance: "1e-6"
    }
    
    Let starting_point be ["2.0", "-1.5"]
    Let result be GradOpt.coordinate_descent_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_block_coordinate_descent" that takes no parameters returns Boolean:
    Note: Test block coordinate descent for structured problems
    Let objective be create_simple_quadratic_objective(1.0)
    Let blocks be [["0"], ["1"]]  # Two 1-dimensional blocks
    
    Let config be GradOpt.BlockCoordinateDescentConfig{
        blocks: blocks,
        selection_rule: "cyclic",
        learning_rate: "0.1",
        max_iterations: 150,
        block_wise_tolerance: "1e-6"
    }
    
    Let starting_point be ["1.5", "1.0"]
    Let result be GradOpt.block_coordinate_descent_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Note: =====================================================================
Note: SUBGRADIENT METHODS TESTS
Note: =====================================================================

Process called "test_subgradient_method_l1" that takes no parameters returns Boolean:
    Note: Test subgradient method on L1 norm (non-smooth)
    Let objective be GradOpt.NonSmoothObjective{
        function: "lambda x: abs(x[0]) + abs(x[1])",
        subgradient: "lambda x: [sign(x[0]), sign(x[1])]",
        dimension: 2,
        convex: True
    }
    
    Let config be GradOpt.SubgradientConfig{
        step_size_rule: "diminishing",
        initial_step_size: "0.1",
        max_iterations: 500
    }
    
    Let starting_point be ["1.0", "-0.8"]
    Let result be GradOpt.subgradient_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Process called "test_projected_subgradient_method" that takes no parameters returns Boolean:
    Note: Test projected subgradient method with constraints
    Let objective be GradOpt.NonSmoothObjective{
        function: "lambda x: max(x[0], x[1])",
        subgradient: "lambda x: compute_max_subgradient(x)",
        dimension: 2,
        convex: True
    }
    
    Let constraint_set be GradOpt.ConstraintSet{
        type: "box",
        lower_bounds: ["-1.0", "-1.0"],
        upper_bounds: ["1.0", "1.0"]
    }
    
    Let config be GradOpt.ProjectedSubgradientConfig{
        constraint_set: constraint_set,
        step_size_rule: "constant",
        step_size: "0.01",
        max_iterations: 300
    }
    
    Let starting_point be ["0.5", "0.3"]
    Let result be GradOpt.projected_subgradient_optimize(objective, starting_point, config)
    
    Assert.IsTrue(assert_gradient_result_valid(result))
    Return True

Note: =====================================================================
Note: COMPREHENSIVE TEST RUNNER FUNCTIONS
Note: =====================================================================

Process called "run_adaptive_methods_tests" that takes no parameters returns Boolean:
    Note: Run all adaptive gradient method tests
    Let tests be [
        "test_adagrad_basic_convergence",
        "test_adagrad_learning_rate_adaptation",
        "test_adagrad_diagonal_accumulation",
        "test_rmsprop_exponential_averaging",
        "test_rmsprop_vs_adagrad_comparison",
        "test_rmsprop_decay_parameter_sensitivity"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_adam_family_tests" that takes no parameters returns Boolean:
    Note: Run all Adam family optimization tests
    Let tests be [
        "test_adam_basic_optimization",
        "test_adam_bias_correction",
        "test_adam_momentum_parameters",
        "test_adam_vs_sgd_comparison",
        "test_adadelta_parameter_free",
        "test_adadelta_delta_accumulation",
        "test_nadam_nesterov_acceleration",
        "test_nadam_vs_adam_acceleration"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_momentum_tests" that takes no parameters returns Boolean:
    Note: Run all momentum-based optimization tests
    Let tests be [
        "test_momentum_sgd_basic",
        "test_nesterov_momentum",
        "test_momentum_parameter_sensitivity"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_proximal_methods_tests" that takes no parameters returns Boolean:
    Note: Run all proximal gradient method tests
    Let tests be [
        "test_proximal_gradient_l1_regularization",
        "test_fista_acceleration",
        "test_proximal_operators_accuracy"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_stochastic_methods_tests" that takes no parameters returns Boolean:
    Note: Run all stochastic gradient method tests
    Let tests be [
        "test_stochastic_gradient_descent_basic",
        "test_mini_batch_gradient_descent",
        "test_batch_size_effect"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_gradient_clipping_tests" that takes no parameters returns Boolean:
    Note: Run all gradient clipping tests
    Let tests be [
        "test_gradient_clipping_by_norm",
        "test_gradient_clipping_by_value",
        "test_gradient_clipping_effectiveness"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_learning_rate_schedule_tests" that takes no parameters returns Boolean:
    Note: Run all learning rate scheduling tests
    Let tests be [
        "test_exponential_decay_schedule",
        "test_step_decay_schedule", 
        "test_cosine_annealing_schedule",
        "test_plateau_reduction_schedule"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_variance_reduction_tests" that takes no parameters returns Boolean:
    Note: Run all variance reduction method tests
    Let tests be [
        "test_svrg_variance_reduction",
        "test_saga_method",
        "test_sag_method"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_coordinate_descent_tests" that takes no parameters returns Boolean:
    Note: Run all coordinate descent method tests
    Let tests be [
        "test_coordinate_descent_basic",
        "test_randomized_coordinate_descent",
        "test_block_coordinate_descent"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_subgradient_method_tests" that takes no parameters returns Boolean:
    Note: Run all subgradient method tests
    Let tests be [
        "test_subgradient_method_l1",
        "test_projected_subgradient_method"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ‚úì " + test_name + " PASSED"
            Else:
                Print "  ‚úó " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ‚úó " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_all_tests" that takes no parameters returns Boolean:
    Note: Run all gradient optimization module tests
    Print "=" * 80
    Print "GRADIENT OPTIMIZATION MODULE COMPREHENSIVE TEST SUITE"
    Print "=" * 80
    Print ""
    
    Let test_categories be [
        ["Adaptive Methods (AdaGrad, RMSprop)", "run_adaptive_methods_tests"],
        ["Adam Family (Adam, AdaDelta, Nadam)", "run_adam_family_tests"],
        ["Momentum-Based Methods", "run_momentum_tests"],
        ["Proximal Gradient Methods", "run_proximal_methods_tests"],
        ["Stochastic Methods", "run_stochastic_methods_tests"],
        ["Gradient Clipping", "run_gradient_clipping_tests"],
        ["Learning Rate Scheduling", "run_learning_rate_schedule_tests"],
        ["Variance Reduction Methods", "run_variance_reduction_tests"],
        ["Coordinate Descent", "run_coordinate_descent_tests"],
        ["Subgradient Methods", "run_subgradient_method_tests"]
    ]
    
    Let overall_success be True
    Let passed_categories be 0
    Let total_categories be test_categories.length
    
    For category_info in test_categories:
        Let category_name be category_info[0]
        Let test_runner be category_info[1]
        
        Print "Testing " + category_name + "..."
        Print "-" * (9 + Length(category_name))
        
        Let category_result be Call(test_runner)
        If category_result:
            Print "‚úì " + category_name + ": ALL TESTS PASSED"
            Set passed_categories to passed_categories + 1
        Else:
            Print "‚úó " + category_name + ": SOME TESTS FAILED"
            Set overall_success to False
        Print ""
    
    Print "=" * 80
    Print "GRADIENT OPTIMIZATION TEST SUMMARY"
    Print "=" * 80
    Print "Categories tested: " + ToString(total_categories)
    Print "Categories passed: " + ToString(passed_categories)
    Print "Categories failed: " + ToString(total_categories - passed_categories)
    
    Let success_rate be (passed_categories * 100.0) / total_categories
    Print "Success rate: " + ToString(success_rate) + "%"
    
    If overall_success:
        Print "\nüéâ ALL GRADIENT OPTIMIZATION TESTS PASSED! üéâ"
        Print "The gradient optimization module is ready for production use."
    Else:
        Print "\n‚ùå SOME GRADIENT OPTIMIZATION TESTS FAILED ‚ùå"
        Print "Please review and fix failing tests before deployment."
    
    Print "=" * 80
    
    Return overall_success