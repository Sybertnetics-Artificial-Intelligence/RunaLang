Note:
tests/unit/libraries/math/engine/optimization/core_test.runa
Unit Tests for Math Engine Optimization Core Module

This test suite provides comprehensive testing for the math engine optimization core module including:
- Line search methods (Armijo, Wolfe conditions, backtracking, golden section)
- Trust region methods and robustness algorithms
- Gradient descent and steepest descent algorithms
- Conjugate gradient methods (Fletcher-Reeves, Polak-Ribiere, Hestenes-Stiefel, Dai-Yuan)
- Quasi-Newton methods (BFGS, L-BFGS, DFP, SR1)
- Newton's method with Hessian modifications
- Univariate optimization methods (golden section search, ternary search, bracketing)
- Convergence criteria and stopping conditions
- Optimization problem formulation and validation
- Performance monitoring and algorithm diagnostics
- Automatic and numerical differentiation integration
- Adaptive parameter selection and algorithm tuning
:End Note

Import "stdlib/math/engine/optimization/core" as OptCore
Import "dev/debug/test_framework/assertions" as Assert
Import "dev/debug/test_framework/test_runner" as TestRunner
Import "dev/debug/test_framework/data_generators" as DataGen
Import "math.core" as MathCore

Note: =====================================================================
Note: HELPER FUNCTIONS AND TEST UTILITIES
Note: =====================================================================

Process called "create_quadratic_function" that takes a as Float, b as Float, c as Float returns OptimizationProblem:
    Note: Create a simple quadratic test function f(x) = ax² + bx + c
    Let objective_function be "lambda x: " + ToString(a) + "*x*x + " + ToString(b) + "*x + " + ToString(c)
    Let gradient_function be "lambda x: 2*" + ToString(a) + "*x + " + ToString(b)
    Let hessian_function be "lambda x: 2*" + ToString(a)
    
    Return OptCore.OptimizationProblem{
        objective: objective_function,
        gradient: gradient_function,
        hessian: hessian_function,
        dimension: 1,
        bounds: ["-100.0", "100.0"],
        constraints: [],
        problem_type: "unconstrained"
    }

Process called "create_rosenbrock_function" that takes dimension as Integer returns OptimizationProblem:
    Note: Create Rosenbrock test function for multi-dimensional optimization
    Let objective_function be "rosenbrock_" + ToString(dimension) + "d"
    Let gradient_function be "rosenbrock_gradient_" + ToString(dimension) + "d"
    Let hessian_function be "rosenbrock_hessian_" + ToString(dimension) + "d"
    
    Return OptCore.OptimizationProblem{
        objective: objective_function,
        gradient: gradient_function,
        hessian: hessian_function,
        dimension: dimension,
        bounds: ["-5.0", "5.0"],
        constraints: [],
        problem_type: "unconstrained"
    }

Process called "create_test_convergence_criteria" returns ConvergenceCriteria:
    Note: Create test convergence criteria with reasonable tolerances
    Return OptCore.ConvergenceCriteria{
        gradient_tolerance: "1e-6",
        function_tolerance: "1e-8",
        parameter_tolerance: "1e-10",
        max_iterations: 1000,
        max_function_evaluations: 5000,
        absolute_tolerance: "1e-12",
        relative_tolerance: "1e-8"
    }

Process called "assert_optimization_result_valid" that takes result as OptimizationResult returns Boolean:
    Note: Assert that optimization result has valid structure and values
    Assert.IsNotNull(result)
    Assert.IsTrue(result.converged == "true" or result.converged == "false")
    Assert.IsTrue(result.iterations >= 0)
    Assert.IsTrue(result.function_evaluations >= 0)
    Assert.IsNotEmpty(result.solution)
    Assert.IsNotEmpty(result.objective_value)
    Assert.IsNotEmpty(result.gradient_norm)
    Return True

Process called "generate_test_starting_points" that takes dimension as Integer returns List[List[String]]:
    Note: Generate various starting points for testing robustness
    Let starting_points be Collections.create_list()
    
    Note: Zero starting point
    Let zero_point be Collections.create_list()
    For i from 0 to dimension - 1:
        zero_point.append("0.0")
    starting_points.append(zero_point)
    
    Note: Random starting points
    For trial from 0 to 4:
        Let random_point be Collections.create_list()
        For i from 0 to dimension - 1:
            Let random_value be DataGen.generate_random_float(-2.0, 2.0)
            random_point.append(ToString(random_value))
        starting_points.append(random_point)
    
    Return starting_points

Note: =====================================================================
Note: LINE SEARCH METHODS TESTS
Note: =====================================================================

Process called "test_armijo_line_search_basic" that takes no parameters returns Boolean:
    Note: Test basic Armijo line search functionality
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.method to "armijo"
    
    Let starting_point be ["2.0"]
    Let search_direction be ["-1.0"]
    Let step_size be OptCore.armijo_line_search(problem, starting_point, search_direction, config)
    
    Assert.IsTrue(MathCore.parse_float(step_size) > 0.0)
    Assert.IsTrue(MathCore.parse_float(step_size) <= MathCore.parse_float(config.max_step_size))
    Return True

Process called "test_wolfe_line_search_conditions" that takes no parameters returns Boolean:
    Note: Test Wolfe line search conditions satisfaction
    Let problem be create_quadratic_function(2.0, -4.0, 3.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.method to "wolfe"
    Set config.curvature_constant to "0.9"
    
    Let starting_point be ["0.0"]
    Let search_direction be ["1.0"]
    Let step_size be OptCore.wolfe_line_search(problem, starting_point, search_direction, config)
    
    Assert.IsTrue(MathCore.parse_float(step_size) > 0.0)
    Let satisfied be OptCore.check_wolfe_conditions(problem, starting_point, search_direction, step_size, config)
    Assert.IsTrue(satisfied)
    Return True

Process called "test_backtracking_line_search_convergence" that takes no parameters returns Boolean:
    Note: Test backtracking line search convergence properties
    Let problem be create_quadratic_function(3.0, -6.0, 2.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.method to "backtracking"
    Set config.contraction_factor to "0.5"
    
    Let starting_point be ["1.0"]
    Let search_direction be ["-1.0"]
    Let step_size be OptCore.backtracking_line_search(problem, starting_point, search_direction, config)
    
    Assert.IsTrue(MathCore.parse_float(step_size) > 0.0)
    Assert.IsTrue(MathCore.parse_float(step_size) >= MathCore.parse_float(config.min_step_size))
    Return True

Process called "test_golden_section_line_search_accuracy" that takes no parameters returns Boolean:
    Note: Test golden section line search accuracy for univariate functions
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.method to "golden_section"
    
    Let bracket_a be "0.0"
    Let bracket_b be "3.0"
    Let optimal_point be OptCore.golden_section_search(problem, bracket_a, bracket_b, config)
    
    Note: For f(x) = x² - 2x + 1, minimum is at x = 1
    Let expected_minimum be 1.0
    Let actual_minimum be MathCore.parse_float(optimal_point)
    Assert.IsTrue(AbsoluteValue(actual_minimum - expected_minimum) < 1e-6)
    Return True

Process called "test_line_search_edge_cases" that takes no parameters returns Boolean:
    Note: Test line search methods with edge cases
    Let problem be create_quadratic_function(0.1, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.min_step_size to "1e-15"
    Set config.max_step_size to "1e15"
    
    Note: Test with very small gradient
    Let starting_point be ["1e-10"]
    Let search_direction be ["-1e-15"]
    Let step_size be OptCore.armijo_line_search(problem, starting_point, search_direction, config)
    Assert.IsNotNull(step_size)
    
    Note: Test with very large step direction
    Set search_direction to ["-1e10"]
    Let step_size_large be OptCore.armijo_line_search(problem, starting_point, search_direction, config)
    Assert.IsNotNull(step_size_large)
    Return True

Note: =====================================================================
Note: TRUST REGION METHODS TESTS
Note: =====================================================================

Process called "test_trust_region_basic_functionality" that takes no parameters returns Boolean:
    Note: Test basic trust region method functionality
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.TrustRegionConfig{
        initial_radius: "1.0",
        max_radius: "10.0",
        eta1: "0.1",
        eta2: "0.75",
        gamma1: "0.5",
        gamma2: "2.0",
        max_iterations: 100
    }
    
    Let starting_point be ["2.0"]
    Let result be OptCore.trust_region_optimize(problem, starting_point, config)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_trust_region_radius_adaptation" that takes no parameters returns Boolean:
    Note: Test trust region radius adaptation mechanisms
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.TrustRegionConfig{
        initial_radius: "0.1",
        max_radius: "5.0",
        eta1: "0.25",
        eta2: "0.75",
        gamma1: "0.25",
        gamma2: "4.0",
        max_iterations: 200
    }
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be OptCore.trust_region_optimize(problem, starting_point, config)
    Let history be result.history
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(history.trust_radii.length > 5)
    Return True

Process called "test_trust_region_convergence_criteria" that takes no parameters returns Boolean:
    Note: Test trust region convergence with different criteria
    Let problem be create_quadratic_function(2.0, -4.0, 1.0)
    Let config be OptCore.TrustRegionConfig{
        initial_radius: "1.0",
        max_radius: "10.0",
        eta1: "0.1",
        eta2: "0.9",
        gamma1: "0.5",
        gamma2: "2.0",
        max_iterations: 50
    }
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.gradient_tolerance to "1e-8"
    
    Let starting_point be ["1.0"]
    Let result be OptCore.trust_region_optimize_with_criteria(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(MathCore.parse_float(result.gradient_norm) < 1e-7)
    Return True

Note: =====================================================================
Note: GRADIENT DESCENT METHODS TESTS
Note: =====================================================================

Process called "test_steepest_descent_basic" that takes no parameters returns Boolean:
    Note: Test basic steepest descent algorithm
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["3.0"]
    Let result be OptCore.steepest_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Check that solution is close to optimal (x = 0)
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value) < 1e-5)
    Return True

Process called "test_steepest_descent_multidimensional" that takes no parameters returns Boolean:
    Note: Test steepest descent on multidimensional problems
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.initial_step_size to "0.01"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 2000
    Set convergence.gradient_tolerance to "1e-4"
    
    Let starting_point be ["-1.2", "1.0"]
    Let result be OptCore.steepest_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.iterations > 10)
    Return True

Process called "test_gradient_descent_with_momentum" that takes no parameters returns Boolean:
    Note: Test gradient descent with momentum acceleration
    Let problem be create_quadratic_function(2.0, -4.0, 3.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.momentum_factor to "0.9"
    
    Let convergence be create_test_convergence_criteria()
    Let starting_point be ["0.0"]
    
    Let result be OptCore.gradient_descent_with_momentum(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Solution should be x = 1 for this quadratic
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value - 1.0) < 1e-5)
    Return True

Note: =====================================================================
Note: CONJUGATE GRADIENT METHODS TESTS
Note: =====================================================================

Process called "test_conjugate_gradient_fletcher_reeves" that takes no parameters returns Boolean:
    Note: Test Fletcher-Reeves conjugate gradient method
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["0.0"]
    Let result be OptCore.conjugate_gradient(problem, starting_point, "fletcher_reeves", config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Check solution accuracy (minimum at x = 1)
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value - 1.0) < 1e-6)
    Return True

Process called "test_conjugate_gradient_polak_ribiere" that takes no parameters returns Boolean:
    Note: Test Polak-Ribiere conjugate gradient method
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.initial_step_size to "0.1"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 1000
    Set convergence.gradient_tolerance to "1e-5"
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be OptCore.conjugate_gradient(problem, starting_point, "polak_ribiere", config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.iterations > 5)
    Return True

Process called "test_conjugate_gradient_hestenes_stiefel" that takes no parameters returns Boolean:
    Note: Test Hestenes-Stiefel conjugate gradient method
    Let problem be create_quadratic_function(3.0, -6.0, 2.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["0.0"]
    Let result be OptCore.conjugate_gradient(problem, starting_point, "hestenes_stiefel", config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Check solution (minimum at x = 1)
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value - 1.0) < 1e-6)
    Return True

Process called "test_conjugate_gradient_dai_yuan" that takes no parameters returns Boolean:
    Note: Test Dai-Yuan conjugate gradient method
    Let problem be create_quadratic_function(0.5, -1.0, 0.5)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["2.0"]
    Let result be OptCore.conjugate_gradient(problem, starting_point, "dai_yuan", config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_conjugate_gradient_variant_comparison" that takes no parameters returns Boolean:
    Note: Compare different conjugate gradient variants on the same problem
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 50
    
    Let starting_point be ["3.0"]
    Let variants be ["fletcher_reeves", "polak_ribiere", "hestenes_stiefel", "dai_yuan"]
    
    Let results be Collections.create_list()
    For variant in variants:
        Let result be OptCore.conjugate_gradient(problem, starting_point, variant, config, convergence)
        Assert.IsTrue(assert_optimization_result_valid(result))
        results.append(result)
    
    Note: All variants should converge for this simple quadratic
    For result in results:
        Assert.IsTrue(result.converged == "true")
        Let solution_value be MathCore.parse_float(result.solution[0])
        Assert.IsTrue(AbsoluteValue(solution_value) < 1e-5)
    
    Return True

Note: =====================================================================
Note: QUASI-NEWTON METHODS TESTS
Note: =====================================================================

Process called "test_bfgs_basic_optimization" that takes no parameters returns Boolean:
    Note: Test basic BFGS quasi-Newton method
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 100
    
    Let starting_point be ["0.0"]
    Let result be OptCore.bfgs_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: BFGS should converge quickly for quadratics
    Assert.IsTrue(result.iterations < 20)
    
    Note: Check solution accuracy
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value - 1.0) < 1e-8)
    Return True

Process called "test_lbfgs_memory_efficiency" that takes no parameters returns Boolean:
    Note: Test L-BFGS limited memory approach
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.lbfgs_memory_size to 5
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 500
    
    Let starting_point be ["-1.2", "1.0"]
    Let result be OptCore.lbfgs_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.function_evaluations < 2000)
    Return True

Process called "test_dfp_quasi_newton" that takes no parameters returns Boolean:
    Note: Test DFP (Davidon-Fletcher-Powell) quasi-Newton method
    Let problem be create_quadratic_function(2.0, -4.0, 3.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["0.0"]
    Let result be OptCore.dfp_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_sr1_quasi_newton" that takes no parameters returns Boolean:
    Note: Test SR1 (Symmetric Rank-One) quasi-Newton method
    Let problem be create_quadratic_function(1.5, -3.0, 2.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.sr1_skip_threshold to "1e-8"
    
    Let convergence be create_test_convergence_criteria()
    Let starting_point be ["1.0"]
    
    Let result be OptCore.sr1_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_quasi_newton_hessian_approximation" that takes no parameters returns Boolean:
    Note: Test Hessian approximation quality in quasi-Newton methods
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 10
    
    Let starting_point be ["2.0"]
    Let result be OptCore.bfgs_optimize_with_tracking(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    
    Note: Check that Hessian approximation improves over iterations
    Let hessian_history be result.history.hessian_approximations
    Assert.IsTrue(hessian_history.length > 2)
    Return True

Note: =====================================================================
Note: NEWTON'S METHOD TESTS
Note: =====================================================================

Process called "test_newton_method_basic" that takes no parameters returns Boolean:
    Note: Test basic Newton's method with exact Hessian
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 10
    
    Let starting_point be ["0.0"]
    Let result be OptCore.newton_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    
    Note: Newton's method should converge in one step for quadratics
    Assert.IsTrue(result.iterations <= 2)
    
    Let solution_value be MathCore.parse_float(result.solution[0])
    Assert.IsTrue(AbsoluteValue(solution_value - 1.0) < 1e-10)
    Return True

Process called "test_newton_method_with_modifications" that takes no parameters returns Boolean:
    Note: Test Newton's method with Hessian modifications
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.hessian_modification to "regularization"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 100
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be OptCore.modified_newton_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.iterations < 200)
    Return True

Process called "test_newton_method_eigenvalue_modification" that takes no parameters returns Boolean:
    Note: Test Newton's method with eigenvalue-based Hessian modification
    Let problem be create_quadratic_function(0.1, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    Set config.hessian_modification to "eigenvalue_modification"
    Set config.eigenvalue_threshold to "1e-6"
    
    Let convergence be create_test_convergence_criteria()
    Let starting_point be ["1.0"]
    
    Let result be OptCore.modified_newton_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Note: =====================================================================
Note: UNIVARIATE OPTIMIZATION TESTS
Note: =====================================================================

Process called "test_golden_section_search_convergence" that takes no parameters returns Boolean:
    Note: Test golden section search convergence properties
    Let problem be create_quadratic_function(1.0, -4.0, 3.0)
    Let tolerance be 1e-8
    
    Let bracket_a be "0.0"
    Let bracket_b be "5.0"
    Let result be OptCore.golden_section_search_detailed(problem, bracket_a, bracket_b, tolerance)
    
    Assert.IsNotNull(result.optimal_point)
    Assert.IsTrue(result.converged)
    Assert.IsTrue(result.iterations > 0)
    
    Note: Check accuracy (minimum at x = 2)
    Let optimal_value be MathCore.parse_float(result.optimal_point)
    Assert.IsTrue(AbsoluteValue(optimal_value - 2.0) < tolerance * 10)
    Return True

Process called "test_ternary_search_accuracy" that takes no parameters returns Boolean:
    Note: Test ternary search method accuracy
    Let problem be create_quadratic_function(2.0, -8.0, 6.0)
    Let tolerance be 1e-6
    
    Let bracket_a be "0.0"
    Let bracket_b be "6.0"
    Let result be OptCore.ternary_search(problem, bracket_a, bracket_b, tolerance)
    
    Assert.IsNotNull(result.optimal_point)
    Assert.IsTrue(result.converged)
    
    Note: Check accuracy (minimum at x = 2)
    Let optimal_value be MathCore.parse_float(result.optimal_point)
    Assert.IsTrue(AbsoluteValue(optimal_value - 2.0) < tolerance * 10)
    Return True

Process called "test_bracketing_methods" that takes no parameters returns Boolean:
    Note: Test bracketing methods for finding intervals containing minima
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let starting_point be "0.0"
    Let step_size be "0.5"
    
    Let bracket be OptCore.bracket_minimum(problem, starting_point, step_size)
    
    Assert.IsTrue(bracket.found)
    Assert.IsTrue(MathCore.parse_float(bracket.left) < MathCore.parse_float(bracket.right))
    
    Note: Verify that minimum is indeed in the bracket
    Let left_val be MathCore.parse_float(bracket.left)
    Let right_val be MathCore.parse_float(bracket.right)
    Assert.IsTrue(left_val <= 1.0 and 1.0 <= right_val)
    Return True

Note: =====================================================================
Note: CONVERGENCE CRITERIA AND STOPPING CONDITIONS TESTS
Note: =====================================================================

Process called "test_gradient_tolerance_convergence" that takes no parameters returns Boolean:
    Note: Test convergence based on gradient tolerance
    Let problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let config be OptCore.create_default_line_search_config()
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.gradient_tolerance to "1e-10"
    Set convergence.function_tolerance to "1e-15"
    
    Let starting_point be ["1.0"]
    Let result be OptCore.steepest_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Assert.IsTrue(MathCore.parse_float(result.gradient_norm) < 1e-9)
    Return True

Process called "test_function_tolerance_convergence" that takes no parameters returns Boolean:
    Note: Test convergence based on function value tolerance
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.function_tolerance to "1e-12"
    Set convergence.gradient_tolerance to "1e-6"
    
    Let starting_point be ["0.5"]
    Let result be OptCore.bfgs_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_parameter_tolerance_convergence" that takes no parameters returns Boolean:
    Note: Test convergence based on parameter change tolerance
    Let problem be create_quadratic_function(2.0, -4.0, 3.0)
    Let config be OptCore.create_default_line_search_config()
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.parameter_tolerance to "1e-10"
    Set convergence.gradient_tolerance to "1e-4"
    
    Let starting_point be ["0.0"]
    Let result be OptCore.newton_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_max_iterations_limit" that takes no parameters returns Boolean:
    Note: Test that optimization stops at maximum iterations limit
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.initial_step_size to "1e-6"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 10
    Set convergence.gradient_tolerance to "1e-15"
    
    Let starting_point be ["-2.0", "2.0"]
    Let result be OptCore.steepest_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.iterations == 10)
    Assert.IsTrue(result.converged == "false")
    Return True

Process called "test_max_function_evaluations_limit" that takes no parameters returns Boolean:
    Note: Test that optimization stops at function evaluation limit
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_function_evaluations to 50
    Set convergence.max_iterations to 1000
    Set convergence.gradient_tolerance to "1e-10"
    
    Let starting_point be ["-1.5", "1.5"]
    Let result be OptCore.bfgs_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.function_evaluations <= 50)
    Return True

Note: =====================================================================
Note: OPTIMIZATION PROBLEM FORMULATION TESTS
Note: =====================================================================

Process called "test_optimization_problem_validation" that takes no parameters returns Boolean:
    Note: Test optimization problem validation and error handling
    Let valid_problem be create_quadratic_function(1.0, 0.0, 0.0)
    Let is_valid be OptCore.validate_optimization_problem(valid_problem)
    Assert.IsTrue(is_valid)
    
    Note: Test invalid problem (missing objective function)
    Let invalid_problem be OptCore.OptimizationProblem{
        objective: "",
        gradient: "lambda x: 2*x",
        hessian: "lambda x: 2",
        dimension: 1,
        bounds: ["-10.0", "10.0"],
        constraints: [],
        problem_type: "unconstrained"
    }
    
    Let is_invalid be OptCore.validate_optimization_problem(invalid_problem)
    Assert.IsFalse(is_invalid)
    Return True

Process called "test_optimization_problem_bounds_checking" that takes no parameters returns Boolean:
    Note: Test optimization problem bounds validation
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Set problem.bounds to ["0.0", "2.0"]
    
    Let starting_point be ["1.5"]
    Let bounds_satisfied be OptCore.check_bounds_satisfaction(starting_point, problem.bounds)
    Assert.IsTrue(bounds_satisfied)
    
    Let invalid_point be ["3.0"]
    Let bounds_violated be OptCore.check_bounds_satisfaction(invalid_point, problem.bounds)
    Assert.IsFalse(bounds_violated)
    Return True

Process called "test_optimization_result_history_tracking" that takes no parameters returns Boolean:
    Note: Test optimization history tracking functionality
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 20
    
    Let starting_point be ["0.0"]
    Let result be OptCore.bfgs_optimize_with_history(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsNotNull(result.history)
    Assert.IsTrue(result.history.objective_values.length > 0)
    Assert.IsTrue(result.history.gradient_norms.length > 0)
    Assert.IsTrue(result.history.step_sizes.length > 0)
    Assert.IsTrue(result.history.iterations_points.length > 0)
    Return True

Note: =====================================================================
Note: PERFORMANCE AND ALGORITHM DIAGNOSTICS TESTS
Note: =====================================================================

Process called "test_algorithm_performance_comparison" that takes no parameters returns Boolean:
    Note: Compare performance of different optimization algorithms
    Let problem be create_quadratic_function(1.0, -4.0, 3.0)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 100
    
    Let starting_point be ["0.0"]
    Let algorithms be ["steepest_descent", "bfgs", "newton", "conjugate_gradient"]
    
    Let performance_results be Collections.create_list()
    For algorithm in algorithms:
        Let result be OptCore.optimize_with_algorithm(problem, starting_point, algorithm, config, convergence)
        Assert.IsTrue(assert_optimization_result_valid(result))
        performance_results.append(result)
    
    Note: Newton should be fastest for quadratics
    Let newton_result be performance_results[2]
    Assert.IsTrue(newton_result.iterations <= 3)
    Return True

Process called "test_optimization_diagnostics" that takes no parameters returns Boolean:
    Note: Test optimization diagnostic information collection
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 50
    
    Let starting_point be ["-1.2", "1.0"]
    Let result be OptCore.bfgs_optimize_with_diagnostics(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsNotNull(result.diagnostics)
    Assert.IsTrue(result.diagnostics.condition_number > 0.0)
    Assert.IsTrue(result.diagnostics.final_step_size > 0.0)
    Assert.IsNotEmpty(result.diagnostics.algorithm_specific_info)
    Return True

Note: =====================================================================
Note: DIFFERENTIATION INTEGRATION TESTS
Note: =====================================================================

Process called "test_automatic_differentiation_integration" that takes no parameters returns Boolean:
    Note: Test integration with automatic differentiation
    Let objective_func be "lambda x: x*x*x - 3*x*x + 2*x"
    Let problem be OptCore.OptimizationProblem{
        objective: objective_func,
        gradient: "auto",
        hessian: "auto",
        dimension: 1,
        bounds: ["-5.0", "5.0"],
        constraints: [],
        problem_type: "unconstrained"
    }
    
    Let config be OptCore.create_default_line_search_config()
    Let convergence be create_test_convergence_criteria()
    
    Let starting_point be ["0.0"]
    Let result be OptCore.bfgs_optimize(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsTrue(result.converged == "true")
    Return True

Process called "test_numerical_differentiation_fallback" that takes no parameters returns Boolean:
    Note: Test numerical differentiation as fallback method
    Let objective_func be "lambda x: sin(x*x) + cos(x)"
    Let problem be OptCore.OptimizationProblem{
        objective: objective_func,
        gradient: "numerical",
        hessian: "numerical",
        dimension: 1,
        bounds: ["-3.0", "3.0"],
        constraints: [],
        problem_type: "unconstrained"
    }
    
    Let config be OptCore.create_default_line_search_config()
    Set config.numerical_diff_step to "1e-8"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.gradient_tolerance to "1e-6"
    
    Let starting_point be ["1.0"]
    Let result be OptCore.steepest_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Return True

Note: =====================================================================
Note: ADAPTIVE PARAMETER SELECTION TESTS
Note: =====================================================================

Process called "test_adaptive_step_size_selection" that takes no parameters returns Boolean:
    Note: Test adaptive step size selection mechanisms
    Let problem be create_rosenbrock_function(2)
    Let config be OptCore.create_default_line_search_config()
    Set config.adaptive_step_size to "true"
    Set config.step_adaptation_factor to "1.5"
    
    Let convergence be create_test_convergence_criteria()
    Set convergence.max_iterations to 100
    
    Let starting_point be ["-1.0", "1.0"]
    Let result be OptCore.adaptive_gradient_descent(problem, starting_point, config, convergence)
    
    Assert.IsTrue(assert_optimization_result_valid(result))
    Assert.IsNotNull(result.history)
    Assert.IsTrue(result.history.step_sizes.length > 5)
    
    Note: Check that step sizes vary over iterations
    Let step_sizes be result.history.step_sizes
    Let first_step be MathCore.parse_float(step_sizes[0])
    Let last_step be MathCore.parse_float(step_sizes[step_sizes.length - 1])
    Assert.IsTrue(first_step != last_step)
    Return True

Process called "test_algorithm_parameter_tuning" that takes no parameters returns Boolean:
    Note: Test automatic algorithm parameter tuning
    Let problem be create_quadratic_function(1.0, -2.0, 1.0)
    Let base_config be OptCore.create_default_line_search_config()
    
    Let tuning_config be OptCore.TuningConfig{
        parameter_ranges: [
            ["initial_step_size", "0.1", "2.0"],
            ["armijo_constant", "1e-5", "1e-3"],
            ["contraction_factor", "0.1", "0.9"]
        ],
        tuning_method: "grid_search",
        evaluation_metric: "iterations"
    }
    
    Let starting_point be ["0.0"]
    Let tuned_config be OptCore.tune_algorithm_parameters(problem, starting_point, base_config, tuning_config)
    
    Assert.IsNotNull(tuned_config)
    Assert.IsTrue(MathCore.parse_float(tuned_config.initial_step_size) >= 0.1)
    Assert.IsTrue(MathCore.parse_float(tuned_config.initial_step_size) <= 2.0)
    Return True

Note: =====================================================================
Note: COMPREHENSIVE TEST RUNNER FUNCTIONS
Note: =====================================================================

Process called "run_line_search_tests" that takes no parameters returns Boolean:
    Note: Run all line search method tests
    Let tests be [
        "test_armijo_line_search_basic",
        "test_wolfe_line_search_conditions", 
        "test_backtracking_line_search_convergence",
        "test_golden_section_line_search_accuracy",
        "test_line_search_edge_cases"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_trust_region_tests" that takes no parameters returns Boolean:
    Note: Run all trust region method tests
    Let tests be [
        "test_trust_region_basic_functionality",
        "test_trust_region_radius_adaptation",
        "test_trust_region_convergence_criteria"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_gradient_descent_tests" that takes no parameters returns Boolean:
    Note: Run all gradient descent method tests
    Let tests be [
        "test_steepest_descent_basic",
        "test_steepest_descent_multidimensional",
        "test_gradient_descent_with_momentum"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_conjugate_gradient_tests" that takes no parameters returns Boolean:
    Note: Run all conjugate gradient method tests
    Let tests be [
        "test_conjugate_gradient_fletcher_reeves",
        "test_conjugate_gradient_polak_ribiere",
        "test_conjugate_gradient_hestenes_stiefel",
        "test_conjugate_gradient_dai_yuan",
        "test_conjugate_gradient_variant_comparison"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_quasi_newton_tests" that takes no parameters returns Boolean:
    Note: Run all quasi-Newton method tests
    Let tests be [
        "test_bfgs_basic_optimization",
        "test_lbfgs_memory_efficiency",
        "test_dfp_quasi_newton",
        "test_sr1_quasi_newton",
        "test_quasi_newton_hessian_approximation"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_newton_method_tests" that takes no parameters returns Boolean:
    Note: Run all Newton method tests
    Let tests be [
        "test_newton_method_basic",
        "test_newton_method_with_modifications",
        "test_newton_method_eigenvalue_modification"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_univariate_optimization_tests" that takes no parameters returns Boolean:
    Note: Run all univariate optimization tests
    Let tests be [
        "test_golden_section_search_convergence",
        "test_ternary_search_accuracy",
        "test_bracketing_methods"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_convergence_tests" that takes no parameters returns Boolean:
    Note: Run all convergence criteria tests
    Let tests be [
        "test_gradient_tolerance_convergence",
        "test_function_tolerance_convergence",
        "test_parameter_tolerance_convergence",
        "test_max_iterations_limit",
        "test_max_function_evaluations_limit"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_problem_formulation_tests" that takes no parameters returns Boolean:
    Note: Run all optimization problem formulation tests
    Let tests be [
        "test_optimization_problem_validation",
        "test_optimization_problem_bounds_checking",
        "test_optimization_result_history_tracking"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_performance_tests" that takes no parameters returns Boolean:
    Note: Run all performance and diagnostics tests
    Let tests be [
        "test_algorithm_performance_comparison",
        "test_optimization_diagnostics"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_differentiation_tests" that takes no parameters returns Boolean:
    Note: Run all differentiation integration tests
    Let tests be [
        "test_automatic_differentiation_integration",
        "test_numerical_differentiation_fallback"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_adaptive_tests" that takes no parameters returns Boolean:
    Note: Run all adaptive parameter selection tests
    Let tests be [
        "test_adaptive_step_size_selection",
        "test_algorithm_parameter_tuning"
    ]
    
    Let all_passed be True
    For test_name in tests:
        Try:
            Print "Running " + test_name + "..."
            Let result be Call(test_name)
            If result:
                Print "  ✓ " + test_name + " PASSED"
            Else:
                Print "  ✗ " + test_name + " FAILED"
                Set all_passed to False
        Catch error:
            Print "  ✗ " + test_name + " ERROR: " + ToString(error)
            Set all_passed to False
    
    Return all_passed

Process called "run_all_tests" that takes no parameters returns Boolean:
    Note: Run all optimization core module tests
    Print "=" * 80
    Print "OPTIMIZATION CORE MODULE COMPREHENSIVE TEST SUITE"
    Print "=" * 80
    Print ""
    
    Let test_categories be [
        ["Line Search Methods", "run_line_search_tests"],
        ["Trust Region Methods", "run_trust_region_tests"],
        ["Gradient Descent Methods", "run_gradient_descent_tests"],
        ["Conjugate Gradient Methods", "run_conjugate_gradient_tests"],
        ["Quasi-Newton Methods", "run_quasi_newton_tests"],
        ["Newton Methods", "run_newton_method_tests"],
        ["Univariate Optimization", "run_univariate_optimization_tests"],
        ["Convergence Criteria", "run_convergence_tests"],
        ["Problem Formulation", "run_problem_formulation_tests"],
        ["Performance & Diagnostics", "run_performance_tests"],
        ["Differentiation Integration", "run_differentiation_tests"],
        ["Adaptive Parameters", "run_adaptive_tests"]
    ]
    
    Let overall_success be True
    Let passed_categories be 0
    Let total_categories be test_categories.length
    
    For category_info in test_categories:
        Let category_name be category_info[0]
        Let test_runner be category_info[1]
        
        Print "Testing " + category_name + "..."
        Print "-" * (9 + Length(category_name))
        
        Let category_result be Call(test_runner)
        If category_result:
            Print "✓ " + category_name + ": ALL TESTS PASSED"
            Set passed_categories to passed_categories + 1
        Else:
            Print "✗ " + category_name + ": SOME TESTS FAILED"
            Set overall_success to False
        Print ""
    
    Print "=" * 80
    Print "OPTIMIZATION CORE TEST SUMMARY"
    Print "=" * 80
    Print "Categories tested: " + ToString(total_categories)
    Print "Categories passed: " + ToString(passed_categories)  
    Print "Categories failed: " + ToString(total_categories - passed_categories)
    
    Let success_rate be (passed_categories * 100.0) / total_categories
    Print "Success rate: " + ToString(success_rate) + "%"
    
    If overall_success:
        Print "\n🎉 ALL OPTIMIZATION CORE TESTS PASSED! 🎉"
        Print "The optimization core module is ready for production use."
    Else:
        Print "\n❌ SOME OPTIMIZATION CORE TESTS FAILED ❌"
        Print "Please review and fix failing tests before deployment."
    
    Print "=" * 80
    
    Return overall_success