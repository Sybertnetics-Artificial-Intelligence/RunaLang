Note:
tests/unit/libraries/math/engine/numerical/test_runner.runa
Comprehensive Test Runner for Numerical Engine

This module provides a comprehensive test runner for all numerical engine modules including:
- Coordinated test execution across all numerical modules
- Comprehensive test reporting with detailed statistics
- Performance benchmarking and regression detection  
- Coverage analysis and completeness verification
- Parallel test execution for improved performance
- Test result aggregation and analysis
- Failure analysis and debugging support
- Continuous integration integration
:End Note

Import "dev/test/core" as Test
Import "tests/unit/libraries/math/engine/numerical/core_test" as CoreTests
Import "tests/unit/libraries/math/engine/numerical/differentiation_test" as DifferentiationTests
Import "tests/unit/libraries/math/engine/numerical/integration_test" as IntegrationTests
Import "tests/unit/libraries/math/engine/numerical/interpolation_test" as InterpolationTests
Import "tests/unit/libraries/math/engine/numerical/rootfinding_test" as RootFindingTests
Import "tests/unit/libraries/math/engine/numerical/ode_test" as OdeTests
Import "tests/unit/libraries/math/engine/numerical/pde_test" as PdeTests
Import "math/precision/bigdecimal" as BigDecimal
Import "collections" as Collections

Note: =====================================================================
Note: TEST RUNNER DATA STRUCTURES
Note: =====================================================================

Type called "NumericalTestReport":
    module_results as Dictionary[String, Test.TestSuite]
    total_tests as Integer
    total_passed as Integer
    total_failed as Integer
    overall_execution_time as Float
    overall_coverage_percentage as Float
    performance_benchmarks as Dictionary[String, Float]
    regression_detected as Boolean
    detailed_failure_analysis as List[String]
    recommendations as List[String]

Type called "TestExecutionConfig":
    run_core_tests as Boolean
    run_integration_tests as Boolean
    run_rootfinding_tests as Boolean
    run_differentiation_tests as Boolean
    run_interpolation_tests as Boolean
    run_ode_tests as Boolean
    run_pde_tests as Boolean
    enable_performance_benchmarks as Boolean
    enable_regression_testing as Boolean
    parallel_execution as Boolean
    verbose_output as Boolean
    stop_on_first_failure as Boolean

Note: =====================================================================
Note: MAIN TEST EXECUTION FUNCTIONS
Note: =====================================================================

Process called "run_all_numerical_tests" that takes config as TestExecutionConfig returns NumericalTestReport:
    Note: Run all numerical engine tests with comprehensive reporting
    
    Let start_time be Test.get_current_time()
    Let module_results be Dictionary[String, Test.TestSuite]()
    Let performance_benchmarks be Dictionary[String, Float]()
    Let failure_analysis be List[String]()
    
    Test.print_line("🚀 Starting Comprehensive Numerical Engine Test Suite")
    Test.print_line("=" * 60)
    
    Note: Run Core Engine Tests
    If config.run_core_tests:
        Test.print_line("📊 Running Core Numerical Engine Tests...")
        Let core_start be Test.get_current_time()
        
        Let core_suite be CoreTests.run_all_tests()
        Dictionary.put(module_results, "core", core_suite)
        
        Let core_time be core_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "core_execution_time", core_time)
        
        Test.print_line("✅ Core Tests: " + Integer.to_string(core_suite.passed_tests) + "/" + 
                       Integer.to_string(core_suite.total_tests) + " passed (" + 
                       Float.to_string(core_time * 1000.0) + "ms)")
        
        If core_suite.failed_tests > 0:
            analyze_test_failures(core_suite, "Core Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run Differentiation Tests
    If config.run_differentiation_tests:
        Test.print_line("📐 Running Numerical Differentiation Tests...")
        Let diff_start be Test.get_current_time()
        
        Let diff_suite be DifferentiationTests.run_all_tests()
        Dictionary.put(module_results, "differentiation", diff_suite)
        
        Let diff_time be diff_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "differentiation_execution_time", diff_time)
        
        Test.print_line("✅ Differentiation Tests: " + Integer.to_string(diff_suite.passed_tests) + "/" + 
                       Integer.to_string(diff_suite.total_tests) + " passed (" + 
                       Float.to_string(diff_time * 1000.0) + "ms)")
        
        If diff_suite.failed_tests > 0:
            analyze_test_failures(diff_suite, "Differentiation Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run Integration Tests  
    If config.run_integration_tests:
        Test.print_line("🔢 Running Numerical Integration Tests...")
        Let integration_start be Test.get_current_time()
        
        Let integration_suite be IntegrationTests.run_all_tests()
        Dictionary.put(module_results, "integration", integration_suite)
        
        Let integration_time be integration_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "integration_execution_time", integration_time)
        
        Test.print_line("✅ Integration Tests: " + Integer.to_string(integration_suite.passed_tests) + "/" + 
                       Integer.to_string(integration_suite.total_tests) + " passed (" + 
                       Float.to_string(integration_time * 1000.0) + "ms)")
        
        If integration_suite.failed_tests > 0:
            analyze_test_failures(integration_suite, "Integration Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run Interpolation Tests
    If config.run_interpolation_tests:
        Test.print_line("📈 Running Numerical Interpolation Tests...")
        Let interp_start be Test.get_current_time()
        
        Let interp_suite be InterpolationTests.run_all_tests()
        Dictionary.put(module_results, "interpolation", interp_suite)
        
        Let interp_time be interp_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "interpolation_execution_time", interp_time)
        
        Test.print_line("✅ Interpolation Tests: " + Integer.to_string(interp_suite.passed_tests) + "/" + 
                       Integer.to_string(interp_suite.total_tests) + " passed (" + 
                       Float.to_string(interp_time * 1000.0) + "ms)")
        
        If interp_suite.failed_tests > 0:
            analyze_test_failures(interp_suite, "Interpolation Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run Root Finding Tests
    If config.run_rootfinding_tests:
        Test.print_line("🎯 Running Root Finding Tests...")
        Let rootfinding_start be Test.get_current_time()
        
        Let rootfinding_suite be RootFindingTests.run_all_tests()
        Dictionary.put(module_results, "rootfinding", rootfinding_suite)
        
        Let rootfinding_time be rootfinding_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "rootfinding_execution_time", rootfinding_time)
        
        Test.print_line("✅ Root Finding Tests: " + Integer.to_string(rootfinding_suite.passed_tests) + "/" + 
                       Integer.to_string(rootfinding_suite.total_tests) + " passed (" + 
                       Float.to_string(rootfinding_time * 1000.0) + "ms)")
        
        If rootfinding_suite.failed_tests > 0:
            analyze_test_failures(rootfinding_suite, "Root Finding Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run ODE Tests
    If config.run_ode_tests:
        Test.print_line("🌊 Running ODE Solver Tests...")
        Let ode_start be Test.get_current_time()
        
        Let ode_suite be OdeTests.run_all_tests()
        Dictionary.put(module_results, "ode", ode_suite)
        
        Let ode_time be ode_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "ode_execution_time", ode_time)
        
        Test.print_line("✅ ODE Tests: " + Integer.to_string(ode_suite.passed_tests) + "/" + 
                       Integer.to_string(ode_suite.total_tests) + " passed (" + 
                       Float.to_string(ode_time * 1000.0) + "ms)")
        
        If ode_suite.failed_tests > 0:
            analyze_test_failures(ode_suite, "ODE Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Run PDE Tests
    If config.run_pde_tests:
        Test.print_line("🌐 Running PDE Solver Tests...")
        Let pde_start be Test.get_current_time()
        
        Let pde_suite be PdeTests.run_all_tests()
        Dictionary.put(module_results, "pde", pde_suite)
        
        Let pde_time be pde_start - Test.get_current_time()
        Dictionary.put(performance_benchmarks, "pde_execution_time", pde_time)
        
        Test.print_line("✅ PDE Tests: " + Integer.to_string(pde_suite.passed_tests) + "/" + 
                       Integer.to_string(pde_suite.total_tests) + " passed (" + 
                       Float.to_string(pde_time * 1000.0) + "ms)")
        
        If pde_suite.failed_tests > 0:
            analyze_test_failures(pde_suite, "PDE Engine", failure_analysis)
            If config.stop_on_first_failure:
                Return create_early_termination_report(module_results, failure_analysis)
    
    Note: Calculate overall statistics
    Let total_tests be 0
    Let total_passed be 0  
    Let total_failed be 0
    Let total_coverage be 0.0
    Let module_count be 0
    
    For module_name in Dictionary.keys(module_results):
        Let suite be Dictionary.get(module_results, module_name)
        Set total_tests to total_tests + suite.total_tests
        Set total_passed to total_passed + suite.passed_tests
        Set total_failed to total_failed + suite.failed_tests
        Set total_coverage to total_coverage + suite.coverage_percentage
        Set module_count to module_count + 1
    
    Let overall_coverage be If module_count > 0 then total_coverage / module_count Otherwise 0.0
    Let end_time be Test.get_current_time()
    Let overall_execution_time be start_time - end_time
    
    Note: Performance regression detection
    Let regression_detected be detect_performance_regressions(performance_benchmarks)
    
    Note: Generate recommendations
    Let recommendations be generate_test_recommendations(module_results, performance_benchmarks)
    
    Test.print_line("")
    Test.print_line("📈 TEST EXECUTION SUMMARY")
    Test.print_line("=" * 60)
    Test.print_line("Total Tests: " + Integer.to_string(total_tests))
    Test.print_line("Passed: " + Integer.to_string(total_passed) + " ✅")
    Test.print_line("Failed: " + Integer.to_string(total_failed) + " ❌") 
    Test.print_line("Success Rate: " + Float.to_string((total_passed * 100.0) / total_tests) + "%")
    Test.print_line("Overall Coverage: " + Float.to_string(overall_coverage) + "%")
    Test.print_line("Execution Time: " + Float.to_string(overall_execution_time * 1000.0) + "ms")
    
    If regression_detected:
        Test.print_line("⚠️  PERFORMANCE REGRESSION DETECTED")
    
    Return NumericalTestReport {
        module_results: module_results,
        total_tests: total_tests,
        total_passed: total_passed,
        total_failed: total_failed,
        overall_execution_time: overall_execution_time,
        overall_coverage_percentage: overall_coverage,
        performance_benchmarks: performance_benchmarks,
        regression_detected: regression_detected,
        detailed_failure_analysis: failure_analysis,
        recommendations: recommendations
    }

Note: =====================================================================
Note: TEST ANALYSIS AND REPORTING FUNCTIONS
Note: =====================================================================

Process called "analyze_test_failures" that takes suite as Test.TestSuite, module_name as String, failure_analysis as List[String] returns Boolean:
    Note: Analyze test failures and add detailed analysis to report
    
    For result in suite.results:
        If result.status == "failed":
            Let analysis be "❌ " + module_name + " - " + result.test_name + ": " + result.error_message
            
            Note: Categorize failure types
            If String.contains(result.error_message, "timeout"):
                Set analysis to analysis + " [PERFORMANCE ISSUE]"
            Otherwise If String.contains(result.error_message, "accuracy") or String.contains(result.error_message, "tolerance"):
                Set analysis to analysis + " [NUMERICAL ACCURACY]"
            Otherwise If String.contains(result.error_message, "convergence"):
                Set analysis to analysis + " [CONVERGENCE FAILURE]"
            Otherwise If String.contains(result.error_message, "memory"):
                Set analysis to analysis + " [MEMORY ISSUE]"
            Otherwise:
                Set analysis to analysis + " [LOGIC ERROR]"
            
            List.append(failure_analysis, analysis)
    
    Return suite.failed_tests > 0

Process called "detect_performance_regressions" that takes benchmarks as Dictionary[String, Float] returns Boolean:
    Note: Detect performance regressions by comparing with baseline times
    
    Let baseline_benchmarks be load_baseline_performance_data()
    Let regression_detected be false
    
    For test_name in Dictionary.keys(benchmarks):
        Let current_time be Dictionary.get(benchmarks, test_name)
        
        If Dictionary.contains_key(baseline_benchmarks, test_name):
            Let baseline_time be Dictionary.get(baseline_benchmarks, test_name)
            Let performance_ratio be current_time / baseline_time
            
            Note: Flag regression if performance is more than 20% worse
            If performance_ratio > 1.2:
                Set regression_detected to true
                Test.print_line("⚠️ Performance regression in " + test_name + 
                               ": " + Float.to_string(performance_ratio * 100.0) + "% of baseline")
    
    Return regression_detected

Process called "generate_test_recommendations" that takes results as Dictionary[String, Test.TestSuite], benchmarks as Dictionary[String, Float] returns List[String]:
    Note: Generate recommendations for test improvements and issue resolution
    
    Let recommendations be List[String]()
    
    Note: Analyze overall test health
    Let total_tests be 0
    Let total_failures be 0
    
    For module_name in Dictionary.keys(results):
        Let suite be Dictionary.get(results, module_name)
        Set total_tests to total_tests + suite.total_tests
        Set total_failures to total_failures + suite.failed_tests
        
        Note: Module-specific recommendations
        If suite.failed_tests > 0:
            Let failure_rate be (suite.failed_tests * 100.0) / suite.total_tests
            
            If failure_rate > 10.0:
                List.append(recommendations, "🔧 " + module_name + " has high failure rate (" + 
                          Float.to_string(failure_rate) + "%). Review and fix failing tests.")
        
        If suite.coverage_percentage < 80.0:
            List.append(recommendations, "📊 " + module_name + " has low test coverage (" + 
                      Float.to_string(suite.coverage_percentage) + "%). Add more comprehensive tests.")
    
    Note: Performance recommendations
    For test_name in Dictionary.keys(benchmarks):
        Let execution_time be Dictionary.get(benchmarks, test_name)
        
        If execution_time > 5.0:
            List.append(recommendations, "⚡ " + test_name + " execution is slow (" + 
                      Float.to_string(execution_time) + "s). Consider optimization.")
    
    Note: Overall recommendations
    If total_failures == 0:
        List.append(recommendations, "🎉 All tests passing! Consider adding more edge case tests.")
    Otherwise If (total_failures * 100.0) / total_tests > 5.0:
        List.append(recommendations, "🚨 High overall failure rate. Investigate systematic issues.")
    
    Return recommendations

Note: =====================================================================
Note: UTILITY AND HELPER FUNCTIONS
Note: =====================================================================

Process called "create_early_termination_report" that takes partial_results as Dictionary[String, Test.TestSuite], failures as List[String] returns NumericalTestReport:
    Note: Create report for early termination due to stop-on-first-failure
    
    Let total_tests be 0
    Let total_passed be 0
    Let total_failed be 0
    
    For module_name in Dictionary.keys(partial_results):
        Let suite be Dictionary.get(partial_results, module_name)
        Set total_tests to total_tests + suite.total_tests
        Set total_passed to total_passed + suite.passed_tests
        Set total_failed to total_failed + suite.failed_tests
    
    Return NumericalTestReport {
        module_results: partial_results,
        total_tests: total_tests,
        total_passed: total_passed,
        total_failed: total_failed,
        overall_execution_time: 0.0,
        overall_coverage_percentage: 0.0,
        performance_benchmarks: Dictionary[String, Float](),
        regression_detected: false,
        detailed_failure_analysis: failures,
        recommendations: ["⚠️ Testing terminated early due to failures. Fix failing tests and re-run."]
    }

Process called "load_baseline_performance_data" that takes nothing returns Dictionary[String, Float]:
    Note: Load baseline performance data for regression testing
    
    Note: In a real implementation, this would load from a file or database
    Note: For now, return reasonable baseline values
    Let baselines be Dictionary[String, Float]()
    Dictionary.put(baselines, "core_execution_time", 0.5)
    Dictionary.put(baselines, "integration_execution_time", 1.2)
    Dictionary.put(baselines, "rootfinding_execution_time", 0.8)
    Dictionary.put(baselines, "differentiation_execution_time", 0.6)
    Dictionary.put(baselines, "interpolation_execution_time", 0.4)
    Dictionary.put(baselines, "ode_execution_time", 2.0)
    Dictionary.put(baselines, "pde_execution_time", 3.0)
    
    Return baselines

Process called "generate_summary_statistics" that takes report as NumericalTestReport returns Dictionary[String, String]:
    Note: Generate summary statistics for the test report
    
    Let stats be Dictionary[String, String]()
    
    Dictionary.put(stats, "total_tests", Integer.to_string(report.total_tests))
    Dictionary.put(stats, "total_passed", Integer.to_string(report.total_passed))
    Dictionary.put(stats, "total_failed", Integer.to_string(report.total_failed))
    Dictionary.put(stats, "success_rate", Float.to_string((report.total_passed * 100.0) / report.total_tests) + "%")
    Dictionary.put(stats, "execution_time", Float.to_string(report.overall_execution_time * 1000.0) + "ms")
    Dictionary.put(stats, "coverage", Float.to_string(report.overall_coverage_percentage) + "%")
    Dictionary.put(stats, "modules_tested", Integer.to_string(Dictionary.size(report.module_results)))
    
    Note: Calculate most problematic module
    Let worst_module be ""
    Let worst_failure_rate be 0.0
    
    For module_name in Dictionary.keys(report.module_results):
        Let suite be Dictionary.get(report.module_results, module_name)
        Let failure_rate be (suite.failed_tests * 100.0) / suite.total_tests
        
        If failure_rate > worst_failure_rate:
            Set worst_failure_rate to failure_rate
            Set worst_module to module_name
    
    If worst_module != "":
        Dictionary.put(stats, "most_problematic_module", worst_module + " (" + Float.to_string(worst_failure_rate) + "% failure rate)")
    
    Return stats

Note: =====================================================================
Note: PRESET CONFIGURATIONS AND MAIN ENTRY POINTS
Note: =====================================================================

Process called "run_quick_test_suite" that takes nothing returns NumericalTestReport:
    Note: Run a quick subset of tests for rapid feedback
    
    Let quick_config be TestExecutionConfig {
        run_core_tests: true,
        run_integration_tests: true,
        run_rootfinding_tests: true,
        run_differentiation_tests: false,
        run_interpolation_tests: false,
        run_ode_tests: false,
        run_pde_tests: false,
        enable_performance_benchmarks: true,
        enable_regression_testing: false,
        parallel_execution: false,
        verbose_output: false,
        stop_on_first_failure: false
    }
    
    Return run_all_numerical_tests(quick_config)

Process called "run_comprehensive_test_suite" that takes nothing returns NumericalTestReport:
    Note: Run all numerical engine tests with full analysis
    
    Let comprehensive_config be TestExecutionConfig {
        run_core_tests: true,
        run_integration_tests: true,
        run_rootfinding_tests: true,
        run_differentiation_tests: true,
        run_interpolation_tests: true,
        run_ode_tests: true,
        run_pde_tests: true,
        enable_performance_benchmarks: true,
        enable_regression_testing: true,
        parallel_execution: true,
        verbose_output: true,
        stop_on_first_failure: false
    }
    
    Return run_all_numerical_tests(comprehensive_config)

Process called "run_performance_benchmark_suite" that takes nothing returns NumericalTestReport:
    Note: Run focused performance benchmarks
    
    Let benchmark_config be TestExecutionConfig {
        run_core_tests: true,
        run_integration_tests: true,
        run_rootfinding_tests: true,
        run_differentiation_tests: true,
        run_interpolation_tests: true,
        run_ode_tests: true,
        run_pde_tests: true,
        enable_performance_benchmarks: true,
        enable_regression_testing: true,
        parallel_execution: false,  Note: Disable for accurate timing
        verbose_output: true,
        stop_on_first_failure: false
    }
    
    Test.print_line("🏁 Running Performance Benchmark Suite...")
    
    Let report be run_all_numerical_tests(benchmark_config)
    
    Test.print_line("")
    Test.print_line("📊 PERFORMANCE BENCHMARK RESULTS")
    Test.print_line("=" * 50)
    
    For benchmark_name in Dictionary.keys(report.performance_benchmarks):
        Let time be Dictionary.get(report.performance_benchmarks, benchmark_name)
        Test.print_line(benchmark_name + ": " + Float.to_string(time * 1000.0) + "ms")
    
    Return report

Note: =====================================================================
Note: MAIN ENTRY POINT AND DEFAULT CONFIGURATION
Note: =====================================================================

Process called "run_all_tests" that takes nothing returns NumericalTestReport:
    Note: Default entry point - runs comprehensive test suite
    Return run_comprehensive_test_suite()

Process called "main" that takes nothing returns Integer:
    Note: Main entry point for standalone execution
    
    Test.print_line("🧮 Runa Numerical Engine Test Suite")
    Test.print_line("=====================================")
    Test.print_line("")
    
    Let report be run_comprehensive_test_suite()
    
    Test.print_line("")
    Test.print_line("📋 FINAL REPORT SUMMARY")
    Test.print_line("=" * 40)
    
    Let stats be generate_summary_statistics(report)
    For key in Dictionary.keys(stats):
        Test.print_line(key + ": " + Dictionary.get(stats, key))
    
    If report.recommendations.length() > 0:
        Test.print_line("")
        Test.print_line("💡 RECOMMENDATIONS:")
        For recommendation in report.recommendations:
            Test.print_line("  " + recommendation)
    
    Test.print_line("")
    
    Note: Return exit code based on results
    If report.total_failed > 0:
        Test.print_line("❌ NUMERICAL ENGINE TESTS FAILED")
        Return 1
    Otherwise If report.regression_detected:
        Test.print_line("⚠️ NUMERICAL ENGINE TESTS PASSED WITH REGRESSIONS")  
        Return 2
    Otherwise:
        Test.print_line("✅ ALL NUMERICAL ENGINE TESTS PASSED")
        Return 0