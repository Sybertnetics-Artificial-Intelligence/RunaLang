Note: Master Test Runner for Dynamical Systems Module

This is the comprehensive test runner for the entire dynamical systems module,
coordinating all tests across dynamical systems, chaos theory, and bifurcation
theory components. It provides unified test execution, reporting, and performance
benchmarking for the complete dynamical analysis functionality.

Test Module Coverage:
- systems_test.runa: Dynamical systems analysis and phase portraits
- chaos_test.runa: Chaos theory, attractors, and Lyapunov exponents  
- bifurcation_test.runa: Bifurcation analysis and normal forms

Features:
- Comprehensive test execution across all dynamical modules
- Detailed test reporting with pass/fail statistics
- Performance benchmarking and timing analysis
- Error handling and test isolation
- Summary reporting with module breakdown
- Integration testing across module boundaries

:End Note

Import "tests/unit/libraries/math/dynamical/systems_test" as SystemsTest
Import "tests/unit/libraries/math/dynamical/chaos_test" as ChaosTest  
Import "tests/unit/libraries/math/dynamical/bifurcation_test" as BifurcationTest
Import "math/core/operations" as MathOps
Import "os/system/time" as SystemTime

Note: ===== Test Suite Coordination =====

Type called "ModuleTestResult":
    module_name as String
    tests_passed as Integer
    tests_total as Integer
    execution_time_ms as Float64
    success_rate as Float64
    errors_encountered as List[String]

Type called "DynamicalTestReport":
    total_tests_passed as Integer
    total_tests_run as Integer
    overall_success_rate as Float64
    total_execution_time_ms as Float64
    module_results as List[ModuleTestResult]
    performance_benchmarks as Dictionary[String, Float64]
    critical_failures as List[String]
    recommendations as List[String]

Note: ===== Individual Module Test Execution =====

Process called "run_systems_module_tests" that takes returns ModuleTestResult:
    Note: Executes all dynamical systems tests with error handling and timing
    
    Let start_time be SystemTime.current_timestamp_microseconds()
    Let errors_list be MathOps.create_empty_list(10)
    
    Let test_result be [0, 0]
    
    Try:
        test_result = SystemsTest.run_all_systems_tests()
    Catch error:
        errors_list.append("Systems module test execution failed: " + error.message)
        test_result = [0, 1]
    
    Let end_time be SystemTime.current_timestamp_microseconds()
    Let execution_time be MathOps.integer_to_float(end_time - start_time) / 1000.0
    
    Let success_rate be 0.0
    If test_result[1] > 0:
        success_rate = MathOps.integer_to_float(test_result[0]) / MathOps.integer_to_float(test_result[1])
    
    Let module_result be ModuleTestResult{
        module_name: "Dynamical Systems",
        tests_passed: test_result[0],
        tests_total: test_result[1], 
        execution_time_ms: execution_time,
        success_rate: success_rate,
        errors_encountered: errors_list
    }
    
    Return module_result

Process called "run_chaos_module_tests" that takes returns ModuleTestResult:
    Note: Executes all chaos theory tests with error handling and timing
    
    Let start_time be SystemTime.current_timestamp_microseconds()
    Let errors_list be MathOps.create_empty_list(10)
    
    Let test_result be [0, 0]
    
    Try:
        test_result = ChaosTest.run_all_chaos_tests()
    Catch error:
        errors_list.append("Chaos module test execution failed: " + error.message)
        test_result = [0, 1]
    
    Let end_time be SystemTime.current_timestamp_microseconds()
    Let execution_time be MathOps.integer_to_float(end_time - start_time) / 1000.0
    
    Let success_rate be 0.0
    If test_result[1] > 0:
        success_rate = MathOps.integer_to_float(test_result[0]) / MathOps.integer_to_float(test_result[1])
    
    Let module_result be ModuleTestResult{
        module_name: "Chaos Theory",
        tests_passed: test_result[0],
        tests_total: test_result[1],
        execution_time_ms: execution_time,
        success_rate: success_rate,
        errors_encountered: errors_list
    }
    
    Return module_result

Process called "run_bifurcation_module_tests" that takes returns ModuleTestResult:
    Note: Executes all bifurcation theory tests with error handling and timing
    
    Let start_time be SystemTime.current_timestamp_microseconds()
    Let errors_list be MathOps.create_empty_list(10)
    
    Let test_result be [0, 0]
    
    Try:
        test_result = BifurcationTest.run_all_bifurcation_tests()
    Catch error:
        errors_list.append("Bifurcation module test execution failed: " + error.message)
        test_result = [0, 1]
    
    Let end_time be SystemTime.current_timestamp_microseconds()
    Let execution_time be MathOps.integer_to_float(end_time - start_time) / 1000.0
    
    Let success_rate be 0.0
    If test_result[1] > 0:
        success_rate = MathOps.integer_to_float(test_result[0]) / MathOps.integer_to_float(test_result[1])
    
    Let module_result be ModuleTestResult{
        module_name: "Bifurcation Theory",
        tests_passed: test_result[0],
        tests_total: test_result[1],
        execution_time_ms: execution_time,
        success_rate: success_rate,
        errors_encountered: errors_list
    }
    
    Return module_result

Note: ===== Integration Tests =====

Process called "run_integration_tests" that takes returns ModuleTestResult:
    Note: Runs integration tests across dynamical systems modules
    
    Let start_time be SystemTime.current_timestamp_microseconds()
    Let errors_list be MathOps.create_empty_list(5)
    Let passed_count be 0
    Let total_count be 5
    
    Let test_results be [
        test_systems_chaos_integration(),
        test_chaos_bifurcation_integration(),
        test_bifurcation_systems_integration(),
        test_cross_module_performance(),
        test_end_to_end_workflow()
    ]
    
    For result in test_results:
        If result:
            passed_count = passed_count + 1
    
    Let end_time be SystemTime.current_timestamp_microseconds()
    Let execution_time be MathOps.integer_to_float(end_time - start_time) / 1000.0
    
    Let success_rate be MathOps.integer_to_float(passed_count) / MathOps.integer_to_float(total_count)
    
    Let integration_result be ModuleTestResult{
        module_name: "Integration Tests",
        tests_passed: passed_count,
        tests_total: total_count,
        execution_time_ms: execution_time,
        success_rate: success_rate,
        errors_encountered: errors_list
    }
    
    Return integration_result

Process called "test_systems_chaos_integration" that takes returns Boolean:
    Note: Tests integration between dynamical systems and chaos analysis
    
    Let lorenz_system be (state: List[Float64]) => [
        10.0 * (state[1] - state[0]),
        state[0] * (28.0 - state[2]) - state[1], 
        state[0] * state[1] - (8.0/3.0) * state[2]
    ]
    
    Let initial_state be [1.0, 1.0, 1.0]
    Let time_span be [0.0, 50.0]
    
    Try:
        Let continuous_system be DynamicalSystems.create_continuous_system(lorenz_system, initial_state, time_span)
        Let trajectory be DynamicalSystems.integrate_trajectory(lorenz_system, initial_state, time_span, 0.01)
        
        If trajectory.state_history.size == 0:
            Return false
        
        Let time_series be MathOps.create_empty_list(trajectory.state_history.size)
        For i from 0 to trajectory.state_history.size - 1:
            time_series[i] = trajectory.state_history[i][0]
        
        Let lyapunov_exponent be Chaos.compute_largest_lyapunov_exponent(time_series)
        
        If lyapunov_exponent <= 0.0:
            Return false
        
        Let attractor be Chaos.reconstruct_attractor(time_series, 3, 10)
        
        If attractor.embedding_vectors.size == 0:
            Return false
        
        Return true
    Catch error:
        Return false

Process called "test_chaos_bifurcation_integration" that takes returns Boolean:
    Note: Tests integration between chaos theory and bifurcation analysis
    
    Let logistic_map be (state: List[Float64], parameter: Float64) => [parameter * state[0] * (1.0 - state[0])]
    
    Try:
        Let parameter_range be [3.0, 4.0]
        Let bifurcation_diagram be Bifurcation.analyze_period_doubling_cascade(logistic_map, parameter_range)
        
        If bifurcation_diagram.bifurcation_points.size == 0:
            Return false
        
        Let chaotic_parameter be 3.99
        Let chaotic_series be MathOps.create_empty_list(1000)
        Let current_value be 0.5
        
        For i from 0 to 999:
            current_value = logistic_map([current_value], chaotic_parameter)[0]
            chaotic_series[i] = current_value
        
        Let chaos_detected be Chaos.detect_chaos(chaotic_series)
        
        If not chaos_detected.is_chaotic:
            Return false
        
        If chaos_detected.largest_lyapunov_exponent <= 0.0:
            Return false
        
        Return true
    Catch error:
        Return false

Process called "test_bifurcation_systems_integration" that takes returns Boolean:
    Note: Tests integration between bifurcation theory and dynamical systems
    
    Let hopf_system be (state: List[Float64], parameter: Float64) => [
        parameter * state[0] - state[1] - state[0] * (state[0] * state[0] + state[1] * state[1]),
        state[0] + parameter * state[1] - state[1] * (state[0] * state[0] + state[1] * state[1])
    ]
    
    Try:
        Let parameter_range be [-1.0, 1.0]
        Let hopf_diagram be Bifurcation.analyze_hopf_bifurcation(hopf_system, parameter_range)
        
        If hopf_diagram.bifurcation_points.size == 0:
            Return false
        
        Let post_hopf_parameter be 0.1
        Let post_hopf_system be (state: List[Float64]) => hopf_system(state, post_hopf_parameter)
        Let initial_state be [0.1, 0.1]
        Let time_span be [0.0, 50.0]
        
        Let trajectory be DynamicalSystems.integrate_trajectory(post_hopf_system, initial_state, time_span, 0.01)
        
        If trajectory.state_history.size == 0:
            Return false
        
        Let limit_cycles be DynamicalSystems.detect_limit_cycles(post_hopf_system, [initial_state])
        
        If limit_cycles.size == 0:
            Return false
        
        Return true
    Catch error:
        Return false

Process called "test_cross_module_performance" that takes returns Boolean:
    Note: Tests performance across all dynamical modules working together
    
    Let start_time be SystemTime.current_timestamp_microseconds()
    
    Try:
        Let van_der_pol_system be (state: List[Float64]) => [
            state[1],
            -state[0] + 0.1 * (1.0 - state[0] * state[0]) * state[1]
        ]
        
        Let initial_state be [0.1, 0.1]
        Let time_span be [0.0, 100.0]
        Let trajectory be DynamicalSystems.integrate_trajectory(van_der_pol_system, initial_state, time_span, 0.01)
        
        Let x_series be MathOps.create_empty_list(trajectory.state_history.size)
        For i from 0 to trajectory.state_history.size - 1:
            x_series[i] = trajectory.state_history[i][0]
        
        Let attractor be Chaos.reconstruct_attractor(x_series, 2, 10)
        Let correlation_dim be Chaos.compute_correlation_dimension(attractor)
        
        Let equilibrium be [0.0, 0.0]
        Let stability_result be DynamicalSystems.analyze_linear_stability(van_der_pol_system, equilibrium)
        
        Let end_time be SystemTime.current_timestamp_microseconds()
        Let total_time_ms be MathOps.integer_to_float(end_time - start_time) / 1000.0
        
        If total_time_ms > 5000.0:
            Return false
        
        If correlation_dim <= 0.0:
            Return false
        
        Return true
    Catch error:
        Return false

Process called "test_end_to_end_workflow" that takes returns Boolean:
    Note: Tests complete end-to-end dynamical analysis workflow
    
    Try:
        Let duffing_system be (state: List[Float64]) => [
            state[1],
            -0.1 * state[1] - state[0] * state[0] * state[0] + state[0] + 0.3 * MathOps.cosine(1.2 * SystemTime.current_timestamp_microseconds() / 1000000.0)
        ]
        
        Let initial_state be [1.0, 0.0]
        Let time_span be [0.0, 100.0]
        
        Let system be DynamicalSystems.create_continuous_system(duffing_system, initial_state, time_span)
        Let trajectory be DynamicalSystems.integrate_trajectory(duffing_system, initial_state, time_span, 0.01)
        
        If trajectory.state_history.size == 0:
            Return false
        
        Let equilibria be DynamicalSystems.find_equilibrium_points(duffing_system, [[-2.0, 2.0], [-2.0, 2.0]])
        
        For equilibrium in equilibria:
            Let stability be DynamicalSystems.analyze_linear_stability(duffing_system, equilibrium.position)
            If stability.stability_classification == "unknown":
                Return false
        
        Let x_series be MathOps.create_empty_list(trajectory.state_history.size)
        For i from 0 to trajectory.state_history.size - 1:
            x_series[i] = trajectory.state_history[i][0]
        
        Let chaos_result be Chaos.detect_chaos(x_series)
        Let attractor be Chaos.reconstruct_attractor(x_series, 3, 15)
        
        If attractor.embedding_vectors.size == 0:
            Return false
        
        Return true
    Catch error:
        Return false

Note: ===== Performance Benchmarking =====

Process called "run_performance_benchmarks" that takes returns Dictionary[String, Float64]:
    Note: Runs comprehensive performance benchmarks across all modules
    
    Let benchmarks be MathOps.create_empty_dictionary()
    
    Let trajectory_start_time be SystemTime.current_timestamp_microseconds()
    Let _ be benchmark_trajectory_integration()
    Let trajectory_end_time be SystemTime.current_timestamp_microseconds()
    benchmarks["trajectory_integration_ms"] = MathOps.integer_to_float(trajectory_end_time - trajectory_start_time) / 1000.0
    
    Let lyapunov_start_time be SystemTime.current_timestamp_microseconds()
    Let _ be benchmark_lyapunov_computation()
    Let lyapunov_end_time be SystemTime.current_timestamp_microseconds()
    benchmarks["lyapunov_computation_ms"] = MathOps.integer_to_float(lyapunov_end_time - lyapunov_start_time) / 1000.0
    
    Let bifurcation_start_time be SystemTime.current_timestamp_microseconds()
    Let _ be benchmark_bifurcation_analysis()
    Let bifurcation_end_time be SystemTime.current_timestamp_microseconds()
    benchmarks["bifurcation_analysis_ms"] = MathOps.integer_to_float(bifurcation_end_time - bifurcation_start_time) / 1000.0
    
    Let attractor_start_time be SystemTime.current_timestamp_microseconds()
    Let _ be benchmark_attractor_reconstruction()
    Let attractor_end_time be SystemTime.current_timestamp_microseconds()
    benchmarks["attractor_reconstruction_ms"] = MathOps.integer_to_float(attractor_end_time - attractor_start_time) / 1000.0
    
    Return benchmarks

Process called "benchmark_trajectory_integration" that takes returns Boolean:
    Let lorenz_system be (state: List[Float64]) => [
        10.0 * (state[1] - state[0]),
        state[0] * (28.0 - state[2]) - state[1],
        state[0] * state[1] - (8.0/3.0) * state[2]
    ]
    
    Let initial_state be [1.0, 1.0, 1.0]
    Let time_span be [0.0, 100.0]
    Let trajectory be DynamicalSystems.integrate_trajectory(lorenz_system, initial_state, time_span, 0.001)
    
    Return trajectory.state_history.size > 50000

Process called "benchmark_lyapunov_computation" that takes returns Boolean:
    Let henon_series be MathOps.create_empty_list(5000)
    Let current_state be [0.1, 0.1]
    
    For i from 0 to 4999:
        current_state = [1.0 - 1.4 * current_state[0] * current_state[0] + current_state[1], 0.3 * current_state[0]]
        henon_series[i] = current_state[0]
    
    Let lyapunov_exponent be Chaos.compute_largest_lyapunov_exponent(henon_series)
    
    Return lyapunov_exponent > 0.0

Process called "benchmark_bifurcation_analysis" that takes returns Boolean:
    Let logistic_map be (state: List[Float64], parameter: Float64) => [parameter * state[0] * (1.0 - state[0])]
    Let parameter_range be [2.5, 4.0]
    
    Let bifurcation_diagram be Bifurcation.analyze_period_doubling_cascade(logistic_map, parameter_range)
    
    Return bifurcation_diagram.bifurcation_points.size > 5

Process called "benchmark_attractor_reconstruction" that takes returns Boolean:
    Let rossler_series be MathOps.create_empty_list(2000)
    Let current_state be [1.0, 1.0, 1.0]
    Let dt be 0.01
    
    For i from 0 to 1999:
        Let dx be (-current_state[1] - current_state[2]) * dt
        Let dy be (current_state[0] + 0.2 * current_state[1]) * dt  
        Let dz be (0.2 + current_state[2] * (current_state[0] - 5.7)) * dt
        
        current_state[0] = current_state[0] + dx
        current_state[1] = current_state[1] + dy
        current_state[2] = current_state[2] + dz
        
        rossler_series[i] = current_state[0]
    
    Let attractor be Chaos.reconstruct_attractor(rossler_series, 3, 10)
    
    Return attractor.embedding_vectors.size > 1000

Note: ===== Report Generation =====

Process called "generate_test_report" that takes module_results as List[ModuleTestResult], benchmarks as Dictionary[String, Float64] returns DynamicalTestReport:
    Note: Generates comprehensive test report with analysis and recommendations
    
    Let total_passed be 0
    Let total_run be 0
    Let total_time be 0.0
    Let critical_failures be MathOps.create_empty_list(10)
    Let recommendations be MathOps.create_empty_list(10)
    
    For module_result in module_results:
        total_passed = total_passed + module_result.tests_passed
        total_run = total_run + module_result.tests_total
        total_time = total_time + module_result.execution_time_ms
        
        If module_result.success_rate < 0.8:
            critical_failures.append("Module " + module_result.module_name + " has low success rate: " + MathOps.float_to_string(module_result.success_rate * 100.0) + "%")
        
        If module_result.execution_time_ms > 2000.0:
            recommendations.append("Consider optimizing " + module_result.module_name + " module - execution time: " + MathOps.float_to_string(module_result.execution_time_ms) + "ms")
        
        For error in module_result.errors_encountered:
            critical_failures.append(error)
    
    Let overall_success_rate be 0.0
    If total_run > 0:
        overall_success_rate = MathOps.integer_to_float(total_passed) / MathOps.integer_to_float(total_run)
    
    If overall_success_rate >= 0.95:
        recommendations.append("Excellent test coverage! All dynamical systems modules performing well.")
    Otherwise if overall_success_rate >= 0.8:
        recommendations.append("Good test coverage. Consider investigating failed tests for improvements.")
    Otherwise:
        recommendations.append("Critical: Multiple test failures detected. Immediate investigation required.")
        critical_failures.append("Overall success rate below 80%: " + MathOps.float_to_string(overall_success_rate * 100.0) + "%")
    
    For benchmark_name in benchmarks.keys():
        Let benchmark_time be benchmarks[benchmark_name]
        If benchmark_time > 1000.0:
            recommendations.append("Performance concern in " + benchmark_name + ": " + MathOps.float_to_string(benchmark_time) + "ms")
    
    Let report be DynamicalTestReport{
        total_tests_passed: total_passed,
        total_tests_run: total_run,
        overall_success_rate: overall_success_rate,
        total_execution_time_ms: total_time,
        module_results: module_results,
        performance_benchmarks: benchmarks,
        critical_failures: critical_failures,
        recommendations: recommendations
    }
    
    Return report

Process called "print_detailed_report" that takes report as DynamicalTestReport returns None:
    Note: Prints comprehensive test report to console
    
    MathOps.print_string("=" * 80)
    MathOps.print_string("DYNAMICAL SYSTEMS MODULE - COMPREHENSIVE TEST REPORT")
    MathOps.print_string("=" * 80)
    MathOps.print_string("")
    
    MathOps.print_string("OVERALL SUMMARY:")
    MathOps.print_string("  Total Tests Run: " + MathOps.integer_to_string(report.total_tests_run))
    MathOps.print_string("  Total Tests Passed: " + MathOps.integer_to_string(report.total_tests_passed))
    MathOps.print_string("  Overall Success Rate: " + MathOps.float_to_string(report.overall_success_rate * 100.0) + "%")
    MathOps.print_string("  Total Execution Time: " + MathOps.float_to_string(report.total_execution_time_ms) + " ms")
    MathOps.print_string("")
    
    MathOps.print_string("MODULE BREAKDOWN:")
    For module_result in report.module_results:
        MathOps.print_string("  " + module_result.module_name + ":")
        MathOps.print_string("    Tests: " + MathOps.integer_to_string(module_result.tests_passed) + "/" + MathOps.integer_to_string(module_result.tests_total))
        MathOps.print_string("    Success Rate: " + MathOps.float_to_string(module_result.success_rate * 100.0) + "%")
        MathOps.print_string("    Execution Time: " + MathOps.float_to_string(module_result.execution_time_ms) + " ms")
        
        If module_result.errors_encountered.size > 0:
            MathOps.print_string("    Errors:")
            For error in module_result.errors_encountered:
                MathOps.print_string("      - " + error)
        
        MathOps.print_string("")
    
    MathOps.print_string("PERFORMANCE BENCHMARKS:")
    For benchmark_name in report.performance_benchmarks.keys():
        Let benchmark_time be report.performance_benchmarks[benchmark_name]
        MathOps.print_string("  " + benchmark_name + ": " + MathOps.float_to_string(benchmark_time) + " ms")
    MathOps.print_string("")
    
    If report.critical_failures.size > 0:
        MathOps.print_string("CRITICAL FAILURES:")
        For failure in report.critical_failures:
            MathOps.print_string("  ! " + failure)
        MathOps.print_string("")
    
    MathOps.print_string("RECOMMENDATIONS:")
    For recommendation in report.recommendations:
        MathOps.print_string("  * " + recommendation)
    
    MathOps.print_string("")
    MathOps.print_string("=" * 80)

Note: ===== Main Test Runner =====

Process called "run_all_dynamical_tests" that takes returns DynamicalTestReport:
    Note: Executes all dynamical systems tests and generates comprehensive report
    
    MathOps.print_string("Starting Dynamical Systems Module Test Suite...")
    MathOps.print_string("Testing modules: systems, chaos, bifurcation, integration")
    MathOps.print_string("")
    
    Let overall_start_time be SystemTime.current_timestamp_microseconds()
    
    MathOps.print_string("Running Dynamical Systems Tests...")
    Let systems_result be run_systems_module_tests()
    
    MathOps.print_string("Running Chaos Theory Tests...")
    Let chaos_result be run_chaos_module_tests()
    
    MathOps.print_string("Running Bifurcation Theory Tests...")
    Let bifurcation_result be run_bifurcation_module_tests()
    
    MathOps.print_string("Running Integration Tests...")
    Let integration_result be run_integration_tests()
    
    MathOps.print_string("Running Performance Benchmarks...")
    Let benchmarks be run_performance_benchmarks()
    
    Let overall_end_time be SystemTime.current_timestamp_microseconds()
    
    Let module_results be [systems_result, chaos_result, bifurcation_result, integration_result]
    Let report be generate_test_report(module_results, benchmarks)
    
    Let total_suite_time be MathOps.integer_to_float(overall_end_time - overall_start_time) / 1000.0
    MathOps.print_string("")
    MathOps.print_string("Test suite completed in " + MathOps.float_to_string(total_suite_time) + " ms")
    MathOps.print_string("")
    
    print_detailed_report(report)
    
    Return report

Process called "run_quick_dynamical_tests" that takes returns Tuple[Integer, Integer]:
    Note: Runs abbreviated test suite for quick validation
    
    MathOps.print_string("Running Quick Dynamical Systems Test Suite...")
    
    Let systems_result be run_systems_module_tests()
    Let chaos_result be run_chaos_module_tests()
    Let bifurcation_result be run_bifurcation_module_tests()
    
    Let total_passed be systems_result.tests_passed + chaos_result.tests_passed + bifurcation_result.tests_passed
    Let total_run be systems_result.tests_total + chaos_result.tests_total + bifurcation_result.tests_total
    
    MathOps.print_string("")
    MathOps.print_string("QUICK TEST SUMMARY:")
    MathOps.print_string("  Systems: " + MathOps.integer_to_string(systems_result.tests_passed) + "/" + MathOps.integer_to_string(systems_result.tests_total))
    MathOps.print_string("  Chaos: " + MathOps.integer_to_string(chaos_result.tests_passed) + "/" + MathOps.integer_to_string(chaos_result.tests_total))
    MathOps.print_string("  Bifurcation: " + MathOps.integer_to_string(bifurcation_result.tests_passed) + "/" + MathOps.integer_to_string(bifurcation_result.tests_total))
    MathOps.print_string("  Total: " + MathOps.integer_to_string(total_passed) + "/" + MathOps.integer_to_string(total_run))
    
    Let success_rate be MathOps.integer_to_float(total_passed) / MathOps.integer_to_float(total_run)
    MathOps.print_string("  Success Rate: " + MathOps.float_to_string(success_rate * 100.0) + "%")
    
    Return [total_passed, total_run]