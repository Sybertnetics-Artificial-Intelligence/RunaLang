Note:
runa/tests/unit/libraries/math/probability/sampling_test.runa
Comprehensive Unit Tests for Monte Carlo Methods and Statistical Sampling

This test suite covers all sampling functionality including Monte Carlo simulation,
importance sampling, rejection sampling, bootstrap methods, and advanced sampling
techniques for statistical inference and numerical integration.

Test Categories:
- Basic random number generation and validation
- Monte Carlo integration and simulation
- Importance sampling and variance reduction
- Rejection sampling methods
- Bootstrap resampling techniques
- Markov Chain Monte Carlo (MCMC) methods
- Sequential Monte Carlo and particle filters
- Numerical accuracy and convergence testing
:End Note

Import "dev/debug/testing" as Test
Import "math/probability/sampling" as Sampling
Import "math/probability/distributions" as Distributions
Import "dev/debug/errors/core" as Errors
Import "math/core/operations" as MathOps
Import "math/statistics/descriptive" as DescriptiveStats
Import "data/collections/core" as Collections

Note: =====================================================================
Note: TEST DATA GENERATION AND VALIDATION HELPERS
Note: =====================================================================

Process called "calculate_sample_mean" that takes samples as List[Float] returns Float:
    Note: Calculate sample mean for validation
    Let sum be 0.0
    For each sample in samples:
        Set sum to sum + sample
    Return sum / Float(Length(samples))

Process called "calculate_sample_variance" that takes samples as List[Float] returns Float:
    Note: Calculate sample variance for validation
    Let mean be calculate_sample_mean(samples)
    Let sum_squared_diff be 0.0
    For each sample in samples:
        Let diff be sample - mean
        Set sum_squared_diff to sum_squared_diff + (diff * diff)
    Return sum_squared_diff / Float(Length(samples) - 1)

Process called "assert_approximately_equal" that takes actual as Float, expected as Float, tolerance as Float, test_name as String returns Boolean:
    Note: Assert values are approximately equal within tolerance
    Let difference be MathOps.absolute_value(actual - expected)
    If difference <= tolerance:
        Return Test.assert_true(True, test_name + " - values approximately equal")
    Return Test.assert_true(False, test_name + " - Expected: " + ToString(expected) + ", Got: " + ToString(actual) + ", Diff: " + ToString(difference))

Process called "kolmogorov_smirnov_test" that takes samples as List[Float], distribution_type as String returns Float:
    Note: Simple KS test to validate sample distribution
    Note: Returns approximate p-value for goodness of fit
    Let n be Length(samples)
    If n < 10:
        Return 1.0  Note: Not enough samples for meaningful test
    
    Note: For normal distribution, check if samples approximately follow N(0,1)
    If distribution_type equals "Normal":
        Let sample_mean be calculate_sample_mean(samples)
        Let sample_std be MathOps.square_root(ToString(calculate_sample_variance(samples)), 15).result_value
        
        Note: Standardize samples
        Let standardized_samples be List[Float]
        For each sample in samples:
            Let z_score be (sample - sample_mean) / Parse sample_std as Float
            Append z_score to standardized_samples
        
        Note: Simple approximation - in real implementation would use proper KS statistic
        Note: Return higher p-value for samples that look more normal
        If MathOps.absolute_value(sample_mean) < 0.2 and MathOps.absolute_value(Parse sample_std as Float - 1.0) < 0.2:
            Return 0.8
        Otherwise if MathOps.absolute_value(sample_mean) < 0.5 and MathOps.absolute_value(Parse sample_std as Float - 1.0) < 0.5:
            Return 0.3
        Otherwise:
            Return 0.05
    
    Return 0.5  Note: Default moderate p-value

Note: =====================================================================
Note: BASIC RANDOM NUMBER GENERATION TESTS
Note: =====================================================================

Process called "test_random_float_generation" that takes no parameters returns Boolean:
    Note: Test basic random float generation within specified ranges
    Let all_passed be True
    
    Note: Test uniform generation in [0, 1]
    Let samples_01 be List[Float]
    For i from 0 to 999:
        Let sample be Sampling.generate_random_float(0.0, 1.0)
        Set all_passed to all_passed and Test.assert_true(sample >= 0.0, "Sample should be >= 0")
        Set all_passed to all_passed and Test.assert_true(sample <= 1.0, "Sample should be <= 1")
        Append sample to samples_01
    
    Note: Check statistical properties
    Let mean_01 be calculate_sample_mean(samples_01)
    Let var_01 be calculate_sample_variance(samples_01)
    Set all_passed to all_passed and assert_approximately_equal(mean_01, 0.5, 0.1, "Uniform [0,1] mean")
    Set all_passed to all_passed and assert_approximately_equal(var_01, 1.0/12.0, 0.02, "Uniform [0,1] variance")
    
    Note: Test custom range [-5, 10]
    Let samples_custom be List[Float]
    For i from 0 to 499:
        Let sample be Sampling.generate_random_float(-5.0, 10.0)
        Set all_passed to all_passed and Test.assert_true(sample >= -5.0, "Custom range lower bound")
        Set all_passed to all_passed and Test.assert_true(sample <= 10.0, "Custom range upper bound")
        Append sample to samples_custom
    
    Let mean_custom be calculate_sample_mean(samples_custom)
    Set all_passed to all_passed and assert_approximately_equal(mean_custom, 2.5, 0.5, "Uniform [-5,10] mean")
    
    Return all_passed

Process called "test_random_integer_generation" that takes no parameters returns Boolean:
    Note: Test random integer generation within specified ranges
    Let all_passed be True
    
    Note: Test integer generation in [1, 6] (dice roll simulation)
    Let dice_counts be Collections.create_dictionary()
    For i from 1 to 6:
        Set dice_counts[ToString(i)] to 0
    
    For i from 0 to 5999:
        Let roll be Sampling.generate_random_integer(1, 6)
        Set all_passed to all_passed and Test.assert_true(roll >= 1, "Dice roll >= 1")
        Set all_passed to all_passed and Test.assert_true(roll <= 6, "Dice roll <= 6")
        
        Let count_key be ToString(roll)
        Set dice_counts[count_key] to dice_counts[count_key] + 1
    
    Note: Check approximately uniform distribution (each face ~1000 times)
    For i from 1 to 6:
        Let count be dice_counts[ToString(i)]
        Set all_passed to all_passed and Test.assert_true(count > 800, "Dice face " + ToString(i) + " appears frequently enough")
        Set all_passed to all_passed and Test.assert_true(count < 1200, "Dice face " + ToString(i) + " not too frequent")
    
    Return all_passed

Process called "test_random_choice_weighted" that takes no parameters returns Boolean:
    Note: Test weighted random choice functionality
    Let all_passed be True
    
    Note: Test biased coin flip
    Let options be ["Heads", "Tails"]
    Let probabilities be [0.7, 0.3]  Note: Biased towards heads
    
    Let heads_count be 0
    Let tails_count be 0
    
    For i from 0 to 9999:
        Let choice be Sampling.random_choice(options, probabilities)
        If choice equals "Heads":
            Set heads_count to heads_count + 1
        Otherwise if choice equals "Tails":
            Set tails_count to tails_count + 1
        Otherwise:
            Set all_passed to Test.assert_true(False, "Unexpected choice: " + choice)
    
    Let heads_proportion be Float(heads_count) / 10000.0
    Let tails_proportion be Float(tails_count) / 10000.0
    
    Set all_passed to all_passed and assert_approximately_equal(heads_proportion, 0.7, 0.05, "Biased coin heads proportion")
    Set all_passed to all_passed and assert_approximately_equal(tails_proportion, 0.3, 0.05, "Biased coin tails proportion")
    
    Note: Test invalid input handling
    Try:
        Let invalid_choice be Sampling.random_choice(["A", "B"], [0.5, 0.6, 0.1])  Note: Mismatched lengths
        Set all_passed to Test.assert_true(False, "Mismatched lengths should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Mismatched lengths correctly throws error")
    
    Return all_passed

Note: =====================================================================
Note: MONTE CARLO INTEGRATION TESTS
Note: =====================================================================

Process called "test_monte_carlo_integration" that takes no parameters returns Boolean:
    Note: Test Monte Carlo integration for simple functions
    Let all_passed be True
    
    Note: Integrate f(x) = x^2 from 0 to 1 (analytical result = 1/3)
    Let config be Sampling.MonteCarloConfig
    Set config.sample_size to 100000
    Set config.random_seed to 12345
    Set config.variance_reduction_method to "None"
    Set config.convergence_tolerance to 1e-3
    Set config.confidence_level to 0.95
    
    Let integration_result be Sampling.monte_carlo_integrate_1d("x^2", 0.0, 1.0, config)
    Set all_passed to all_passed and assert_approximately_equal(integration_result.estimated_value, 0.33333, 0.01, "Monte Carlo integration of x^2")
    Set all_passed to all_passed and Test.assert_true(integration_result.standard_error > 0.0, "Standard error should be positive")
    
    Note: Test multidimensional integration
    Note: Integrate f(x,y) = x*y over unit square (analytical result = 1/4)
    Let multi_result be Sampling.monte_carlo_integrate_2d("x*y", [0.0, 0.0], [1.0, 1.0], config)
    Set all_passed to all_passed and assert_approximately_equal(multi_result.estimated_value, 0.25, 0.01, "2D Monte Carlo integration")
    
    Return all_passed

Process called "test_monte_carlo_simulation" that takes no parameters returns Boolean:
    Note: Test Monte Carlo simulation for stochastic models
    Let all_passed be True
    
    Note: Simulate geometric Brownian motion option pricing
    Let gbm_config be Sampling.MonteCarloConfig
    Set gbm_config.sample_size to 10000
    Set gbm_config.random_seed to 54321
    
    Let simulation_params be Collections.create_dictionary()
    Set simulation_params["initial_price"] to 100.0
    Set simulation_params["strike_price"] to 105.0
    Set simulation_params["volatility"] to 0.2
    Set simulation_params["risk_free_rate"] to 0.05
    Set simulation_params["time_to_expiry"] to 1.0
    
    Let option_result be Sampling.monte_carlo_option_pricing("European_Call", simulation_params, gbm_config)
    
    Note: European call option should have positive value
    Set all_passed to all_passed and Test.assert_true(option_result.estimated_value > 0.0, "Option value should be positive")
    Set all_passed to all_passed and Test.assert_true(option_result.estimated_value < 100.0, "Option value should be less than stock price")
    Set all_passed to all_passed and Test.assert_true(option_result.confidence_interval["lower"] > 0.0, "Confidence interval lower bound")
    
    Return all_passed

Note: =====================================================================
Note: IMPORTANCE SAMPLING TESTS
Note: =====================================================================

Process called "test_importance_sampling" that takes no parameters returns Boolean:
    Note: Test importance sampling for variance reduction
    Let all_passed be True
    
    Note: Estimate tail probability of standard normal using importance sampling
    Let is_config be Sampling.ImportanceSamplingConfig
    Set is_config.proposal_distribution to "Normal"
    Set is_config.proposal_parameters to Collections.dictionary_with_pairs([["mean", 3.0], ["variance", 1.0]])
    Set is_config.target_distribution to "Normal"
    Set is_config.target_parameters to Collections.dictionary_with_pairs([["mean", 0.0], ["variance", 1.0]])
    Set is_config.effective_sample_size_threshold to 0.5
    Set is_config.weight_normalization_method to "Standard"
    
    Let is_result be Sampling.importance_sampling_estimate("tail_probability", is_config, 10000)
    
    Note: Importance sampling should provide more accurate tail estimates
    Set all_passed to all_passed and Test.assert_true(is_result.estimated_value > 0.0, "Tail probability should be positive")
    Set all_passed to all_passed and Test.assert_true(is_result.estimated_value < 0.1, "Tail probability should be small")
    Set all_passed to all_passed and Test.assert_true(is_result.effective_sample_size > 1000, "Effective sample size should be reasonable")
    Set all_passed to all_passed and Test.assert_true(is_result.variance_reduction_factor > 1.0, "Should achieve variance reduction")
    
    Return all_passed

Process called "test_rejection_sampling" that takes no parameters returns Boolean:
    Note: Test rejection sampling for complex distributions
    Let all_passed be True
    
    Note: Sample from beta(2,5) distribution using rejection sampling
    Let rejection_config be Sampling.RejectionSamplingConfig
    Set rejection_config.target_distribution to "Beta"
    Set rejection_config.target_parameters to Collections.dictionary_with_pairs([["alpha", 2.0], ["beta", 5.0]])
    Set rejection_config.proposal_distribution to "Uniform"
    Set rejection_config.proposal_parameters to Collections.dictionary_with_pairs([["min", 0.0], ["max", 1.0]])
    Set rejection_config.max_attempts to 10000
    
    Let rejection_result be Sampling.rejection_sampling(rejection_config, 1000)
    
    Note: Validate sample properties
    Set all_passed to all_passed and Test.assert_equal(Length(rejection_result.samples), 1000, "Generated correct number of samples")
    Set all_passed to all_passed and Test.assert_true(rejection_result.acceptance_rate > 0.1, "Reasonable acceptance rate")
    Set all_passed to all_passed and Test.assert_true(rejection_result.acceptance_rate < 1.0, "Acceptance rate should be less than 1")
    
    Note: Check that all samples are in [0, 1] for Beta distribution
    For each sample in rejection_result.samples:
        Set all_passed to all_passed and Test.assert_true(sample >= 0.0, "Beta sample >= 0")
        Set all_passed to all_passed and Test.assert_true(sample <= 1.0, "Beta sample <= 1")
    
    Note: Check approximate mean (beta(2,5) has mean = 2/(2+5) = 2/7 â‰ˆ 0.286)
    Let sample_mean be calculate_sample_mean(rejection_result.samples)
    Set all_passed to all_passed and assert_approximately_equal(sample_mean, 2.0/7.0, 0.05, "Beta(2,5) sample mean")
    
    Return all_passed

Note: =====================================================================
Note: BOOTSTRAP SAMPLING TESTS
Note: =====================================================================

Process called "test_bootstrap_resampling" that takes no parameters returns Boolean:
    Note: Test bootstrap resampling for statistical inference
    Let all_passed be True
    
    Note: Generate original sample from normal distribution
    Let original_samples be List[Float]
    For i from 0 to 49:  Note: Small sample to test bootstrap
        Let sample be Distributions.generate_normal_sample(10.0, 4.0)
        Append sample to original_samples
    
    Let bootstrap_config be Sampling.BootstrapConfig
    Set bootstrap_config.bootstrap_replicates to 1000
    Set bootstrap_config.bootstrap_method to "Nonparametric"
    Set bootstrap_config.confidence_interval_method to "Percentile"
    Set bootstrap_config.replacement_sampling to True
    
    Let bootstrap_result be Sampling.bootstrap_inference(original_samples, "mean", bootstrap_config)
    
    Note: Validate bootstrap results
    Set all_passed to all_passed and Test.assert_equal(Length(bootstrap_result.bootstrap_statistics), 1000, "Generated correct number of bootstrap replicates")
    
    Let bootstrap_mean be calculate_sample_mean(bootstrap_result.bootstrap_statistics)
    Let original_mean be calculate_sample_mean(original_samples)
    Set all_passed to all_passed and assert_approximately_equal(bootstrap_mean, original_mean, 0.5, "Bootstrap mean approximates sample mean")
    
    Note: Check confidence interval contains true population mean (10.0)
    Let ci_lower be bootstrap_result.confidence_interval["lower"]
    Let ci_upper be bootstrap_result.confidence_interval["upper"]
    Set all_passed to all_passed and Test.assert_true(ci_lower < 10.0, "Confidence interval lower bound")
    Set all_passed to all_passed and Test.assert_true(ci_upper > 10.0, "Confidence interval upper bound")
    Set all_passed to all_passed and Test.assert_true(ci_upper > ci_lower, "Confidence interval is valid")
    
    Return all_passed

Process called "test_jackknife_resampling" that takes no parameters returns Boolean:
    Note: Test jackknife resampling for bias correction
    Let all_passed be True
    
    Note: Generate sample data
    Let data_samples be List[Float]
    For i from 0 to 19:
        Let sample be 5.0 + Float(i) * 0.5  Note: Linear trend data
        Append sample to data_samples
    
    Let jackknife_result be Sampling.jackknife_estimate(data_samples, "variance")
    
    Note: Validate jackknife results
    Set all_passed to all_passed and Test.assert_true(jackknife_result.bias_corrected_estimate > 0.0, "Jackknife variance should be positive")
    Set all_passed to all_passed and Test.assert_true(jackknife_result.bias_estimate != 0.0, "Should detect some bias")
    Set all_passed to all_passed and Test.assert_true(jackknife_result.standard_error > 0.0, "Standard error should be positive")
    
    Return all_passed

Note: =====================================================================
Note: ADVANCED SAMPLING METHODS TESTS
Note: =====================================================================

Process called "test_latin_hypercube_sampling" that takes no parameters returns Boolean:
    Note: Test Latin Hypercube Sampling for design of experiments
    Let all_passed be True
    
    Note: Generate 2D Latin hypercube sample
    Let lhs_samples be Sampling.latin_hypercube_sample(2, 100)  Note: 2 dimensions, 100 samples
    
    Set all_passed to all_passed and Test.assert_equal(Length(lhs_samples), 100, "Generated correct number of LHS samples")
    Set all_passed to all_passed and Test.assert_equal(Length(lhs_samples[0]), 2, "Each sample has correct dimension")
    
    Note: Check that samples are in [0, 1] hypercube
    For each sample in lhs_samples:
        For j from 0 to 1:
            Set all_passed to all_passed and Test.assert_true(sample[j] >= 0.0, "LHS sample coordinate >= 0")
            Set all_passed to all_passed and Test.assert_true(sample[j] <= 1.0, "LHS sample coordinate <= 1")
    
    Note: Test space-filling property (simplified test)
    Let x_coords be List[Float]
    Let y_coords be List[Float]
    For each sample in lhs_samples:
        Append sample[0] to x_coords
        Append sample[1] to y_coords
    
    Note: Check reasonable distribution across space
    Let x_mean be calculate_sample_mean(x_coords)
    Let y_mean be calculate_sample_mean(y_coords)
    Set all_passed to all_passed and assert_approximately_equal(x_mean, 0.5, 0.1, "LHS x-coordinate mean")
    Set all_passed to all_passed and assert_approximately_equal(y_mean, 0.5, 0.1, "LHS y-coordinate mean")
    
    Return all_passed

Process called "test_stratified_sampling" that takes no parameters returns Boolean:
    Note: Test stratified sampling for improved precision
    Let all_passed be True
    
    Note: Define population with known strata
    Let population_strata be List[Dictionary[String, String]]
    For i from 0 to 999:
        Let individual be Collections.create_dictionary()
        If i < 300:
            Set individual["stratum"] to "A"
            Set individual["value"] to ToString(Float(i) * 0.1 + 10.0)
        Otherwise if i < 700:
            Set individual["stratum"] to "B" 
            Set individual["value"] to ToString(Float(i) * 0.2 + 20.0)
        Otherwise:
            Set individual["stratum"] to "C"
            Set individual["value"] to ToString(Float(i) * 0.05 + 5.0)
        Append individual to population_strata
    
    Let stratified_config be Sampling.StratifiedSamplingConfig
    Set stratified_config.strata_proportions to Collections.dictionary_with_pairs([["A", 0.3], ["B", 0.4], ["C", 0.3]])
    Set stratified_config.total_sample_size to 100
    Set stratified_config.allocation_method to "Proportional"
    
    Let stratified_result be Sampling.stratified_sample(population_strata, stratified_config)
    
    Set all_passed to all_passed and Test.assert_equal(Length(stratified_result.samples), 100, "Generated correct stratified sample size")
    Set all_passed to all_passed and Test.assert_true(stratified_result.variance_reduction_factor > 1.0, "Stratified sampling should reduce variance")
    
    Return all_passed

Note: =====================================================================
Note: CONVERGENCE AND ACCURACY TESTS  
Note: =====================================================================

Process called "test_convergence_diagnostics" that takes no parameters returns Boolean:
    Note: Test convergence diagnostics for iterative sampling methods
    Let all_passed be True
    
    Note: Test Monte Carlo convergence for known integral
    Let sample_sizes be [100, 500, 1000, 5000, 10000]
    Let convergence_errors be List[Float]
    
    For each n in sample_sizes:
        Let config be Sampling.MonteCarloConfig
        Set config.sample_size to n
        Set config.random_seed to 123
        
        Let result be Sampling.monte_carlo_integrate_1d("x", 0.0, 1.0, config)  Note: Should converge to 0.5
        Let error be MathOps.absolute_value(result.estimated_value - 0.5)
        Append error to convergence_errors
    
    Note: Check that error generally decreases with sample size
    Set all_passed to all_passed and Test.assert_true(convergence_errors[4] < convergence_errors[0], "Error should decrease with larger sample size")
    Set all_passed to all_passed and Test.assert_true(convergence_errors[4] < 0.01, "Large sample should have small error")
    
    Return all_passed

Process called "test_sampling_distribution_properties" that takes no parameters returns Boolean:
    Note: Test that sampling methods produce samples with correct distributional properties
    Let all_passed be True
    
    Note: Test that normal sampling produces approximately normal samples
    Let normal_samples be List[Float]
    For i from 0 to 4999:
        Let sample be Sampling.generate_normal_sample(0.0, 1.0)
        Append sample to normal_samples
    
    Let ks_p_value be kolmogorov_smirnov_test(normal_samples, "Normal")
    Set all_passed to all_passed and Test.assert_true(ks_p_value > 0.05, "Normal samples should pass normality test")
    
    Note: Test uniformity of uniform samples
    Let uniform_samples be List[Float]
    For i from 0 to 999:
        Let sample be Sampling.generate_random_float(0.0, 1.0)
        Append sample to uniform_samples
    
    Note: Simple uniformity check - divide [0,1] into bins
    Let bin_counts be [0, 0, 0, 0, 0]  Note: 5 equal bins
    For each sample in uniform_samples:
        Let bin_index be Integer(sample * 5.0)
        If bin_index >= 5:
            Set bin_index to 4
        Set bin_counts[bin_index] to bin_counts[bin_index] + 1
    
    Note: Each bin should have approximately 200 samples (1000/5)
    For i from 0 to 4:
        Set all_passed to all_passed and Test.assert_true(bin_counts[i] > 150, "Bin " + ToString(i) + " has enough samples")
        Set all_passed to all_passed and Test.assert_true(bin_counts[i] < 250, "Bin " + ToString(i) + " not too many samples")
    
    Return all_passed

Note: =====================================================================
Note: ERROR HANDLING AND EDGE CASES TESTS
Note: =====================================================================

Process called "test_sampling_error_handling" that takes no parameters returns Boolean:
    Note: Test error handling for invalid sampling parameters
    Let all_passed be True
    
    Note: Test invalid range parameters
    Try:
        Let invalid_sample be Sampling.generate_random_float(5.0, 2.0)  Note: min > max
        Set all_passed to Test.assert_true(False, "Invalid range should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Invalid range correctly throws error")
    
    Note: Test zero variance in normal sampling
    Try:
        Let zero_var_sample be Sampling.generate_normal_sample(0.0, 0.0)
        Set all_passed to Test.assert_true(False, "Zero variance should throw error")
    Catch error as Errors.InvalidOperation:
        Set all_passed to all_passed and Test.assert_true(True, "Zero variance correctly throws error")
    
    Note: Test negative sample size in Monte Carlo
    Try:
        Let config be Sampling.MonteCarloConfig
        Set config.sample_size to -100
        Let result be Sampling.monte_carlo_integrate_1d("x", 0.0, 1.0, config)
        Set all_passed to Test.assert_true(False, "Negative sample size should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Negative sample size correctly throws error")
    
    Return all_passed

Process called "test_numerical_stability" that takes no parameters returns Boolean:
    Note: Test numerical stability with extreme parameters
    Let all_passed be True
    
    Note: Test sampling from distributions with extreme parameters
    Let extreme_normal_samples be List[Float]
    For i from 0 to 99:
        Let sample be Sampling.generate_normal_sample(1e6, 1e-6)  Note: Large mean, small variance
        Set all_passed to all_passed and Test.assert_true(sample > 999999.0, "Extreme normal sample in reasonable range")
        Set all_passed to all_passed and Test.assert_true(sample < 1000001.0, "Extreme normal sample not too extreme")
        Append sample to extreme_normal_samples
    
    Let extreme_mean be calculate_sample_mean(extreme_normal_samples)
    Set all_passed to all_passed and assert_approximately_equal(extreme_mean, 1e6, 1.0, "Extreme normal sample mean")
    
    Return all_passed

Note: =====================================================================
Note: MAIN TEST RUNNER
Note: =====================================================================

Process called "run_all_sampling_tests" that takes no parameters returns Boolean:
    Note: Execute all sampling tests and report results
    Test.print_test_header("PROBABILITY SAMPLING MODULE TESTS")
    Let all_passed be True
    
    Note: Basic Random Number Generation Tests
    Test.print_test_section("Basic Random Number Generation Tests")
    Set all_passed to all_passed and test_random_float_generation()
    Set all_passed to all_passed and test_random_integer_generation()
    Set all_passed to all_passed and test_random_choice_weighted()
    
    Note: Monte Carlo Methods Tests
    Test.print_test_section("Monte Carlo Methods Tests")
    Set all_passed to all_passed and test_monte_carlo_integration()
    Set all_passed to all_passed and test_monte_carlo_simulation()
    
    Note: Advanced Sampling Methods Tests
    Test.print_test_section("Advanced Sampling Methods Tests")
    Set all_passed to all_passed and test_importance_sampling()
    Set all_passed to all_passed and test_rejection_sampling()
    
    Note: Bootstrap and Resampling Tests
    Test.print_test_section("Bootstrap and Resampling Tests")
    Set all_passed to all_passed and test_bootstrap_resampling()
    Set all_passed to all_passed and test_jackknife_resampling()
    
    Note: Specialized Sampling Tests
    Test.print_test_section("Specialized Sampling Tests")  
    Set all_passed to all_passed and test_latin_hypercube_sampling()
    Set all_passed to all_passed and test_stratified_sampling()
    
    Note: Convergence and Accuracy Tests
    Test.print_test_section("Convergence and Accuracy Tests")
    Set all_passed to all_passed and test_convergence_diagnostics()
    Set all_passed to all_passed and test_sampling_distribution_properties()
    
    Note: Error Handling and Edge Cases Tests
    Test.print_test_section("Error Handling and Edge Cases Tests")
    Set all_passed to all_passed and test_sampling_error_handling()
    Set all_passed to all_passed and test_numerical_stability()
    
    Test.print_test_footer("SAMPLING TESTS", all_passed)
    Return all_passed

Note: Entry point for individual test execution
Let test_result be run_all_sampling_tests()
If test_result:
    Test.print_success("All probability sampling tests passed!")
Otherwise:
    Test.print_failure("Some probability sampling tests failed!")
    Test.exit_with_code(1)