Note:
runa/tests/unit/libraries/math/probability/bayesian_test.runa
Comprehensive Unit Tests for Bayesian Methods and Statistical Inference

This test suite covers all Bayesian functionality including Bayesian inference,
prior and posterior distributions, Bayes factors, credible intervals, MCMC
methods, model selection, and hierarchical modeling for principled statistical
analysis and uncertainty quantification.

Test Categories:
- Prior distribution specification and conjugate priors
- Bayesian parameter estimation and posterior inference
- Markov Chain Monte Carlo (MCMC) algorithms
- Model comparison and Bayes factors
- Credible intervals and posterior predictive checks
- Hierarchical Bayesian modeling
- Variational inference methods
- Computational accuracy and convergence testing
:End Note

Import "dev/debug/testing" as Test
Import "math/probability/bayesian" as Bayesian
Import "math/probability/distributions" as Distributions
Import "math/probability/sampling" as Sampling
Import "dev/debug/errors/core" as Errors
Import "math/core/operations" as MathOps
Import "math/engine/linalg/core" as LinAlg
Import "data/collections/core" as Collections

Note: =====================================================================
Note: TEST DATA GENERATION AND VALIDATION HELPERS
Note: =====================================================================

Process called "generate_normal_data" that takes mean as Float, variance as Float, n as Integer returns List[Float]:
    Note: Generate sample data from normal distribution for testing
    Let samples be List[Float]
    For i from 0 to n - 1:
        Let sample be Distributions.generate_normal_sample(mean, variance)
        Append sample to samples
    Return samples

Process called "calculate_sample_statistics" that takes data as List[Float] returns Dictionary[String, Float]:
    Note: Calculate basic statistics for validation
    Let stats be Collections.create_dictionary()
    Let n be Length(data)
    
    Let sum be 0.0
    For each value in data:
        Set sum to sum + value
    Set stats["mean"] to sum / Float(n)
    
    Let variance_sum be 0.0
    For each value in data:
        Let diff be value - stats["mean"]
        Set variance_sum to variance_sum + (diff * diff)
    Set stats["variance"] to variance_sum / Float(n - 1)
    Set stats["std_dev"] to MathOps.square_root(ToString(stats["variance"]), 15).result_value
    Set stats["n"] to Float(n)
    
    Return stats

Process called "assert_approximately_equal" that takes actual as Float, expected as Float, tolerance as Float, test_name as String returns Boolean:
    Note: Assert values are approximately equal within tolerance
    Let difference be MathOps.absolute_value(actual - expected)
    If difference <= tolerance:
        Return Test.assert_true(True, test_name + " - values approximately equal")
    Return Test.assert_true(False, test_name + " - Expected: " + ToString(expected) + ", Got: " + ToString(actual))

Process called "assert_within_credible_interval" that takes value as Float, interval as Dictionary[String, Float], confidence as Float, test_name as String returns Boolean:
    Note: Check if value falls within credible interval
    Let lower be interval["lower"]
    Let upper be interval["upper"]
    Let within_interval be (value >= lower) and (value <= upper)
    Return Test.assert_true(within_interval, test_name + " - value within " + ToString(confidence*100.0) + "% credible interval")

Note: =====================================================================
Note: PRIOR DISTRIBUTION SPECIFICATION TESTS
Note: =====================================================================

Process called "test_conjugate_prior_selection" that takes no parameters returns Boolean:
    Note: Test automatic conjugate prior selection for different likelihood families
    Let all_passed be True
    
    Note: Test conjugate prior for normal likelihood with known variance
    Let normal_data_characteristics be Collections.create_dictionary()
    Set normal_data_characteristics["sample_mean"] to 2.5
    Set normal_data_characteristics["sample_variance"] to 1.0
    Set normal_data_characteristics["sample_size"] to 50.0
    
    Let normal_prior be Bayesian.conjugate_prior_selection("Normal", normal_data_characteristics)
    Set all_passed to all_passed and Test.assert_equal(normal_prior.distribution_type, "Normal-Gamma", "Normal likelihood uses Normal-Gamma prior")
    Set all_passed to all_passed and Test.assert_equal(normal_prior.prior_type, "Conjugate", "Prior marked as conjugate")
    Set all_passed to all_passed and Test.assert_true(normal_prior.parameters["mu_0"] == 2.5, "Prior mean initialized to sample mean")
    
    Note: Test conjugate prior for binomial likelihood
    Let binomial_data_characteristics be Collections.create_dictionary()
    Set binomial_data_characteristics["successes"] to 15.0
    Set binomial_data_characteristics["trials"] to 50.0
    Set binomial_data_characteristics["success_rate"] to 0.3
    
    Let binomial_prior be Bayesian.conjugate_prior_selection("Binomial", binomial_data_characteristics)
    Set all_passed to all_passed and Test.assert_equal(binomial_prior.distribution_type, "Beta", "Binomial likelihood uses Beta prior")
    Set all_passed to all_passed and Test.assert_true(binomial_prior.parameters["alpha"] > 0.0, "Beta alpha parameter positive")
    Set all_passed to all_passed and Test.assert_true(binomial_prior.parameters["beta"] > 0.0, "Beta beta parameter positive")
    
    Note: Test conjugate prior for Poisson likelihood
    Let poisson_data_characteristics be Collections.create_dictionary()
    Set poisson_data_characteristics["sample_mean"] to 3.2
    Set poisson_data_characteristics["sample_size"] to 100.0
    
    Let poisson_prior be Bayesian.conjugate_prior_selection("Poisson", poisson_data_characteristics)
    Set all_passed to all_passed and Test.assert_equal(poisson_prior.distribution_type, "Gamma", "Poisson likelihood uses Gamma prior")
    Set all_passed to all_passed and Test.assert_true(poisson_prior.parameters["shape"] > 0.0, "Gamma shape parameter positive")
    Set all_passed to all_passed and Test.assert_true(poisson_prior.parameters["rate"] > 0.0, "Gamma rate parameter positive")
    
    Return all_passed

Process called "test_prior_elicitation" that takes no parameters returns Boolean:
    Note: Test expert prior elicitation methods
    Let all_passed be True
    
    Note: Test quantile-based prior elicitation for normal mean
    Let quantile_info be Collections.create_dictionary()
    Set quantile_info["q25"] to 8.0   Note: Expert believes 25th percentile is 8
    Set quantile_info["q50"] to 10.0  Note: Median belief is 10
    Set quantile_info["q75"] to 12.0  Note: 75th percentile belief is 12
    
    Let elicited_prior be Bayesian.elicit_normal_prior_from_quantiles(quantile_info)
    Set all_passed to all_passed and Test.assert_equal(elicited_prior.distribution_type, "Normal", "Elicited prior is normal")
    Set all_passed to all_passed and assert_approximately_equal(elicited_prior.parameters["mean"], 10.0, 0.1, "Elicited prior mean matches median")
    Set all_passed to all_passed and Test.assert_true(elicited_prior.parameters["variance"] > 0.0, "Elicited prior variance positive")
    
    Note: Test moment-based prior elicitation
    Let moment_info be Collections.create_dictionary() 
    Set moment_info["prior_mean"] to 5.0
    Set moment_info["prior_variance"] to 4.0
    Set moment_info["confidence"] to 0.8
    
    Let moment_prior be Bayesian.elicit_normal_prior_from_moments(moment_info)
    Set all_passed to all_passed and assert_approximately_equal(moment_prior.parameters["mean"], 5.0, 1e-10, "Moment-based prior mean")
    Set all_passed to all_passed and assert_approximately_equal(moment_prior.parameters["variance"], 4.0, 1e-10, "Moment-based prior variance")
    
    Return all_passed

Note: =====================================================================
Note: BAYESIAN PARAMETER ESTIMATION TESTS
Note: =====================================================================

Process called "test_bayesian_normal_inference" that takes no parameters returns Boolean:
    Note: Test Bayesian inference for normal distribution parameters
    Let all_passed be True
    
    Note: Generate known data and test posterior inference
    Let true_mean be 5.0
    Let true_variance be 2.25  Note: std = 1.5
    Let observed_data be generate_normal_data(true_mean, true_variance, 100)
    
    Note: Set up conjugate prior
    Let prior_params be Collections.create_dictionary()
    Set prior_params["mu_0"] to 0.0      Note: Prior mean
    Set prior_params["lambda_0"] to 1.0   Note: Prior precision on mean
    Set prior_params["alpha_0"] to 2.0    Note: Prior shape for precision
    Set prior_params["beta_0"] to 2.0     Note: Prior rate for precision
    
    Let posterior_result be Bayesian.bayesian_normal_inference(observed_data, prior_params)
    
    Note: Check posterior statistics
    Set all_passed to all_passed and Test.assert_true(posterior_result.posterior_statistics["mean"]["mean"] > 4.0, "Posterior mean reasonable lower bound")
    Set all_passed to all_passed and Test.assert_true(posterior_result.posterior_statistics["mean"]["mean"] < 6.0, "Posterior mean reasonable upper bound")
    
    Note: Check that credible interval contains true value
    Let mean_ci be posterior_result.credible_intervals["mean"]["95%"]
    Set all_passed to all_passed and assert_within_credible_interval(true_mean, mean_ci, 0.95, "True mean in credible interval")
    
    Note: Check posterior variance estimate
    Set all_passed to all_passed and Test.assert_true(posterior_result.posterior_statistics["variance"]["mean"] > 1.0, "Posterior variance estimate reasonable")
    Set all_passed to all_passed and Test.assert_true(posterior_result.posterior_statistics["variance"]["mean"] < 4.0, "Posterior variance not too large")
    
    Return all_passed

Process called "test_bayesian_binomial_inference" that takes no parameters returns Boolean:
    Note: Test Bayesian inference for binomial proportion
    Let all_passed be True
    
    Note: Simulate binomial data with known success probability
    Let true_probability be 0.3
    Let n_trials be 200
    Let successes be 0
    
    For i from 0 to n_trials - 1:
        Let trial_result be Sampling.generate_random_float(0.0, 1.0)
        If trial_result < true_probability:
            Set successes to successes + 1
    
    Note: Set up Beta prior
    Let prior_params be Collections.create_dictionary()
    Set prior_params["alpha"] to 1.0  Note: Uniform prior (Jeffrey's prior would be 0.5, 0.5)
    Set prior_params["beta"] to 1.0
    
    Let binomial_data be Collections.create_dictionary()
    Set binomial_data["successes"] to Float(successes)
    Set binomial_data["trials"] to Float(n_trials)
    
    Let posterior_result be Bayesian.bayesian_binomial_inference(binomial_data, prior_params)
    
    Note: Check posterior statistics
    Let posterior_mean be posterior_result.posterior_statistics["probability"]["mean"]
    Set all_passed to all_passed and Test.assert_true(posterior_mean > 0.2, "Posterior probability reasonable lower bound")
    Set all_passed to all_passed and Test.assert_true(posterior_mean < 0.4, "Posterior probability reasonable upper bound")
    
    Note: Check credible interval
    Let prob_ci be posterior_result.credible_intervals["probability"]["95%"]
    Set all_passed to all_passed and assert_within_credible_interval(true_probability, prob_ci, 0.95, "True probability in credible interval")
    
    Return all_passed

Process called "test_bayesian_regression" that takes no parameters returns Boolean:
    Note: Test Bayesian linear regression
    Let all_passed be True
    
    Note: Generate regression data y = 2.0 + 1.5*x + noise
    Let true_intercept be 2.0
    Let true_slope be 1.5
    Let noise_variance be 0.25
    
    Let x_values be List[Float]
    Let y_values be List[Float]
    
    For i from 0 to 49:
        Let x be Float(i) / 10.0  Note: x from 0 to 4.9
        Let noise be Distributions.generate_normal_sample(0.0, noise_variance)
        Let y be true_intercept + true_slope * x + noise
        Append x to x_values
        Append y to y_values
    
    Note: Set up regression prior
    Let regression_prior be Collections.create_dictionary()
    Set regression_prior["beta_mean"] to [0.0, 0.0]  Note: Prior means for intercept and slope
    Set regression_prior["beta_precision"] to [[0.1, 0.0], [0.0, 0.1]]  Note: Prior precision matrix
    Set regression_prior["noise_shape"] to 1.0
    Set regression_prior["noise_rate"] to 1.0
    
    Let regression_result be Bayesian.bayesian_linear_regression(x_values, y_values, regression_prior)
    
    Note: Check coefficient estimates
    Let intercept_estimate be regression_result.posterior_statistics["intercept"]["mean"]
    Let slope_estimate be regression_result.posterior_statistics["slope"]["mean"]
    
    Set all_passed to all_passed and assert_approximately_equal(intercept_estimate, true_intercept, 0.3, "Bayesian regression intercept")
    Set all_passed to all_passed and assert_approximately_equal(slope_estimate, true_slope, 0.2, "Bayesian regression slope")
    
    Note: Check credible intervals contain true values
    Let intercept_ci be regression_result.credible_intervals["intercept"]["95%"]
    Let slope_ci be regression_result.credible_intervals["slope"]["95%"]
    Set all_passed to all_passed and assert_within_credible_interval(true_intercept, intercept_ci, 0.95, "True intercept in CI")
    Set all_passed to all_passed and assert_within_credible_interval(true_slope, slope_ci, 0.95, "True slope in CI")
    
    Return all_passed

Note: =====================================================================
Note: MARKOV CHAIN MONTE CARLO TESTS
Note: =====================================================================

Process called "test_metropolis_hastings_sampling" that takes no parameters returns Boolean:
    Note: Test Metropolis-Hastings MCMC sampling
    Let all_passed be True
    
    Note: Sample from standard normal using MH algorithm
    Let mh_config be Bayesian.MCMCConfig
    Set mh_config.algorithm_type to "Metropolis-Hastings"
    Set mh_config.chain_length to 10000
    Set mh_config.burn_in_period to 1000
    Set mh_config.thinning_interval to 5
    Set mh_config.target_acceptance_rate to 0.44
    
    Let target_distribution be Collections.create_dictionary()
    Set target_distribution["type"] to "Normal"
    Set target_distribution["parameters"] to Collections.dictionary_with_pairs([["mean", 0.0], ["variance", 1.0]])
    
    Let mh_result be Bayesian.metropolis_hastings_sample(target_distribution, mh_config)
    
    Note: Check chain properties
    Let expected_samples be (mh_config.chain_length - mh_config.burn_in_period) / mh_config.thinning_interval
    Set all_passed to all_passed and Test.assert_equal(Length(mh_result.samples), expected_samples, "Correct number of MCMC samples")
    Set all_passed to all_passed and Test.assert_true(mh_result.acceptance_rate > 0.2, "Reasonable acceptance rate")
    Set all_passed to all_passed and Test.assert_true(mh_result.acceptance_rate < 0.8, "Not too high acceptance rate")
    
    Note: Check convergence diagnostics
    Set all_passed to all_passed and Test.assert_true(mh_result.convergence_diagnostics["effective_sample_size"] > 1000, "Adequate effective sample size")
    Set all_passed to all_passed and Test.assert_true(mh_result.convergence_diagnostics["rhat"] < 1.1, "Gelman-Rubin diagnostic acceptable")
    
    Note: Check sample statistics approximate target distribution
    Let sample_mean be calculate_sample_statistics(mh_result.samples)["mean"]
    Let sample_variance be calculate_sample_statistics(mh_result.samples)["variance"]
    Set all_passed to all_passed and assert_approximately_equal(sample_mean, 0.0, 0.1, "MCMC sample mean")
    Set all_passed to all_passed and assert_approximately_equal(sample_variance, 1.0, 0.2, "MCMC sample variance")
    
    Return all_passed

Process called "test_gibbs_sampling" that takes no parameters returns Boolean:
    Note: Test Gibbs sampling for multivariate distributions
    Let all_passed be True
    
    Note: Sample from bivariate normal using Gibbs sampling
    Let gibbs_config be Bayesian.MCMCConfig
    Set gibbs_config.algorithm_type to "Gibbs"
    Set gibbs_config.chain_length to 5000
    Set gibbs_config.burn_in_period to 500
    Set gibbs_config.thinning_interval to 1
    
    Let bivariate_params be Collections.create_dictionary()
    Set bivariate_params["mean"] to [1.0, 2.0]
    Set bivariate_params["covariance"] to [[1.0, 0.5], [0.5, 1.0]]
    
    Let gibbs_result be Bayesian.gibbs_sample_bivariate_normal(bivariate_params, gibbs_config)
    
    Note: Check sample dimensions
    Set all_passed to all_passed and Test.assert_equal(Length(gibbs_result.samples), 2, "Bivariate samples have 2 components")
    Set all_passed to all_passed and Test.assert_equal(Length(gibbs_result.samples[0]), 4500, "Correct number of x samples")
    Set all_passed to all_passed and Test.assert_equal(Length(gibbs_result.samples[1]), 4500, "Correct number of y samples")
    
    Note: Check marginal distributions
    Let x_mean be calculate_sample_statistics(gibbs_result.samples[0])["mean"]
    Let y_mean be calculate_sample_statistics(gibbs_result.samples[1])["mean"]
    Set all_passed to all_passed and assert_approximately_equal(x_mean, 1.0, 0.1, "Gibbs x marginal mean")
    Set all_passed to all_passed and assert_approximately_equal(y_mean, 2.0, 0.1, "Gibbs y marginal mean")
    
    Note: Check sample covariance
    Let sample_covariance be calculate_empirical_covariance(gibbs_result.samples[0], gibbs_result.samples[1])
    Set all_passed to all_passed and assert_approximately_equal(sample_covariance, 0.5, 0.1, "Sample covariance approximates true value")
    
    Return all_passed

Process called "calculate_empirical_covariance" that takes x_samples as List[Float], y_samples as List[Float] returns Float:
    Note: Helper function to calculate empirical covariance
    Let n be Length(x_samples)
    Let x_mean be calculate_sample_statistics(x_samples)["mean"]
    Let y_mean be calculate_sample_statistics(y_samples)["mean"]
    
    Let covariance_sum be 0.0
    For i from 0 to n - 1:
        Let x_dev be x_samples[i] - x_mean
        Let y_dev be y_samples[i] - y_mean
        Set covariance_sum to covariance_sum + (x_dev * y_dev)
    
    Return covariance_sum / Float(n - 1)

Process called "test_hamiltonian_monte_carlo" that takes no parameters returns Boolean:
    Note: Test Hamiltonian Monte Carlo (HMC) sampling
    Let all_passed be True
    
    Note: Sample from multivariate normal using HMC
    Let hmc_config be Bayesian.MCMCConfig
    Set hmc_config.algorithm_type to "HMC"
    Set hmc_config.chain_length to 2000
    Set hmc_config.burn_in_period to 200
    Set hmc_config.thinning_interval to 1
    
    Let hmc_params be Collections.create_dictionary()
    Set hmc_params["step_size"] to 0.1
    Set hmc_params["num_leapfrog_steps"] to 10
    Set hmc_params["mass_matrix"] to [[1.0, 0.0], [0.0, 1.0]]
    
    Let target_params be Collections.create_dictionary()
    Set target_params["mean"] to [0.0, 0.0]
    Set target_params["precision"] to [[2.0, 0.5], [0.5, 1.0]]
    
    Let hmc_result be Bayesian.hamiltonian_monte_carlo_sample(target_params, hmc_params, hmc_config)
    
    Note: HMC should have high acceptance rate and good mixing
    Set all_passed to all_passed and Test.assert_true(hmc_result.acceptance_rate > 0.8, "HMC should have high acceptance rate")
    Set all_passed to all_passed and Test.assert_true(hmc_result.convergence_diagnostics["effective_sample_size"] > 1500, "HMC effective sample size")
    
    Note: Check sample quality
    Let hmc_x_mean be calculate_sample_statistics(hmc_result.samples[0])["mean"]
    Let hmc_y_mean be calculate_sample_statistics(hmc_result.samples[1])["mean"]
    Set all_passed to all_passed and assert_approximately_equal(hmc_x_mean, 0.0, 0.1, "HMC x component mean")
    Set all_passed to all_passed and assert_approximately_equal(hmc_y_mean, 0.0, 0.1, "HMC y component mean")
    
    Return all_passed

Note: =====================================================================
Note: MODEL COMPARISON AND BAYES FACTORS TESTS
Note: =====================================================================

Process called "test_bayes_factor_calculation" that takes no parameters returns Boolean:
    Note: Test Bayes factor computation for model comparison
    Let all_passed be True
    
    Note: Compare two normal models with different means
    Let data be generate_normal_data(1.0, 1.0, 50)  Note: Data generated from model with mean=1
    
    Note: Model 1: Normal with mean=0, variance=1
    Let model1_params be Collections.create_dictionary()
    Set model1_params["mean"] to 0.0
    Set model1_params["variance"] to 1.0
    
    Note: Model 2: Normal with mean=1, variance=1  
    Let model2_params be Collections.create_dictionary()
    Set model2_params["mean"] to 1.0
    Set model2_params["variance"] to 1.0
    
    Let bayes_factor_result be Bayesian.calculate_bayes_factor(data, "Normal", model1_params, model2_params)
    
    Note: Model 2 should be favored since data was generated from mean=1
    Set all_passed to all_passed and Test.assert_true(bayes_factor_result.bf_21 > 1.0, "Model 2 should be favored over Model 1")
    Set all_passed to all_passed and Test.assert_true(bayes_factor_result.log_bf_21 > 0.0, "Log Bayes factor should be positive")
    
    Note: Check evidence values
    Set all_passed to all_passed and Test.assert_true(bayes_factor_result.model1_evidence < bayes_factor_result.model2_evidence, "Model 2 evidence higher")
    
    Return all_passed

Process called "test_model_selection_criteria" that takes no parameters returns Boolean:
    Note: Test information criteria for model selection
    Let all_passed be True
    
    Note: Fit models of different complexity to data
    Let data be generate_normal_data(2.0, 1.0, 100)
    
    Note: Simple model: Normal with unknown mean, known variance
    Let simple_model_result be Bayesian.fit_bayesian_model(data, "Normal_Known_Variance", Collections.create_dictionary())
    
    Note: Complex model: Normal with unknown mean and variance
    Let complex_model_result be Bayesian.fit_bayesian_model(data, "Normal_Unknown_Variance", Collections.create_dictionary())
    
    Note: Calculate model comparison criteria
    Let model_comparison be Bayesian.compare_bayesian_models([simple_model_result, complex_model_result])
    
    Note: Check information criteria values
    Set all_passed to all_passed and Test.assert_true(model_comparison.information_criteria["Model_1"]["DIC"] > 0.0, "DIC values should be positive")
    Set all_passed to all_passed and Test.assert_true(model_comparison.information_criteria["Model_1"]["WAIC"] > 0.0, "WAIC values should be positive")
    
    Note: Model probabilities should sum to 1
    Let prob_sum be model_comparison.model_probabilities["Model_1"] + model_comparison.model_probabilities["Model_2"]
    Set all_passed to all_passed and assert_approximately_equal(prob_sum, 1.0, 1e-10, "Model probabilities sum to 1")
    
    Return all_passed

Note: =====================================================================
Note: HIERARCHICAL BAYESIAN MODELING TESTS
Note: =====================================================================

Process called "test_hierarchical_normal_model" that takes no parameters returns Boolean:
    Note: Test hierarchical normal model with multiple groups
    Let all_passed be True
    
    Note: Simulate hierarchical data
    Let group_means be [2.0, 2.5, 1.8, 2.2, 1.9]  Note: Group-level means
    Let within_group_variance be 0.5
    Let between_group_variance be 0.1
    
    Let hierarchical_data be Collections.create_dictionary()
    For g from 0 to 4:
        Let group_name be "Group_" + ToString(g + 1)
        Let group_data be generate_normal_data(group_means[g], within_group_variance, 20)
        Set hierarchical_data[group_name] to group_data
    
    Note: Fit hierarchical model
    Let hierarchical_prior be Collections.create_dictionary()
    Set hierarchical_prior["global_mean_prior"] to Collections.dictionary_with_pairs([["mean", 0.0], ["variance", 100.0]])
    Set hierarchical_prior["between_group_variance_prior"] to Collections.dictionary_with_pairs([["shape", 1.0], ["rate", 1.0]])
    Set hierarchical_prior["within_group_variance_prior"] to Collections.dictionary_with_pairs([["shape", 1.0], ["rate", 1.0]])
    
    Let hierarchical_result be Bayesian.fit_hierarchical_normal_model(hierarchical_data, hierarchical_prior)
    
    Note: Check global parameters
    Let global_mean_estimate be hierarchical_result.global_parameters["mean"]["posterior_mean"]
    Set all_passed to all_passed and Test.assert_true(global_mean_estimate > 1.5, "Global mean reasonable lower bound")
    Set all_passed to all_passed and Test.assert_true(global_mean_estimate < 2.5, "Global mean reasonable upper bound")
    
    Note: Check group-level parameters
    For g from 0 to 4:
        Let group_name be "Group_" + ToString(g + 1)
        Let group_mean_estimate be hierarchical_result.group_parameters[group_name]["mean"]["posterior_mean"]
        Let expected_mean be group_means[g]
        Set all_passed to all_passed and assert_approximately_equal(group_mean_estimate, expected_mean, 0.3, "Group " + ToString(g + 1) + " mean estimate")
    
    Note: Check shrinkage effect
    Note: Group estimates should be shrunk toward global mean
    Let group1_raw_mean be calculate_sample_statistics(hierarchical_data["Group_1"])["mean"]
    Let group1_bayes_mean be hierarchical_result.group_parameters["Group_1"]["mean"]["posterior_mean"]
    Let shrinkage_observed be MathOps.absolute_value(group1_bayes_mean - global_mean_estimate) < MathOps.absolute_value(group1_raw_mean - global_mean_estimate)
    Set all_passed to all_passed and Test.assert_true(shrinkage_observed, "Hierarchical model shows shrinkage effect")
    
    Return all_passed

Note: =====================================================================
Note: POSTERIOR PREDICTIVE CHECKS TESTS
Note: =====================================================================

Process called "test_posterior_predictive_checks" that takes no parameters returns Boolean:
    Note: Test posterior predictive checking for model validation
    Let all_passed be True
    
    Note: Generate data and fit model
    Let observed_data be generate_normal_data(3.0, 2.0, 100)
    
    Let prior_params be Collections.create_dictionary()
    Set prior_params["mu_0"] to 0.0
    Set prior_params["lambda_0"] to 1.0
    Set prior_params["alpha_0"] to 2.0  
    Set prior_params["beta_0"] to 2.0
    
    Let posterior_result be Bayesian.bayesian_normal_inference(observed_data, prior_params)
    
    Note: Generate posterior predictive samples
    Let ppc_config be Bayesian.PosteriorPredictiveConfig
    Set ppc_config.num_predictive_samples to 1000
    Set ppc_config.test_statistics to ["mean", "variance", "min", "max", "median"]
    
    Let ppc_result be Bayesian.posterior_predictive_check(posterior_result, observed_data, ppc_config)
    
    Note: Check test statistics
    For each statistic in ppc_config.test_statistics:
        Let observed_stat be ppc_result.observed_test_statistics[statistic]
        Let predictive_stats be ppc_result.predictive_test_statistics[statistic]
        Let p_value be ppc_result.bayesian_p_values[statistic]
        
        Set all_passed to all_passed and Test.assert_true(p_value >= 0.0, "P-value " + statistic + " non-negative")
        Set all_passed to all_passed and Test.assert_true(p_value <= 1.0, "P-value " + statistic + " not greater than 1")
        
        Note: For good model fit, p-values should not be extreme (not close to 0 or 1)
        Set all_passed to all_passed and Test.assert_true(p_value > 0.05, "P-value " + statistic + " not too extreme (low)")
        Set all_passed to all_passed and Test.assert_true(p_value < 0.95, "P-value " + statistic + " not too extreme (high)")
    
    Return all_passed

Note: =====================================================================
Note: VARIATIONAL INFERENCE TESTS
Note: =====================================================================

Process called "test_variational_inference" that takes no parameters returns Boolean:
    Note: Test variational Bayes approximation methods
    Let all_passed be True
    
    Note: Apply variational inference to normal-normal conjugate model
    Let vi_data be generate_normal_data(2.0, 1.0, 200)
    
    Let vi_prior be Collections.create_dictionary()
    Set vi_prior["mean_prior"] to Collections.dictionary_with_pairs([["mean", 0.0], ["variance", 10.0]])
    Set vi_prior["variance_prior"] to Collections.dictionary_with_pairs([["shape", 1.0], ["rate", 1.0]])
    
    Let vi_config be Bayesian.VariationalConfig
    Set vi_config.inference_method to "Mean_Field"
    Set vi_config.max_iterations to 1000
    Set vi_config.convergence_tolerance to 1e-6
    Set vi_config.learning_rate to 0.01
    
    Let vi_result be Bayesian.variational_inference(vi_data, "Normal", vi_prior, vi_config)
    
    Note: Check convergence
    Set all_passed to all_passed and Test.assert_true(vi_result.converged, "Variational inference should converge")
    Set all_passed to all_passed and Test.assert_true(vi_result.final_elbo > vi_result.initial_elbo, "ELBO should improve")
    
    Note: Compare with exact posterior (should be close for conjugate case)
    Let exact_posterior be Bayesian.bayesian_normal_inference(vi_data, vi_prior)
    Let vi_mean_estimate be vi_result.variational_parameters["mean"]["mean"]
    Let exact_mean_estimate be exact_posterior.posterior_statistics["mean"]["mean"]
    
    Set all_passed to all_passed and assert_approximately_equal(vi_mean_estimate, exact_mean_estimate, 0.1, "VI approximates exact posterior mean")
    
    Return all_passed

Note: =====================================================================
Note: ERROR HANDLING AND EDGE CASES TESTS
Note: =====================================================================

Process called "test_bayesian_error_handling" that takes no parameters returns Boolean:
    Note: Test error handling for invalid Bayesian analysis inputs
    Let all_passed be True
    
    Note: Test invalid prior parameters
    Try:
        Let invalid_prior be Collections.create_dictionary()
        Set invalid_prior["alpha"] to -1.0  Note: Negative shape parameter
        Set invalid_prior["beta"] to 1.0
        
        Let dummy_data be [1.0, 2.0, 3.0]
        Let result be Bayesian.bayesian_binomial_inference(
            Collections.dictionary_with_pairs([["successes", 5.0], ["trials", 10.0]]), 
            invalid_prior)
        Set all_passed to Test.assert_true(False, "Negative prior parameters should throw error")
    Catch error as Errors.InvalidOperation:
        Set all_passed to all_passed and Test.assert_true(True, "Invalid prior parameters correctly caught")
    
    Note: Test empty data
    Try:
        Let empty_data be List[Float]
        Let valid_prior be Collections.create_dictionary()
        Set valid_prior["mu_0"] to 0.0
        Set valid_prior["lambda_0"] to 1.0
        Set valid_prior["alpha_0"] to 1.0
        Set valid_prior["beta_0"] to 1.0
        
        Let result be Bayesian.bayesian_normal_inference(empty_data, valid_prior)
        Set all_passed to Test.assert_true(False, "Empty data should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Empty data correctly caught")
    
    Note: Test MCMC with invalid chain parameters
    Try:
        Let invalid_config be Bayesian.MCMCConfig
        Set invalid_config.chain_length to -100  Note: Negative chain length
        Set invalid_config.burn_in_period to 50
        
        Let target_dist be Collections.dictionary_with_pairs([["type", "Normal"], ["parameters", Collections.dictionary_with_pairs([["mean", 0.0], ["variance", 1.0]])]])
        Let result be Bayesian.metropolis_hastings_sample(target_dist, invalid_config)
        Set all_passed to Test.assert_true(False, "Invalid MCMC config should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Invalid MCMC config correctly caught")
    
    Return all_passed

Note: =====================================================================
Note: MAIN TEST RUNNER
Note: =====================================================================

Process called "run_all_bayesian_tests" that takes no parameters returns Boolean:
    Note: Execute all Bayesian analysis tests and report results
    Test.print_test_header("BAYESIAN METHODS MODULE TESTS")
    Let all_passed be True
    
    Note: Prior Distribution Tests
    Test.print_test_section("Prior Distribution Specification Tests")
    Set all_passed to all_passed and test_conjugate_prior_selection()
    Set all_passed to all_passed and test_prior_elicitation()
    
    Note: Bayesian Parameter Estimation Tests
    Test.print_test_section("Bayesian Parameter Estimation Tests")
    Set all_passed to all_passed and test_bayesian_normal_inference()
    Set all_passed to all_passed and test_bayesian_binomial_inference() 
    Set all_passed to all_passed and test_bayesian_regression()
    
    Note: MCMC Methods Tests
    Test.print_test_section("Markov Chain Monte Carlo Tests")
    Set all_passed to all_passed and test_metropolis_hastings_sampling()
    Set all_passed to all_passed and test_gibbs_sampling()
    Set all_passed to all_passed and test_hamiltonian_monte_carlo()
    
    Note: Model Comparison Tests
    Test.print_test_section("Model Comparison and Selection Tests")
    Set all_passed to all_passed and test_bayes_factor_calculation()
    Set all_passed to all_passed and test_model_selection_criteria()
    
    Note: Hierarchical Modeling Tests
    Test.print_test_section("Hierarchical Bayesian Modeling Tests")
    Set all_passed to all_passed and test_hierarchical_normal_model()
    
    Note: Model Checking Tests
    Test.print_test_section("Posterior Predictive Checking Tests")
    Set all_passed to all_passed and test_posterior_predictive_checks()
    
    Note: Variational Inference Tests
    Test.print_test_section("Variational Inference Tests")
    Set all_passed to all_passed and test_variational_inference()
    
    Note: Error Handling Tests
    Test.print_test_section("Error Handling and Edge Cases Tests")
    Set all_passed to all_passed and test_bayesian_error_handling()
    
    Test.print_test_footer("BAYESIAN TESTS", all_passed)
    Return all_passed

Note: Entry point for individual test execution
Let test_result be run_all_bayesian_tests()
If test_result:
    Test.print_success("All Bayesian methods tests passed!")
Otherwise:
    Test.print_failure("Some Bayesian methods tests failed!")
    Test.exit_with_code(1)