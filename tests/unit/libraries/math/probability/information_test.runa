Note:
runa/tests/unit/libraries/math/probability/information_test.runa
Comprehensive Unit Tests for Information Theory and Entropy Measures

This test suite covers all information theory functionality including entropy
calculation, mutual information, Kullback-Leibler divergence, Fisher information,
channel capacity, and data compression bounds for quantifying information content
and communication efficiency.

Test Categories:
- Shannon entropy and differential entropy calculation
- Mutual information and conditional entropy
- Kullback-Leibler divergence and Jensen-Shannon divergence
- Fisher information matrix computation
- Communication channel capacity analysis
- Data compression and coding theory
- Information-theoretic inequalities
- Numerical accuracy and edge case testing
:End Note

Import "dev/debug/testing" as Test
Import "math/probability/information" as Information
Import "math/probability/distributions" as Distributions
Import "dev/debug/errors/core" as Errors
Import "math/core/operations" as MathOps
Import "math/engine/linalg/core" as LinAlg
Import "data/collections/core" as Collections
Import "algorithms/sorting/core" as Sorting

Note: =====================================================================
Note: TEST DATA GENERATION AND VALIDATION HELPERS
Note: =====================================================================

Process called "create_uniform_distribution" that takes alphabet_size as Integer returns Dictionary[String, Float]:
    Note: Create uniform probability distribution for testing
    Let uniform_dist be Collections.create_dictionary()
    Let prob be 1.0 / Float(alphabet_size)
    
    For i from 0 to alphabet_size - 1:
        Let symbol be "Symbol_" + ToString(i)
        Set uniform_dist[symbol] to prob
    
    Return uniform_dist

Process called "create_biased_distribution" that takes p as Float returns Dictionary[String, Float]:
    Note: Create biased binary distribution for testing
    Let biased_dist be Collections.create_dictionary()
    Set biased_dist["0"] to p
    Set biased_dist["1"] to 1.0 - p
    Return biased_dist

Process called "create_joint_distribution" that takes prob_00 as Float, prob_01 as Float, prob_10 as Float, prob_11 as Float returns Dictionary[String, Dictionary[String, Float]]:
    Note: Create joint probability distribution for two binary variables
    Let joint_dist be Collections.create_dictionary()
    
    Set joint_dist["X=0"] to Collections.create_dictionary()
    Set joint_dist["X=0"]["Y=0"] to prob_00
    Set joint_dist["X=0"]["Y=1"] to prob_01
    
    Set joint_dist["X=1"] to Collections.create_dictionary()
    Set joint_dist["X=1"]["Y=0"] to prob_10
    Set joint_dist["X=1"]["Y=1"] to prob_11
    
    Return joint_dist

Process called "assert_approximately_equal" that takes actual as Float, expected as Float, tolerance as Float, test_name as String returns Boolean:
    Note: Assert values are approximately equal within tolerance
    Let difference be MathOps.absolute_value(actual - expected)
    If difference <= tolerance:
        Return Test.assert_true(True, test_name + " - values approximately equal")
    Return Test.assert_true(False, test_name + " - Expected: " + ToString(expected) + ", Got: " + ToString(actual))

Process called "validate_probability_distribution" that takes distribution as Dictionary[String, Float] returns Boolean:
    Note: Validate that a dictionary represents a proper probability distribution
    Let sum be 0.0
    For each symbol in distribution.keys():
        Let prob be distribution[symbol]
        If prob < 0.0 or prob > 1.0:
            Return False
        Set sum to sum + prob
    Return MathOps.absolute_value(sum - 1.0) < 1e-10

Note: =====================================================================
Note: SHANNON ENTROPY TESTS
Note: =====================================================================

Process called "test_shannon_entropy_calculation" that takes no parameters returns Boolean:
    Note: Test Shannon entropy calculation for discrete distributions
    Let all_passed be True
    
    Note: Test uniform distribution entropy
    Let uniform_2 be create_uniform_distribution(2)  Note: Binary uniform
    Let entropy_2 be Information.shannon_entropy(uniform_2, 2.0)  Note: Base 2 (bits)
    Set all_passed to all_passed and assert_approximately_equal(entropy_2, 1.0, 1e-10, "Uniform binary entropy (bits)")
    
    Let uniform_4 be create_uniform_distribution(4)
    Let entropy_4 be Information.shannon_entropy(uniform_4, 2.0)
    Set all_passed to all_passed and assert_approximately_equal(entropy_4, 2.0, 1e-10, "Uniform quaternary entropy (bits)")
    
    Note: Test entropy in nats (base e)
    Let entropy_2_nats be Information.shannon_entropy(uniform_2, MathOps.e_constant())
    Let expected_nats be MathOps.logarithm_natural("2", 15).result_value
    Set all_passed to all_passed and assert_approximately_equal(entropy_2_nats, Parse expected_nats as Float, 1e-10, "Uniform binary entropy (nats)")
    
    Note: Test biased binary distribution
    Let biased_dist be create_biased_distribution(0.8)
    Let biased_entropy be Information.shannon_entropy(biased_dist, 2.0)
    
    Note: Entropy should be less than 1 bit for biased distribution
    Set all_passed to all_passed and Test.assert_true(biased_entropy < 1.0, "Biased distribution has lower entropy")
    Set all_passed to all_passed and Test.assert_true(biased_entropy > 0.0, "Biased distribution has positive entropy")
    
    Note: Calculate expected value: -0.8*log₂(0.8) - 0.2*log₂(0.2)
    Let log2_08 be MathOps.logarithm_arbitrary_base("0.8", "2", 15).result
    Let log2_02 be MathOps.logarithm_arbitrary_base("0.2", "2", 15).result
    Let expected_biased be -(0.8 * Parse log2_08 as Float + 0.2 * Parse log2_02 as Float)
    Set all_passed to all_passed and assert_approximately_equal(biased_entropy, expected_biased, 1e-10, "Biased binary entropy calculation")
    
    Return all_passed

Process called "test_differential_entropy" that takes no parameters returns Boolean:
    Note: Test differential entropy for continuous distributions
    Let all_passed be True
    
    Note: Test normal distribution differential entropy
    Note: For N(μ, σ²): h(X) = (1/2)log(2πeσ²)
    Let normal_params be Collections.create_dictionary()
    Set normal_params["pdf_expression"] to "normal"
    Set normal_params["mean"] to "0"
    Set normal_params["variance"] to "1"
    
    Let normal_bounds be Collections.create_dictionary()
    Set normal_bounds["lower"] to -10.0
    Set normal_bounds["upper"] to 10.0
    
    Let normal_diff_entropy be Information.differential_entropy(normal_params, normal_bounds)
    
    Note: Standard normal differential entropy = (1/2)log(2πe) ≈ 1.4189 nats
    Let expected_normal_entropy be 0.5 * MathOps.logarithm_natural(ToString(2.0 * MathOps.pi_constant() * MathOps.e_constant()), 15).result_value
    Set all_passed to all_passed and assert_approximately_equal(normal_diff_entropy, Parse expected_normal_entropy as Float, 0.01, "Standard normal differential entropy")
    
    Note: Test uniform distribution differential entropy
    Let uniform_params be Collections.create_dictionary()
    Set uniform_params["pdf_expression"] to "uniform"
    Set uniform_params["a"] to "0"
    Set uniform_params["b"] to "2"
    
    Let uniform_bounds be Collections.create_dictionary()
    Set uniform_bounds["lower"] to 0.0
    Set uniform_bounds["upper"] to 2.0
    
    Let uniform_diff_entropy be Information.differential_entropy(uniform_params, uniform_bounds)
    
    Note: Uniform[0,2] differential entropy = log(2-0) = log(2) ≈ 0.693 nats
    Let expected_uniform_entropy be MathOps.logarithm_natural("2", 15).result_value
    Set all_passed to all_passed and assert_approximately_equal(uniform_diff_entropy, Parse expected_uniform_entropy as Float, 0.01, "Uniform differential entropy")
    
    Return all_passed

Process called "test_conditional_entropy" that takes no parameters returns Boolean:
    Note: Test conditional entropy H(Y|X) calculation
    Let all_passed be True
    
    Note: Create joint distribution for testing
    Let joint_probs be create_joint_distribution(0.25, 0.25, 0.25, 0.25)  Note: Independent case
    
    Let conditional_entropy be Information.conditional_entropy(joint_probs, "Y", "X")
    
    Note: For independent variables, H(Y|X) = H(Y)
    Let marginal_y be Collections.create_dictionary()
    Set marginal_y["0"] to 0.5  Note: P(Y=0) = P(X=0,Y=0) + P(X=1,Y=0)
    Set marginal_y["1"] to 0.5  Note: P(Y=1) = P(X=0,Y=1) + P(X=1,Y=1)
    
    Let entropy_y be Information.shannon_entropy(marginal_y, 2.0)
    Set all_passed to all_passed and assert_approximately_equal(conditional_entropy, entropy_y, 1e-10, "Conditional entropy for independent variables")
    
    Note: Test dependent case
    Let dependent_probs be create_joint_distribution(0.4, 0.1, 0.1, 0.4)  Note: Dependent variables
    Let dependent_conditional be Information.conditional_entropy(dependent_probs, "Y", "X")
    
    Note: For dependent variables, H(Y|X) < H(Y)
    Set all_passed to all_passed and Test.assert_true(dependent_conditional < entropy_y, "Conditional entropy lower for dependent variables")
    
    Return all_passed

Note: =====================================================================
Note: MUTUAL INFORMATION TESTS
Note: =====================================================================

Process called "test_mutual_information" that takes no parameters returns Boolean:
    Note: Test mutual information I(X;Y) calculation
    Let all_passed be True
    
    Note: Test independent variables (I(X;Y) = 0)
    Let independent_joint be create_joint_distribution(0.25, 0.25, 0.25, 0.25)
    Let independent_mi be Information.mutual_information(independent_joint, "X", "Y")
    Set all_passed to all_passed and assert_approximately_equal(independent_mi, 0.0, 1e-10, "Mutual information for independent variables")
    
    Note: Test perfectly dependent variables (deterministic relationship)
    Let perfect_joint be create_joint_distribution(0.5, 0.0, 0.0, 0.5)  Note: Y = X
    Let perfect_mi be Information.mutual_information(perfect_joint, "X", "Y")
    
    Note: For perfect dependence, I(X;Y) = H(X) = H(Y)
    Let marginal_x be Collections.create_dictionary()
    Set marginal_x["0"] to 0.5
    Set marginal_x["1"] to 0.5
    Let entropy_x be Information.shannon_entropy(marginal_x, 2.0)
    
    Set all_passed to all_passed and assert_approximately_equal(perfect_mi, entropy_x, 1e-10, "Mutual information for perfect dependence")
    
    Note: Test partial dependence
    Let partial_joint be create_joint_distribution(0.4, 0.1, 0.1, 0.4)
    Let partial_mi be Information.mutual_information(partial_joint, "X", "Y")
    
    Note: Partial dependence should give 0 < I(X;Y) < H(X)
    Set all_passed to all_passed and Test.assert_true(partial_mi > 0.0, "Partial dependence MI positive")
    Set all_passed to all_passed and Test.assert_true(partial_mi < entropy_x, "Partial dependence MI less than marginal entropy")
    
    Note: Test symmetry: I(X;Y) = I(Y;X)
    Let symmetric_mi be Information.mutual_information(partial_joint, "Y", "X")
    Set all_passed to all_passed and assert_approximately_equal(partial_mi, symmetric_mi, 1e-10, "Mutual information symmetry")
    
    Return all_passed

Process called "test_conditional_mutual_information" that takes no parameters returns Boolean:
    Note: Test conditional mutual information I(X;Y|Z)
    Let all_passed be True
    
    Note: Create 3-variable joint distribution
    Let three_var_joint be Collections.create_dictionary()
    
    Note: Set up joint probabilities P(X,Y,Z)
    Set three_var_joint["X=0,Y=0,Z=0"] to 0.125
    Set three_var_joint["X=0,Y=0,Z=1"] to 0.125
    Set three_var_joint["X=0,Y=1,Z=0"] to 0.125
    Set three_var_joint["X=0,Y=1,Z=1"] to 0.125
    Set three_var_joint["X=1,Y=0,Z=0"] to 0.125
    Set three_var_joint["X=1,Y=0,Z=1"] to 0.125
    Set three_var_joint["X=1,Y=1,Z=0"] to 0.125
    Set three_var_joint["X=1,Y=1,Z=1"] to 0.125
    
    Let conditional_mi be Information.conditional_mutual_information(three_var_joint, "X", "Y", "Z")
    
    Note: For uniform distribution, all variables are independent
    Set all_passed to all_passed and assert_approximately_equal(conditional_mi, 0.0, 1e-10, "Conditional MI for independent variables")
    
    Note: Test chain rule: I(X;Y,Z) = I(X;Y) + I(X;Z|Y)
    Let joint_xy_z be Information.mutual_information_multivariate(three_var_joint, ["X"], ["Y", "Z"])
    Let mi_xy be Information.mutual_information_bivariate(three_var_joint, "X", "Y")
    Let cond_mi_xz_y be Information.conditional_mutual_information(three_var_joint, "X", "Z", "Y")
    
    Set all_passed to all_passed and assert_approximately_equal(joint_xy_z, mi_xy + cond_mi_xz_y, 1e-10, "Chain rule for mutual information")
    
    Return all_passed

Note: =====================================================================
Note: KULLBACK-LEIBLER DIVERGENCE TESTS
Note: =====================================================================

Process called "test_kl_divergence" that takes no parameters returns Boolean:
    Note: Test Kullback-Leibler divergence calculation
    Let all_passed be True
    
    Note: Test KL divergence with identical distributions (should be 0)
    Let dist_p be create_uniform_distribution(4)
    Let dist_q be create_uniform_distribution(4)
    Let kl_identical be Information.kullback_leibler_divergence(dist_p, dist_q)
    Set all_passed to all_passed and assert_approximately_equal(kl_identical, 0.0, 1e-10, "KL divergence for identical distributions")
    
    Note: Test KL divergence with different distributions
    Let biased_p be create_biased_distribution(0.8)
    Let uniform_q be create_biased_distribution(0.5)
    Let kl_different be Information.kullback_leibler_divergence(biased_p, uniform_q)
    
    Note: KL divergence should be positive for different distributions
    Set all_passed to all_passed and Test.assert_true(kl_different > 0.0, "KL divergence positive for different distributions")
    
    Note: Test asymmetry: KL(P||Q) ≠ KL(Q||P) in general
    Let kl_reverse be Information.kullback_leibler_divergence(uniform_q, biased_p)
    Set all_passed to all_passed and Test.assert_true(MathOps.absolute_value(kl_different - kl_reverse) > 1e-6, "KL divergence asymmetric")
    
    Note: Test known analytical result
    Note: KL(Ber(p)||Ber(q)) = p*log(p/q) + (1-p)*log((1-p)/(1-q))
    Let p be 0.3
    Let q be 0.7
    Let ber_p be create_biased_distribution(p)
    Let ber_q be create_biased_distribution(q)
    
    Let kl_analytical be Information.kullback_leibler_divergence(ber_p, ber_q)
    
    Note: Calculate expected value
    Let log_p_q be MathOps.logarithm_natural(ToString(p / q), 15).result_value
    Let log_1p_1q be MathOps.logarithm_natural(ToString((1.0 - p) / (1.0 - q)), 15).result_value
    Let expected_kl be p * Parse log_p_q as Float + (1.0 - p) * Parse log_1p_1q as Float
    
    Set all_passed to all_passed and assert_approximately_equal(kl_analytical, expected_kl, 1e-10, "KL divergence analytical calculation")
    
    Return all_passed

Process called "test_jensen_shannon_divergence" that takes no parameters returns Boolean:
    Note: Test Jensen-Shannon divergence (symmetric version of KL)
    Let all_passed be True
    
    Note: Test JS divergence with identical distributions
    Let dist_p be create_uniform_distribution(3)
    Let dist_q be create_uniform_distribution(3)
    Let js_identical be Information.jensen_shannon_divergence(dist_p, dist_q)
    Set all_passed to all_passed and assert_approximately_equal(js_identical, 0.0, 1e-10, "JS divergence for identical distributions")
    
    Note: Test JS divergence with different distributions
    Let biased_p be create_biased_distribution(0.9)
    Let uniform_q be create_biased_distribution(0.5)
    Let js_different be Information.jensen_shannon_divergence(biased_p, uniform_q)
    
    Note: JS divergence should be positive and bounded by log(2)
    Set all_passed to all_passed and Test.assert_true(js_different > 0.0, "JS divergence positive for different distributions")
    Let log2_bound be MathOps.logarithm_natural("2", 15).result_value
    Set all_passed to all_passed and Test.assert_true(js_different <= Parse log2_bound as Float, "JS divergence bounded by log(2)")
    
    Note: Test symmetry: JS(P,Q) = JS(Q,P)
    Let js_reverse be Information.jensen_shannon_divergence(uniform_q, biased_p)
    Set all_passed to all_passed and assert_approximately_equal(js_different, js_reverse, 1e-10, "JS divergence symmetry")
    
    Note: Test triangle inequality (JS is a metric)
    Let dist_r be create_biased_distribution(0.2)
    Let js_pr be Information.jensen_shannon_divergence(biased_p, dist_r)
    Let js_qr be Information.jensen_shannon_divergence(uniform_q, dist_r)
    
    Note: Triangle inequality: JS(P,R) ≤ JS(P,Q) + JS(Q,R)
    Set all_passed to all_passed and Test.assert_true(js_pr <= js_different + js_qr + 1e-10, "JS divergence triangle inequality")
    
    Return all_passed

Note: =====================================================================
Note: FISHER INFORMATION TESTS
Note: =====================================================================

Process called "test_fisher_information_matrix" that takes no parameters returns Boolean:
    Note: Test Fisher information matrix computation
    Let all_passed be True
    
    Note: Test Fisher information for normal distribution
    Let normal_params be ["mean", "variance"]
    Let sample_data be [1.2, 0.8, 1.5, 0.9, 1.1, 1.3, 0.7, 1.4, 1.0, 1.2]
    
    Let fisher_result be Information.calculate_fisher_information_matrix("Normal", normal_params, sample_data)
    
    Set all_passed to all_passed and Test.assert_equal(Length(fisher_result.parameter_names), 2, "Fisher matrix has correct parameter count")
    Set all_passed to all_passed and Test.assert_equal(Length(fisher_result.information_matrix), 2, "Fisher matrix correct dimensions")
    Set all_passed to all_passed and Test.assert_equal(Length(fisher_result.information_matrix[0]), 2, "Fisher matrix square")
    
    Note: Fisher information matrix should be positive definite
    Set all_passed to all_passed and Test.assert_true(fisher_result.determinant > 0.0, "Fisher matrix positive definite")
    Set all_passed to all_passed and Test.assert_true(fisher_result.information_matrix[0][0] > 0.0, "Fisher matrix diagonal positive")
    Set all_passed to all_passed and Test.assert_true(fisher_result.information_matrix[1][1] > 0.0, "Fisher matrix diagonal positive")
    
    Note: Test Cramér-Rao bound relationship
    Note: Variance of MLE ≥ inverse of Fisher information
    Set all_passed to all_passed and Test.assert_true(Length(fisher_result.asymptotic_variance) == 2, "Asymptotic variance matrix computed")
    
    Note: Test Fisher information for exponential distribution
    Let exp_params be ["rate"]
    Let exp_data be [0.5, 1.2, 0.8, 2.1, 0.3, 1.5, 0.9, 1.8, 0.7, 1.1]
    
    Let exp_fisher be Information.calculate_fisher_information_matrix("Exponential", exp_params, exp_data)
    Set all_passed to all_passed and Test.assert_equal(Length(exp_fisher.parameter_names), 1, "Exponential Fisher matrix 1D")
    Set all_passed to all_passed and Test.assert_true(exp_fisher.information_matrix[0][0] > 0.0, "Exponential Fisher information positive")
    
    Return all_passed

Process called "test_observed_fisher_information" that takes no parameters returns Boolean:
    Note: Test observed Fisher information (negative Hessian of log-likelihood)
    Let all_passed be True
    
    Note: Generate data from known distribution
    Let true_mean be 2.0
    Let true_var be 0.25
    Let normal_data be List[Float]
    
    For i from 0 to 99:
        Let sample be Distributions.generate_normal_sample(true_mean, true_var)
        Append sample to normal_data
    
    Let mle_estimates be Collections.create_dictionary()
    Set mle_estimates["mean"] to 2.1  Note: Close to true value
    Set mle_estimates["variance"] to 0.26
    
    Let observed_fisher be Information.calculate_observed_fisher_information("Normal", mle_estimates, normal_data)
    
    Note: Observed Fisher information should be close to expected Fisher information
    Set all_passed to all_passed and Test.assert_true(observed_fisher.information_matrix[0][0] > 0.0, "Observed Fisher information positive")
    Set all_passed to all_passed and Test.assert_true(observed_fisher.condition_number < 1000.0, "Observed Fisher matrix well-conditioned")
    
    Return all_passed

Note: =====================================================================
Note: COMMUNICATION CHANNEL TESTS
Note: =====================================================================

Process called "test_channel_capacity" that takes no parameters returns Boolean:
    Note: Test communication channel capacity calculation
    Let all_passed be True
    
    Note: Test binary symmetric channel
    Let bsc_input_alphabet be ["0", "1"]
    Let bsc_output_alphabet be ["0", "1"]
    Let crossover_prob be 0.1  Note: 10% error probability
    
    Let bsc_transition_probs be Collections.create_dictionary()
    Set bsc_transition_probs["0"] to Collections.dictionary_with_pairs([["0", 1.0 - crossover_prob], ["1", crossover_prob]])
    Set bsc_transition_probs["1"] to Collections.dictionary_with_pairs([["0", crossover_prob], ["1", 1.0 - crossover_prob]])
    
    Let bsc_channel be Information.CommunicationChannel
    Set bsc_channel.input_alphabet to bsc_input_alphabet
    Set bsc_channel.output_alphabet to bsc_output_alphabet
    Set bsc_channel.transition_probabilities to bsc_transition_probs
    
    Let bsc_capacity be Information.calculate_channel_capacity(bsc_channel)
    
    Note: BSC capacity = 1 - H(p) where p is crossover probability
    Let h_p be Information.shannon_entropy(Collections.dictionary_with_pairs([["0", 1.0 - crossover_prob], ["1", crossover_prob]]), 2.0)
    Let expected_capacity be 1.0 - h_p
    
    Set all_passed to all_passed and assert_approximately_equal(bsc_capacity, expected_capacity, 1e-6, "Binary symmetric channel capacity")
    
    Note: Test noiseless channel (capacity should be log(|input alphabet|))
    Let noiseless_probs be Collections.create_dictionary()
    Set noiseless_probs["0"] to Collections.dictionary_with_pairs([["0", 1.0], ["1", 0.0]])
    Set noiseless_probs["1"] to Collections.dictionary_with_pairs([["0", 0.0], ["1", 1.0]])
    
    Let noiseless_channel be Information.CommunicationChannel
    Set noiseless_channel.input_alphabet to ["0", "1"]
    Set noiseless_channel.output_alphabet to ["0", "1"]
    Set noiseless_channel.transition_probabilities to noiseless_probs
    
    Let noiseless_capacity be Information.calculate_channel_capacity(noiseless_channel)
    Set all_passed to all_passed and assert_approximately_equal(noiseless_capacity, 1.0, 1e-6, "Noiseless binary channel capacity")
    
    Note: Test completely noisy channel (capacity should be 0)
    Let noisy_probs be Collections.create_dictionary()
    Set noisy_probs["0"] to Collections.dictionary_with_pairs([["0", 0.5], ["1", 0.5]])
    Set noisy_probs["1"] to Collections.dictionary_with_pairs([["0", 0.5], ["1", 0.5]])
    
    Let noisy_channel be Information.CommunicationChannel
    Set noisy_channel.input_alphabet to ["0", "1"]
    Set noisy_channel.output_alphabet to ["0", "1"]
    Set noisy_channel.transition_probabilities to noisy_probs
    
    Let noisy_capacity be Information.calculate_channel_capacity(noisy_channel)
    Set all_passed to all_passed and assert_approximately_equal(noisy_capacity, 0.0, 1e-6, "Completely noisy channel capacity")
    
    Return all_passed

Process called "test_channel_mutual_information" that takes no parameters returns Boolean:
    Note: Test mutual information calculation for communication channels
    Let all_passed be True
    
    Note: Create channel and input distribution
    Let channel_probs be Collections.create_dictionary()
    Set channel_probs["A"] to Collections.dictionary_with_pairs([["X", 0.8], ["Y", 0.2]])
    Set channel_probs["B"] to Collections.dictionary_with_pairs([["X", 0.3], ["Y", 0.7]])
    
    Let test_channel be Information.CommunicationChannel
    Set test_channel.input_alphabet to ["A", "B"]
    Set test_channel.output_alphabet to ["X", "Y"]
    Set test_channel.transition_probabilities to channel_probs
    
    Note: Test uniform input distribution
    Let uniform_input be Collections.dictionary_with_pairs([["A", 0.5], ["B", 0.5]])
    Let channel_mi_uniform be Information.calculate_channel_mutual_information(test_channel, uniform_input)
    
    Set all_passed to all_passed and Test.assert_true(channel_mi_uniform >= 0.0, "Channel mutual information non-negative")
    
    Note: Test biased input distribution
    Let biased_input be Collections.dictionary_with_pairs([["A", 0.9], ["B", 0.1]])
    Let channel_mi_biased be Information.calculate_channel_mutual_information(test_channel, biased_input)
    
    Set all_passed to all_passed and Test.assert_true(channel_mi_biased >= 0.0, "Biased input MI non-negative")
    
    Note: Capacity is the maximum over all input distributions
    Note: So both calculated MI values should be ≤ capacity
    Let capacity be Information.calculate_channel_capacity(test_channel)
    Set all_passed to all_passed and Test.assert_true(channel_mi_uniform <= capacity + 1e-6, "Uniform input MI ≤ capacity")
    Set all_passed to all_passed and Test.assert_true(channel_mi_biased <= capacity + 1e-6, "Biased input MI ≤ capacity")
    
    Return all_passed

Note: =====================================================================
Note: DATA COMPRESSION TESTS
Note: =====================================================================

Process called "test_compression_bounds" that takes no parameters returns Boolean:
    Note: Test data compression bounds and coding theory
    Let all_passed be True
    
    Note: Test Shannon's source coding theorem
    Let text_sequence be ["A", "B", "A", "C", "A", "B", "A", "A", "C", "B", "A", "A"]
    
    Let compression_analysis be Information.analyze_compression_potential(text_sequence)
    
    Set all_passed to all_passed and Test.assert_true(compression_analysis.empirical_entropy > 0.0, "Empirical entropy positive")
    Set all_passed to all_passed and Test.assert_true(compression_analysis.optimal_code_length >= compression_analysis.empirical_entropy, "Optimal code length ≥ entropy")
    
    Note: Calculate theoretical entropy
    Let symbol_counts be Collections.create_dictionary()
    For each symbol in text_sequence:
        If symbol_counts.has_key(symbol):
            Set symbol_counts[symbol] to symbol_counts[symbol] + 1
        Otherwise:
            Set symbol_counts[symbol] to 1
    
    Let symbol_probs be Collections.create_dictionary()
    Let total_symbols be Length(text_sequence)
    For each symbol in symbol_counts.keys():
        Set symbol_probs[symbol] to Float(symbol_counts[symbol]) / Float(total_symbols)
    
    Let theoretical_entropy be Information.shannon_entropy(symbol_probs, 2.0)
    Set all_passed to all_passed and assert_approximately_equal(compression_analysis.empirical_entropy, theoretical_entropy, 1e-6, "Empirical entropy matches theoretical")
    
    Note: Test Huffman coding optimality
    Let huffman_result be Information.huffman_coding_analysis(text_sequence)
    Set all_passed to all_passed and Test.assert_true(huffman_result.average_code_length >= theoretical_entropy, "Huffman code length ≥ entropy")
    Set all_passed to all_passed and Test.assert_true(huffman_result.average_code_length < theoretical_entropy + 1.0, "Huffman code length < entropy + 1")
    
    Return all_passed

Process called "test_lempel_ziv_complexity" that takes no parameters returns Boolean:
    Note: Test Lempel-Ziv compression complexity
    Let all_passed be True
    
    Note: Test periodic sequence (low complexity)
    Let periodic_sequence be ["A", "B", "A", "B", "A", "B", "A", "B", "A", "B"]
    Let periodic_complexity be Information.lempel_ziv_complexity(periodic_sequence)
    
    Note: Test random sequence (high complexity) 
    Let random_sequence be ["A", "C", "B", "A", "D", "B", "C", "D", "A", "B", "C", "A", "D", "C", "B", "D"]
    Let random_complexity be Information.lempel_ziv_complexity(random_sequence)
    
    Set all_passed to all_passed and Test.assert_true(random_complexity > periodic_complexity, "Random sequence has higher LZ complexity")
    
    Note: Test constant sequence (minimal complexity)
    Let constant_sequence be ["X", "X", "X", "X", "X", "X", "X", "X"]
    Let constant_complexity be Information.lempel_ziv_complexity(constant_sequence)
    
    Set all_passed to all_passed and Test.assert_true(constant_complexity <= periodic_complexity, "Constant sequence has low LZ complexity")
    Set all_passed to all_passed and Test.assert_true(constant_complexity > 0, "LZ complexity positive")
    
    Return all_passed

Note: =====================================================================
Note: INFORMATION-THEORETIC INEQUALITIES TESTS
Note: =====================================================================

Process called "test_information_inequalities" that takes no parameters returns Boolean:
    Note: Test fundamental information-theoretic inequalities
    Let all_passed be True
    
    Note: Test data processing inequality: I(X;Z) ≤ I(X;Y) for Markov chain X → Y → Z
    Let markov_chain_joint be Collections.create_dictionary()
    
    Note: Set up Markov chain probabilities
    Set markov_chain_joint["X=0,Y=0,Z=0"] to 0.2
    Set markov_chain_joint["X=0,Y=0,Z=1"] to 0.1
    Set markov_chain_joint["X=0,Y=1,Z=0"] to 0.05
    Set markov_chain_joint["X=0,Y=1,Z=1"] to 0.15
    Set markov_chain_joint["X=1,Y=0,Z=0"] to 0.1
    Set markov_chain_joint["X=1,Y=0,Z=1"] to 0.2
    Set markov_chain_joint["X=1,Y=1,Z=0"] to 0.15
    Set markov_chain_joint["X=1,Y=1,Z=1"] to 0.05
    
    Let mi_xy be Information.mutual_information_trivariate(markov_chain_joint, "X", "Y")
    Let mi_xz be Information.mutual_information_trivariate(markov_chain_joint, "X", "Z")
    
    Set all_passed to all_passed and Test.assert_true(mi_xz <= mi_xy + 1e-10, "Data processing inequality")
    
    Note: Test submodularity: I(X;Y,Z) ≤ I(X;Y) + I(X;Z)
    Let mi_xyz be Information.mutual_information_multivariate(markov_chain_joint, ["X"], ["Y", "Z"])
    Set all_passed to all_passed and Test.assert_true(mi_xyz <= mi_xy + mi_xz + 1e-10, "Mutual information submodularity")
    
    Note: Test Fano's inequality for error probability bounds
    Let conditional_entropy_yx be Information.conditional_entropy_trivariate(markov_chain_joint, "Y", "X")
    Let error_prob_bound be Information.fano_inequality_bound(conditional_entropy_yx, 2)  Note: 2 possible values for Y
    
    Set all_passed to all_passed and Test.assert_true(error_prob_bound >= 0.0, "Fano bound non-negative")
    Set all_passed to all_passed and Test.assert_true(error_prob_bound <= 1.0, "Fano bound ≤ 1")
    
    Return all_passed

Note: =====================================================================
Note: ERROR HANDLING AND EDGE CASES TESTS
Note: =====================================================================

Process called "test_information_error_handling" that takes no parameters returns Boolean:
    Note: Test error handling for invalid information theory inputs
    Let all_passed be True
    
    Note: Test entropy with invalid probability distribution
    Try:
        Let invalid_dist be Collections.dictionary_with_pairs([["A", 0.7], ["B", 0.4]])  Note: Sums to > 1
        Let invalid_entropy be Information.shannon_entropy(invalid_dist, 2.0)
        Set all_passed to Test.assert_true(False, "Invalid probability distribution should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Invalid distribution correctly rejected")
    
    Note: Test KL divergence with zero probabilities in Q
    Try:
        Let p_dist be Collections.dictionary_with_pairs([["X", 0.5], ["Y", 0.5]])
        Let q_dist be Collections.dictionary_with_pairs([["X", 1.0], ["Y", 0.0]])  Note: Zero probability
        Let invalid_kl be Information.kullback_leibler_divergence(p_dist, q_dist)
        Set all_passed to Test.assert_true(False, "KL divergence with zero in Q should throw error or return infinity")
    Catch error as Errors.InvalidOperation:
        Set all_passed to all_passed and Test.assert_true(True, "KL with zero probability correctly handled")
    
    Note: Test Fisher information with insufficient data
    Try:
        Let tiny_data be [1.0]  Note: Only one data point
        Let tiny_fisher be Information.calculate_fisher_information_matrix("Normal", ["mean", "variance"], tiny_data)
        Set all_passed to Test.assert_true(False, "Insufficient data should throw error")
    Catch error as Errors.InvalidArgument:
        Set all_passed to all_passed and Test.assert_true(True, "Insufficient data correctly rejected")
    
    Return all_passed

Process called "test_numerical_stability" that takes no parameters returns Boolean:
    Note: Test numerical stability with extreme probability values
    Let all_passed be True
    
    Note: Test entropy with very small probabilities
    Let extreme_dist be Collections.dictionary_with_pairs([["A", 1e-10], ["B", 1.0 - 1e-10]])
    Let extreme_entropy be Information.shannon_entropy(extreme_dist, 2.0)
    
    Set all_passed to all_passed and Test.assert_true(extreme_entropy >= 0.0, "Entropy with extreme probabilities non-negative")
    Set all_passed to all_passed and Test.assert_true(extreme_entropy < 0.01, "Entropy with extreme probabilities very small")
    
    Note: Test KL divergence with very similar distributions
    Let p_similar be Collections.dictionary_with_pairs([["X", 0.5000001], ["Y", 0.4999999]])
    Let q_similar be Collections.dictionary_with_pairs([["X", 0.5], ["Y", 0.5]])
    Let similar_kl be Information.kullback_leibler_divergence(p_similar, q_similar)
    
    Set all_passed to all_passed and Test.assert_true(similar_kl >= 0.0, "KL divergence non-negative for similar distributions")
    Set all_passed to all_passed and Test.assert_true(similar_kl < 1e-6, "KL divergence very small for similar distributions")
    
    Return all_passed

Note: =====================================================================
Note: MAIN TEST RUNNER
Note: =====================================================================

Process called "run_all_information_tests" that takes no parameters returns Boolean:
    Note: Execute all information theory tests and report results
    Test.print_test_header("INFORMATION THEORY MODULE TESTS")
    Let all_passed be True
    
    Note: Shannon Entropy Tests
    Test.print_test_section("Shannon Entropy Tests")
    Set all_passed to all_passed and test_shannon_entropy_calculation()
    Set all_passed to all_passed and test_differential_entropy()
    Set all_passed to all_passed and test_conditional_entropy()
    
    Note: Mutual Information Tests
    Test.print_test_section("Mutual Information Tests")
    Set all_passed to all_passed and test_mutual_information()
    Set all_passed to all_passed and test_conditional_mutual_information()
    
    Note: Divergence Measures Tests
    Test.print_test_section("Divergence Measures Tests")
    Set all_passed to all_passed and test_kl_divergence()
    Set all_passed to all_passed and test_jensen_shannon_divergence()
    
    Note: Fisher Information Tests
    Test.print_test_section("Fisher Information Tests")
    Set all_passed to all_passed and test_fisher_information_matrix()
    Set all_passed to all_passed and test_observed_fisher_information()
    
    Note: Communication Channel Tests
    Test.print_test_section("Communication Channel Tests")
    Set all_passed to all_passed and test_channel_capacity()
    Set all_passed to all_passed and test_channel_mutual_information()
    
    Note: Data Compression Tests
    Test.print_test_section("Data Compression Tests")
    Set all_passed to all_passed and test_compression_bounds()
    Set all_passed to all_passed and test_lempel_ziv_complexity()
    
    Note: Information Inequalities Tests
    Test.print_test_section("Information Inequalities Tests")
    Set all_passed to all_passed and test_information_inequalities()
    
    Note: Error Handling and Edge Cases Tests
    Test.print_test_section("Error Handling and Edge Cases Tests")
    Set all_passed to all_passed and test_information_error_handling()
    Set all_passed to all_passed and test_numerical_stability()
    
    Test.print_test_footer("INFORMATION TESTS", all_passed)
    Return all_passed

Note: Entry point for individual test execution
Let test_result be run_all_information_tests()
If test_result:
    Test.print_success("All information theory tests passed!")
Otherwise:
    Test.print_failure("Some information theory tests failed!")
    Test.exit_with_code(1)