Note:
runa/tests/unit/libraries/math/probability/probability_test_runner.runa
Master Test Runner for Probability Mathematics Module

This master test runner orchestrates all probability module tests including
distributions, sampling, Bayesian methods, Markov chains, stochastic processes,
and information theory. It provides comprehensive test execution, reporting,
benchmarking, and coverage analysis for the entire probability subsystem.

Module Coverage:
- distributions.runa - Probability distribution functions and statistical analysis
- sampling.runa - Monte Carlo methods and statistical sampling
- bayesian.runa - Bayesian methods and statistical inference
- markov.runa - Markov chains and Monte Carlo Markov Chain methods
- stochastic.runa - Stochastic processes and random dynamical systems
- information.runa - Information theory and entropy measures

Test Features:
- Individual module test execution
- Comprehensive error reporting and logging
- Performance benchmarking and timing analysis
- Memory usage monitoring
- Test coverage statistics
- Regression test validation
:End Note

Import "dev/debug/testing" as Test
Import "dev/debug/errors/core" as Errors
Import "system/time/measurement" as TimeMeasurement
Import "system/memory/profiling" as MemoryProfiler
Import "data/collections/core" as Collections
Import "text/string/manipulation" as StringOps
Import "io/filesystem/operations" as FileSystem

Note: Import individual module test suites
Import "runa/tests/unit/libraries/math/probability/distributions_test" as DistributionsTest
Import "runa/tests/unit/libraries/math/probability/sampling_test" as SamplingTest
Import "runa/tests/unit/libraries/math/probability/bayesian_test" as BayesianTest
Import "runa/tests/unit/libraries/math/probability/markov_test" as MarkovTest
Import "runa/tests/unit/libraries/math/probability/stochastic_test" as StochasticTest
Import "runa/tests/unit/libraries/math/probability/information_test" as InformationTest

Note: =====================================================================
Note: TEST EXECUTION COORDINATION DATA STRUCTURES
Note: =====================================================================

Type called "ModuleTestResult":
    module_name as String
    test_passed as Boolean
    execution_time_ms as Integer
    memory_used_bytes as Integer
    tests_run as Integer
    tests_passed as Integer
    tests_failed as Integer
    error_messages as List[String]
    performance_metrics as Dictionary[String, Float]

Type called "TestSuiteConfiguration":
    enable_benchmarking as Boolean
    enable_memory_profiling as Boolean
    enable_coverage_analysis as Boolean
    parallel_execution as Boolean
    timeout_seconds as Integer
    verbose_output as Boolean
    save_detailed_logs as Boolean

Type called "TestSuiteResults":
    total_modules_tested as Integer
    modules_passed as Integer
    modules_failed as Integer
    total_execution_time_ms as Integer
    total_memory_used_bytes as Integer
    overall_success_rate as Float
    module_results as List[ModuleTestResult]
    performance_summary as Dictionary[String, Float]

Note: =====================================================================
Note: CONFIGURATION AND SETUP
Note: =====================================================================

Process called "create_default_test_configuration" that takes no parameters returns TestSuiteConfiguration:
    Note: Create default configuration for probability test suite
    Let config be TestSuiteConfiguration
    Set config.enable_benchmarking to True
    Set config.enable_memory_profiling to True
    Set config.enable_coverage_analysis to False  Note: Expensive operation
    Set config.parallel_execution to False  Note: Sequential for better debugging
    Set config.timeout_seconds to 300  Note: 5 minutes per module
    Set config.verbose_output to True
    Set config.save_detailed_logs to True
    Return config

Process called "initialize_test_environment" that takes config as TestSuiteConfiguration returns Boolean:
    Note: Initialize test environment and validate dependencies
    Test.print_info("Initializing probability module test environment...")
    
    Note: Verify test framework availability
    If not Test.framework_available():
        Test.print_error("Test framework not available")
        Return False
    
    Note: Initialize memory profiler if enabled
    If config.enable_memory_profiling:
        If not MemoryProfiler.initialize():
            Test.print_warning("Memory profiling not available - continuing without it")
            Set config.enable_memory_profiling to False
    
    Note: Initialize timing measurement
    If not TimeMeasurement.initialize_high_precision():
        Test.print_warning("High precision timing not available - using standard timing")
    
    Note: Create log directory if needed
    If config.save_detailed_logs:
        Let log_dir_created be FileSystem.create_directory("logs/probability_tests")
        If not log_dir_created:
            Test.print_warning("Could not create log directory - logs will not be saved")
            Set config.save_detailed_logs to False
    
    Test.print_success("Test environment initialized successfully")
    Return True

Note: =====================================================================
Note: INDIVIDUAL MODULE TEST EXECUTION
Note: =====================================================================

Process called "execute_module_test" that takes module_name as String, config as TestSuiteConfiguration returns ModuleTestResult:
    Note: Execute tests for a single probability module with comprehensive monitoring
    Let result be ModuleTestResult
    Set result.module_name to module_name
    Set result.error_messages to List[String]
    Set result.performance_metrics to Collections.create_dictionary()
    
    Note: Initialize monitoring
    Let start_time be TimeMeasurement.get_current_time_ms()
    Let start_memory be 0
    If config.enable_memory_profiling:
        Set start_memory to MemoryProfiler.get_current_usage_bytes()
    
    Test.print_info("Executing " + module_name + " module tests...")
    
    Try:
        Note: Execute appropriate test suite based on module name
        Let test_success be False
        
        If module_name equals "distributions":
            Set test_success to DistributionsTest.run_all_distributions_tests()
        Otherwise if module_name equals "sampling":
            Set test_success to SamplingTest.run_all_sampling_tests()
        Otherwise if module_name equals "bayesian":
            Set test_success to BayesianTest.run_all_bayesian_tests()
        Otherwise if module_name equals "markov":
            Set test_success to MarkovTest.run_all_markov_tests()
        Otherwise if module_name equals "stochastic":
            Set test_success to StochasticTest.run_all_stochastic_tests()
        Otherwise if module_name equals "information":
            Set test_success to InformationTest.run_all_information_tests()
        Otherwise:
            Throw Errors.InvalidArgument with "Unknown module: " + module_name
        
        Set result.test_passed to test_success
        
    Catch error as Errors:
        Set result.test_passed to False
        Append error.message to result.error_messages
        Test.print_error("Module " + module_name + " failed: " + error.message)
    
    Note: Calculate execution metrics
    Let end_time be TimeMeasurement.get_current_time_ms()
    Set result.execution_time_ms to end_time - start_time
    
    If config.enable_memory_profiling:
        Let end_memory be MemoryProfiler.get_current_usage_bytes()
        Set result.memory_used_bytes to end_memory - start_memory
    
    Note: Collect additional performance metrics if benchmarking enabled
    If config.enable_benchmarking:
        Set result.performance_metrics["avg_test_time_ms"] to Float(result.execution_time_ms) / Float(result.tests_run)
        Set result.performance_metrics["memory_efficiency"] to Float(result.memory_used_bytes) / Float(result.tests_run)
        Set result.performance_metrics["success_rate"] to Float(result.tests_passed) / Float(result.tests_run)
    
    Note: Log detailed results if enabled
    If config.save_detailed_logs:
        save_module_test_log(result)
    
    If result.test_passed:
        Test.print_success("Module " + module_name + " tests completed successfully")
    Otherwise:
        Test.print_error("Module " + module_name + " tests failed")
    
    Return result

Process called "save_module_test_log" that takes result as ModuleTestResult returns Boolean:
    Note: Save detailed test results to log file
    Let log_filename be "logs/probability_tests/" + result.module_name + "_test_log.txt"
    
    Let log_content be "PROBABILITY MODULE TEST LOG\n"
    Set log_content to log_content + "Module: " + result.module_name + "\n"
    Set log_content to log_content + "Execution Time: " + ToString(result.execution_time_ms) + " ms\n"
    Set log_content to log_content + "Memory Used: " + ToString(result.memory_used_bytes) + " bytes\n"
    Set log_content to log_content + "Tests Run: " + ToString(result.tests_run) + "\n"
    Set log_content to log_content + "Tests Passed: " + ToString(result.tests_passed) + "\n"
    Set log_content to log_content + "Tests Failed: " + ToString(result.tests_failed) + "\n"
    Set log_content to log_content + "Overall Result: " + If(result.test_passed, "PASSED", "FAILED") + "\n\n"
    
    If Length(result.error_messages) > 0:
        Set log_content to log_content + "ERROR MESSAGES:\n"
        For each error in result.error_messages:
            Set log_content to log_content + "- " + error + "\n"
        Set log_content to log_content + "\n"
    
    Set log_content to log_content + "PERFORMANCE METRICS:\n"
    For each metric in result.performance_metrics.keys():
        Set log_content to log_content + metric + ": " + ToString(result.performance_metrics[metric]) + "\n"
    
    Return FileSystem.write_text_file(log_filename, log_content)

Note: =====================================================================
Note: BENCHMARKING AND PERFORMANCE ANALYSIS
Note: =====================================================================

Process called "run_performance_benchmarks" that takes module_results as List[ModuleTestResult] returns Dictionary[String, Float]:
    Note: Analyze performance characteristics across all probability modules
    Let benchmarks be Collections.create_dictionary()
    
    Note: Calculate aggregate performance metrics
    Let total_execution_time be 0
    Let total_memory_used be 0
    Let total_tests_run be 0
    
    For each result in module_results:
        Set total_execution_time to total_execution_time + result.execution_time_ms
        Set total_memory_used to total_memory_used + result.memory_used_bytes
        Set total_tests_run to total_tests_run + result.tests_run
    
    Set benchmarks["total_execution_time_ms"] to Float(total_execution_time)
    Set benchmarks["total_memory_usage_bytes"] to Float(total_memory_used)
    Set benchmarks["average_test_time_ms"] to Float(total_execution_time) / Float(total_tests_run)
    Set benchmarks["memory_per_test_bytes"] to Float(total_memory_used) / Float(total_tests_run)
    
    Note: Find performance leaders and laggards
    Let fastest_module be ""
    Let fastest_time be 999999999
    Let slowest_module be ""
    Let slowest_time be 0
    
    For each result in module_results:
        Let avg_time_per_test be result.execution_time_ms / result.tests_run
        
        If avg_time_per_test < fastest_time:
            Set fastest_time to avg_time_per_test
            Set fastest_module to result.module_name
            
        If avg_time_per_test > slowest_time:
            Set slowest_time to avg_time_per_test
            Set slowest_module to result.module_name
    
    Set benchmarks["fastest_module_time_ms"] to Float(fastest_time)
    Set benchmarks["slowest_module_time_ms"] to Float(slowest_time)
    
    Note: Calculate memory efficiency metrics
    Let most_efficient_module be ""
    Let most_efficient_ratio be 999999999.0
    Let least_efficient_module be ""
    Let least_efficient_ratio be 0.0
    
    For each result in module_results:
        If result.tests_run > 0:
            Let memory_per_test be Float(result.memory_used_bytes) / Float(result.tests_run)
            
            If memory_per_test < most_efficient_ratio:
                Set most_efficient_ratio to memory_per_test
                Set most_efficient_module to result.module_name
                
            If memory_per_test > least_efficient_ratio:
                Set least_efficient_ratio to memory_per_test
                Set least_efficient_module to result.module_name
    
    Set benchmarks["most_efficient_memory_bytes_per_test"] to most_efficient_ratio
    Set benchmarks["least_efficient_memory_bytes_per_test"] to least_efficient_ratio
    
    Test.print_info("PERFORMANCE BENCHMARKS:")
    Test.print_info("Fastest module (avg per test): " + fastest_module + " (" + ToString(fastest_time) + " ms)")
    Test.print_info("Slowest module (avg per test): " + slowest_module + " (" + ToString(slowest_time) + " ms)")
    Test.print_info("Most memory efficient: " + most_efficient_module)
    Test.print_info("Least memory efficient: " + least_efficient_module)
    
    Return benchmarks

Process called "analyze_test_coverage" that takes module_results as List[ModuleTestResult] returns Dictionary[String, Float]:
    Note: Analyze test coverage across probability modules
    Let coverage be Collections.create_dictionary()
    
    Note: Calculate overall coverage statistics
    Let total_functions_covered be 0
    Let total_functions_available be 0
    Let modules_with_full_coverage be 0
    
    Note: Estimated function counts based on module complexity
    Let estimated_functions be Collections.create_dictionary()
    Set estimated_functions["distributions"] to 45.0  Note: PDF, CDF, quantile, fitting functions
    Set estimated_functions["sampling"] to 25.0      Note: Monte Carlo, MCMC, bootstrap methods
    Set estimated_functions["bayesian"] to 35.0      Note: Prior, posterior, MCMC, model selection
    Set estimated_functions["markov"] to 20.0        Note: Chain analysis, transition matrices
    Set estimated_functions["stochastic"] to 30.0    Note: Process simulation, SDE solvers
    Set estimated_functions["information"] to 25.0    Note: Entropy, MI, divergence measures
    
    For each result in module_results:
        If result.test_passed:
            Let estimated_count be estimated_functions[result.module_name]
            Let coverage_ratio be Float(result.tests_passed) / estimated_count
            
            Set coverage["coverage_" + result.module_name] to coverage_ratio
            Set total_functions_covered to total_functions_covered + result.tests_passed
            Set total_functions_available to total_functions_available + Integer(estimated_count)
            
            If coverage_ratio > 0.95:  Note: Consider >95% as full coverage
                Set modules_with_full_coverage to modules_with_full_coverage + 1
    
    Set coverage["overall_coverage"] to Float(total_functions_covered) / Float(total_functions_available)
    Set coverage["modules_with_full_coverage"] to Float(modules_with_full_coverage)
    Set coverage["coverage_completeness_ratio"] to Float(modules_with_full_coverage) / 6.0  Note: 6 total modules
    
    Return coverage

Note: =====================================================================
Note: REGRESSION TESTING AND VALIDATION
Note: =====================================================================

Process called "run_regression_tests" that takes current_results as List[ModuleTestResult] returns Boolean:
    Note: Compare current test results with previous baselines to detect regressions
    Test.print_info("Running regression analysis...")
    
    Note: Load previous test results if available
    Let baseline_file be "logs/probability_tests/baseline_results.json"
    Let baseline_exists be FileSystem.file_exists(baseline_file)
    
    If not baseline_exists:
        Test.print_info("No baseline results found - creating new baseline")
        save_baseline_results(current_results)
        Return True
    
    Let baseline_results be load_baseline_results(baseline_file)
    Let regression_detected be False
    
    Note: Compare each module against baseline
    For each current_result in current_results:
        Let baseline_result be find_baseline_result(baseline_results, current_result.module_name)
        
        If baseline_result != null:
            Note: Check for performance regressions (>20% slowdown)
            Let current_avg_time be Float(current_result.execution_time_ms) / Float(current_result.tests_run)
            Let baseline_avg_time be Float(baseline_result.execution_time_ms) / Float(baseline_result.tests_run)
            Let performance_ratio be current_avg_time / baseline_avg_time
            
            If performance_ratio > 1.2:
                Test.print_warning("Performance regression in " + current_result.module_name + ": " + ToString((performance_ratio - 1.0) * 100.0) + "% slower")
                Set regression_detected to True
            
            Note: Check for memory usage regressions (>25% increase)
            If baseline_result.memory_used_bytes > 0:
                Let memory_ratio be Float(current_result.memory_used_bytes) / Float(baseline_result.memory_used_bytes)
                If memory_ratio > 1.25:
                    Test.print_warning("Memory regression in " + current_result.module_name + ": " + ToString((memory_ratio - 1.0) * 100.0) + "% more memory")
                    Set regression_detected to True
            
            Note: Check for test count regressions
            If current_result.tests_run < baseline_result.tests_run:
                Test.print_warning("Test coverage regression in " + current_result.module_name + ": fewer tests run")
                Set regression_detected to True
    
    If not regression_detected:
        Test.print_success("No regressions detected - updating baseline")
        save_baseline_results(current_results)
    
    Return not regression_detected

Process called "save_baseline_results" that takes results as List[ModuleTestResult] returns Boolean:
    Note: Save current results as new baseline for future regression testing
    Let baseline_content be "{\n  \"baseline_timestamp\": \"" + TimeMeasurement.get_current_timestamp() + "\",\n  \"modules\": [\n"
    
    For i from 0 to Length(results) - 1:
        Let result be results[i]
        Set baseline_content to baseline_content + "    {\n"
        Set baseline_content to baseline_content + "      \"module_name\": \"" + result.module_name + "\",\n"
        Set baseline_content to baseline_content + "      \"execution_time_ms\": " + ToString(result.execution_time_ms) + ",\n"
        Set baseline_content to baseline_content + "      \"memory_used_bytes\": " + ToString(result.memory_used_bytes) + ",\n"
        Set baseline_content to baseline_content + "      \"tests_run\": " + ToString(result.tests_run) + ",\n"
        Set baseline_content to baseline_content + "      \"tests_passed\": " + ToString(result.tests_passed) + "\n"
        
        If i < Length(results) - 1:
            Set baseline_content to baseline_content + "    },\n"
        Otherwise:
            Set baseline_content to baseline_content + "    }\n"
    
    Set baseline_content to baseline_content + "  ]\n}"
    
    Return FileSystem.write_text_file("logs/probability_tests/baseline_results.json", baseline_content)

Process called "load_baseline_results" that takes filename as String returns List[ModuleTestResult]:
    Note: Load baseline results from file (simplified implementation)
    Note: In a real implementation, this would parse JSON
    Return List[ModuleTestResult]  Note: Return empty list for now

Process called "find_baseline_result" that takes baseline_results as List[ModuleTestResult], module_name as String returns ModuleTestResult:
    Note: Find baseline result for specific module
    For each result in baseline_results:
        If result.module_name equals module_name:
            Return result
    Return null

Note: =====================================================================
Note: COMPREHENSIVE REPORTING
Note: =====================================================================

Process called "generate_comprehensive_report" that takes suite_results as TestSuiteResults, config as TestSuiteConfiguration returns String:
    Note: Generate detailed test execution report
    Let report be "PROBABILITY MODULE TEST SUITE COMPREHENSIVE REPORT\n"
    Set report to report + StringOps.repeat("=", 60) + "\n\n"
    
    Note: Executive Summary
    Set report to report + "EXECUTIVE SUMMARY:\n"
    Set report to report + "Total Modules Tested: " + ToString(suite_results.total_modules_tested) + "\n"
    Set report to report + "Modules Passed: " + ToString(suite_results.modules_passed) + "\n"
    Set report to report + "Modules Failed: " + ToString(suite_results.modules_failed) + "\n"
    Set report to report + "Overall Success Rate: " + ToString(suite_results.overall_success_rate * 100.0) + "%\n"
    Set report to report + "Total Execution Time: " + ToString(suite_results.total_execution_time_ms) + " ms\n"
    
    If config.enable_memory_profiling:
        Set report to report + "Total Memory Used: " + ToString(suite_results.total_memory_used_bytes) + " bytes\n"
    
    Set report to report + "\n" + StringOps.repeat("-", 40) + "\n\n"
    
    Note: Individual Module Results
    Set report to report + "INDIVIDUAL MODULE RESULTS:\n"
    
    For each module_result in suite_results.module_results:
        Set report to report + "\n" + module_result.module_name.to_upper() + " MODULE:\n"
        Set report to report + "  Status: " + If(module_result.test_passed, "PASSED", "FAILED") + "\n"
        Set report to report + "  Execution Time: " + ToString(module_result.execution_time_ms) + " ms\n"
        Set report to report + "  Tests Run: " + ToString(module_result.tests_run) + "\n"
        Set report to report + "  Tests Passed: " + ToString(module_result.tests_passed) + "\n"
        
        If module_result.tests_failed > 0:
            Set report to report + "  Tests Failed: " + ToString(module_result.tests_failed) + "\n"
        
        If config.enable_memory_profiling:
            Set report to report + "  Memory Used: " + ToString(module_result.memory_used_bytes) + " bytes\n"
        
        If Length(module_result.error_messages) > 0:
            Set report to report + "  Errors:\n"
            For each error in module_result.error_messages:
                Set report to report + "    - " + error + "\n"
    
    Note: Performance Analysis
    If config.enable_benchmarking:
        Set report to report + "\n" + StringOps.repeat("-", 40) + "\n\n"
        Set report to report + "PERFORMANCE ANALYSIS:\n"
        
        For each metric in suite_results.performance_summary.keys():
            Set report to report + "  " + metric + ": " + ToString(suite_results.performance_summary[metric]) + "\n"
    
    Note: Recommendations
    Set report to report + "\n" + StringOps.repeat("-", 40) + "\n\n"
    Set report to report + "RECOMMENDATIONS:\n"
    
    If suite_results.modules_failed > 0:
        Set report to report + "- Address failing modules before production deployment\n"
    
    If suite_results.overall_success_rate < 0.95:
        Set report to report + "- Improve test coverage and reliability\n"
    
    Let avg_time_per_test be Float(suite_results.total_execution_time_ms) / Float(suite_results.total_modules_tested * 50)  Note: Estimate 50 tests per module
    If avg_time_per_test > 100.0:
        Set report to report + "- Consider optimizing slow test cases\n"
    
    Set report to report + "- Regular regression testing recommended\n"
    Set report to report + "- Monitor memory usage in production environment\n"
    
    Set report to report + "\nReport generated at: " + TimeMeasurement.get_current_timestamp() + "\n"
    
    Return report

Note: =====================================================================
Note: MAIN TEST SUITE EXECUTION
Note: =====================================================================

Process called "run_probability_test_suite" that takes config as TestSuiteConfiguration returns TestSuiteResults:
    Note: Execute the complete probability module test suite
    Let suite_results be TestSuiteResults
    Set suite_results.module_results to List[ModuleTestResult]
    Set suite_results.performance_summary to Collections.create_dictionary()
    
    Note: Initialize test environment
    If not initialize_test_environment(config):
        Test.print_error("Failed to initialize test environment")
        Set suite_results.total_modules_tested to 0
        Return suite_results
    
    Test.print_test_header("PROBABILITY MODULE COMPREHENSIVE TEST SUITE")
    Let suite_start_time be TimeMeasurement.get_current_time_ms()
    
    Note: Define modules to test in execution order
    Let modules_to_test be ["distributions", "sampling", "bayesian", "markov", "stochastic", "information"]
    Set suite_results.total_modules_tested to Length(modules_to_test)
    
    Note: Execute tests for each module
    Let total_execution_time be 0
    Let total_memory_used be 0
    Let modules_passed be 0
    
    For each module_name in modules_to_test:
        Test.print_test_section("Testing " + module_name + " module")
        
        Let module_result be execute_module_test(module_name, config)
        Append module_result to suite_results.module_results
        
        Set total_execution_time to total_execution_time + module_result.execution_time_ms
        Set total_memory_used to total_memory_used + module_result.memory_used_bytes
        
        If module_result.test_passed:
            Set modules_passed to modules_passed + 1
            Test.print_success("âœ“ " + module_name + " module tests passed")
        Otherwise:
            Test.print_error("âœ— " + module_name + " module tests failed")
    
    Note: Calculate overall results
    Set suite_results.modules_passed to modules_passed
    Set suite_results.modules_failed to suite_results.total_modules_tested - modules_passed
    Set suite_results.total_execution_time_ms to total_execution_time
    Set suite_results.total_memory_used_bytes to total_memory_used
    Set suite_results.overall_success_rate to Float(modules_passed) / Float(suite_results.total_modules_tested)
    
    Note: Run performance benchmarks if enabled
    If config.enable_benchmarking:
        Set suite_results.performance_summary to run_performance_benchmarks(suite_results.module_results)
    
    Note: Run coverage analysis if enabled
    If config.enable_coverage_analysis:
        Let coverage_results be analyze_test_coverage(suite_results.module_results)
        For each metric in coverage_results.keys():
            Set suite_results.performance_summary[metric] to coverage_results[metric]
    
    Note: Run regression tests
    Let no_regressions be run_regression_tests(suite_results.module_results)
    If not no_regressions:
        Test.print_warning("Regressions detected - see detailed logs")
    
    Let suite_end_time be TimeMeasurement.get_current_time_ms()
    Let total_suite_time be suite_end_time - suite_start_time
    
    Note: Generate and save comprehensive report
    Let comprehensive_report be generate_comprehensive_report(suite_results, config)
    
    If config.save_detailed_logs:
        Let report_saved be FileSystem.write_text_file("logs/probability_tests/comprehensive_report.txt", comprehensive_report)
        If report_saved:
            Test.print_info("Comprehensive report saved to logs/probability_tests/comprehensive_report.txt")
    
    Note: Print summary
    Test.print_test_footer("PROBABILITY TEST SUITE", suite_results.overall_success_rate == 1.0)
    Test.print_info("Suite execution completed in " + ToString(total_suite_time) + " ms")
    Test.print_info("Overall success rate: " + ToString(suite_results.overall_success_rate * 100.0) + "%")
    
    If suite_results.modules_failed == 0:
        Test.print_success("All probability modules passed their tests! ðŸŽ‰")
    Otherwise:
        Test.print_error(ToString(suite_results.modules_failed) + " module(s) failed - see detailed logs")
    
    Return suite_results

Note: =====================================================================
Note: COMMAND LINE INTERFACE AND ENTRY POINTS
Note: =====================================================================

Process called "run_with_custom_configuration" that takes enable_benchmarking as Boolean, enable_profiling as Boolean, verbose as Boolean returns Boolean:
    Note: Run test suite with custom configuration
    Let config be create_default_test_configuration()
    Set config.enable_benchmarking to enable_benchmarking
    Set config.enable_memory_profiling to enable_profiling
    Set config.verbose_output to verbose
    
    Let results be run_probability_test_suite(config)
    Return results.overall_success_rate == 1.0

Process called "run_single_module_test" that takes module_name as String returns Boolean:
    Note: Run tests for a single probability module
    Let config be create_default_test_configuration()
    Set config.verbose_output to True
    
    If not initialize_test_environment(config):
        Return False
    
    Test.print_info("Running single module test: " + module_name)
    Let result be execute_module_test(module_name, config)
    
    Return result.test_passed

Process called "run_quick_validation" that takes no parameters returns Boolean:
    Note: Run quick validation tests (minimal configuration)
    Let config be create_default_test_configuration()
    Set config.enable_benchmarking to False
    Set config.enable_memory_profiling to False
    Set config.enable_coverage_analysis to False
    Set config.save_detailed_logs to False
    Set config.verbose_output to False
    
    Let results be run_probability_test_suite(config)
    Return results.modules_failed == 0

Note: =====================================================================
Note: DEFAULT EXECUTION ENTRY POINT
Note: =====================================================================

Note: Default execution with comprehensive configuration
Let default_config be create_default_test_configuration()
Let test_suite_results be run_probability_test_suite(default_config)

Note: Exit with appropriate code
If test_suite_results.overall_success_rate == 1.0:
    Test.print_success("All probability module tests completed successfully!")
    Test.exit_with_code(0)
Otherwise:
    Test.print_failure("Probability module test suite completed with failures")
    Test.print_info("Failed modules: " + ToString(test_suite_results.modules_failed) + "/" + ToString(test_suite_results.total_modules_tested))
    Test.exit_with_code(1)