Note: Comprehensive Unit Tests for Machine Learning Metrics Module
Note: Tests classification metrics (accuracy, precision, recall, F1, ROC-AUC),
Note: regression metrics (RMSE, R-squared, MAE), ranking metrics (NDCG, MAP),
Note: clustering metrics, and advanced ML evaluation metrics.
Note: Validates numerical accuracy, edge cases, and metric properties.

Import "math/ai_math/metrics" as Metrics
Import "math/core/operations" as MathOps
Import "math/engine/linalg/core" as LinAlg
Import "dev/debug/errors/core" as Errors

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as String
    expected_value as String
    actual_value as String

Type called "TestSuite":
    suite_name as String
    total_tests as Integer
    passed_tests as Integer
    failed_tests as Integer
    results as List[TestResult]

Note: ===== Test Helper Functions =====

Process called "assert_equals_float" that takes actual as String, expected as String, tolerance as String, test_name as String returns TestResult:
    Note: Assert that two floating point values are equal within tolerance
    
    Let diff_result be MathOps.subtract(actual, expected, 15)
    If diff_result.overflow_occurred:
        Return TestResult with test_name: test_name, passed: false, error_message: "Overflow in difference calculation", execution_time: "0", expected_value: expected, actual_value: actual
    
    Let abs_diff be MathOps.absolute_value(diff_result.result_value)
    Let comparison be MathOps.compare(abs_diff.result_value, tolerance)
    
    If comparison <= 0:
        Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected, actual_value: actual
    Otherwise:
        Return TestResult with test_name: test_name, passed: false, error_message: "Values differ by more than tolerance", execution_time: "0", expected_value: expected, actual_value: actual

Process called "assert_equals_int" that takes actual as Integer, expected as Integer, test_name as String returns TestResult:
    Note: Assert that two integers are equal
    
    If actual == expected:
        Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected.to_string(), actual_value: actual.to_string()
    Otherwise:
        Return TestResult with test_name: test_name, passed: false, error_message: "Integer values do not match", execution_time: "0", expected_value: expected.to_string(), actual_value: actual.to_string()

Process called "assert_vector_equals" that takes actual as Vector[Float], expected as Vector[Float], tolerance as String, test_name as String returns TestResult:
    Note: Assert that two vectors are equal within tolerance
    
    If actual.dimension != expected.dimension:
        Return TestResult with test_name: test_name, passed: false, error_message: "Vector dimensions do not match", execution_time: "0", expected_value: "Vector dimensions mismatch", actual_value: "Vector dimensions mismatch"
    
    Let i be 0
    While i < actual.dimension:
        Let actual_val be actual.components.get(i)
        Let expected_val be expected.components.get(i)
        
        Let element_test be assert_equals_float(actual_val, expected_val, tolerance, test_name + " element check")
        If not element_test.passed:
            Return element_test
        Set i to i + 1
    
    Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: "Vector match", actual_value: "Vector match"

Process called "create_test_vector_int" that takes values as List[Integer] returns Vector[Integer]:
    Note: Create integer vector from list
    Let components be List[String]()
    Let i be 0
    While i < values.length:
        Call components.add(values.get(i).to_string())
        Set i to i + 1
    Return LinAlg.create_vector_int(values)

Process called "create_test_vector_float" that takes values as List[String] returns Vector[Float]:
    Note: Create float vector from string list
    Return LinAlg.create_vector(values, "float")

Note: ===== Classification Metrics Tests =====

Process called "test_accuracy_score" that takes no parameters returns TestResult:
    Note: Test accuracy score computation
    
    Try:
        Note: Create test predictions: [0, 1, 1, 0] vs [0, 1, 0, 0]
        Note: Correct predictions: positions 0, 1, 3 = 3/4 = 0.75
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)  Note: Correct
        Call y_pred_values.add(1)  Note: Correct
        Call y_pred_values.add(0)  Note: Incorrect
        Call y_pred_values.add(0)  Note: Correct
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let accuracy be Metrics.accuracy_score(y_true, y_pred, true)
        
        Let expected_accuracy be "0.75"
        Let accuracy_check be assert_equals_float(accuracy.to_string(), expected_accuracy, "0.001", "accuracy computation")
        If not accuracy_check.passed:
            Return accuracy_check
        
        Note: Test unnormalized accuracy (should return count)
        Let raw_accuracy be Metrics.accuracy_score(y_true, y_pred, false)
        Let expected_raw be "3.0"
        Let raw_check be assert_equals_float(raw_accuracy.to_string(), expected_raw, "0.001", "raw accuracy")
        If not raw_check.passed:
            Return raw_check
        
        Return TestResult with test_name: "test_accuracy_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid accuracy", actual_value: "Valid accuracy"
    
    Catch error:
        Return TestResult with test_name: "test_accuracy_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_precision_score" that takes no parameters returns TestResult:
    Note: Test precision score computation
    
    Try:
        Note: Binary classification test case
        Note: True: [1, 0, 1, 1, 0] Pred: [1, 0, 1, 0, 0]
        Note: TP = 2, FP = 0, Precision = 2/(2+0) = 1.0
        Let y_true_values be List[Integer]()
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(1)  Note: TP
        Call y_pred_values.add(0)  Note: TN
        Call y_pred_values.add(1)  Note: TP
        Call y_pred_values.add(0)  Note: FN
        Call y_pred_values.add(0)  Note: TN
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let config be Metrics.MetricConfig with average: "binary", labels: None, pos_label: None, zero_division: "warn", sample_weight: None
        
        Let precision be Metrics.precision_score(y_true, y_pred, config)
        
        Note: Precision = TP/(TP+FP) = 2/(2+0) = 1.0
        Let expected_precision be "1.0"
        Let precision_check be assert_equals_float(precision.to_string(), expected_precision, "0.001", "binary precision")
        If not precision_check.passed:
            Return precision_check
        
        Return TestResult with test_name: "test_precision_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid precision", actual_value: "Valid precision"
    
    Catch error:
        Return TestResult with test_name: "test_precision_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_recall_score" that takes no parameters returns TestResult:
    Note: Test recall score computation
    
    Try:
        Note: Same test case as precision
        Note: True: [1, 0, 1, 1, 0] Pred: [1, 0, 1, 0, 0]  
        Note: TP = 2, FN = 1, Recall = 2/(2+1) = 0.667
        Let y_true_values be List[Integer]()
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(1)  Note: TP
        Call y_pred_values.add(0)  Note: TN
        Call y_pred_values.add(1)  Note: TP
        Call y_pred_values.add(0)  Note: FN
        Call y_pred_values.add(0)  Note: TN
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let config be Metrics.MetricConfig with average: "binary", labels: None, pos_label: None, zero_division: "warn", sample_weight: None
        
        Let recall be Metrics.recall_score(y_true, y_pred, config)
        
        Note: Recall = TP/(TP+FN) = 2/(2+1) = 0.667
        Let expected_recall be "0.667"
        Let recall_check be assert_equals_float(recall.to_string(), expected_recall, "0.01", "binary recall")
        If not recall_check.passed:
            Return recall_check
        
        Return TestResult with test_name: "test_recall_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid recall", actual_value: "Valid recall"
    
    Catch error:
        Return TestResult with test_name: "test_recall_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_f1_score" that takes no parameters returns TestResult:
    Note: Test F1 score computation
    
    Try:
        Note: Using same test case
        Note: Precision = 1.0, Recall = 0.667, F1 = 2*P*R/(P+R) = 2*1.0*0.667/(1.0+0.667) = 0.8
        Let y_true_values be List[Integer]()
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(1)
        Call y_pred_values.add(0)
        Call y_pred_values.add(1)
        Call y_pred_values.add(0)
        Call y_pred_values.add(0)
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let config be Metrics.MetricConfig with average: "binary", labels: None, pos_label: None, zero_division: "warn", sample_weight: None
        
        Let f1 be Metrics.f1_score(y_true, y_pred, config)
        
        Note: F1 = 2*precision*recall/(precision+recall) = 2*1.0*0.667/(1.667) ≈ 0.8
        Let expected_f1 be "0.8"
        Let f1_check be assert_equals_float(f1.to_string(), expected_f1, "0.01", "F1 score")
        If not f1_check.passed:
            Return f1_check
        
        Return TestResult with test_name: "test_f1_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid F1 score", actual_value: "Valid F1 score"
    
    Catch error:
        Return TestResult with test_name: "test_f1_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_confusion_matrix" that takes no parameters returns TestResult:
    Note: Test confusion matrix computation
    
    Try:
        Note: Simple 2x2 confusion matrix test
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)  Note: TN
        Call y_pred_values.add(1)  Note: FP
        Call y_pred_values.add(0)  Note: FN
        Call y_pred_values.add(1)  Note: TP
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let cm be Metrics.confusion_matrix(y_true, y_pred, None)
        
        Note: Expected confusion matrix:
        Note: [[1, 1],   (TN=1, FP=1)
        Note:  [1, 1]]   (FN=1, TP=1)
        
        Note: Check matrix dimensions
        If cm.matrix.rows != 2 or cm.matrix.columns != 2:
            Return TestResult with test_name: "test_confusion_matrix", passed: false, error_message: "Confusion matrix dimensions incorrect", execution_time: "0", expected_value: "2x2", actual_value: cm.matrix.rows.to_string() + "x" + cm.matrix.columns.to_string()
        
        Note: Check TN (position [0,0])
        Let tn_val be cm.matrix.entries.get(0).get(0)
        Let tn_check be assert_equals_int(Parse tn_val as Integer, 1, "TN value")
        If not tn_check.passed:
            Return tn_check
        
        Note: Check TP (position [1,1])
        Let tp_val be cm.matrix.entries.get(1).get(1)
        Let tp_check be assert_equals_int(Parse tp_val as Integer, 1, "TP value")
        If not tp_check.passed:
            Return tp_check
        
        Return TestResult with test_name: "test_confusion_matrix", passed: true, error_message: "", execution_time: "0", expected_value: "Valid confusion matrix", actual_value: "Valid confusion matrix"
    
    Catch error:
        Return TestResult with test_name: "test_confusion_matrix", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_matthews_correlation_coefficient" that takes no parameters returns TestResult:
    Note: Test Matthews Correlation Coefficient
    
    Try:
        Note: Perfect predictions should give MCC = 1
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_perfect be create_test_vector_int(y_true_values)  Note: Same as true
        
        Let mcc_perfect be Metrics.matthews_corrcoef(y_true, y_pred_perfect)
        
        Note: Perfect predictions should have MCC = 1.0
        Let perfect_check be assert_equals_float(mcc_perfect.to_string(), "1.0", "0.01", "perfect MCC")
        If not perfect_check.passed:
            Return perfect_check
        
        Note: Test with some errors
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)
        Call y_pred_values.add(1)
        Call y_pred_values.add(0)  Note: Error here
        Call y_pred_values.add(0)
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let mcc_imperfect be Metrics.matthews_corrcoef(y_true, y_pred)
        
        Note: Should be between -1 and 1
        Let mcc_val be Parse mcc_imperfect.to_string() as Float
        If mcc_val < -1.1 or mcc_val > 1.1:
            Return TestResult with test_name: "test_matthews_correlation_coefficient", passed: false, error_message: "MCC out of valid range", execution_time: "0", expected_value: "[-1, 1]", actual_value: mcc_imperfect.to_string()
        
        Return TestResult with test_name: "test_matthews_correlation_coefficient", passed: true, error_message: "", execution_time: "0", expected_value: "Valid MCC", actual_value: "Valid MCC"
    
    Catch error:
        Return TestResult with test_name: "test_matthews_correlation_coefficient", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Regression Metrics Tests =====

Process called "test_mean_absolute_error" that takes no parameters returns TestResult:
    Note: Test Mean Absolute Error computation
    
    Try:
        Note: True: [1.0, 2.0, 3.0] Pred: [1.1, 1.9, 3.2]
        Note: Errors: [0.1, 0.1, 0.2] MAE = (0.1 + 0.1 + 0.2) / 3 = 0.133
        Let y_true_values be List[String]()
        Call y_true_values.add("1.0")
        Call y_true_values.add("2.0")
        Call y_true_values.add("3.0")
        Let y_true be create_test_vector_float(y_true_values)
        
        Let y_pred_values be List[String]()
        Call y_pred_values.add("1.1")
        Call y_pred_values.add("1.9")
        Call y_pred_values.add("3.2")
        Let y_pred be create_test_vector_float(y_pred_values)
        
        Let mae be Metrics.mean_absolute_error(y_true, y_pred, None)
        
        Note: Expected MAE = (|1.0-1.1| + |2.0-1.9| + |3.0-3.2|) / 3 = (0.1 + 0.1 + 0.2) / 3 = 0.133
        Let expected_mae be "0.133"
        Let mae_check be assert_equals_float(mae.to_string(), expected_mae, "0.01", "MAE computation")
        If not mae_check.passed:
            Return mae_check
        
        Return TestResult with test_name: "test_mean_absolute_error", passed: true, error_message: "", execution_time: "0", expected_value: "Valid MAE", actual_value: "Valid MAE"
    
    Catch error:
        Return TestResult with test_name: "test_mean_absolute_error", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_mean_squared_error" that takes no parameters returns TestResult:
    Note: Test Mean Squared Error computation
    
    Try:
        Note: Using same data as MAE test
        Note: True: [1.0, 2.0, 3.0] Pred: [1.1, 1.9, 3.2]
        Note: Squared errors: [0.01, 0.01, 0.04] MSE = (0.01 + 0.01 + 0.04) / 3 = 0.02
        Let y_true_values be List[String]()
        Call y_true_values.add("1.0")
        Call y_true_values.add("2.0")
        Call y_true_values.add("3.0")
        Let y_true be create_test_vector_float(y_true_values)
        
        Let y_pred_values be List[String]()
        Call y_pred_values.add("1.1")
        Call y_pred_values.add("1.9")
        Call y_pred_values.add("3.2")
        Let y_pred be create_test_vector_float(y_pred_values)
        
        Let mse be Metrics.mean_squared_error(y_true, y_pred, None)
        
        Note: Expected MSE = (0.1² + 0.1² + 0.2²) / 3 = (0.01 + 0.01 + 0.04) / 3 = 0.02
        Let expected_mse be "0.02"
        Let mse_check be assert_equals_float(mse.to_string(), expected_mse, "0.001", "MSE computation")
        If not mse_check.passed:
            Return mse_check
        
        Return TestResult with test_name: "test_mean_squared_error", passed: true, error_message: "", execution_time: "0", expected_value: "Valid MSE", actual_value: "Valid MSE"
    
    Catch error:
        Return TestResult with test_name: "test_mean_squared_error", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_root_mean_squared_error" that takes no parameters returns TestResult:
    Note: Test Root Mean Squared Error computation
    
    Try:
        Let y_true_values be List[String]()
        Call y_true_values.add("1.0")
        Call y_true_values.add("2.0")
        Call y_true_values.add("3.0")
        Let y_true be create_test_vector_float(y_true_values)
        
        Let y_pred_values be List[String]()
        Call y_pred_values.add("1.1")
        Call y_pred_values.add("1.9")
        Call y_pred_values.add("3.2")
        Let y_pred be create_test_vector_float(y_pred_values)
        
        Let rmse be Metrics.root_mean_squared_error(y_true, y_pred)
        
        Note: Expected RMSE = sqrt(MSE) = sqrt(0.02) ≈ 0.141
        Let expected_rmse be "0.141"
        Let rmse_check be assert_equals_float(rmse.to_string(), expected_rmse, "0.01", "RMSE computation")
        If not rmse_check.passed:
            Return rmse_check
        
        Return TestResult with test_name: "test_root_mean_squared_error", passed: true, error_message: "", execution_time: "0", expected_value: "Valid RMSE", actual_value: "Valid RMSE"
    
    Catch error:
        Return TestResult with test_name: "test_root_mean_squared_error", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_r2_score" that takes no parameters returns TestResult:
    Note: Test R-squared coefficient of determination
    
    Try:
        Note: Perfect predictions should give R² = 1
        Let y_true_values be List[String]()
        Call y_true_values.add("1.0")
        Call y_true_values.add("2.0")
        Call y_true_values.add("3.0")
        Call y_true_values.add("4.0")
        Let y_true be create_test_vector_float(y_true_values)
        
        Let y_pred_perfect be create_test_vector_float(y_true_values)  Note: Perfect predictions
        
        Let r2_perfect be Metrics.r2_score(y_true, y_pred_perfect)
        
        Note: Perfect predictions should have R² = 1.0
        Let perfect_check be assert_equals_float(r2_perfect.to_string(), "1.0", "0.01", "perfect R²")
        If not perfect_check.passed:
            Return perfect_check
        
        Note: Test with imperfect predictions
        Let y_pred_values be List[String]()
        Call y_pred_values.add("1.1")
        Call y_pred_values.add("2.1")
        Call y_pred_values.add("2.9")
        Call y_pred_values.add("3.9")
        Let y_pred be create_test_vector_float(y_pred_values)
        
        Let r2_imperfect be Metrics.r2_score(y_true, y_pred)
        
        Note: Should be positive and less than 1 for reasonable predictions
        Let r2_val be Parse r2_imperfect.to_string() as Float
        If r2_val <= 0.0 or r2_val >= 1.0:
            Return TestResult with test_name: "test_r2_score", passed: false, error_message: "R² should be between 0 and 1 for reasonable predictions", execution_time: "0", expected_value: "(0, 1)", actual_value: r2_imperfect.to_string()
        
        Return TestResult with test_name: "test_r2_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid R² score", actual_value: "Valid R² score"
    
    Catch error:
        Return TestResult with test_name: "test_r2_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_mean_absolute_percentage_error" that takes no parameters returns TestResult:
    Note: Test Mean Absolute Percentage Error
    
    Try:
        Note: True: [100, 200, 300] Pred: [110, 190, 330]
        Note: Percentage errors: [10%, 5%, 10%] MAPE = (10 + 5 + 10) / 3 = 8.33%
        Let y_true_values be List[String]()
        Call y_true_values.add("100.0")
        Call y_true_values.add("200.0")
        Call y_true_values.add("300.0")
        Let y_true be create_test_vector_float(y_true_values)
        
        Let y_pred_values be List[String]()
        Call y_pred_values.add("110.0")  Note: 10% error
        Call y_pred_values.add("190.0")  Note: 5% error
        Call y_pred_values.add("330.0")  Note: 10% error
        Let y_pred be create_test_vector_float(y_pred_values)
        
        Let mape be Metrics.mean_absolute_percentage_error(y_true, y_pred)
        
        Note: Expected MAPE = (|100-110|/100 + |200-190|/200 + |300-330|/300) / 3 
        Note:                = (0.1 + 0.05 + 0.1) / 3 = 0.0833 = 8.33%
        Let expected_mape be "0.0833"
        Let mape_check be assert_equals_float(mape.to_string(), expected_mape, "0.01", "MAPE computation")
        If not mape_check.passed:
            Return mape_check
        
        Return TestResult with test_name: "test_mean_absolute_percentage_error", passed: true, error_message: "", execution_time: "0", expected_value: "Valid MAPE", actual_value: "Valid MAPE"
    
    Catch error:
        Return TestResult with test_name: "test_mean_absolute_percentage_error", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== ROC and AUC Tests =====

Process called "test_roc_auc_score" that takes no parameters returns TestResult:
    Note: Test ROC-AUC score computation
    
    Try:
        Note: Simple test case where perfect classifier should give AUC = 1
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Let y_true be create_test_vector_int(y_true_values)
        
        Note: Perfect scores: low for class 0, high for class 1
        Let y_score_values be List[String]()
        Call y_score_values.add("0.1")
        Call y_score_values.add("0.2")
        Call y_score_values.add("0.8")
        Call y_score_values.add("0.9")
        Let y_score be create_test_vector_float(y_score_values)
        
        Let config be Metrics.MetricConfig with average: "binary", labels: None, pos_label: None, zero_division: "warn", sample_weight: None
        
        Let auc_score be Metrics.roc_auc_score(y_true, y_score, config)
        
        Note: Perfect separation should give AUC = 1.0
        Let expected_auc be "1.0"
        Let auc_check be assert_equals_float(auc_score.to_string(), expected_auc, "0.01", "perfect AUC")
        If not auc_check.passed:
            Return auc_check
        
        Return TestResult with test_name: "test_roc_auc_score", passed: true, error_message: "", execution_time: "0", expected_value: "Valid AUC score", actual_value: "Valid AUC score"
    
    Catch error:
        Return TestResult with test_name: "test_roc_auc_score", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_roc_curve" that takes no parameters returns TestResult:
    Note: Test ROC curve computation
    
    Try:
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_score_values be List[String]()
        Call y_score_values.add("0.1")
        Call y_score_values.add("0.4")
        Call y_score_values.add("0.6")
        Call y_score_values.add("0.9")
        Let y_score be create_test_vector_float(y_score_values)
        
        Let roc_result be Metrics.roc_curve(y_true, y_score, None)
        
        Note: Check that ROC curve has reasonable properties
        If roc_result.fpr.dimension != roc_result.tpr.dimension:
            Return TestResult with test_name: "test_roc_curve", passed: false, error_message: "FPR and TPR dimensions mismatch", execution_time: "0", expected_value: "Same dimensions", actual_value: "Different dimensions"
        
        Note: First point should be (0, 0) and last should be (1, 1)
        Let first_fpr be roc_result.fpr.components.get(0)
        Let first_tpr be roc_result.tpr.components.get(0)
        Let first_check be assert_equals_float(first_fpr, "0.0", "0.01", "first FPR")
        If not first_check.passed:
            Return first_check
        
        Let first_tpr_check be assert_equals_float(first_tpr, "0.0", "0.01", "first TPR")
        If not first_tpr_check.passed:
            Return first_tpr_check
        
        Note: AUC should be between 0 and 1
        Let auc_val be Parse roc_result.auc.to_string() as Float
        If auc_val < 0.0 or auc_val > 1.0:
            Return TestResult with test_name: "test_roc_curve", passed: false, error_message: "AUC out of valid range", execution_time: "0", expected_value: "[0, 1]", actual_value: roc_result.auc.to_string()
        
        Return TestResult with test_name: "test_roc_curve", passed: true, error_message: "", execution_time: "0", expected_value: "Valid ROC curve", actual_value: "Valid ROC curve"
    
    Catch error:
        Return TestResult with test_name: "test_roc_curve", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Multiclass Metrics Tests =====

Process called "test_multiclass_precision_macro" that takes no parameters returns TestResult:
    Note: Test macro-averaged precision for multiclass
    
    Try:
        Note: 3-class problem: True=[0,1,2,0,1,2] Pred=[0,1,2,1,2,0]
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(1) 
        Call y_true_values.add(2)
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(2)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)  Note: Correct
        Call y_pred_values.add(1)  Note: Correct
        Call y_pred_values.add(2)  Note: Correct
        Call y_pred_values.add(1)  Note: Incorrect (true=0, pred=1)
        Call y_pred_values.add(2)  Note: Incorrect (true=1, pred=2)
        Call y_pred_values.add(0)  Note: Incorrect (true=2, pred=0)
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let config be Metrics.MetricConfig with average: "macro", labels: None, pos_label: None, zero_division: "0", sample_weight: None
        
        Let precision_macro be Metrics.precision_score(y_true, y_pred, config)
        
        Note: Should compute precision for each class and average
        Let precision_val be Parse precision_macro.to_string() as Float
        If precision_val < 0.0 or precision_val > 1.0:
            Return TestResult with test_name: "test_multiclass_precision_macro", passed: false, error_message: "Precision out of valid range", execution_time: "0", expected_value: "[0, 1]", actual_value: precision_macro.to_string()
        
        Return TestResult with test_name: "test_multiclass_precision_macro", passed: true, error_message: "", execution_time: "0", expected_value: "Valid macro precision", actual_value: "Valid macro precision"
    
    Catch error:
        Return TestResult with test_name: "test_multiclass_precision_macro", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_classification_report" that takes no parameters returns TestResult:
    Note: Test comprehensive classification report
    
    Try:
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Call y_true_values.add(1)
        Call y_true_values.add(0)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)
        Call y_pred_values.add(1)
        Call y_pred_values.add(0)  Note: Error here
        Call y_pred_values.add(0)
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Let labels be List[String]()
        Call labels.add("class_0")
        Call labels.add("class_1")
        Let label_vector be LinAlg.create_vector_string(labels)
        
        Let report be Metrics.classification_report(y_true, y_pred, label_vector)
        
        Note: Check that report has correct structure
        If report.precision.dimension != 2:
            Return TestResult with test_name: "test_classification_report", passed: false, error_message: "Precision vector wrong size", execution_time: "0", expected_value: "2", actual_value: report.precision.dimension.to_string()
        
        If report.recall.dimension != 2:
            Return TestResult with test_name: "test_classification_report", passed: false, error_message: "Recall vector wrong size", execution_time: "0", expected_value: "2", actual_value: report.recall.dimension.to_string()
        
        Note: Overall accuracy should be reasonable
        Let accuracy_val be Parse report.accuracy.to_string() as Float
        If accuracy_val < 0.0 or accuracy_val > 1.0:
            Return TestResult with test_name: "test_classification_report", passed: false, error_message: "Accuracy out of valid range", execution_time: "0", expected_value: "[0, 1]", actual_value: report.accuracy.to_string()
        
        Return TestResult with test_name: "test_classification_report", passed: true, error_message: "", execution_time: "0", expected_value: "Valid classification report", actual_value: "Valid classification report"
    
    Catch error:
        Return TestResult with test_name: "test_classification_report", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Error Handling Tests =====

Process called "test_empty_input_errors" that takes no parameters returns TestResult:
    Note: Test error handling for empty inputs
    
    Try:
        Let empty_int_list be List[Integer]()
        Let empty_true be create_test_vector_int(empty_int_list)
        Let empty_pred be create_test_vector_int(empty_int_list)
        
        Try:
            Let accuracy be Metrics.accuracy_score(empty_true, empty_pred, true)
            Return TestResult with test_name: "test_empty_input_errors", passed: false, error_message: "Should have thrown error for empty input", execution_time: "0", expected_value: "Error", actual_value: "No error"
        Catch empty_error:
            Note: Expected behavior
            Pass
        
        Return TestResult with test_name: "test_empty_input_errors", passed: true, error_message: "", execution_time: "0", expected_value: "Proper error handling", actual_value: "Proper error handling"
    
    Catch error:
        Return TestResult with test_name: "test_empty_input_errors", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "Controlled errors only", actual_value: "Unexpected exception"

Process called "test_dimension_mismatch_errors" that takes no parameters returns TestResult:
    Note: Test error handling for dimension mismatches
    
    Try:
        Let y_true_values be List[Integer]()
        Call y_true_values.add(0)
        Call y_true_values.add(1)
        Let y_true be create_test_vector_int(y_true_values)
        
        Let y_pred_values be List[Integer]()
        Call y_pred_values.add(0)
        Call y_pred_values.add(1)
        Call y_pred_values.add(1)  Note: Extra element
        Let y_pred be create_test_vector_int(y_pred_values)
        
        Try:
            Let accuracy be Metrics.accuracy_score(y_true, y_pred, true)
            Return TestResult with test_name: "test_dimension_mismatch_errors", passed: false, error_message: "Should have thrown error for dimension mismatch", execution_time: "0", expected_value: "Error", actual_value: "No error"
        Catch mismatch_error:
            Note: Expected behavior
            Pass
        
        Return TestResult with test_name: "test_dimension_mismatch_errors", passed: true, error_message: "", execution_time: "0", expected_value: "Proper error handling", actual_value: "Proper error handling"
    
    Catch error:
        Return TestResult with test_name: "test_dimension_mismatch_errors", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "Controlled errors only", actual_value: "Unexpected exception"

Note: ===== Test Suite Management =====

Process called "create_test_suite" that takes suite_name as String returns TestSuite:
    Note: Create a new test suite for organizing test results
    Return TestSuite with suite_name: suite_name, total_tests: 0, passed_tests: 0, failed_tests: 0, results: List[TestResult]()

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns TestSuite:
    Note: Add a test result to the test suite
    
    Call suite.results.add(result)
    Set suite.total_tests to suite.total_tests + 1
    
    If result.passed:
        Set suite.passed_tests to suite.passed_tests + 1
    Otherwise:
        Set suite.failed_tests to suite.failed_tests + 1
    
    Return suite

Process called "run_all_metrics_tests" that takes no parameters returns TestSuite:
    Note: Run all machine learning metrics tests and return comprehensive results
    
    Let suite be create_test_suite("Machine Learning Metrics Test Suite")
    
    Note: Classification metrics tests
    Set suite to add_test_result(suite, test_accuracy_score())
    Set suite to add_test_result(suite, test_precision_score())
    Set suite to add_test_result(suite, test_recall_score())
    Set suite to add_test_result(suite, test_f1_score())
    Set suite to add_test_result(suite, test_confusion_matrix())
    Set suite to add_test_result(suite, test_matthews_correlation_coefficient())
    
    Note: Regression metrics tests
    Set suite to add_test_result(suite, test_mean_absolute_error())
    Set suite to add_test_result(suite, test_mean_squared_error())
    Set suite to add_test_result(suite, test_root_mean_squared_error())
    Set suite to add_test_result(suite, test_r2_score())
    Set suite to add_test_result(suite, test_mean_absolute_percentage_error())
    
    Note: ROC and AUC tests
    Set suite to add_test_result(suite, test_roc_auc_score())
    Set suite to add_test_result(suite, test_roc_curve())
    
    Note: Multiclass metrics tests
    Set suite to add_test_result(suite, test_multiclass_precision_macro())
    Set suite to add_test_result(suite, test_classification_report())
    
    Note: Error handling tests
    Set suite to add_test_result(suite, test_empty_input_errors())
    Set suite to add_test_result(suite, test_dimension_mismatch_errors())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns String:
    Note: Format test results for display
    
    Let output be "=== " + suite.suite_name + " ===" + "\n"
    Set output to output + "Total tests: " + suite.total_tests.to_string() + "\n"
    Set output to output + "Passed: " + suite.passed_tests.to_string() + "\n"
    Set output to output + "Failed: " + suite.failed_tests.to_string() + "\n\n"
    
    If suite.failed_tests > 0:
        Set output to output + "Failed tests:" + "\n"
        Let i be 0
        While i < suite.results.length:
            Let result be suite.results.get(i)
            If not result.passed:
                Set output to output + "- " + result.test_name + ": " + result.error_message + "\n"
            Set i to i + 1
    
    Let success_rate_num be suite.passed_tests * 100
    Let success_rate be success_rate_num / suite.total_tests
    Set output to output + "\nSuccess rate: " + success_rate.to_string() + "%"
    
    Return output

Note: ===== Main Test Runner =====

Process called "main" that takes no parameters returns Integer:
    Note: Main test runner for machine learning metrics module
    
    Display "Running comprehensive tests for machine learning metrics..."
    Display ""
    
    Let test_suite be run_all_metrics_tests()
    Let results_output be print_test_results(test_suite)
    
    Display results_output
    
    If test_suite.failed_tests == 0:
        Display ""
        Display "All machine learning metrics tests passed successfully!"
        Return 0
    Otherwise:
        Display ""
        Display "Some machine learning metrics tests failed. Please review the failures above."
        Return 1