Note:
tests/unit/libraries/math/ai_math/neural_ops_test.runa
Comprehensive tests for Neural Network Operations

This test suite provides comprehensive testing for neural network operations including
activation functions, forward and backward propagation, weight initialization strategies,
batch normalization, dropout regularization, and fundamental layer operations for
modern deep learning applications.
:End Note

Import "math/ai_math/neural_ops" as NeuralOps
Import "math/core/operations" as MathOps
Import "math/engine/linalg/core" as LinAlg
Import "collections" as Collections

Note: =====================================================================
Note: TEST HELPER FUNCTIONS
Note: =====================================================================

Process called "assert_float_equals" that takes actual as Float, expected as Float, tolerance as Float returns Boolean:
    Let difference be MathOps.abs(MathOps.subtract(actual.to_string(), expected.to_string(), 50).result_value).result_value
    Let diff_float be MathOps.string_to_float(difference, 50).result_value
    If diff_float <= tolerance:
        Return true
    Otherwise:
        Print("Expected: " + expected.to_string() + ", Actual: " + actual.to_string() + ", Difference: " + diff_float.to_string())
        Return false

Process called "assert_vector_equals" that takes actual as Vector[Float], expected as Vector[Float], tolerance as Float returns Boolean:
    If actual.dimension != expected.dimension:
        Print("Vector dimensions don't match: " + actual.dimension.to_string() + " vs " + expected.dimension.to_string())
        Return false
    
    Let i be 0
    While i < actual.dimension:
        Let actual_val be MathOps.string_to_float(actual.components.get(i), 50).result_value
        Let expected_val be MathOps.string_to_float(expected.components.get(i), 50).result_value
        If !assert_float_equals(actual_val, expected_val, tolerance):
            Print("Vector element " + i.to_string() + " mismatch")
            Return false
        Set i to i + 1
    
    Return true

Process called "assert_matrix_equals" that takes actual as Matrix[Float], expected as Matrix[Float], tolerance as Float returns Boolean:
    If actual.rows != expected.rows || actual.columns != expected.columns:
        Print("Matrix dimensions don't match")
        Return false
    
    Let i be 0
    While i < actual.rows:
        Let j be 0
        While j < actual.columns:
            Let actual_val be MathOps.string_to_float(actual.elements.get(i).get(j), 50).result_value
            Let expected_val be MathOps.string_to_float(expected.elements.get(i).get(j), 50).result_value
            If !assert_float_equals(actual_val, expected_val, tolerance):
                Print("Matrix element (" + i.to_string() + "," + j.to_string() + ") mismatch")
                Return false
            Set j to j + 1
        Set i to i + 1
    
    Return true

Process called "create_test_vector" that takes values as List[Float] returns Vector[Float]:
    Let components be Collections.List[String]()
    Let i be 0
    While i < values.length:
        components.add(values.get(i).to_string())
        Set i to i + 1
    
    Let result be LinAlg.Vector[Float]
    Set result.dimension to values.length
    Set result.components to components
    Return result

Process called "create_test_matrix" that takes values as List[List[Float]] returns Matrix[Float]:
    Let elements be Collections.List[List[String]]()
    Let i be 0
    While i < values.length:
        Let row be Collections.List[String]()
        Let j be 0
        While j < values.get(i).length:
            row.add(values.get(i).get(j).to_string())
            Set j to j + 1
        elements.add(row)
        Set i to i + 1
    
    Let result be LinAlg.Matrix[Float]
    Set result.rows to values.length
    Set result.columns to values.get(0).length
    Set result.elements to elements
    Return result

Note: =====================================================================
Note: ACTIVATION FUNCTION TESTS
Note: =====================================================================

Process called "test_activation_functions" that returns [Integer, Integer]:
    Print("Testing Activation Functions...")
    
    Let passed be 0
    Let total be 0
    
    Note: Test ReLU activation
    Set total to total + 1
    Let test_input be create_test_vector(Collections.List[Float]([-2.0, -1.0, 0.0, 1.0, 2.0]))
    Let relu_result be NeuralOps.relu_activation(test_input)
    Let expected_relu be create_test_vector(Collections.List[Float]([0.0, 0.0, 0.0, 1.0, 2.0]))
    
    If assert_vector_equals(relu_result, expected_relu, 1e-6):
        Set passed to passed + 1
        Print("‚úì ReLU activation test passed")
    Otherwise:
        Print("‚úó ReLU activation test failed")
    
    Note: Test Leaky ReLU activation
    Set total to total + 1
    Let leaky_relu_result be NeuralOps.leaky_relu_activation(test_input, 0.1)
    Let expected_leaky be create_test_vector(Collections.List[Float]([-0.2, -0.1, 0.0, 1.0, 2.0]))
    
    If assert_vector_equals(leaky_relu_result, expected_leaky, 1e-6):
        Set passed to passed + 1
        Print("‚úì Leaky ReLU activation test passed")
    Otherwise:
        Print("‚úó Leaky ReLU activation test failed")
    
    Note: Test Sigmoid activation
    Set total to total + 1
    Let sigmoid_input be create_test_vector(Collections.List[Float]([0.0]))
    Let sigmoid_result be NeuralOps.sigmoid_activation(sigmoid_input)
    
    Note: sigmoid(0) should be 0.5
    Let sigmoid_val be MathOps.string_to_float(sigmoid_result.components.get(0), 50).result_value
    If assert_float_equals(sigmoid_val, 0.5, 1e-6):
        Set passed to passed + 1
        Print("‚úì Sigmoid activation test passed")
    Otherwise:
        Print("‚úó Sigmoid activation test failed")
    
    Note: Test Tanh activation
    Set total to total + 1
    Let tanh_input be create_test_vector(Collections.List[Float]([0.0]))
    Let tanh_result be NeuralOps.tanh_activation(tanh_input)
    
    Note: tanh(0) should be 0.0
    Let tanh_val be MathOps.string_to_float(tanh_result.components.get(0), 50).result_value
    If assert_float_equals(tanh_val, 0.0, 1e-6):
        Set passed to passed + 1
        Print("‚úì Tanh activation test passed")
    Otherwise:
        Print("‚úó Tanh activation test failed")
    
    Note: Test Softmax activation
    Set total to total + 1
    Let softmax_input be create_test_vector(Collections.List[Float]([1.0, 2.0, 3.0]))
    Let softmax_result be NeuralOps.softmax_activation(softmax_input, -1)
    
    Note: Check if softmax sums to 1
    Let sum be 0.0
    Let i be 0
    While i < softmax_result.dimension:
        Let val be MathOps.string_to_float(softmax_result.components.get(i), 50).result_value
        Set sum to sum + val
        Set i to i + 1
    
    If assert_float_equals(sum, 1.0, 1e-6):
        Set passed to passed + 1
        Print("‚úì Softmax activation test passed")
    Otherwise:
        Print("‚úó Softmax activation test failed")
    
    Note: Test GELU activation
    Set total to total + 1
    Let gelu_input be create_test_vector(Collections.List[Float]([0.0]))
    Let gelu_result be NeuralOps.gelu_activation(gelu_input)
    
    Note: GELU(0) should be 0.0
    Let gelu_val be MathOps.string_to_float(gelu_result.components.get(0), 50).result_value
    If assert_float_equals(gelu_val, 0.0, 1e-6):
        Set passed to passed + 1
        Print("‚úì GELU activation test passed")
    Otherwise:
        Print("‚úó GELU activation test failed")
    
    Note: Test Swish activation
    Set total to total + 1
    Let swish_result be NeuralOps.swish_activation(sigmoid_input, 1.0)
    
    Note: Swish(0, Œ≤=1) = 0 * sigmoid(0) = 0 * 0.5 = 0
    Let swish_val be MathOps.string_to_float(swish_result.components.get(0), 50).result_value
    If assert_float_equals(swish_val, 0.0, 1e-6):
        Set passed to passed + 1
        Print("‚úì Swish activation test passed")
    Otherwise:
        Print("‚úó Swish activation test failed")
    
    Print("Activation Functions Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_activation_derivatives" that returns [Integer, Integer]:
    Print("Testing Activation Function Derivatives...")
    
    Let passed be 0
    Let total be 0
    
    Note: Test ReLU derivative
    Set total to total + 1
    Let test_input be create_test_vector(Collections.List[Float]([-1.0, 0.0, 1.0]))
    Let relu_derivative be NeuralOps.relu_derivative(test_input)
    Let expected_relu_grad be create_test_vector(Collections.List[Float]([0.0, 0.0, 1.0]))
    
    If assert_vector_equals(relu_derivative, expected_relu_grad, 1e-6):
        Set passed to passed + 1
        Print("‚úì ReLU derivative test passed")
    Otherwise:
        Print("‚úó ReLU derivative test failed")
    
    Note: Test Sigmoid derivative
    Set total to total + 1
    Let sigmoid_input be create_test_vector(Collections.List[Float]([0.0]))
    Let sigmoid_derivative be NeuralOps.sigmoid_derivative(sigmoid_input)
    
    Note: sigmoid'(0) = sigmoid(0) * (1 - sigmoid(0)) = 0.5 * 0.5 = 0.25
    Let sigmoid_grad_val be MathOps.string_to_float(sigmoid_derivative.components.get(0), 50).result_value
    If assert_float_equals(sigmoid_grad_val, 0.25, 1e-6):
        Set passed to passed + 1
        Print("‚úì Sigmoid derivative test passed")
    Otherwise:
        Print("‚úó Sigmoid derivative test failed")
    
    Note: Test Softmax derivative
    Set total to total + 1
    Let softmax_input be create_test_vector(Collections.List[Float]([1.0, 2.0]))
    Let softmax_output be NeuralOps.softmax_activation(softmax_input, -1)
    Let softmax_jacobian be NeuralOps.softmax_derivative(softmax_input, softmax_output)
    
    Note: Check if Jacobian has correct dimensions
    If softmax_jacobian.rows == 2 && softmax_jacobian.columns == 2:
        Set passed to passed + 1
        Print("‚úì Softmax derivative test passed")
    Otherwise:
        Print("‚úó Softmax derivative test failed")
    
    Print("Activation Derivatives Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_layer_operations" that returns [Integer, Integer]:
    Print("Testing Layer Operations...")
    
    Let passed be 0
    Let total be 0
    
    Note: Test linear forward pass
    Set total to total + 1
    Let input_vector be create_test_vector(Collections.List[Float]([1.0, 2.0]))
    Let weight_matrix be create_test_matrix(Collections.List[List[Float]]([[0.5, 1.0], [1.5, 0.5]]))
    Let bias_vector be create_test_vector(Collections.List[Float]([0.1, 0.2]))
    
    Let linear_result be NeuralOps.linear_forward(input_vector, weight_matrix, bias_vector)
    
    Note: Expected: [0.5*1 + 1.0*2 + 0.1, 1.5*1 + 0.5*2 + 0.2] = [2.6, 2.7]
    Let expected_linear be create_test_vector(Collections.List[Float]([2.6, 2.7]))
    
    If assert_vector_equals(linear_result, expected_linear, 1e-6):
        Set passed to passed + 1
        Print("‚úì Linear forward test passed")
    Otherwise:
        Print("‚úó Linear forward test failed")
    
    Note: Test linear backward pass
    Set total to total + 1
    Let grad_output be create_test_vector(Collections.List[Float]([1.0, 1.0]))
    Let [grad_input, grad_weights, grad_bias] be NeuralOps.linear_backward(grad_output, input_vector, weight_matrix)
    
    Note: Check if gradients have correct dimensions
    If grad_input.dimension == input_vector.dimension &&
       grad_weights.rows == weight_matrix.rows &&
       grad_weights.columns == weight_matrix.columns &&
       grad_bias.dimension == bias_vector.dimension:
        Set passed to passed + 1
        Print("‚úì Linear backward test passed")
    Otherwise:
        Print("‚úó Linear backward test failed")
    
    Print("Layer Operations Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_weight_initialization" that returns [Integer, Integer]:
    Print("Testing Weight Initialization...")
    
    Let passed be 0
    Let total be 0
    
    Note: Test Xavier uniform initialization
    Set total to total + 1
    Let shape be LinAlg.Tuple[Integer, Integer]
    Set shape.first to 3
    Set shape.second to 4
    Let xavier_weights be NeuralOps.xavier_uniform_init(shape, 1.0)
    
    If xavier_weights.rows == 3 && xavier_weights.columns == 4:
        Set passed to passed + 1
        Print("‚úì Xavier uniform initialization test passed")
    Otherwise:
        Print("‚úó Xavier uniform initialization test failed")
    
    Note: Test He normal initialization
    Set total to total + 1
    Let he_weights be NeuralOps.he_normal_init(shape, 1.0)
    
    If he_weights.rows == 3 && he_weights.columns == 4:
        Set passed to passed + 1
        Print("‚úì He normal initialization test passed")
    Otherwise:
        Print("‚úó He normal initialization test failed")
    
    Note: Test orthogonal initialization
    Set total to total + 1
    Let orthogonal_weights be NeuralOps.orthogonal_init(shape, 1.0)
    
    If orthogonal_weights.rows == 3 && orthogonal_weights.columns == 4:
        Set passed to passed + 1
        Print("‚úì Orthogonal initialization test passed")
    Otherwise:
        Print("‚úó Orthogonal initialization test failed")
    
    Print("Weight Initialization Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_pooling_operations" that returns [Integer, Integer]:
    Print("Testing Pooling Operations...")
    
    Let passed be 0
    Let total be 0
    
    Note: Create a test tensor for pooling (simplified 2D tensor as matrix)
    Let test_tensor be LinAlg.Tensor[Float]
    Set test_tensor.dimensions to Collections.List[Integer]([2, 2, 4, 4])
    Set test_tensor.data to Collections.List[String]()
    
    Note: Fill with test data
    Let i be 0
    While i < 64:
        test_tensor.data.add((i % 9 + 1).to_string())
        Set i to i + 1
    
    Note: Test max pooling
    Set total to total + 1
    Let pool_kernel be LinAlg.Tuple[Integer, Integer]
    Set pool_kernel.first to 2
    Set pool_kernel.second to 2
    Let pool_stride be LinAlg.Tuple[Integer, Integer]
    Set pool_stride.first to 2
    Set pool_stride.second to 2
    
    Let max_pool_result be NeuralOps.max_pool_2d(test_tensor, pool_kernel, pool_stride)
    
    If max_pool_result.dimensions.get(2) == 2 && max_pool_result.dimensions.get(3) == 2:
        Set passed to passed + 1
        Print("‚úì Max pooling test passed")
    Otherwise:
        Print("‚úó Max pooling test failed")
    
    Note: Test average pooling
    Set total to total + 1
    Let avg_pool_result be NeuralOps.avg_pool_2d(test_tensor, pool_kernel, pool_stride)
    
    If avg_pool_result.dimensions.get(2) == 2 && avg_pool_result.dimensions.get(3) == 2:
        Set passed to passed + 1
        Print("‚úì Average pooling test passed")
    Otherwise:
        Print("‚úó Average pooling test failed")
    
    Note: Test adaptive average pooling
    Set total to total + 1
    Let adaptive_size be LinAlg.Tuple[Integer, Integer]
    Set adaptive_size.first to 1
    Set adaptive_size.second to 1
    Let adaptive_result be NeuralOps.adaptive_avg_pool(test_tensor, adaptive_size)
    
    If adaptive_result.dimensions.get(2) == 1 && adaptive_result.dimensions.get(3) == 1:
        Set passed to passed + 1
        Print("‚úì Adaptive average pooling test passed")
    Otherwise:
        Print("‚úó Adaptive average pooling test failed")
    
    Print("Pooling Operations Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_regularization_operations" that returns [Integer, Integer]:
    Print("Testing Regularization Operations...")
    
    Let passed be 0
    Let total be 0
    
    Note: Create test tensor for dropout
    Let test_tensor be LinAlg.Tensor[Float]
    Set test_tensor.dimensions to Collections.List[Integer]([2, 3])
    Set test_tensor.data to Collections.List[String](["1.0", "2.0", "3.0", "4.0", "5.0", "6.0"])
    
    Note: Test dropout forward (training mode)
    Set total to total + 1
    Let [dropout_output, dropout_mask] be NeuralOps.dropout_forward(test_tensor, 0.5, true)
    
    If dropout_output.dimensions.length == test_tensor.dimensions.length:
        Set passed to passed + 1
        Print("‚úì Dropout forward test passed")
    Otherwise:
        Print("‚úó Dropout forward test failed")
    
    Note: Test dropout backward
    Set total to total + 1
    Let grad_input_tensor be LinAlg.Tensor[Float]
    Set grad_input_tensor.dimensions to Collections.List[Integer]([2, 3])
    Set grad_input_tensor.data to Collections.List[String](["1.0", "1.0", "1.0", "1.0", "1.0", "1.0"])
    
    Let dropout_grad be NeuralOps.dropout_backward(grad_input_tensor, dropout_mask, 0.5)
    
    If dropout_grad.dimensions.length == grad_input_tensor.dimensions.length:
        Set passed to passed + 1
        Print("‚úì Dropout backward test passed")
    Otherwise:
        Print("‚úó Dropout backward test failed")
    
    Note: Test layer normalization
    Set total to total + 1
    Let gamma be create_test_vector(Collections.List[Float]([1.0, 1.0, 1.0]))
    Let beta be create_test_vector(Collections.List[Float]([0.0, 0.0, 0.0]))
    Let layer_norm_result be NeuralOps.layer_normalize(test_tensor, gamma, beta, 1e-5)
    
    If layer_norm_result.dimensions.length == test_tensor.dimensions.length:
        Set passed to passed + 1
        Print("‚úì Layer normalization test passed")
    Otherwise:
        Print("‚úó Layer normalization test failed")
    
    Print("Regularization Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Process called "test_gradient_operations" that returns [Integer, Integer]:
    Print("Testing Gradient Operations...")
    
    Let passed be 0
    Let total be 0
    
    Note: Create test gradients
    Let gradients be Collections.List[Tensor[Float]]()
    Let grad1 be LinAlg.Tensor[Float]
    Set grad1.dimensions to Collections.List[Integer]([2, 2])
    Set grad1.data to Collections.List[String](["2.0", "3.0", "4.0", "5.0"])
    gradients.add(grad1)
    
    Let grad2 be LinAlg.Tensor[Float]
    Set grad2.dimensions to Collections.List[Integer]([3])
    Set grad2.data to Collections.List[String](["1.0", "2.0", "3.0"])
    gradients.add(grad2)
    
    Note: Test gradient norm computation
    Set total to total + 1
    Let grad_norm be NeuralOps.compute_gradient_norm(gradients)
    
    If grad_norm > 0.0:
        Set passed to passed + 1
        Print("‚úì Gradient norm computation test passed")
    Otherwise:
        Print("‚úó Gradient norm computation test failed")
    
    Note: Test gradient clipping
    Set total to total + 1
    Let clipped_gradients be NeuralOps.gradient_clipping(gradients, 1.0)
    
    If clipped_gradients.length == gradients.length:
        Set passed to passed + 1
        Print("‚úì Gradient clipping test passed")
    Otherwise:
        Print("‚úó Gradient clipping test failed")
    
    Print("Gradient Operations Tests: " + passed.to_string() + "/" + total.to_string() + " passed")
    Return [passed, total]

Note: =====================================================================
Note: MAIN TEST RUNNER
Note: =====================================================================

Process called "run_all_neural_ops_tests" that returns [Integer, Integer]:
    Print("=".repeat(70))
    Print("NEURAL OPERATIONS COMPREHENSIVE TEST SUITE")
    Print("=".repeat(70))
    
    Let total_passed be 0
    Let total_tests be 0
    
    Note: Run all test suites
    Let [p1, t1] be test_activation_functions()
    Set total_passed to total_passed + p1
    Set total_tests to total_tests + t1
    
    Let [p2, t2] be test_activation_derivatives()
    Set total_passed to total_passed + p2
    Set total_tests to total_tests + t2
    
    Let [p3, t3] be test_layer_operations()
    Set total_passed to total_passed + p3
    Set total_tests to total_tests + t3
    
    Let [p4, t4] be test_weight_initialization()
    Set total_passed to total_passed + p4
    Set total_tests to total_tests + t4
    
    Let [p5, t5] be test_pooling_operations()
    Set total_passed to total_passed + p5
    Set total_tests to total_tests + t5
    
    Let [p6, t6] be test_regularization_operations()
    Set total_passed to total_passed + p6
    Set total_tests to total_tests + t6
    
    Let [p7, t7] be test_gradient_operations()
    Set total_passed to total_passed + p7
    Set total_tests to total_tests + t7
    
    Print("=".repeat(70))
    Print("NEURAL OPERATIONS TEST SUMMARY")
    Print("Total Tests: " + total_tests.to_string())
    Print("Passed: " + total_passed.to_string())
    Print("Failed: " + (total_tests - total_passed).to_string())
    
    If total_passed == total_tests:
        Print("üéâ ALL TESTS PASSED! üéâ")
    Otherwise:
        Print("‚ùå Some tests failed. Please review output above.")
    
    Print("=".repeat(70))
    
    Return [total_passed, total_tests]