Note: Comprehensive Unit Tests for Reinforcement Learning Mathematics Module
Note: Tests value functions, Q-learning, policy gradients, actor-critic methods,
Note: exploration strategies, multi-agent RL, and Bellman equations.
Note: Validates convergence properties, numerical stability, and algorithm correctness.

Import "math/ai_math/reinforcement" as RL
Import "math/core/operations" as MathOps
Import "math/core/comparison" as MathCompare
Import "math/engine/linalg/core" as LinAlg
Import "math/engine/linalg/tensor" as TensorOps
Import "dev/debug/errors/core" as Errors

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as String
    expected_value as String
    actual_value as String

Type called "TestSuite":
    suite_name as String
    total_tests as Integer
    passed_tests as Integer
    failed_tests as Integer
    results as List[TestResult]

Note: ===== Test Helper Functions =====

Process called "assert_equals_float" that takes actual as String, expected as String, tolerance as String, test_name as String returns TestResult:
    Note: Assert that two floating point values are equal within tolerance
    
    Let diff_result be MathOps.subtract(actual, expected, 15)
    If diff_result.overflow_occurred:
        Return TestResult with test_name: test_name, passed: false, error_message: "Overflow in difference calculation", execution_time: "0", expected_value: expected, actual_value: actual
    
    Let abs_diff be MathOps.absolute_value(diff_result.result_value)
    Let comparison be MathOps.compare(abs_diff.result_value, tolerance)
    
    If comparison <= 0:
        Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected, actual_value: actual
    Otherwise:
        Return TestResult with test_name: test_name, passed: false, error_message: "Values differ by more than tolerance", execution_time: "0", expected_value: expected, actual_value: actual

Process called "assert_equals_int" that takes actual as Integer, expected as Integer, test_name as String returns TestResult:
    Note: Assert that two integers are equal
    
    If actual == expected:
        Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected.to_string(), actual_value: actual.to_string()
    Otherwise:
        Return TestResult with test_name: test_name, passed: false, error_message: "Integer values do not match", execution_time: "0", expected_value: expected.to_string(), actual_value: actual.to_string()

Process called "assert_matrix_equals" that takes actual as Matrix[Float], expected as Matrix[Float], tolerance as String, test_name as String returns TestResult:
    Note: Assert that two matrices are equal within tolerance
    
    If actual.rows != expected.rows or actual.columns != expected.columns:
        Return TestResult with test_name: test_name, passed: false, error_message: "Matrix dimensions do not match", execution_time: "0", expected_value: "Matrix dimensions mismatch", actual_value: "Matrix dimensions mismatch"
    
    Let i be 0
    While i < actual.rows:
        Let j be 0
        While j < actual.columns:
            Let actual_val be actual.entries.get(i).get(j)
            Let expected_val be expected.entries.get(i).get(j)
            
            Let element_test be assert_equals_float(actual_val, expected_val, tolerance, test_name + " element check")
            If not element_test.passed:
                Return element_test
            Set j to j + 1
        Set i to i + 1
    
    Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: "Matrix match", actual_value: "Matrix match"

Process called "assert_vector_equals" that takes actual as Vector[Float], expected as Vector[Float], tolerance as String, test_name as String returns TestResult:
    Note: Assert that two vectors are equal within tolerance
    
    If actual.dimension != expected.dimension:
        Return TestResult with test_name: test_name, passed: false, error_message: "Vector dimensions do not match", execution_time: "0", expected_value: "Vector dimensions mismatch", actual_value: "Vector dimensions mismatch"
    
    Let i be 0
    While i < actual.dimension:
        Let actual_val be actual.components.get(i)
        Let expected_val be expected.components.get(i)
        
        Let element_test be assert_equals_float(actual_val, expected_val, tolerance, test_name + " element check")
        If not element_test.passed:
            Return element_test
        Set i to i + 1
    
    Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: "Vector match", actual_value: "Vector match"

Process called "create_test_matrix" that takes rows as Integer, cols as Integer, pattern as String returns Matrix[Float]:
    Note: Create test matrix with specified pattern
    
    Let entries be List[List[String]]()
    
    If pattern == "identity":
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                If i == j:
                    Call row.add("1.0")
                Otherwise:
                    Call row.add("0.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "ones":
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add("1.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "sequential":
        Let value be 1
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add(value.to_string())
                Set value to value + 1
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "small_values":
        Let value be 0.1
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add(value.to_string())
                Set value to value + 0.1
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise:
        Note: Default to zeros
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add("0.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    
    Return LinAlg.create_matrix(entries, "float")

Process called "create_test_vector" that takes size as Integer, pattern as String returns Vector[Float]:
    Note: Create test vector with specified pattern
    
    Let components be List[String]()
    
    If pattern == "ones":
        Let i be 0
        While i < size:
            Call components.add("1.0")
            Set i to i + 1
    Otherwise If pattern == "sequential":
        Let i be 0
        While i < size:
            Let value be (i + 1).to_string()
            Call components.add(value)
            Set i to i + 1
    Otherwise If pattern == "uniform":
        Let uniform_value be (1.0 / size.to_float()).to_string()
        Let i be 0
        While i < size:
            Call components.add(uniform_value)
            Set i to i + 1
    Otherwise If pattern == "decreasing":
        Let i be 0
        While i < size:
            Let value be (1.0 / (i + 1).to_float()).to_string()
            Call components.add(value)
            Set i to i + 1
    Otherwise:
        Note: Default to zeros
        Let i be 0
        While i < size:
            Call components.add("0.0")
            Set i to i + 1
    
    Return LinAlg.create_vector(components, "float")

Note: ===== Helper Function Tests =====

Process called "test_argmax_function" that takes no parameters returns TestResult:
    Note: Test argmax utility function
    
    Try:
        Let test_values be create_test_vector(4, "sequential")
        Let max_index be RL.argmax(test_values)
        
        Note: Sequential pattern [1,2,3,4] should have max at index 3
        Let expected_index be 3
        Let index_check be assert_equals_int(max_index, expected_index, "argmax index")
        If not index_check.passed:
            Return index_check
        
        Note: Test with equal values
        Let equal_values_components be List[String]()
        Call equal_values_components.add("2.0")
        Call equal_values_components.add("2.0")
        Call equal_values_components.add("2.0")
        Let equal_values be LinAlg.create_vector(equal_values_components, "float")
        
        Let equal_max_index be RL.argmax(equal_values)
        Note: Should return first occurrence (index 0)
        Let first_occurrence_check be assert_equals_int(equal_max_index, 0, "argmax first occurrence")
        If not first_occurrence_check.passed:
            Return first_occurrence_check
        
        Return TestResult with test_name: "test_argmax_function", passed: true, error_message: "", execution_time: "0", expected_value: "Valid argmax", actual_value: "Valid argmax"
    
    Catch error:
        Return TestResult with test_name: "test_argmax_function", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_random_choice" that takes no parameters returns TestResult:
    Note: Test random choice from probability distribution
    
    Try:
        Note: Create uniform probability distribution
        Let prob_components be List[String]()
        Call prob_components.add("0.25")
        Call prob_components.add("0.25")
        Call prob_components.add("0.25")
        Call prob_components.add("0.25")
        Let uniform_probs be LinAlg.create_vector(prob_components, "float")
        
        Let choice be RL.generate_random_choice(uniform_probs)
        
        Note: Choice should be valid index (0-3)
        If choice < 0 or choice >= 4:
            Return TestResult with test_name: "test_random_choice", passed: false, error_message: "Choice index out of bounds", execution_time: "0", expected_value: "[0,3]", actual_value: choice.to_string()
        
        Note: Test with deterministic distribution
        Let deterministic_components be List[String]()
        Call deterministic_components.add("0.0")
        Call deterministic_components.add("1.0")
        Call deterministic_components.add("0.0")
        Let deterministic_probs be LinAlg.create_vector(deterministic_components, "float")
        
        Let deterministic_choice be RL.generate_random_choice(deterministic_probs)
        Let deterministic_check be assert_equals_int(deterministic_choice, 1, "deterministic choice")
        If not deterministic_check.passed:
            Return deterministic_check
        
        Return TestResult with test_name: "test_random_choice", passed: true, error_message: "", execution_time: "0", expected_value: "Valid random choice", actual_value: "Valid random choice"
    
    Catch error:
        Return TestResult with test_name: "test_random_choice", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Value Function Tests =====

Process called "test_bellman_expectation_equation" that takes no parameters returns TestResult:
    Note: Test Bellman expectation equation computation
    
    Try:
        Let num_states be 2
        Let num_actions be 2
        Let discount be 0.9
        
        Note: Create simple policy (uniform random)
        Let policy be create_test_matrix(num_states, num_actions, "ones")
        Let policy_normalized_entries be List[List[String]]()
        Let i be 0
        While i < num_states:
            Let policy_row be List[String]()
            Let j be 0
            While j < num_actions:
                Call policy_row.add("0.5")  Note: Uniform policy
                Set j to j + 1
            Call policy_normalized_entries.add(policy_row)
            Set i to i + 1
        Let policy_normalized be LinAlg.create_matrix(policy_normalized_entries, "float")
        
        Note: Create simple reward matrix
        Let reward_matrix be create_test_matrix(num_states, num_actions, "ones")
        
        Note: Create simple transition tensor (deterministic transitions)
        Let transition_data be List[String]()
        Let tensor_size be num_states * num_actions * num_states
        Let k be 0
        While k < tensor_size:
            If k % 2 == 0:
                Call transition_data.add("1.0")
            Otherwise:
                Call transition_data.add("0.0")
            Set k to k + 1
        
        Let transition_shape be List[Integer]()
        Call transition_shape.add(num_states)
        Call transition_shape.add(num_actions)
        Call transition_shape.add(num_states)
        Let transition_tensor be TensorOps.create_tensor(transition_data, transition_shape, "float")
        
        Let state be 0
        Let bellman_value be RL.bellman_expectation_equation(state, policy_normalized, reward_matrix, transition_tensor, discount)
        
        Note: Check that computed value is reasonable (positive for positive rewards)
        Let value_float be Parse bellman_value.to_string() as Float
        If value_float <= 0.0:
            Return TestResult with test_name: "test_bellman_expectation_equation", passed: false, error_message: "Bellman value should be positive for positive rewards", execution_time: "0", expected_value: "Positive value", actual_value: bellman_value.to_string()
        
        Return TestResult with test_name: "test_bellman_expectation_equation", passed: true, error_message: "", execution_time: "0", expected_value: "Valid Bellman expectation", actual_value: "Valid Bellman expectation"
    
    Catch error:
        Return TestResult with test_name: "test_bellman_expectation_equation", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_value_iteration" that takes no parameters returns TestResult:
    Note: Test value iteration algorithm
    
    Try:
        Let num_states be 2
        Let num_actions be 2
        
        Note: Create reward matrix
        Let reward_entries be List[List[String]]()
        Let reward_row1 be List[String]()
        Call reward_row1.add("1.0")
        Call reward_row1.add("0.5")
        Call reward_entries.add(reward_row1)
        Let reward_row2 be List[String]()
        Call reward_row2.add("0.5")
        Call reward_row2.add("1.0")
        Call reward_entries.add(reward_row2)
        Let reward_matrix be LinAlg.create_matrix(reward_entries, "float")
        
        Note: Create transition tensor
        Let transition_data be List[String]()
        Call transition_data.add("0.8")  Note: P(s0|s0,a0) = 0.8
        Call transition_data.add("0.2")  Note: P(s1|s0,a0) = 0.2
        Call transition_data.add("0.3")  Note: P(s0|s0,a1) = 0.3
        Call transition_data.add("0.7")  Note: P(s1|s0,a1) = 0.7
        Call transition_data.add("0.4")  Note: P(s0|s1,a0) = 0.4
        Call transition_data.add("0.6")  Note: P(s1|s1,a0) = 0.6
        Call transition_data.add("0.1")  Note: P(s0|s1,a1) = 0.1
        Call transition_data.add("0.9")  Note: P(s1|s1,a1) = 0.9
        
        Let transition_shape be List[Integer]()
        Call transition_shape.add(2)  Note: num_states
        Call transition_shape.add(2)  Note: num_actions
        Call transition_shape.add(2)  Note: num_states
        Let transition_tensor be TensorOps.create_tensor(transition_data, transition_shape, "float")
        
        Let discount be 0.9
        Let tolerance be 0.001
        
        Let value_function be RL.value_iteration(reward_matrix, transition_tensor, discount, tolerance)
        
        Note: Check that value function has correct parameters dimensions
        If value_function.parameters.rows != num_states or value_function.parameters.columns != 1:
            Return TestResult with test_name: "test_value_iteration", passed: false, error_message: "Value function dimensions incorrect", execution_time: "0", expected_value: "2x1", actual_value: value_function.parameters.rows.to_string() + "x" + value_function.parameters.columns.to_string()
        
        Note: Check that values are positive (positive rewards should yield positive values)
        Let value_0 be value_function.parameters.entries.get(0).get(0)
        Let value_1 be value_function.parameters.entries.get(1).get(0)
        
        Let value_0_float be Parse value_0 as Float
        Let value_1_float be Parse value_1 as Float
        
        If value_0_float <= 0.0 or value_1_float <= 0.0:
            Return TestResult with test_name: "test_value_iteration", passed: false, error_message: "Values should be positive", execution_time: "0", expected_value: "Positive values", actual_value: "Non-positive values"
        
        Return TestResult with test_name: "test_value_iteration", passed: true, error_message: "", execution_time: "0", expected_value: "Valid value iteration", actual_value: "Valid value iteration"
    
    Catch error:
        Return TestResult with test_name: "test_value_iteration", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Q-Learning Tests =====

Process called "test_q_learning_update" that takes no parameters returns TestResult:
    Note: Test Q-learning update rule
    
    Try:
        Let num_states be 3
        Let num_actions be 2
        
        Note: Initialize Q-table with small values
        Let q_table be create_test_matrix(num_states, num_actions, "small_values")
        
        Let config be RL.QLearningConfig with learning_rate: 0.1, discount_factor: 0.9, epsilon: 0.1, epsilon_decay: 0.99, min_epsilon: 0.01, double_q: false
        
        Let state be 0
        Let action be 1
        Let reward be 1.0
        Let next_state be 1
        
        Let updated_q_table be RL.q_learning_update(q_table, state, action, reward, next_state, config)
        
        Note: Check that Q-table dimensions are preserved
        If updated_q_table.rows != num_states or updated_q_table.columns != num_actions:
            Return TestResult with test_name: "test_q_learning_update", passed: false, error_message: "Q-table dimensions changed", execution_time: "0", expected_value: num_states.to_string() + "x" + num_actions.to_string(), actual_value: updated_q_table.rows.to_string() + "x" + updated_q_table.columns.to_string()
        
        Note: Check that the specific Q(s,a) was updated
        Let original_value be q_table.entries.get(state).get(action)
        Let updated_value be updated_q_table.entries.get(state).get(action)
        
        Let comparison be MathCompare.compare(updated_value, original_value, 15)
        If comparison == 0:
            Return TestResult with test_name: "test_q_learning_update", passed: false, error_message: "Q-value was not updated", execution_time: "0", expected_value: "Different values", actual_value: "Same values"
        
        Note: Check that other Q-values remained unchanged
        Let other_state be 2
        Let other_action be 0
        Let other_original be q_table.entries.get(other_state).get(other_action)
        Let other_updated be updated_q_table.entries.get(other_state).get(other_action)
        
        Let other_check be assert_equals_float(other_updated, other_original, "0.0001", "unchanged Q-value")
        If not other_check.passed:
            Return other_check
        
        Return TestResult with test_name: "test_q_learning_update", passed: true, error_message: "", execution_time: "0", expected_value: "Valid Q-learning update", actual_value: "Valid Q-learning update"
    
    Catch error:
        Return TestResult with test_name: "test_q_learning_update", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_double_q_learning" that takes no parameters returns TestResult:
    Note: Test double Q-learning algorithm
    
    Try:
        Let num_states be 2
        Let num_actions be 2
        
        Let q1 be create_test_matrix(num_states, num_actions, "small_values")
        Let q2 be create_test_matrix(num_states, num_actions, "small_values")
        
        Let state be 0
        Let action be 1
        Let reward be 1.0
        Let next_state be 1
        Let learning_rate be 0.1
        Let discount be 0.9
        
        Let update_result be RL.double_q_learning_update(q1, q2, state, action, reward, next_state, learning_rate, discount)
        Let updated_q1 be update_result.first
        Let updated_q2 be update_result.second
        
        Note: Check dimensions are preserved
        If updated_q1.rows != num_states or updated_q1.columns != num_actions:
            Return TestResult with test_name: "test_double_q_learning", passed: false, error_message: "Q1 dimensions changed", execution_time: "0", expected_value: "Preserved dimensions", actual_value: "Changed dimensions"
        
        If updated_q2.rows != num_states or updated_q2.columns != num_actions:
            Return TestResult with test_name: "test_double_q_learning", passed: false, error_message: "Q2 dimensions changed", execution_time: "0", expected_value: "Preserved dimensions", actual_value: "Changed dimensions"
        
        Note: Check that one of the Q-tables was updated (double Q-learning randomly updates one)
        Let q1_original be q1.entries.get(state).get(action)
        Let q1_updated be updated_q1.entries.get(state).get(action)
        Let q2_original be q2.entries.get(state).get(action)
        Let q2_updated be updated_q2.entries.get(state).get(action)
        
        Let q1_changed be MathCompare.compare(q1_updated, q1_original, 15) != 0
        Let q2_changed be MathCompare.compare(q2_updated, q2_original, 15) != 0
        
        If not q1_changed and not q2_changed:
            Return TestResult with test_name: "test_double_q_learning", passed: false, error_message: "Neither Q-table was updated", execution_time: "0", expected_value: "At least one update", actual_value: "No updates"
        
        Return TestResult with test_name: "test_double_q_learning", passed: true, error_message: "", execution_time: "0", expected_value: "Valid double Q-learning", actual_value: "Valid double Q-learning"
    
    Catch error:
        Return TestResult with test_name: "test_double_q_learning", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_sarsa_update" that takes no parameters returns TestResult:
    Note: Test SARSA update rule
    
    Try:
        Let num_states be 3
        Let num_actions be 2
        
        Let q_table be create_test_matrix(num_states, num_actions, "small_values")
        
        Let state be 0
        Let action be 1
        Let reward be 1.0
        Let next_state be 1
        Let next_action be 0
        Let learning_rate be 0.1
        Let discount be 0.9
        
        Let updated_q_table be RL.sarsa_update(q_table, state, action, reward, next_state, next_action, learning_rate, discount)
        
        Note: Check dimensions preserved
        If updated_q_table.rows != num_states or updated_q_table.columns != num_actions:
            Return TestResult with test_name: "test_sarsa_update", passed: false, error_message: "Q-table dimensions changed", execution_time: "0", expected_value: "Preserved dimensions", actual_value: "Changed dimensions"
        
        Note: Check that Q(s,a) was updated
        Let original_value be q_table.entries.get(state).get(action)
        Let updated_value be updated_q_table.entries.get(state).get(action)
        
        Let comparison be MathCompare.compare(updated_value, original_value, 15)
        If comparison == 0:
            Return TestResult with test_name: "test_sarsa_update", passed: false, error_message: "Q-value was not updated", execution_time: "0", expected_value: "Different values", actual_value: "Same values"
        
        Return TestResult with test_name: "test_sarsa_update", passed: true, error_message: "", execution_time: "0", expected_value: "Valid SARSA update", actual_value: "Valid SARSA update"
    
    Catch error:
        Return TestResult with test_name: "test_sarsa_update", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Policy Gradient Tests =====

Process called "test_vanilla_policy_gradient" that takes no parameters returns TestResult:
    Note: Test vanilla policy gradient computation
    
    Try:
        Note: Create test log probabilities and rewards
        Let log_prob_components be List[String]()
        Call log_prob_components.add("-0.5")  Note: log(0.6) ≈ -0.5
        Call log_prob_components.add("-0.7")  Note: log(0.5) ≈ -0.7
        Call log_prob_components.add("-0.3")  Note: log(0.7) ≈ -0.3
        Let log_probs be LinAlg.create_vector(log_prob_components, "float")
        
        Let reward_components be List[String]()
        Call reward_components.add("1.0")
        Call reward_components.add("0.5")
        Call reward_components.add("2.0")
        Let rewards be LinAlg.create_vector(reward_components, "float")
        
        Let discount be 0.9
        
        Let pg_loss be RL.vanilla_policy_gradient(log_probs, rewards, discount)
        
        Note: Policy gradient loss should be a finite value
        Let pg_loss_str be pg_loss.to_string()
        Let pg_loss_float be Parse pg_loss_str as Float
        
        If pg_loss_float != pg_loss_float:  Note: Check for NaN
            Return TestResult with test_name: "test_vanilla_policy_gradient", passed: false, error_message: "Policy gradient loss is NaN", execution_time: "0", expected_value: "Finite value", actual_value: "NaN"
        
        Return TestResult with test_name: "test_vanilla_policy_gradient", passed: true, error_message: "", execution_time: "0", expected_value: "Valid policy gradient", actual_value: "Valid policy gradient"
    
    Catch error:
        Return TestResult with test_name: "test_vanilla_policy_gradient", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_compute_returns" that takes no parameters returns TestResult:
    Note: Test computation of discounted returns
    
    Try:
        Let reward_components be List[String]()
        Call reward_components.add("1.0")
        Call reward_components.add("2.0")
        Call reward_components.add("3.0")
        Let rewards be LinAlg.create_vector(reward_components, "float")
        
        Let discount be 0.5
        Let normalize be false
        
        Let returns be RL.compute_returns(rewards, discount, normalize)
        
        Note: Check that returns vector has same dimension as rewards
        If returns.dimension != rewards.dimension:
            Return TestResult with test_name: "test_compute_returns", passed: false, error_message: "Returns dimension incorrect", execution_time: "0", expected_value: rewards.dimension.to_string(), actual_value: returns.dimension.to_string()
        
        Note: Check manual computation of first return
        Note: G_0 = r_0 + γr_1 + γ²r_2 = 1 + 0.5*2 + 0.25*3 = 1 + 1 + 0.75 = 2.75
        Let expected_first_return be "2.75"
        Let actual_first_return be returns.components.get(0)
        
        Let return_check be assert_equals_float(actual_first_return, expected_first_return, "0.01", "first return")
        If not return_check.passed:
            Return return_check
        
        Note: Check that returns are decreasing (later returns should be smaller)
        Let first_return_float be Parse returns.components.get(0) as Float
        Let second_return_float be Parse returns.components.get(1) as Float
        
        If first_return_float <= second_return_float:
            Return TestResult with test_name: "test_compute_returns", passed: false, error_message: "Returns should be decreasing", execution_time: "0", expected_value: "Decreasing returns", actual_value: "Non-decreasing returns"
        
        Return TestResult with test_name: "test_compute_returns", passed: true, error_message: "", execution_time: "0", expected_value: "Valid returns computation", actual_value: "Valid returns computation"
    
    Catch error:
        Return TestResult with test_name: "test_compute_returns", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_compute_advantages" that takes no parameters returns TestResult:
    Note: Test computation of GAE advantages
    
    Try:
        Let reward_components be List[String]()
        Call reward_components.add("1.0")
        Call reward_components.add("0.5")
        Call reward_components.add("2.0")
        Let rewards be LinAlg.create_vector(reward_components, "float")
        
        Let value_components be List[String]()
        Call value_components.add("0.8")
        Call value_components.add("0.6")
        Call value_components.add("1.2")
        Let values be LinAlg.create_vector(value_components, "float")
        
        Let discount be 0.9
        Let lambda_gae be 0.95
        
        Let advantages be RL.compute_advantages(rewards, values, discount, lambda_gae)
        
        Note: Check that advantages vector has correct dimension
        If advantages.dimension != rewards.dimension:
            Return TestResult with test_name: "test_compute_advantages", passed: false, error_message: "Advantages dimension incorrect", execution_time: "0", expected_value: rewards.dimension.to_string(), actual_value: advantages.dimension.to_string()
        
        Note: Advantages should have zero mean (approximately) after normalization
        Let advantage_sum be "0.0"
        Let i be 0
        While i < advantages.dimension:
            Let advantage_val be advantages.components.get(i)
            Let sum_result be MathOps.add(advantage_sum, advantage_val, 15)
            Set advantage_sum to sum_result.result_value
            Set i to i + 1
        
        Let mean_result be MathOps.divide(advantage_sum, advantages.dimension.to_string(), 15)
        Let mean_advantage be mean_result.result_value
        
        Note: Mean should be close to zero (within reasonable tolerance)
        Let mean_check be assert_equals_float(mean_advantage, "0.0", "0.5", "advantage mean")
        If not mean_check.passed:
            Return mean_check
        
        Return TestResult with test_name: "test_compute_advantages", passed: true, error_message: "", execution_time: "0", expected_value: "Valid advantages computation", actual_value: "Valid advantages computation"
    
    Catch error:
        Return TestResult with test_name: "test_compute_advantages", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Exploration Strategy Tests =====

Process called "test_epsilon_greedy_action" that takes no parameters returns TestResult:
    Note: Test epsilon-greedy action selection
    
    Try:
        Let q_value_components be List[String]()
        Call q_value_components.add("0.1")
        Call q_value_components.add("0.8")  Note: Best action
        Call q_value_components.add("0.3")
        Let q_values be LinAlg.create_vector(q_value_components, "float")
        
        Note: Test with epsilon = 0 (purely greedy)
        Let epsilon be 0.0
        Let greedy_action be RL.epsilon_greedy_action(q_values, epsilon)
        
        Note: Should select action with highest Q-value (index 1)
        Let expected_action be 1
        Let greedy_check be assert_equals_int(greedy_action, expected_action, "greedy action selection")
        If not greedy_check.passed:
            Return greedy_check
        
        Note: Test with epsilon = 1.0 (purely random)
        Let epsilon_random be 1.0
        Let random_action be RL.epsilon_greedy_action(q_values, epsilon_random)
        
        Note: Should be valid action index
        If random_action < 0 or random_action >= 3:
            Return TestResult with test_name: "test_epsilon_greedy_action", passed: false, error_message: "Random action out of bounds", execution_time: "0", expected_value: "[0,2]", actual_value: random_action.to_string()
        
        Return TestResult with test_name: "test_epsilon_greedy_action", passed: true, error_message: "", execution_time: "0", expected_value: "Valid epsilon-greedy", actual_value: "Valid epsilon-greedy"
    
    Catch error:
        Return TestResult with test_name: "test_epsilon_greedy_action", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_boltzmann_action_selection" that takes no parameters returns TestResult:
    Note: Test Boltzmann (softmax) action selection
    
    Try:
        Let q_value_components be List[String]()
        Call q_value_components.add("1.0")
        Call q_value_components.add("3.0")  Note: Much higher Q-value
        Call q_value_components.add("1.5")
        Let q_values be LinAlg.create_vector(q_value_components, "float")
        
        Note: Test with low temperature (more deterministic)
        Let temperature be 0.1
        Let action be RL.boltzmann_action(q_values, temperature)
        
        Note: Should be valid action index
        If action < 0 or action >= 3:
            Return TestResult with test_name: "test_boltzmann_action_selection", passed: false, error_message: "Action out of bounds", execution_time: "0", expected_value: "[0,2]", actual_value: action.to_string()
        
        Note: Test with high temperature (more random)
        Let high_temp be 10.0
        Let random_action be RL.boltzmann_action(q_values, high_temp)
        
        If random_action < 0 or random_action >= 3:
            Return TestResult with test_name: "test_boltzmann_action_selection", passed: false, error_message: "Random action out of bounds", execution_time: "0", expected_value: "[0,2]", actual_value: random_action.to_string()
        
        Return TestResult with test_name: "test_boltzmann_action_selection", passed: true, error_message: "", execution_time: "0", expected_value: "Valid Boltzmann selection", actual_value: "Valid Boltzmann selection"
    
    Catch error:
        Return TestResult with test_name: "test_boltzmann_action_selection", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_ucb_action_selection" that takes no parameters returns TestResult:
    Note: Test UCB (Upper Confidence Bound) action selection
    
    Try:
        Let q_value_components be List[String]()
        Call q_value_components.add("0.5")
        Call q_value_components.add("0.7")
        Call q_value_components.add("0.3")
        Let q_values be LinAlg.create_vector(q_value_components, "float")
        
        Let count_components be List[Integer]()
        Call count_components.add(10)  Note: Action 0 selected 10 times
        Call count_components.add(5)   Note: Action 1 selected 5 times (less explored)
        Call count_components.add(15)  Note: Action 2 selected 15 times
        Let action_counts be LinAlg.create_vector_int(count_components)
        
        Let total_steps be 30
        Let confidence be 2.0
        
        Let ucb_action be RL.ucb_action(q_values, action_counts, total_steps, confidence)
        
        Note: Should be valid action index
        If ucb_action < 0 or ucb_action >= 3:
            Return TestResult with test_name: "test_ucb_action_selection", passed: false, error_message: "UCB action out of bounds", execution_time: "0", expected_value: "[0,2]", actual_value: ucb_action.to_string()
        
        Return TestResult with test_name: "test_ucb_action_selection", passed: true, error_message: "", execution_time: "0", expected_value: "Valid UCB selection", actual_value: "Valid UCB selection"
    
    Catch error:
        Return TestResult with test_name: "test_ucb_action_selection", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Temporal Difference Learning Tests =====

Process called "test_td_error_computation" that takes no parameters returns TestResult:
    Note: Test TD error computation
    
    Try:
        Let current_value be 5.0
        Let reward be 3.0
        Let next_value be 7.0
        Let discount be 0.9
        
        Let td_error be RL.compute_td_error(current_value, reward, next_value, discount)
        
        Note: Manual calculation: δ = r + γV(s') - V(s) = 3 + 0.9*7 - 5 = 3 + 6.3 - 5 = 4.3
        Let expected_error be 4.3
        Let error_check be assert_equals_float(td_error.to_string(), expected_error.to_string(), "0.01", "TD error")
        If not error_check.passed:
            Return error_check
        
        Return TestResult with test_name: "test_td_error_computation", passed: true, error_message: "", execution_time: "0", expected_value: "Valid TD error", actual_value: "Valid TD error"
    
    Catch error:
        Return TestResult with test_name: "test_td_error_computation", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_td_lambda_update" that takes no parameters returns TestResult:
    Note: Test TD(λ) eligibility trace update
    
    Try:
        Let value_components be List[String]()
        Call value_components.add("1.0")
        Call value_components.add("2.0")
        Call value_components.add("1.5")
        Let value_function be LinAlg.create_vector(value_components, "float")
        
        Let state_components be List[Integer]()
        Call state_components.add(0)
        Call state_components.add(1)
        Call state_components.add(2)
        Let states be LinAlg.create_vector_int(state_components)
        
        Let reward_components be List[String]()
        Call reward_components.add("0.5")
        Call reward_components.add("1.0")
        Call reward_components.add("0.8")
        Let rewards be LinAlg.create_vector(reward_components, "float")
        
        Let discount be 0.9
        Let lambda be 0.8
        Let learning_rate be 0.1
        
        Let updated_values be RL.td_lambda_update(value_function, states, rewards, discount, lambda, learning_rate)
        
        Note: Check that dimensions are preserved
        If updated_values.dimension != value_function.dimension:
            Return TestResult with test_name: "test_td_lambda_update", passed: false, error_message: "Value function dimensions changed", execution_time: "0", expected_value: value_function.dimension.to_string(), actual_value: updated_values.dimension.to_string()
        
        Note: Check that at least some values were updated
        Let values_changed be false
        Let i be 0
        While i < value_function.dimension:
            Let original_val be value_function.components.get(i)
            Let updated_val be updated_values.components.get(i)
            Let comparison be MathCompare.compare(updated_val, original_val, 15)
            If comparison != 0:
                Set values_changed to true
                Break
            Set i to i + 1
        
        If not values_changed:
            Return TestResult with test_name: "test_td_lambda_update", passed: false, error_message: "No values were updated", execution_time: "0", expected_value: "Some updates", actual_value: "No updates"
        
        Return TestResult with test_name: "test_td_lambda_update", passed: true, error_message: "", execution_time: "0", expected_value: "Valid TD(λ) update", actual_value: "Valid TD(λ) update"
    
    Catch error:
        Return TestResult with test_name: "test_td_lambda_update", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Error Handling Tests =====

Process called "test_bellman_equation_errors" that takes no parameters returns TestResult:
    Note: Test error handling in Bellman equation computation
    
    Try:
        Let policy be create_test_matrix(2, 2, "ones")
        Let reward_matrix be create_test_matrix(2, 2, "ones")
        Let transition_data be List[String]()
        Let i be 0
        While i < 8:  Note: 2*2*2
            Call transition_data.add("0.25")
            Set i to i + 1
        Let transition_shape be List[Integer]()
        Call transition_shape.add(2)
        Call transition_shape.add(2)
        Call transition_shape.add(2)
        Let transition_tensor be TensorOps.create_tensor(transition_data, transition_shape, "float")
        
        Try:
            Note: Test invalid state
            Let result be RL.bellman_expectation_equation(-1, policy, reward_matrix, transition_tensor, 0.9)
            Return TestResult with test_name: "test_bellman_equation_errors", passed: false, error_message: "Should have thrown error for invalid state", execution_time: "0", expected_value: "Error", actual_value: "No error"
        Catch state_error:
            Note: Expected behavior
            Pass
        
        Try:
            Note: Test invalid discount factor
            Let result be RL.bellman_expectation_equation(0, policy, reward_matrix, transition_tensor, 1.5)
            Return TestResult with test_name: "test_bellman_equation_errors", passed: false, error_message: "Should have thrown error for invalid discount", execution_time: "0", expected_value: "Error", actual_value: "No error"
        Catch discount_error:
            Note: Expected behavior
            Pass
        
        Return TestResult with test_name: "test_bellman_equation_errors", passed: true, error_message: "", execution_time: "0", expected_value: "Proper error handling", actual_value: "Proper error handling"
    
    Catch error:
        Return TestResult with test_name: "test_bellman_equation_errors", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "Controlled errors only", actual_value: "Unexpected exception"

Note: ===== Test Suite Management =====

Process called "create_test_suite" that takes suite_name as String returns TestSuite:
    Note: Create a new test suite for organizing test results
    Return TestSuite with suite_name: suite_name, total_tests: 0, passed_tests: 0, failed_tests: 0, results: List[TestResult]()

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns TestSuite:
    Note: Add a test result to the test suite
    
    Call suite.results.add(result)
    Set suite.total_tests to suite.total_tests + 1
    
    If result.passed:
        Set suite.passed_tests to suite.passed_tests + 1
    Otherwise:
        Set suite.failed_tests to suite.failed_tests + 1
    
    Return suite

Process called "run_all_reinforcement_tests" that takes no parameters returns TestSuite:
    Note: Run all reinforcement learning tests and return comprehensive results
    
    Let suite be create_test_suite("Reinforcement Learning Test Suite")
    
    Note: Helper function tests
    Set suite to add_test_result(suite, test_argmax_function())
    Set suite to add_test_result(suite, test_random_choice())
    
    Note: Value function tests
    Set suite to add_test_result(suite, test_bellman_expectation_equation())
    Set suite to add_test_result(suite, test_value_iteration())
    
    Note: Q-learning tests
    Set suite to add_test_result(suite, test_q_learning_update())
    Set suite to add_test_result(suite, test_double_q_learning())
    Set suite to add_test_result(suite, test_sarsa_update())
    
    Note: Policy gradient tests
    Set suite to add_test_result(suite, test_vanilla_policy_gradient())
    Set suite to add_test_result(suite, test_compute_returns())
    Set suite to add_test_result(suite, test_compute_advantages())
    
    Note: Exploration strategy tests
    Set suite to add_test_result(suite, test_epsilon_greedy_action())
    Set suite to add_test_result(suite, test_boltzmann_action_selection())
    Set suite to add_test_result(suite, test_ucb_action_selection())
    
    Note: Temporal difference learning tests
    Set suite to add_test_result(suite, test_td_error_computation())
    Set suite to add_test_result(suite, test_td_lambda_update())
    
    Note: Error handling tests
    Set suite to add_test_result(suite, test_bellman_equation_errors())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns String:
    Note: Format test results for display
    
    Let output be "=== " + suite.suite_name + " ===" + "\n"
    Set output to output + "Total tests: " + suite.total_tests.to_string() + "\n"
    Set output to output + "Passed: " + suite.passed_tests.to_string() + "\n"
    Set output to output + "Failed: " + suite.failed_tests.to_string() + "\n\n"
    
    If suite.failed_tests > 0:
        Set output to output + "Failed tests:" + "\n"
        Let i be 0
        While i < suite.results.length:
            Let result be suite.results.get(i)
            If not result.passed:
                Set output to output + "- " + result.test_name + ": " + result.error_message + "\n"
            Set i to i + 1
    
    Let success_rate_num be suite.passed_tests * 100
    Let success_rate be success_rate_num / suite.total_tests
    Set output to output + "\nSuccess rate: " + success_rate.to_string() + "%"
    
    Return output

Note: ===== Main Test Runner =====

Process called "main" that takes no parameters returns Integer:
    Note: Main test runner for reinforcement learning module
    
    Display "Running comprehensive tests for reinforcement learning mathematics..."
    Display ""
    
    Let test_suite be run_all_reinforcement_tests()
    Let results_output be print_test_results(test_suite)
    
    Display results_output
    
    If test_suite.failed_tests == 0:
        Display ""
        Display "All reinforcement learning tests passed successfully!"
        Return 0
    Otherwise:
        Display ""
        Display "Some reinforcement learning tests failed. Please review the failures above."
        Return 1