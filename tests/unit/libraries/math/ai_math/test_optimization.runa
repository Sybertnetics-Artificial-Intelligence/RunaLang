Note:
Unit Tests for AI Optimization Module
Testing various optimization algorithms used in machine learning including SGD,
Adam, AdaGrad, RMSprop, and their variants with momentum and learning rate scheduling.
:End Note

Import "math/ai_math/optimization" as Optimization
Import "math/core/operations" as MathOps
Import "math/core/comparison" as MathCompare
Import "dev/debug/errors/core" as Errors

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as Float

Type called "TestSuite":
    suite_name as String
    tests as List[TestResult]
    passed_count as Integer
    failed_count as Integer

Process called "create_test_suite" that takes name as String returns TestSuite:
    Let suite be TestSuite
    Set suite.suite_name to name
    Set suite.tests to List[TestResult].create()
    Set suite.passed_count to 0
    Set suite.failed_count to 0
    Return suite

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns Void:
    suite.tests.add(result)
    If result.passed:
        Set suite.passed_count to suite.passed_count + 1
    Otherwise:
        Set suite.failed_count to suite.failed_count + 1

Process called "assert_equals" that takes actual as Float, expected as Float, tolerance as Float returns Boolean:
    Let difference be MathOps.subtract(actual.to_string(), expected.to_string(), 50).result_value.to_float()
    Let abs_difference be If difference >= 0.0 then difference Otherwise (0.0 - difference)
    Return abs_difference <= tolerance

Process called "assert_vector_equals" that takes actual as List[Float], expected as List[Float], tolerance as Float returns Boolean:
    If actual.length != expected.length:
        Return false
    
    For i in 0 to actual.length - 1:
        If not assert_equals(actual[i], expected[i], tolerance):
            Return false
    
    Return true

Process called "assert_convergence" that takes losses as List[Float] returns Boolean:
    Note: Check that losses generally decrease over time
    If losses.length < 3:
        Return false
    
    Let initial_loss be losses[0]
    Let final_loss be losses[losses.length - 1]
    
    Note: Final loss should be significantly less than initial loss
    Return (final_loss < 0.5 * initial_loss)

Note: ===== SGD Optimizer Tests =====

Process called "test_sgd_basic" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "SGD Basic Optimization"
    
    Try:
        Note: Simple quadratic function: f(x) = x² - 4x + 4 = (x-2)²
        Note: Minimum at x = 2, f(2) = 0
        Note: Gradient: f'(x) = 2x - 4
        
        Let initial_params be [0.0]  Note: Start at x = 0
        Let learning_rate be 0.1
        
        Note: Create SGD optimizer
        Let sgd_optimizer be Optimization.create_sgd_optimizer(learning_rate)
        
        Note: Run optimization steps
        Let current_params be initial_params
        Let losses be List[Float].create()
        
        For iteration in 0 to 49:  Note: 50 iterations
            Note: Calculate gradient: 2x - 4
            Let gradients be [2.0 * current_params[0] - 4.0]
            
            Note: Calculate loss: (x-2)²
            Let loss be (current_params[0] - 2.0) * (current_params[0] - 2.0)
            losses.add(loss)
            
            Note: Update parameters
            Set current_params to Optimization.sgd_update(sgd_optimizer, current_params, gradients)
        
        Note: Check convergence to minimum (x ≈ 2)
        If not assert_equals(current_params[0], 2.0, 0.1):
            Set result.passed to false
            Set result.error_message to "SGD did not converge to minimum. Final x: " + current_params[0]
            Return result
        
        Note: Check that losses decreased
        If not assert_convergence(losses):
            Set result.passed to false
            Set result.error_message to "SGD losses did not converge properly"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_sgd_with_momentum" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "SGD with Momentum"
    
    Try:
        Let initial_params be [0.0, 0.0]  Note: 2D problem
        Let learning_rate be 0.01
        Let momentum be 0.9
        
        Note: Create SGD with momentum optimizer
        Let sgd_momentum be Optimization.create_sgd_momentum_optimizer(learning_rate, momentum)
        
        Note: Optimize Rosenbrock function (harder test case)
        Note: f(x,y) = (1-x)² + 100(y-x²)²
        Note: Minimum at (1,1), f(1,1) = 0
        
        Let current_params be initial_params
        Let losses be List[Float].create()
        
        For iteration in 0 to 99:  Note: 100 iterations
            Let x be current_params[0]
            Let y be current_params[1]
            
            Note: Calculate gradients
            Let grad_x be -2.0 * (1.0 - x) - 400.0 * x * (y - x * x)
            Let grad_y be 200.0 * (y - x * x)
            Let gradients be [grad_x, grad_y]
            
            Note: Calculate loss
            Let loss be (1.0 - x) * (1.0 - x) + 100.0 * (y - x * x) * (y - x * x)
            losses.add(loss)
            
            Note: Update parameters
            Set current_params to Optimization.sgd_momentum_update(sgd_momentum, current_params, gradients)
        
        Note: Check that we're moving towards the minimum (may not fully converge in 100 steps)
        Let final_loss be losses[losses.length - 1]
        Let initial_loss be losses[0]
        
        If final_loss >= initial_loss:
            Set result.passed to false
            Set result.error_message to "SGD with momentum did not reduce loss"
            Return result
        
        Note: Check that momentum helps (should converge faster than plain SGD)
        If not assert_convergence(losses):
            Set result.passed to false
            Set result.error_message to "SGD with momentum did not show convergence"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Adam Optimizer Tests =====

Process called "test_adam_optimizer" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Adam Optimizer"
    
    Try:
        Let initial_params be [0.0]
        Let learning_rate be 0.1
        Let beta1 be 0.9    Note: First moment decay
        Let beta2 be 0.999  Note: Second moment decay
        Let epsilon be 1e-8
        
        Note: Create Adam optimizer
        Let adam_optimizer be Optimization.create_adam_optimizer(learning_rate, beta1, beta2, epsilon)
        
        Note: Optimize simple quadratic: f(x) = (x-3)² + 1
        Note: Minimum at x = 3, f(3) = 1
        
        Let current_params be initial_params
        Let losses be List[Float].create()
        
        For iteration in 0 to 49:  Note: 50 iterations
            Let x be current_params[0]
            
            Note: Gradient: f'(x) = 2(x-3)
            Let gradients be [2.0 * (x - 3.0)]
            
            Note: Loss: (x-3)² + 1
            Let loss be (x - 3.0) * (x - 3.0) + 1.0
            losses.add(loss)
            
            Note: Update parameters
            Set current_params to Optimization.adam_update(adam_optimizer, current_params, gradients, iteration + 1)
        
        Note: Check convergence to minimum
        If not assert_equals(current_params[0], 3.0, 0.1):
            Set result.passed to false
            Set result.error_message to "Adam did not converge to minimum. Final x: " + current_params[0]
            Return result
        
        Note: Check final loss is close to minimum value (1.0)
        Let final_loss be losses[losses.length - 1]
        If not assert_equals(final_loss, 1.0, 0.1):
            Set result.passed to false
            Set result.error_message to "Adam did not reach minimum loss value"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_adam_bias_correction" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Adam Bias Correction"
    
    Try:
        Let initial_params be [0.0]
        Let learning_rate be 0.1
        Let beta1 be 0.9
        Let beta2 be 0.999
        Let epsilon be 1e-8
        
        Let adam_optimizer be Optimization.create_adam_optimizer(learning_rate, beta1, beta2, epsilon)
        
        Note: Test that bias correction works properly in early iterations
        Let current_params be initial_params
        Let gradients be [1.0]  Note: Constant gradient
        
        Note: First few updates should show bias correction effect
        Let step_1_params be Optimization.adam_update(adam_optimizer, current_params, gradients, 1)
        Let step_2_params be Optimization.adam_update(adam_optimizer, step_1_params, gradients, 2)
        Let step_3_params be Optimization.adam_update(adam_optimizer, step_2_params, gradients, 3)
        
        Note: Updates should be progressively larger due to bias correction
        Let update_1 be step_1_params[0] - current_params[0]
        Let update_2 be step_2_params[0] - step_1_params[0]
        Let update_3 be step_3_params[0] - step_2_params[0]
        
        Note: Early updates should be significant due to bias correction
        If update_1 <= 0.0 or update_2 <= 0.0 or update_3 <= 0.0:
            Set result.passed to false
            Set result.error_message to "Adam updates should be positive with positive gradients"
            Return result
        
        Note: All updates should be in same direction (decreasing the parameter)
        If update_1 <= 0.0 or update_2 <= 0.0 or update_3 <= 0.0:
            Set result.passed to false
            Set result.error_message to "Adam bias correction not working properly"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== AdaGrad Optimizer Tests =====

Process called "test_adagrad_optimizer" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "AdaGrad Optimizer"
    
    Try:
        Let initial_params be [0.0, 0.0]
        Let learning_rate be 0.1
        Let epsilon be 1e-8
        
        Note: Create AdaGrad optimizer
        Let adagrad_optimizer be Optimization.create_adagrad_optimizer(learning_rate, epsilon)
        
        Note: Test with different gradient magnitudes for each parameter
        Let current_params be initial_params
        Let losses be List[Float].create()
        
        For iteration in 0 to 29:  Note: 30 iterations
            Note: Different gradient magnitudes: large for param 0, small for param 1
            Let grad_0 be 2.0 * current_params[0] - 4.0  Note: f₀(x₀) = x₀² - 4x₀
            Let grad_1 be 0.1 * current_params[1] - 0.1  Note: f₁(x₁) = 0.05x₁² - 0.1x₁
            Let gradients be [grad_0, grad_1]
            
            Note: Combined loss
            Let loss_0 be current_params[0] * current_params[0] - 4.0 * current_params[0]
            Let loss_1 be 0.05 * current_params[1] * current_params[1] - 0.1 * current_params[1]
            let combined_loss be loss_0 + loss_1
            losses.add(combined_loss)
            
            Note: Update parameters
            Set current_params to Optimization.adagrad_update(adagrad_optimizer, current_params, gradients)
        
        Note: AdaGrad should adapt learning rates based on gradient history
        Note: Parameter with larger gradients should have smaller effective learning rate
        Note: Check that both parameters moved towards their optima
        If not assert_equals(current_params[0], 2.0, 0.2):  Note: Minimum of f₀ at x₀ = 2
            Set result.passed to false
            Set result.error_message to "AdaGrad param 0 convergence issue. Got: " + current_params[0]
            Return result
        
        If not assert_equals(current_params[1], 1.0, 0.2):  Note: Minimum of f₁ at x₁ = 1
            Set result.passed to false
            Set result.error_message to "AdaGrad param 1 convergence issue. Got: " + current_params[1]
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== RMSprop Optimizer Tests =====

Process called "test_rmsprop_optimizer" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "RMSprop Optimizer"
    
    Try:
        Let initial_params be [0.0]
        Let learning_rate be 0.01
        Let decay_rate be 0.9
        Let epsilon be 1e-8
        
        Note: Create RMSprop optimizer
        Let rmsprop_optimizer be Optimization.create_rmsprop_optimizer(learning_rate, decay_rate, epsilon)
        
        Note: Test with oscillating gradients (RMSprop should handle this well)
        Let current_params be initial_params
        Let losses be List[Float].create()
        
        For iteration in 0 to 99:  Note: 100 iterations
            Let x be current_params[0]
            
            Note: Oscillating function: f(x) = x⁴ - 2x² + 1
            Note: Gradients: f'(x) = 4x³ - 4x = 4x(x² - 1)
            Let gradient be 4.0 * x * (x * x - 1.0)
            let gradients be [gradient]
            
            Note: Loss: x⁴ - 2x² + 1
            Let loss be x * x * x * x - 2.0 * x * x + 1.0
            losses.add(loss)
            
            Note: Update parameters
            Set current_params to Optimization.rmsprop_update(rmsprop_optimizer, current_params, gradients)
        
        Note: Function has minima at x = ±1, f(±1) = 0
        Note: Starting from 0, should converge to one of the minima
        Let final_x be current_params[0]
        Let converged_to_minimum be (assert_equals(final_x, 1.0, 0.1) or assert_equals(final_x, -1.0, 0.1))
        
        If not converged_to_minimum:
            Set result.passed to false
            Set result.error_message to "RMSprop did not converge to minimum. Final x: " + final_x
            Return result
        
        Note: Check loss improvement
        If not assert_convergence(losses):
            Set result.passed to false
            Set result.error_message to "RMSprop did not show proper convergence"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Learning Rate Scheduling Tests =====

Process called "test_learning_rate_decay" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Learning Rate Decay Scheduler"
    
    Try:
        Let initial_lr be 0.1
        Let decay_rate be 0.95
        Let decay_steps be 10
        
        Note: Create exponential decay scheduler
        Let lr_scheduler be Optimization.create_exponential_decay_scheduler(initial_lr, decay_rate, decay_steps)
        
        Note: Test learning rate decay over time
        Let step_0_lr be Optimization.get_learning_rate(lr_scheduler, 0)
        Let step_10_lr be Optimization.get_learning_rate(lr_scheduler, 10)
        Let step_20_lr be Optimization.get_learning_rate(lr_scheduler, 20)
        
        Note: Check initial learning rate
        If not assert_equals(step_0_lr, initial_lr, 1e-10):
            Set result.passed to false
            Set result.error_message to "Initial learning rate incorrect"
            Return result
        
        Note: Check decay after 10 steps: lr = initial_lr * decay_rate^(10/10) = 0.1 * 0.95
        Let expected_lr_10 be initial_lr * decay_rate
        If not assert_equals(step_10_lr, expected_lr_10, 1e-6):
            Set result.passed to false
            Set result.error_message to "Learning rate decay after 10 steps incorrect. Expected: " + expected_lr_10 + ", Got: " + step_10_lr
            Return result
        
        Note: Check decay after 20 steps: lr = initial_lr * decay_rate^(20/10) = 0.1 * 0.95²
        Let expected_lr_20 be initial_lr * decay_rate * decay_rate
        If not assert_equals(step_20_lr, expected_lr_20, 1e-6):
            Set result.passed to false
            Set result.error_message to "Learning rate decay after 20 steps incorrect"
            Return result
        
        Note: Learning rate should be monotonically decreasing
        If step_0_lr <= step_10_lr or step_10_lr <= step_20_lr:
            Set result.passed to false
            Set result.error_message to "Learning rate should decrease monotonically"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_cosine_annealing" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Cosine Annealing Scheduler"
    
    Try:
        Let initial_lr be 0.1
        Let min_lr be 0.001
        Let total_steps be 100
        
        Note: Create cosine annealing scheduler
        Let cosine_scheduler be Optimization.create_cosine_annealing_scheduler(initial_lr, min_lr, total_steps)
        
        Note: Test key points in the cosine schedule
        Let step_0_lr be Optimization.get_learning_rate(cosine_scheduler, 0)
        Let step_25_lr be Optimization.get_learning_rate(cosine_scheduler, 25)
        Let step_50_lr be Optimization.get_learning_rate(cosine_scheduler, 50)
        Let step_100_lr be Optimization.get_learning_rate(cosine_scheduler, 100)
        
        Note: Check initial and final learning rates
        If not assert_equals(step_0_lr, initial_lr, 1e-6):
            Set result.passed to false
            Set result.error_message to "Cosine annealing initial LR incorrect"
            Return result
        
        If not assert_equals(step_100_lr, min_lr, 1e-6):
            Set result.passed to false
            Set result.error_message to "Cosine annealing final LR incorrect"
            Return result
        
        Note: At halfway point (step 50), LR should be at midpoint
        Let expected_mid_lr be (initial_lr + min_lr) / 2.0
        If not assert_equals(step_50_lr, expected_mid_lr, 0.01):
            Set result.passed to false
            Set result.error_message to "Cosine annealing midpoint LR incorrect"
            Return result
        
        Note: Learning rate should be decreasing overall
        If step_0_lr <= step_50_lr or step_50_lr <= step_100_lr:
            Set result.passed to false
            Set result.error_message to "Cosine annealing should decrease overall"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Optimizer Comparison Tests =====

Process called "test_optimizer_comparison" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Optimizer Performance Comparison"
    
    Try:
        Note: Test all optimizers on the same problem to compare performance
        Let initial_params be [0.0, 0.0]
        Let iterations be 50
        
        Note: Test function: f(x,y) = x² + 10y² (elliptical)
        Note: Minimum at (0,0), f(0,0) = 0
        
        Note: SGD optimizer
        Let sgd_opt be Optimization.create_sgd_optimizer(0.01)
        Let sgd_params be initial_params
        Let sgd_final_loss be 0.0
        
        For iter in 0 to iterations - 1:
            Let gradients be [2.0 * sgd_params[0], 20.0 * sgd_params[1]]
            Set sgd_params to Optimization.sgd_update(sgd_opt, sgd_params, gradients)
        Set sgd_final_loss to sgd_params[0] * sgd_params[0] + 10.0 * sgd_params[1] * sgd_params[1]
        
        Note: Adam optimizer
        Let adam_opt be Optimization.create_adam_optimizer(0.01, 0.9, 0.999, 1e-8)
        Let adam_params be initial_params
        Let adam_final_loss be 0.0
        
        For iter in 0 to iterations - 1:
            Let gradients be [2.0 * adam_params[0], 20.0 * adam_params[1]]
            Set adam_params to Optimization.adam_update(adam_opt, adam_params, gradients, iter + 1)
        Set adam_final_loss to adam_params[0] * adam_params[0] + 10.0 * adam_params[1] * adam_params[1]
        
        Note: Both should reduce the loss significantly
        If sgd_final_loss >= 0.5:
            Set result.passed to false
            Set result.error_message to "SGD did not optimize well enough"
            Return result
        
        If adam_final_loss >= 0.5:
            Set result.passed to false
            Set result.error_message to "Adam did not optimize well enough"
            Return result
        
        Note: Adam should generally perform better on this type of problem
        If adam_final_loss > sgd_final_loss * 2.0:
            Set result.passed to false
            Set result.error_message to "Adam should perform better than SGD on elliptical problems"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Main Test Runner =====

Process called "run_optimization_tests" that returns TestSuite:
    Let suite be create_test_suite("AI Optimization Tests")
    
    Note: SGD tests
    add_test_result(suite, test_sgd_basic())
    add_test_result(suite, test_sgd_with_momentum())
    
    Note: Adam tests
    add_test_result(suite, test_adam_optimizer())
    add_test_result(suite, test_adam_bias_correction())
    
    Note: AdaGrad tests
    add_test_result(suite, test_adagrad_optimizer())
    
    Note: RMSprop tests
    add_test_result(suite, test_rmsprop_optimizer())
    
    Note: Learning rate scheduling tests
    add_test_result(suite, test_learning_rate_decay())
    add_test_result(suite, test_cosine_annealing())
    
    Note: Comparison tests
    add_test_result(suite, test_optimizer_comparison())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns Void:
    Print("=== " + suite.suite_name + " ===")
    Print("Passed: " + suite.passed_count + ", Failed: " + suite.failed_count)
    Print("")
    
    For Each test in suite.tests:
        Let status be If test.passed then "PASS" Otherwise "FAIL"
        Print("[" + status + "] " + test.test_name)
        If not test.passed:
            Print("  Error: " + test.error_message)
    
    Print("")
    Let success_rate be (suite.passed_count.to_float() / (suite.passed_count + suite.failed_count).to_float()) * 100.0
    Print("Success Rate: " + success_rate + "%")

Note: Run tests if executed directly
Let test_suite be run_optimization_tests()
print_test_results(test_suite)