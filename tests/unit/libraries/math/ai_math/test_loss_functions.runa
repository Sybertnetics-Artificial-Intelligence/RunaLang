Note:
Unit Tests for Loss Functions Module
Testing various loss functions used in machine learning including MSE, cross-entropy,
hinge loss, focal loss, and their derivatives for gradient computation.
:End Note

Import "math/ai_math/loss_functions" as LossFunctions
Import "math/core/operations" as MathOps
Import "math/core/comparison" as MathCompare
Import "dev/debug/errors/core" as Errors

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as Float

Type called "TestSuite":
    suite_name as String
    tests as List[TestResult]
    passed_count as Integer
    failed_count as Integer

Process called "create_test_suite" that takes name as String returns TestSuite:
    Let suite be TestSuite
    Set suite.suite_name to name
    Set suite.tests to List[TestResult].create()
    Set suite.passed_count to 0
    Set suite.failed_count to 0
    Return suite

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns Void:
    suite.tests.add(result)
    If result.passed:
        Set suite.passed_count to suite.passed_count + 1
    Otherwise:
        Set suite.failed_count to suite.failed_count + 1

Process called "assert_equals" that takes actual as Float, expected as Float, tolerance as Float returns Boolean:
    Let difference be MathOps.subtract(actual.to_string(), expected.to_string(), 50).result_value.to_float()
    Let abs_difference be If difference >= 0.0 then difference Otherwise (0.0 - difference)
    Return abs_difference <= tolerance

Process called "assert_vector_equals" that takes actual as List[Float], expected as List[Float], tolerance as Float returns Boolean:
    If actual.length != expected.length:
        Return false
    
    For i in 0 to actual.length - 1:
        If not assert_equals(actual[i], expected[i], tolerance):
            Return false
    
    Return true

Note: ===== Mean Squared Error Tests =====

Process called "test_mse_loss" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Mean Squared Error Loss"
    
    Try:
        Note: Test simple case: predictions = [1, 2, 3], targets = [1, 1, 1]
        Let predictions be [1.0, 2.0, 3.0]
        Let targets be [1.0, 1.0, 1.0]
        
        Note: Expected MSE = ((1-1)² + (2-1)² + (3-1)²) / 3 = (0 + 1 + 4) / 3 = 5/3 ≈ 1.6667
        Let expected_mse be 5.0 / 3.0
        
        Let actual_mse be LossFunctions.mean_squared_error(predictions, targets)
        
        If not assert_equals(actual_mse, expected_mse, 1e-6):
            Set result.passed to false
            Set result.error_message to "MSE calculation incorrect. Expected: " + expected_mse + ", Got: " + actual_mse
            Return result
        
        Note: Test perfect predictions (loss should be 0)
        Let perfect_predictions be [1.0, 2.0, 3.0]
        Let perfect_targets be [1.0, 2.0, 3.0]
        Let perfect_mse be LossFunctions.mean_squared_error(perfect_predictions, perfect_targets)
        
        If not assert_equals(perfect_mse, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "MSE for perfect predictions should be 0"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_mse_derivative" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "MSE Loss Derivative"
    
    Try:
        Let predictions be [2.0, 3.0, 1.0]
        Let targets be [1.0, 2.0, 1.0]
        
        Note: MSE derivative = 2 * (predictions - targets) / n
        Note: Expected: 2 * ([2-1, 3-2, 1-1]) / 3 = 2 * [1, 1, 0] / 3 = [2/3, 2/3, 0]
        Let expected_derivative be [2.0/3.0, 2.0/3.0, 0.0]
        
        Let actual_derivative be LossFunctions.mse_derivative(predictions, targets)
        
        If not assert_vector_equals(actual_derivative, expected_derivative, 1e-6):
            Set result.passed to false
            Set result.error_message to "MSE derivative calculation incorrect"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Cross-Entropy Loss Tests =====

Process called "test_binary_cross_entropy" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Binary Cross-Entropy Loss"
    
    Try:
        Note: Test case: predictions = [0.9, 0.1, 0.8], targets = [1, 0, 1]
        Let predictions be [0.9, 0.1, 0.8]
        Let targets be [1.0, 0.0, 1.0]
        
        Note: BCE = -[1*log(0.9) + 0*log(0.1) + 1*log(0.8)] / 3
        Note: + -[(1-1)*log(1-0.9) + (1-0)*log(1-0.1) + (1-1)*log(1-0.8)] / 3
        Note: = -[log(0.9) + 0 + log(0.8)] / 3 - [0 + log(0.9) + 0] / 3
        Note: = -[log(0.9) + log(0.8) + log(0.9)] / 3
        
        Let actual_bce be LossFunctions.binary_cross_entropy(predictions, targets)
        
        Note: Verify result is positive (cross-entropy should always be positive)
        If actual_bce <= 0.0:
            Set result.passed to false
            Set result.error_message to "Binary cross-entropy should be positive"
            Return result
        
        Note: Test with perfect predictions
        Let perfect_predictions be [1.0, 0.0, 1.0]
        Let perfect_targets be [1.0, 0.0, 1.0]
        Let perfect_bce be LossFunctions.binary_cross_entropy(perfect_predictions, perfect_targets)
        
        Note: Perfect predictions should give very low loss (but not exactly 0 due to log(1) issues)
        If perfect_bce > 1e-6:
            Set result.passed to false
            Set result.error_message to "Binary cross-entropy for perfect predictions should be very low"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_categorical_cross_entropy" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Categorical Cross-Entropy Loss"
    
    Try:
        Note: Test 3-class classification
        Let predictions be [
            [0.7, 0.2, 0.1],  Note: Class 0 (correct)
            [0.1, 0.8, 0.1],  Note: Class 1 (correct) 
            [0.2, 0.3, 0.5]   Note: Class 2 (correct)
        ]
        
        Let targets be [
            [1.0, 0.0, 0.0],  Note: One-hot for class 0
            [0.0, 1.0, 0.0],  Note: One-hot for class 1
            [0.0, 0.0, 1.0]   Note: One-hot for class 2
        ]
        
        Let actual_cce be LossFunctions.categorical_cross_entropy(predictions, targets)
        
        Note: CCE = -[1*log(0.7) + 1*log(0.8) + 1*log(0.5)] / 3
        Note: Should be positive
        If actual_cce <= 0.0:
            Set result.passed to false
            Set result.error_message to "Categorical cross-entropy should be positive"
            Return result
        
        Note: Test with perfect predictions
        Let perfect_predictions be [
            [1.0, 0.0, 0.0],
            [0.0, 1.0, 0.0],
            [0.0, 0.0, 1.0]
        ]
        
        Let perfect_cce be LossFunctions.categorical_cross_entropy(perfect_predictions, targets)
        
        If perfect_cce > 1e-6:
            Set result.passed to false
            Set result.error_message to "Categorical cross-entropy for perfect predictions should be very low"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Hinge Loss Tests =====

Process called "test_hinge_loss" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Hinge Loss for SVM"
    
    Try:
        Note: Test case: predictions = [0.8, -0.3, 1.2], targets = [1, -1, 1]
        Let predictions be [0.8, -0.3, 1.2]
        Let targets be [1.0, -1.0, 1.0]
        
        Note: Hinge loss = max(0, 1 - y_true * y_pred)
        Note: For sample 1: max(0, 1 - 1*0.8) = max(0, 0.2) = 0.2
        Note: For sample 2: max(0, 1 - (-1)*(-0.3)) = max(0, 1 - 0.3) = max(0, 0.7) = 0.7
        Note: For sample 3: max(0, 1 - 1*1.2) = max(0, -0.2) = 0
        Note: Average = (0.2 + 0.7 + 0) / 3 = 0.3
        
        Let expected_hinge be 0.3
        Let actual_hinge be LossFunctions.hinge_loss(predictions, targets)
        
        If not assert_equals(actual_hinge, expected_hinge, 1e-6):
            Set result.passed to false
            Set result.error_message to "Hinge loss calculation incorrect. Expected: " + expected_hinge + ", Got: " + actual_hinge
            Return result
        
        Note: Test with perfect predictions (large margin)
        Let perfect_predictions be [2.0, -2.0, 3.0]
        Let perfect_targets be [1.0, -1.0, 1.0]
        Let perfect_hinge be LossFunctions.hinge_loss(perfect_predictions, perfect_targets)
        
        If not assert_equals(perfect_hinge, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "Hinge loss for well-separated predictions should be 0"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Huber Loss Tests =====

Process called "test_huber_loss" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Huber Loss (Robust to Outliers)"
    
    Try:
        Let predictions be [1.0, 3.0, 10.0]
        Let targets be [1.0, 2.0, 2.0]
        Let delta be 1.0
        
        Note: Huber loss = 0.5 * (y_true - y_pred)² if |error| <= delta
        Note:            = delta * (|error| - 0.5 * delta) otherwise
        
        Note: Sample 1: error = 1-1 = 0, |error| <= 1, so loss = 0.5 * 0² = 0
        Note: Sample 2: error = 2-3 = -1, |error| = 1 <= 1, so loss = 0.5 * 1² = 0.5  
        Note: Sample 3: error = 2-10 = -8, |error| = 8 > 1, so loss = 1 * (8 - 0.5) = 7.5
        Note: Average = (0 + 0.5 + 7.5) / 3 = 8/3 ≈ 2.667
        
        Let expected_huber be 8.0 / 3.0
        Let actual_huber be LossFunctions.huber_loss(predictions, targets, delta)
        
        If not assert_equals(actual_huber, expected_huber, 1e-6):
            Set result.passed to false
            Set result.error_message to "Huber loss calculation incorrect. Expected: " + expected_huber + ", Got: " + actual_huber
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Focal Loss Tests =====

Process called "test_focal_loss" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Focal Loss for Imbalanced Data"
    
    Try:
        Note: Test with easy and hard examples
        Let predictions be [0.9, 0.6, 0.1]  Note: Easy, medium, hard examples
        Let targets be [1.0, 1.0, 1.0]      Note: All positive class
        Let alpha be 0.25
        Let gamma be 2.0
        
        Note: Focal loss = -alpha * (1-p)^gamma * log(p) for positive class
        Note: Sample 1: -0.25 * (1-0.9)² * log(0.9) = -0.25 * 0.01 * log(0.9) (small loss for easy example)
        Note: Sample 2: -0.25 * (1-0.6)² * log(0.6) = -0.25 * 0.16 * log(0.6) (medium loss)
        Note: Sample 3: -0.25 * (1-0.1)² * log(0.1) = -0.25 * 0.81 * log(0.1) (large loss for hard example)
        
        Let actual_focal be LossFunctions.focal_loss(predictions, targets, alpha, gamma)
        
        Note: Focal loss should be positive
        If actual_focal <= 0.0:
            Set result.passed to false
            Set result.error_message to "Focal loss should be positive"
            Return result
        
        Note: Test that focal loss down-weights easy examples compared to cross-entropy
        Let ce_loss be LossFunctions.binary_cross_entropy(predictions, targets)
        
        Note: Focal loss should be less than standard cross-entropy for easy examples
        If actual_focal >= ce_loss:
            Set result.passed to false  
            Set result.error_message to "Focal loss should down-weight easy examples"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Kullback-Leibler Divergence Tests =====

Process called "test_kl_divergence" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Kullback-Leibler Divergence"
    
    Try:
        Note: Test probability distributions
        Let p_distribution be [0.5, 0.3, 0.2]  Note: True distribution
        Let q_distribution be [0.4, 0.4, 0.2]  Note: Predicted distribution
        
        Note: KL(P||Q) = Σ P(i) * log(P(i) / Q(i))
        Note: = 0.5 * log(0.5/0.4) + 0.3 * log(0.3/0.4) + 0.2 * log(0.2/0.2)
        Note: = 0.5 * log(1.25) + 0.3 * log(0.75) + 0.2 * log(1)
        
        Let actual_kl be LossFunctions.kl_divergence(p_distribution, q_distribution)
        
        Note: KL divergence should be non-negative
        If actual_kl < 0.0:
            Set result.passed to false
            Set result.error_message to "KL divergence should be non-negative"
            Return result
        
        Note: Test with identical distributions (should give 0)
        Let identical_kl be LossFunctions.kl_divergence(p_distribution, p_distribution)
        
        If not assert_equals(identical_kl, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "KL divergence for identical distributions should be 0"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Loss Function Properties Tests =====

Process called "test_loss_function_properties" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Loss Function Properties"
    
    Try:
        Let predictions be [0.8, 0.3, 0.9]
        Let targets be [1.0, 0.0, 1.0]
        
        Note: Test that all loss functions are non-negative
        Let mse be LossFunctions.mean_squared_error(predictions, targets)
        Let bce be LossFunctions.binary_cross_entropy(predictions, targets)
        Let hinge be LossFunctions.hinge_loss(predictions, targets)
        
        If mse < 0.0 or bce < 0.0 or hinge < 0.0:
            Set result.passed to false
            Set result.error_message to "All loss functions should be non-negative"
            Return result
        
        Note: Test loss function minimums
        Let perfect_predictions be [1.0, 0.0, 1.0]
        Let perfect_mse be LossFunctions.mean_squared_error(perfect_predictions, targets)
        Let perfect_bce be LossFunctions.binary_cross_entropy(perfect_predictions, targets)
        
        If perfect_mse > 1e-6:
            Set result.passed to false
            Set result.error_message to "MSE should be near zero for perfect predictions"
            Return result
        
        If perfect_bce > 1e-6:
            Set result.passed to false
            Set result.error_message to "BCE should be near zero for perfect predictions"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Main Test Runner =====

Process called "run_loss_functions_tests" that returns TestSuite:
    Let suite be create_test_suite("Loss Functions Tests")
    
    Note: Mean squared error tests
    add_test_result(suite, test_mse_loss())
    add_test_result(suite, test_mse_derivative())
    
    Note: Cross-entropy tests
    add_test_result(suite, test_binary_cross_entropy())
    add_test_result(suite, test_categorical_cross_entropy())
    
    Note: Hinge loss tests
    add_test_result(suite, test_hinge_loss())
    
    Note: Robust loss tests
    add_test_result(suite, test_huber_loss())
    add_test_result(suite, test_focal_loss())
    
    Note: Divergence tests
    add_test_result(suite, test_kl_divergence())
    
    Note: General properties tests
    add_test_result(suite, test_loss_function_properties())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns Void:
    Print("=== " + suite.suite_name + " ===")
    Print("Passed: " + suite.passed_count + ", Failed: " + suite.failed_count)
    Print("")
    
    For Each test in suite.tests:
        Let status be If test.passed then "PASS" Otherwise "FAIL"
        Print("[" + status + "] " + test.test_name)
        If not test.passed:
            Print("  Error: " + test.error_message)
    
    Print("")
    Let success_rate be (suite.passed_count.to_float() / (suite.passed_count + suite.failed_count).to_float()) * 100.0
    Print("Success Rate: " + success_rate + "%")

Note: Run tests if executed directly
Let test_suite be run_loss_functions_tests()
print_test_results(test_suite)