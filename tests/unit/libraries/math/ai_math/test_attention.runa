Note: Comprehensive Unit Tests for Attention Mechanisms Module
Note: Tests attention mechanisms, multi-head attention, positional encoding, 
Note: and transformer architectures. Validates numerical accuracy, edge cases,
Note: and mathematical properties for all attention operations.

Import "math/ai_math/attention" as Attention
Import "math/core/operations" as MathOps
Import "math/engine/linalg/core" as LinAlg
Import "math/engine/linalg/tensor" as TensorOps
Import "dev/debug/errors/core" as Errors

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as String
    expected_value as String
    actual_value as String

Type called "TestSuite":
    suite_name as String
    total_tests as Integer
    passed_tests as Integer
    failed_tests as Integer
    results as List[TestResult]

Note: ===== Test Helper Functions =====

Process called "assert_equals_float" that takes actual as String, expected as String, tolerance as String, test_name as String returns TestResult:
    Note: Assert that two floating point values are equal within tolerance
    
    Let diff_result be MathOps.subtract(actual, expected, 15)
    If diff_result.overflow_occurred:
        Return TestResult with test_name: test_name, passed: false, error_message: "Overflow in difference calculation", execution_time: "0", expected_value: expected, actual_value: actual
    
    Let abs_diff be MathOps.absolute_value(diff_result.result_value)
    Let comparison be MathOps.compare(abs_diff.result_value, tolerance)
    
    If comparison <= 0:
        Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected, actual_value: actual
    Otherwise:
        Return TestResult with test_name: test_name, passed: false, error_message: "Values differ by more than tolerance", execution_time: "0", expected_value: expected, actual_value: actual

Process called "assert_matrix_equals" that takes actual as Matrix[Float], expected as Matrix[Float], tolerance as String, test_name as String returns TestResult:
    Note: Assert that two matrices are equal within tolerance
    
    If actual.rows != expected.rows or actual.columns != expected.columns:
        Return TestResult with test_name: test_name, passed: false, error_message: "Matrix dimensions do not match", execution_time: "0", expected_value: "Matrix dimensions mismatch", actual_value: "Matrix dimensions mismatch"
    
    Let i be 0
    While i < actual.rows:
        Let j be 0
        While j < actual.columns:
            Let actual_val be actual.entries.get(i).get(j)
            Let expected_val be expected.entries.get(i).get(j)
            
            Let element_test be assert_equals_float(actual_val, expected_val, tolerance, test_name + " element check")
            If not element_test.passed:
                Return element_test
            Set j to j + 1
        Set i to i + 1
    
    Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: "Matrix match", actual_value: "Matrix match"

Process called "assert_throws_error" that takes test_operation as String, expected_error as String, test_name as String returns TestResult:
    Note: Assert that operation throws expected error (simplified for testing)
    Return TestResult with test_name: test_name, passed: true, error_message: "", execution_time: "0", expected_value: expected_error, actual_value: "Error thrown as expected"

Process called "create_test_matrix" that takes rows as Integer, cols as Integer, pattern as String returns Matrix[Float]:
    Note: Create test matrix with specified pattern (identity, ones, sequential, random)
    
    Let entries be List[List[String]]()
    
    If pattern == "identity":
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                If i == j:
                    Call row.add("1.0")
                Otherwise:
                    Call row.add("0.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "ones":
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add("1.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "sequential":
        Let value be 1
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add(value.to_string())
                Set value to value + 1
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise If pattern == "small":
        Let value be 0.1
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add(value.to_string())
                Set value to value + 0.1
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    Otherwise:
        Note: Default to zeros
        Let i be 0
        While i < rows:
            Let row be List[String]()
            Let j be 0
            While j < cols:
                Call row.add("0.0")
                Set j to j + 1
            Call entries.add(row)
            Set i to i + 1
    
    Return LinAlg.create_matrix(entries, "float")

Note: ===== Scaled Dot-Product Attention Tests =====

Process called "test_scaled_dot_product_attention_basic" that takes no parameters returns TestResult:
    Note: Test basic scaled dot-product attention computation
    
    Try:
        Let query be create_test_matrix(3, 4, "sequential")
        Let key be create_test_matrix(3, 4, "small")
        Let value be create_test_matrix(3, 4, "ones")
        
        Let result be Attention.scaled_dot_product_attention(query, key, value, None)
        Let output be result.first
        Let attention_weights be result.second
        
        Note: Check output dimensions
        If output.rows != 3 or output.columns != 4:
            Return TestResult with test_name: "scaled_dot_product_attention_basic", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: "3x4", actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Note: Check attention weights dimensions
        If attention_weights.rows != 3 or attention_weights.columns != 3:
            Return TestResult with test_name: "scaled_dot_product_attention_basic", passed: false, error_message: "Attention weights dimensions incorrect", execution_time: "0", expected_value: "3x3", actual_value: attention_weights.rows.to_string() + "x" + attention_weights.columns.to_string()
        
        Note: Check attention weights sum to 1 (approximately)
        Let i be 0
        While i < attention_weights.rows:
            Let row_sum be "0.0"
            Let j be 0
            While j < attention_weights.columns:
                Let weight_val be attention_weights.entries.get(i).get(j)
                Let sum_result be MathOps.add(row_sum, weight_val, 15)
                Set row_sum to sum_result.result_value
                Set j to j + 1
            
            Let sum_check be assert_equals_float(row_sum, "1.0", "0.001", "attention weights sum")
            If not sum_check.passed:
                Return sum_check
            Set i to i + 1
        
        Return TestResult with test_name: "scaled_dot_product_attention_basic", passed: true, error_message: "", execution_time: "0", expected_value: "Valid attention computation", actual_value: "Valid attention computation"
    
    Catch error:
        Return TestResult with test_name: "scaled_dot_product_attention_basic", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_attention_with_mask" that takes no parameters returns TestResult:
    Note: Test attention with causal mask
    
    Try:
        Let query be create_test_matrix(3, 4, "sequential")
        Let key be create_test_matrix(3, 4, "small")
        Let value be create_test_matrix(3, 4, "ones")
        
        Note: Create causal mask (lower triangular)
        Let mask_entries be List[List[Boolean]]()
        Let i be 0
        While i < 3:
            Let mask_row be List[Boolean]()
            Let j be 0
            While j < 3:
                If j <= i:
                    Call mask_row.add(true)
                Otherwise:
                    Call mask_row.add(false)
                Set j to j + 1
            Call mask_entries.add(mask_row)
            Set i to i + 1
        
        Let mask_matrix be LinAlg.create_matrix_boolean(mask_entries)
        Let causal_mask be Attention.AttentionMask with mask_type: "causal", mask_values: mask_matrix, padding_value: -1000000.0
        
        Let result be Attention.scaled_dot_product_attention(query, key, value, causal_mask)
        Let attention_weights be result.second
        
        Note: Check that upper triangular weights are near zero (masked)
        Let upper_weight be attention_weights.entries.get(0).get(2)
        Let near_zero_check be assert_equals_float(upper_weight, "0.0", "0.001", "upper triangular masked")
        If not near_zero_check.passed:
            Return near_zero_check
        
        Return TestResult with test_name: "test_attention_with_mask", passed: true, error_message: "", execution_time: "0", expected_value: "Masked attention working", actual_value: "Masked attention working"
    
    Catch error:
        Return TestResult with test_name: "test_attention_with_mask", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_attention_softmax" that takes no parameters returns TestResult:
    Note: Test attention softmax computation
    
    Try:
        Let test_scores_entries be List[List[String]]()
        Let row1 be List[String]()
        Call row1.add("1.0")
        Call row1.add("2.0")
        Call row1.add("3.0")
        Call test_scores_entries.add(row1)
        
        Let test_scores be LinAlg.create_matrix(test_scores_entries, "float")
        Let softmax_result be Attention.attention_softmax(test_scores)
        
        Note: Check that softmax sums to 1
        Let row_sum be "0.0"
        Let j be 0
        While j < 3:
            Let softmax_val be softmax_result.entries.get(0).get(j)
            Let sum_result be MathOps.add(row_sum, softmax_val, 15)
            Set row_sum to sum_result.result_value
            Set j to j + 1
        
        Let sum_check be assert_equals_float(row_sum, "1.0", "0.001", "softmax sum")
        If not sum_check.passed:
            Return sum_check
        
        Note: Check softmax ordering (higher input should give higher probability)
        Let val1 be softmax_result.entries.get(0).get(0)
        Let val2 be softmax_result.entries.get(0).get(1)
        Let val3 be softmax_result.entries.get(0).get(2)
        
        Let comparison1 be MathOps.compare(val1, val2)
        Let comparison2 be MathOps.compare(val2, val3)
        
        If comparison1 >= 0 or comparison2 >= 0:
            Return TestResult with test_name: "test_attention_softmax", passed: false, error_message: "Softmax ordering incorrect", execution_time: "0", expected_value: "Monotonic increasing", actual_value: "Not monotonic"
        
        Return TestResult with test_name: "test_attention_softmax", passed: true, error_message: "", execution_time: "0", expected_value: "Valid softmax", actual_value: "Valid softmax"
    
    Catch error:
        Return TestResult with test_name: "test_attention_softmax", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Multi-Head Attention Tests =====

Process called "test_multi_head_attention" that takes no parameters returns TestResult:
    Note: Test multi-head attention mechanism
    
    Try:
        Let d_model be 8
        Let num_heads be 2
        Let seq_len be 4
        
        Let query be create_test_matrix(seq_len, d_model, "sequential")
        Let key be create_test_matrix(seq_len, d_model, "small")
        Let value be create_test_matrix(seq_len, d_model, "ones")
        
        Let config be Attention.AttentionConfig with d_model: d_model, num_heads: num_heads, d_k: d_model / num_heads, d_v: d_model / num_heads, dropout: 0.0, temperature: 1.0, causal: false
        
        Let query_weights be create_test_matrix(d_model, d_model, "identity")
        Let key_weights be create_test_matrix(d_model, d_model, "identity")
        Let value_weights be create_test_matrix(d_model, d_model, "identity")
        Let output_weights be create_test_matrix(d_model, d_model, "identity")
        
        Let weights be Attention.AttentionWeights with query_weights: query_weights, key_weights: key_weights, value_weights: value_weights, output_weights: output_weights
        
        Let result be Attention.multi_head_attention(query, key, value, config, weights)
        Let output be result.first
        Let attention_weights_list be result.second
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_multi_head_attention", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Note: Check number of attention heads
        If attention_weights_list.length != num_heads:
            Return TestResult with test_name: "test_multi_head_attention", passed: false, error_message: "Number of attention heads incorrect", execution_time: "0", expected_value: num_heads.to_string(), actual_value: attention_weights_list.length.to_string()
        
        Return TestResult with test_name: "test_multi_head_attention", passed: true, error_message: "", execution_time: "0", expected_value: "Valid multi-head attention", actual_value: "Valid multi-head attention"
    
    Catch error:
        Return TestResult with test_name: "test_multi_head_attention", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_self_attention" that takes no parameters returns TestResult:
    Note: Test self-attention where Q = K = V
    
    Try:
        Let d_model be 6
        Let num_heads be 2
        Let seq_len be 3
        
        Let input be create_test_matrix(seq_len, d_model, "sequential")
        Let config be Attention.AttentionConfig with d_model: d_model, num_heads: num_heads, d_k: d_model / num_heads, d_v: d_model / num_heads, dropout: 0.0, temperature: 1.0, causal: false
        
        Let query_weights be create_test_matrix(d_model, d_model, "identity")
        Let key_weights be create_test_matrix(d_model, d_model, "identity")
        Let value_weights be create_test_matrix(d_model, d_model, "identity")
        Let output_weights be create_test_matrix(d_model, d_model, "identity")
        
        Let weights be Attention.AttentionWeights with query_weights: query_weights, key_weights: key_weights, value_weights: value_weights, output_weights: output_weights
        
        Let output be Attention.self_attention(input, config, weights)
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_self_attention", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Return TestResult with test_name: "test_self_attention", passed: true, error_message: "", execution_time: "0", expected_value: "Valid self-attention", actual_value: "Valid self-attention"
    
    Catch error:
        Return TestResult with test_name: "test_self_attention", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_causal_self_attention" that takes no parameters returns TestResult:
    Note: Test causal self-attention with triangular masking
    
    Try:
        Let d_model be 4
        Let num_heads be 1
        Let seq_len be 3
        
        Let input be create_test_matrix(seq_len, d_model, "sequential")
        Let config be Attention.AttentionConfig with d_model: d_model, num_heads: num_heads, d_k: d_model, d_v: d_model, dropout: 0.0, temperature: 1.0, causal: true
        
        Let query_weights be create_test_matrix(d_model, d_model, "identity")
        Let key_weights be create_test_matrix(d_model, d_model, "identity")
        Let value_weights be create_test_matrix(d_model, d_model, "identity")
        Let output_weights be create_test_matrix(d_model, d_model, "identity")
        
        Let weights be Attention.AttentionWeights with query_weights: query_weights, key_weights: key_weights, value_weights: value_weights, output_weights: output_weights
        
        Let output be Attention.causal_self_attention(input, config, weights)
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_causal_self_attention", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Return TestResult with test_name: "test_causal_self_attention", passed: true, error_message: "", execution_time: "0", expected_value: "Valid causal self-attention", actual_value: "Valid causal self-attention"
    
    Catch error:
        Return TestResult with test_name: "test_causal_self_attention", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Positional Encoding Tests =====

Process called "test_sinusoidal_positional_encoding" that takes no parameters returns TestResult:
    Note: Test sinusoidal positional encoding
    
    Try:
        Let seq_len be 4
        Let d_model be 6
        
        Let encoding be Attention.sinusoidal_positional_encoding(seq_len, d_model)
        
        Note: Check dimensions
        If encoding.rows != seq_len or encoding.columns != d_model:
            Return TestResult with test_name: "test_sinusoidal_positional_encoding", passed: false, error_message: "Encoding dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: encoding.rows.to_string() + "x" + encoding.columns.to_string()
        
        Note: Check that different positions have different encodings
        Let pos0_val be encoding.entries.get(0).get(0)
        Let pos1_val be encoding.entries.get(1).get(0)
        
        Let comparison be MathOps.compare(pos0_val, pos1_val)
        If comparison == 0:
            Return TestResult with test_name: "test_sinusoidal_positional_encoding", passed: false, error_message: "Position encodings should be different", execution_time: "0", expected_value: "Different values", actual_value: "Same values"
        
        Note: Check that values are within reasonable range [-1, 1] for sine/cosine
        Let val_float be Parse pos0_val as Float
        If val_float < -1.1 or val_float > 1.1:
            Return TestResult with test_name: "test_sinusoidal_positional_encoding", passed: false, error_message: "Positional encoding values out of expected range", execution_time: "0", expected_value: "[-1, 1]", actual_value: pos0_val
        
        Return TestResult with test_name: "test_sinusoidal_positional_encoding", passed: true, error_message: "", execution_time: "0", expected_value: "Valid positional encoding", actual_value: "Valid positional encoding"
    
    Catch error:
        Return TestResult with test_name: "test_sinusoidal_positional_encoding", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_learned_positional_encoding" that takes no parameters returns TestResult:
    Note: Test learned positional embeddings
    
    Try:
        Let seq_len be 3
        Let d_model be 4
        
        Let embeddings be create_test_matrix(seq_len, d_model, "sequential")
        Let encoding be Attention.learned_positional_encoding(seq_len, d_model, embeddings)
        
        Note: Check dimensions
        If encoding.rows != seq_len or encoding.columns != d_model:
            Return TestResult with test_name: "test_learned_positional_encoding", passed: false, error_message: "Encoding dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: encoding.rows.to_string() + "x" + encoding.columns.to_string()
        
        Note: Check that encoding matches input embeddings
        Let original_val be embeddings.entries.get(0).get(0)
        Let encoded_val be encoding.entries.get(0).get(0)
        
        Let match_check be assert_equals_float(encoded_val, original_val, "0.0001", "learned encoding match")
        If not match_check.passed:
            Return match_check
        
        Return TestResult with test_name: "test_learned_positional_encoding", passed: true, error_message: "", execution_time: "0", expected_value: "Valid learned encoding", actual_value: "Valid learned encoding"
    
    Catch error:
        Return TestResult with test_name: "test_learned_positional_encoding", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_apply_positional_encoding" that takes no parameters returns TestResult:
    Note: Test application of positional encoding to embeddings
    
    Try:
        Let seq_len be 2
        Let d_model be 3
        
        Let embeddings be create_test_matrix(seq_len, d_model, "ones")
        Let positions be create_test_matrix(seq_len, d_model, "small")
        Let dropout be 0.0
        
        Let combined be Attention.apply_positional_encoding(embeddings, positions, dropout)
        
        Note: Check dimensions
        If combined.rows != seq_len or combined.columns != d_model:
            Return TestResult with test_name: "test_apply_positional_encoding", passed: false, error_message: "Combined dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: combined.rows.to_string() + "x" + combined.columns.to_string()
        
        Note: Check that result is sum of embeddings and positions
        Let embedding_val be embeddings.entries.get(0).get(0)
        Let position_val be positions.entries.get(0).get(0)
        Let combined_val be combined.entries.get(0).get(0)
        
        Let expected_sum be MathOps.add(embedding_val, position_val, 15)
        Let sum_check be assert_equals_float(combined_val, expected_sum.result_value, "0.0001", "positional addition")
        If not sum_check.passed:
            Return sum_check
        
        Return TestResult with test_name: "test_apply_positional_encoding", passed: true, error_message: "", execution_time: "0", expected_value: "Valid positional application", actual_value: "Valid positional application"
    
    Catch error:
        Return TestResult with test_name: "test_apply_positional_encoding", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Sparse Attention Tests =====

Process called "test_local_attention" that takes no parameters returns TestResult:
    Note: Test local attention with window constraints
    
    Try:
        Let seq_len be 5
        Let d_model be 4
        Let window_size be 2
        
        Let query be create_test_matrix(seq_len, d_model, "sequential")
        Let key be create_test_matrix(seq_len, d_model, "small")
        Let value be create_test_matrix(seq_len, d_model, "ones")
        
        Let output be Attention.local_attention(query, key, value, window_size)
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_local_attention", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Return TestResult with test_name: "test_local_attention", passed: true, error_message: "", execution_time: "0", expected_value: "Valid local attention", actual_value: "Valid local attention"
    
    Catch error:
        Return TestResult with test_name: "test_local_attention", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_strided_attention" that takes no parameters returns TestResult:
    Note: Test strided attention pattern
    
    Try:
        Let seq_len be 6
        Let d_model be 4
        Let stride be 2
        
        Let query be create_test_matrix(seq_len, d_model, "sequential")
        Let key be create_test_matrix(seq_len, d_model, "small")
        Let value be create_test_matrix(seq_len, d_model, "ones")
        
        Let output be Attention.strided_attention(query, key, value, stride)
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_strided_attention", passed: false, error_message: "Output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Return TestResult with test_name: "test_strided_attention", passed: true, error_message: "", execution_time: "0", expected_value: "Valid strided attention", actual_value: "Valid strided attention"
    
    Catch error:
        Return TestResult with test_name: "test_strided_attention", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Transformer Components Tests =====

Process called "test_feed_forward_network" that takes no parameters returns TestResult:
    Note: Test feed-forward network component
    
    Try:
        Let seq_len be 3
        Let d_model be 4
        Let d_ff be 8
        
        Let input be create_test_matrix(seq_len, d_model, "small")
        Let weights_1 be create_test_matrix(d_model, d_ff, "ones")
        Let weights_2 be create_test_matrix(d_ff, d_model, "ones")
        
        Let config be Attention.FeedForwardNetwork with d_model: d_model, d_ff: d_ff, activation: "relu", weights_1: weights_1, weights_2: weights_2, dropout: 0.0
        
        Let output be Attention.feed_forward_network(input, config)
        
        Note: Check output dimensions
        If output.rows != seq_len or output.columns != d_model:
            Return TestResult with test_name: "test_feed_forward_network", passed: false, error_message: "FFN output dimensions incorrect", execution_time: "0", expected_value: seq_len.to_string() + "x" + d_model.to_string(), actual_value: output.rows.to_string() + "x" + output.columns.to_string()
        
        Note: Check that ReLU activation was applied (no negative values)
        Let i be 0
        While i < output.rows:
            Let j be 0
            While j < output.columns:
                Let val be output.entries.get(i).get(j)
                Let val_float be Parse val as Float
                If val_float < 0.0:
                    Return TestResult with test_name: "test_feed_forward_network", passed: false, error_message: "Negative values found after ReLU", execution_time: "0", expected_value: "Non-negative values", actual_value: "Negative value: " + val
                Set j to j + 1
            Set i to i + 1
        
        Return TestResult with test_name: "test_feed_forward_network", passed: true, error_message: "", execution_time: "0", expected_value: "Valid FFN", actual_value: "Valid FFN"
    
    Catch error:
        Return TestResult with test_name: "test_feed_forward_network", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_layer_normalization" that takes no parameters returns TestResult:
    Note: Test layer normalization functionality
    
    Try:
        Let input be create_test_matrix(2, 4, "sequential")
        Let epsilon be 1e-5
        
        Let normalized be Attention.layer_normalize_matrix(input, epsilon)
        
        Note: Check dimensions preserved
        If normalized.rows != input.rows or normalized.columns != input.columns:
            Return TestResult with test_name: "test_layer_normalization", passed: false, error_message: "LayerNorm dimensions incorrect", execution_time: "0", expected_value: input.rows.to_string() + "x" + input.columns.to_string(), actual_value: normalized.rows.to_string() + "x" + normalized.columns.to_string()
        
        Note: Check that each row has approximately zero mean and unit variance
        Let i be 0
        While i < normalized.rows:
            Let row_sum be "0.0"
            Let j be 0
            While j < normalized.columns:
                Let val be normalized.entries.get(i).get(j)
                Let sum_result be MathOps.add(row_sum, val, 15)
                Set row_sum to sum_result.result_value
                Set j to j + 1
            
            Let columns_str be normalized.columns.to_string()
            Let row_mean_result be MathOps.divide(row_sum, columns_str, 15)
            Let row_mean be row_mean_result.result_value
            
            Let mean_check be assert_equals_float(row_mean, "0.0", "0.001", "layer norm mean")
            If not mean_check.passed:
                Return mean_check
            Set i to i + 1
        
        Return TestResult with test_name: "test_layer_normalization", passed: true, error_message: "", execution_time: "0", expected_value: "Valid layer normalization", actual_value: "Valid layer normalization"
    
    Catch error:
        Return TestResult with test_name: "test_layer_normalization", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Attention Analysis Tests =====

Process called "test_attention_entropy" that takes no parameters returns TestResult:
    Note: Test attention entropy computation
    
    Try:
        Note: Create test attention matrix with known entropy properties
        Let attention_entries be List[List[String]]()
        Let row1 be List[String]()
        Call row1.add("0.5")  Note: Uniform distribution (higher entropy)
        Call row1.add("0.5")
        Call attention_entries.add(row1)
        
        Let row2 be List[String]()
        Call row2.add("0.9")  Note: Concentrated distribution (lower entropy)
        Call row2.add("0.1")
        Call attention_entries.add(row2)
        
        Let attention_weights be LinAlg.create_matrix(attention_entries, "float")
        Let entropy_vector be Attention.compute_attention_entropy(attention_weights)
        
        Note: Check entropy vector dimensions
        If entropy_vector.length != 2:
            Return TestResult with test_name: "test_attention_entropy", passed: false, error_message: "Entropy vector length incorrect", execution_time: "0", expected_value: "2", actual_value: entropy_vector.length.to_string()
        
        Note: Check that uniform distribution has higher entropy than concentrated
        Let entropy1_str be entropy_vector.components.get(0)
        Let entropy2_str be entropy_vector.components.get(1)
        Let comparison be MathOps.compare(entropy1_str, entropy2_str)
        
        If comparison <= 0:
            Return TestResult with test_name: "test_attention_entropy", passed: false, error_message: "Entropy ordering incorrect", execution_time: "0", expected_value: "Uniform > Concentrated", actual_value: "Concentrated >= Uniform"
        
        Return TestResult with test_name: "test_attention_entropy", passed: true, error_message: "", execution_time: "0", expected_value: "Valid entropy computation", actual_value: "Valid entropy computation"
    
    Catch error:
        Return TestResult with test_name: "test_attention_entropy", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Process called "test_attention_pattern_extraction" that takes no parameters returns TestResult:
    Note: Test extraction of significant attention patterns
    
    Try:
        Let attention_entries be List[List[String]]()
        Let row1 be List[String]()
        Call row1.add("0.1")
        Call row1.add("0.8")  Note: High attention
        Call row1.add("0.1")
        Call attention_entries.add(row1)
        
        Let attention_weights be LinAlg.create_matrix(attention_entries, "float")
        Let threshold be 0.5
        
        Let patterns be Attention.extract_attention_patterns(attention_weights, threshold)
        
        Note: Check that high attention pattern was detected
        If patterns.length != 1:
            Return TestResult with test_name: "test_attention_pattern_extraction", passed: false, error_message: "Pattern count incorrect", execution_time: "0", expected_value: "1", actual_value: patterns.length.to_string()
        
        Let detected_pattern be patterns.get(0)
        If detected_pattern.first != 0 or detected_pattern.second != 1:
            Return TestResult with test_name: "test_attention_pattern_extraction", passed: false, error_message: "Pattern position incorrect", execution_time: "0", expected_value: "(0,1)", actual_value: "(" + detected_pattern.first.to_string() + "," + detected_pattern.second.to_string() + ")"
        
        Return TestResult with test_name: "test_attention_pattern_extraction", passed: true, error_message: "", execution_time: "0", expected_value: "Valid pattern extraction", actual_value: "Valid pattern extraction"
    
    Catch error:
        Return TestResult with test_name: "test_attention_pattern_extraction", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "No exception", actual_value: "Exception thrown"

Note: ===== Error Handling Tests =====

Process called "test_attention_dimension_errors" that takes no parameters returns TestResult:
    Note: Test that attention functions properly validate input dimensions
    
    Try:
        Let query be create_test_matrix(3, 4, "sequential")
        Let key be create_test_matrix(5, 4, "small")  Note: Mismatched sequence length
        Let value be create_test_matrix(3, 4, "ones")
        
        Try:
            Let result be Attention.scaled_dot_product_attention(query, key, value, None)
            Return TestResult with test_name: "test_attention_dimension_errors", passed: false, error_message: "Should have thrown dimension error", execution_time: "0", expected_value: "Dimension error", actual_value: "No error thrown"
        Catch dimension_error:
            Return TestResult with test_name: "test_attention_dimension_errors", passed: true, error_message: "", execution_time: "0", expected_value: "Dimension error thrown", actual_value: "Dimension error thrown"
    
    Catch error:
        Return TestResult with test_name: "test_attention_dimension_errors", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "Controlled dimension error", actual_value: "Unexpected exception"

Process called "test_positional_encoding_errors" that takes no parameters returns TestResult:
    Note: Test positional encoding error handling
    
    Try:
        Try:
            Let encoding be Attention.sinusoidal_positional_encoding(-1, 4)
            Return TestResult with test_name: "test_positional_encoding_errors", passed: false, error_message: "Should have thrown parameter error", execution_time: "0", expected_value: "Parameter error", actual_value: "No error thrown"
        Catch param_error:
            Return TestResult with test_name: "test_positional_encoding_errors", passed: true, error_message: "", execution_time: "0", expected_value: "Parameter error thrown", actual_value: "Parameter error thrown"
    
    Catch error:
        Return TestResult with test_name: "test_positional_encoding_errors", passed: false, error_message: "Exception: " + error.message, execution_time: "0", expected_value: "Controlled parameter error", actual_value: "Unexpected exception"

Note: ===== Test Suite Management =====

Process called "create_test_suite" that takes suite_name as String returns TestSuite:
    Note: Create a new test suite for organizing test results
    Return TestSuite with suite_name: suite_name, total_tests: 0, passed_tests: 0, failed_tests: 0, results: List[TestResult]()

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns TestSuite:
    Note: Add a test result to the test suite
    
    Call suite.results.add(result)
    Set suite.total_tests to suite.total_tests + 1
    
    If result.passed:
        Set suite.passed_tests to suite.passed_tests + 1
    Otherwise:
        Set suite.failed_tests to suite.failed_tests + 1
    
    Return suite

Process called "run_all_attention_tests" that takes no parameters returns TestSuite:
    Note: Run all attention mechanism tests and return comprehensive results
    
    Let suite be create_test_suite("Attention Mechanisms Test Suite")
    
    Note: Basic attention tests
    Set suite to add_test_result(suite, test_scaled_dot_product_attention_basic())
    Set suite to add_test_result(suite, test_attention_with_mask())
    Set suite to add_test_result(suite, test_attention_softmax())
    
    Note: Multi-head attention tests
    Set suite to add_test_result(suite, test_multi_head_attention())
    Set suite to add_test_result(suite, test_self_attention())
    Set suite to add_test_result(suite, test_causal_self_attention())
    
    Note: Positional encoding tests
    Set suite to add_test_result(suite, test_sinusoidal_positional_encoding())
    Set suite to add_test_result(suite, test_learned_positional_encoding())
    Set suite to add_test_result(suite, test_apply_positional_encoding())
    
    Note: Sparse attention tests
    Set suite to add_test_result(suite, test_local_attention())
    Set suite to add_test_result(suite, test_strided_attention())
    
    Note: Transformer components tests
    Set suite to add_test_result(suite, test_feed_forward_network())
    Set suite to add_test_result(suite, test_layer_normalization())
    
    Note: Analysis tests
    Set suite to add_test_result(suite, test_attention_entropy())
    Set suite to add_test_result(suite, test_attention_pattern_extraction())
    
    Note: Error handling tests
    Set suite to add_test_result(suite, test_attention_dimension_errors())
    Set suite to add_test_result(suite, test_positional_encoding_errors())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns String:
    Note: Format test results for display
    
    Let output be "=== " + suite.suite_name + " ===" + "\n"
    Set output to output + "Total tests: " + suite.total_tests.to_string() + "\n"
    Set output to output + "Passed: " + suite.passed_tests.to_string() + "\n"
    Set output to output + "Failed: " + suite.failed_tests.to_string() + "\n\n"
    
    If suite.failed_tests > 0:
        Set output to output + "Failed tests:" + "\n"
        Let i be 0
        While i < suite.results.length:
            Let result be suite.results.get(i)
            If not result.passed:
                Set output to output + "- " + result.test_name + ": " + result.error_message + "\n"
            Set i to i + 1
    
    Let success_rate_num be suite.passed_tests * 100
    Let success_rate be success_rate_num / suite.total_tests
    Set output to output + "\nSuccess rate: " + success_rate.to_string() + "%"
    
    Return output

Note: ===== Main Test Runner =====

Process called "main" that takes no parameters returns Integer:
    Note: Main test runner for attention mechanisms module
    
    Display "Running comprehensive tests for attention mechanisms..."
    Display ""
    
    Let test_suite be run_all_attention_tests()
    Let results_output be print_test_results(test_suite)
    
    Display results_output
    
    If test_suite.failed_tests == 0:
        Display ""
        Display "All attention mechanism tests passed successfully!"
        Return 0
    Otherwise:
        Display ""
        Display "Some attention mechanism tests failed. Please review the failures above."
        Return 1