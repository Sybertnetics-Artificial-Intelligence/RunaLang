Note:
Unit Tests for Neural Operations Module
Testing activation functions, forward/backward propagation, weight initialization,
batch normalization, dropout, and neural layer operations.
:End Note

Import "math/ai_math/neural_ops" as NeuralOps
Import "math/core/operations" as MathOps  
Import "math/core/comparison" as MathCompare
Import "dev/debug/errors/core" as Errors
Import "math/engine/linalg/core" as LinAlg

Note: ===== Test Framework =====

Type called "TestResult":
    test_name as String
    passed as Boolean
    error_message as String
    execution_time as Float

Type called "TestSuite":
    suite_name as String
    tests as List[TestResult]
    passed_count as Integer
    failed_count as Integer

Process called "create_test_suite" that takes name as String returns TestSuite:
    Let suite be TestSuite
    Set suite.suite_name to name
    Set suite.tests to List[TestResult].create()
    Set suite.passed_count to 0
    Set suite.failed_count to 0
    Return suite

Process called "add_test_result" that takes suite as TestSuite, result as TestResult returns Void:
    suite.tests.add(result)
    If result.passed:
        Set suite.passed_count to suite.passed_count + 1
    Otherwise:
        Set suite.failed_count to suite.failed_count + 1

Process called "assert_equals" that takes actual as Float, expected as Float, tolerance as Float returns Boolean:
    Let difference be MathOps.subtract(actual.to_string(), expected.to_string(), 50).result_value.to_float()
    Let abs_difference be If difference >= 0.0 then difference Otherwise (0.0 - difference)
    Return abs_difference <= tolerance

Process called "assert_matrix_equals" that takes actual as Matrix[Float], expected as Matrix[Float], tolerance as Float returns Boolean:
    If actual.rows != expected.rows or actual.cols != expected.cols:
        Return false
    
    For i in 0 to actual.rows - 1:
        For j in 0 to actual.cols - 1:
            If not assert_equals(actual.get(i, j), expected.get(i, j), tolerance):
                Return false
    
    Return true

Note: ===== Activation Function Tests =====

Process called "test_sigmoid_activation" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Sigmoid Activation Function"
    
    Try:
        Note: Test sigmoid(0) = 0.5
        Let sigmoid_result be NeuralOps.sigmoid(0.0)
        If not assert_equals(sigmoid_result, 0.5, 1e-6):
            Set result.passed to false
            Set result.error_message to "sigmoid(0) should equal 0.5"
            Return result
        
        Note: Test sigmoid(large positive) approaches 1
        Let sigmoid_large be NeuralOps.sigmoid(10.0)
        If sigmoid_large <= 0.99:
            Set result.passed to false
            Set result.error_message to "sigmoid(10) should approach 1.0"
            Return result
        
        Note: Test sigmoid(large negative) approaches 0
        Let sigmoid_neg be NeuralOps.sigmoid(-10.0)
        If sigmoid_neg >= 0.01:
            Set result.passed to false
            Set result.error_message to "sigmoid(-10) should approach 0.0"
            Return result
        
        Note: Test sigmoid derivative at 0 = 0.25
        Let sigmoid_deriv be NeuralOps.sigmoid_derivative(0.0)
        If not assert_equals(sigmoid_deriv, 0.25, 1e-6):
            Set result.passed to false
            Set result.error_message to "sigmoid_derivative(0) should equal 0.25"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_relu_activation" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "ReLU Activation Function"
    
    Try:
        Note: Test ReLU(positive) = input
        Let relu_positive be NeuralOps.relu(5.0)
        If not assert_equals(relu_positive, 5.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "ReLU(5.0) should equal 5.0"
            Return result
        
        Note: Test ReLU(negative) = 0
        Let relu_negative be NeuralOps.relu(-3.0)
        If not assert_equals(relu_negative, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "ReLU(-3.0) should equal 0.0"
            Return result
        
        Note: Test ReLU(0) = 0
        Let relu_zero be NeuralOps.relu(0.0)
        If not assert_equals(relu_zero, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "ReLU(0.0) should equal 0.0"
            Return result
        
        Note: Test ReLU derivative
        Let relu_deriv_pos be NeuralOps.relu_derivative(5.0)
        Let relu_deriv_neg be NeuralOps.relu_derivative(-3.0)
        
        If not assert_equals(relu_deriv_pos, 1.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "ReLU derivative for positive should be 1.0"
            Return result
        
        If not assert_equals(relu_deriv_neg, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "ReLU derivative for negative should be 0.0"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_tanh_activation" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Tanh Activation Function"
    
    Try:
        Note: Test tanh(0) = 0
        Let tanh_zero be NeuralOps.tanh(0.0)
        If not assert_equals(tanh_zero, 0.0, 1e-10):
            Set result.passed to false
            Set result.error_message to "tanh(0) should equal 0.0"
            Return result
        
        Note: Test tanh range (-1, 1)
        Let tanh_large_pos be NeuralOps.tanh(10.0)
        Let tanh_large_neg be NeuralOps.tanh(-10.0)
        
        If tanh_large_pos <= 0.99 or tanh_large_pos >= 1.0:
            Set result.passed to false
            Set result.error_message to "tanh(10) should approach 1.0"
            Return result
        
        If tanh_large_neg >= -0.99 or tanh_large_neg <= -1.0:
            Set result.passed to false
            Set result.error_message to "tanh(-10) should approach -1.0"
            Return result
        
        Note: Test tanh derivative at 0 = 1
        Let tanh_deriv be NeuralOps.tanh_derivative(0.0)
        If not assert_equals(tanh_deriv, 1.0, 1e-6):
            Set result.passed to false
            Set result.error_message to "tanh_derivative(0) should equal 1.0"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_softmax_activation" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Softmax Activation Function"
    
    Try:
        Note: Test softmax properties
        Let input_vector be [1.0, 2.0, 3.0, 4.0]
        Let softmax_output be NeuralOps.softmax(input_vector)
        
        Note: Check output length matches input length
        If softmax_output.length != input_vector.length:
            Set result.passed to false
            Set result.error_message to "Softmax output length mismatch"
            Return result
        
        Note: Check sum of probabilities equals 1
        Let sum be 0.0
        For Each prob in softmax_output:
            Set sum to sum + prob
        
        If not assert_equals(sum, 1.0, 1e-6):
            Set result.passed to false
            Set result.error_message to "Softmax probabilities should sum to 1.0, got: " + sum
            Return result
        
        Note: Check all probabilities are positive
        For Each prob in softmax_output:
            If prob <= 0.0:
                Set result.passed to false
                Set result.error_message to "All softmax probabilities should be positive"
                Return result
        
        Note: Check monotonicity (larger inputs should have larger probabilities)
        If softmax_output[3] <= softmax_output[0]:
            Set result.passed to false
            Set result.error_message to "Softmax should preserve input ordering"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Weight Initialization Tests =====

Process called "test_xavier_initialization" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Xavier Weight Initialization"
    
    Try:
        Let input_size be 100
        Let output_size be 50
        
        Let weights be NeuralOps.initialize_weights_xavier(input_size, output_size)
        
        Note: Check matrix dimensions
        If weights.rows != output_size or weights.cols != input_size:
            Set result.passed to false
            Set result.error_message to "Xavier initialization matrix dimensions incorrect"
            Return result
        
        Note: Check variance approximately equals 2/(input_size + output_size)
        Let expected_variance be 2.0 / (input_size + output_size).to_float()
        Let calculated_variance be NeuralOps.calculate_matrix_variance(weights)
        
        If not assert_equals(calculated_variance, expected_variance, 0.1):
            Set result.passed to false
            Set result.error_message to "Xavier initialization variance incorrect. Expected: " + expected_variance + ", Got: " + calculated_variance
            Return result
        
        Note: Check mean is approximately zero
        Let calculated_mean be NeuralOps.calculate_matrix_mean(weights)
        If not assert_equals(calculated_mean, 0.0, 0.1):
            Set result.passed to false
            Set result.error_message to "Xavier initialization mean should be near zero"
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_he_initialization" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "He Weight Initialization"
    
    Try:
        Let input_size be 100
        Let output_size be 50
        
        Let weights be NeuralOps.initialize_weights_he(input_size, output_size)
        
        Note: Check matrix dimensions
        If weights.rows != output_size or weights.cols != input_size:
            Set result.passed to false
            Set result.error_message to "He initialization matrix dimensions incorrect"
            Return result
        
        Note: Check variance approximately equals 2/input_size
        Let expected_variance be 2.0 / input_size.to_float()
        Let calculated_variance be NeuralOps.calculate_matrix_variance(weights)
        
        If not assert_equals(calculated_variance, expected_variance, 0.1):
            Set result.passed to false
            Set result.error_message to "He initialization variance incorrect. Expected: " + expected_variance + ", Got: " + calculated_variance
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Forward Propagation Tests =====

Process called "test_linear_forward_pass" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Linear Layer Forward Pass"
    
    Try:
        Note: Create simple linear layer: 3 inputs -> 2 outputs
        Let input_size be 3
        Let output_size be 2
        Let weights be LinAlg.create_matrix(output_size, input_size)
        Let biases be [0.5, -0.3]
        
        Note: Set known weights for predictable output
        weights.set(0, 0, 1.0)
        weights.set(0, 1, 0.5)
        weights.set(0, 2, -0.2)
        weights.set(1, 0, 0.3)
        weights.set(1, 1, -0.8)
        weights.set(1, 2, 1.2)
        
        Let input_vector be [2.0, -1.0, 3.0]
        
        Note: Expected output: [1*2 + 0.5*(-1) + (-0.2)*3 + 0.5, 0.3*2 + (-0.8)*(-1) + 1.2*3 + (-0.3)]
        Note: = [2 - 0.5 - 0.6 + 0.5, 0.6 + 0.8 + 3.6 - 0.3] = [1.4, 4.7]
        Let expected_output be [1.4, 4.7]
        
        Let actual_output be NeuralOps.linear_forward_pass(input_vector, weights, biases)
        
        For i in 0 to expected_output.length - 1:
            If not assert_equals(actual_output[i], expected_output[i], 1e-6):
                Set result.passed to false
                Set result.error_message to "Linear forward pass output mismatch at index " + i + ". Expected: " + expected_output[i] + ", Got: " + actual_output[i]
                Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Batch Normalization Tests =====

Process called "test_batch_normalization" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Batch Normalization"
    
    Try:
        Note: Create batch of data (3 samples, 4 features each)
        Let batch_data be [
            [1.0, 2.0, 3.0, 4.0],
            [2.0, 4.0, 6.0, 8.0], 
            [3.0, 6.0, 9.0, 12.0]
        ]
        
        Let gamma be [1.0, 1.0, 1.0, 1.0]  Note: Scale parameters
        Let beta be [0.0, 0.0, 0.0, 0.0]   Note: Shift parameters
        Let epsilon be 1e-5
        
        Let normalized_batch be NeuralOps.batch_normalize(batch_data, gamma, beta, epsilon)
        
        Note: Check output dimensions
        If normalized_batch.length != batch_data.length:
            Set result.passed to false
            Set result.error_message to "Batch normalization output size mismatch"
            Return result
        
        Note: Check that each feature has mean ≈ 0 and std ≈ 1 across batch
        For feature_idx in 0 to 3:
            Let feature_values be List[Float].create()
            For sample_idx in 0 to 2:
                feature_values.add(normalized_batch[sample_idx][feature_idx])
            
            Let feature_mean be NeuralOps.calculate_mean(feature_values)
            Let feature_std be NeuralOps.calculate_std(feature_values)
            
            If not assert_equals(feature_mean, 0.0, 1e-5):
                Set result.passed to false
                Set result.error_message to "Batch normalization mean not zero for feature " + feature_idx
                Return result
            
            If not assert_equals(feature_std, 1.0, 1e-5):
                Set result.passed to false
                Set result.error_message to "Batch normalization std not one for feature " + feature_idx  
                Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Dropout Tests =====

Process called "test_dropout_training_mode" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Dropout Training Mode"
    
    Try:
        Let input_vector be [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
        Let dropout_rate be 0.5
        Let training_mode be true
        
        Let dropout_output be NeuralOps.apply_dropout(input_vector, dropout_rate, training_mode)
        
        Note: Check output dimensions
        If dropout_output.length != input_vector.length:
            Set result.passed to false
            Set result.error_message to "Dropout output size mismatch"
            Return result
        
        Note: Check that some values are zero (dropped) and others are scaled
        Let zero_count be 0
        Let non_zero_count be 0
        
        For i in 0 to dropout_output.length - 1:
            If dropout_output[i] == 0.0:
                Set zero_count to zero_count + 1
            Otherwise:
                Set non_zero_count to non_zero_count + 1
                Note: Non-zero values should be scaled by 1/(1-dropout_rate) = 2.0
                Let expected_scaled_value be input_vector[i] * 2.0
                If not assert_equals(dropout_output[i], expected_scaled_value, 1e-6):
                    Set result.passed to false
                    Set result.error_message to "Dropout scaling incorrect"
                    Return result
        
        Note: Check that we have some dropped values (statistical test)
        If zero_count < 2 or zero_count > 8:  Note: Allow some variance due to randomness
            Set result.passed to false
            Set result.error_message to "Dropout rate seems incorrect. Zero count: " + zero_count
            Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Process called "test_dropout_inference_mode" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "Dropout Inference Mode"
    
    Try:
        Let input_vector be [1.0, 2.0, 3.0, 4.0, 5.0]
        Let dropout_rate be 0.3
        Let training_mode be false
        
        Let dropout_output be NeuralOps.apply_dropout(input_vector, dropout_rate, training_mode)
        
        Note: In inference mode, dropout should be identity function
        For i in 0 to input_vector.length - 1:
            If not assert_equals(dropout_output[i], input_vector[i], 1e-10):
                Set result.passed to false
                Set result.error_message to "Dropout should be identity in inference mode"
                Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Convolution Tests =====

Process called "test_convolution_2d" that returns TestResult:
    Let result be TestResult
    Set result.test_name to "2D Convolution Operation"
    
    Try:
        Note: Simple 3x3 input with 2x2 kernel
        Let input_matrix be [
            [1.0, 2.0, 3.0],
            [4.0, 5.0, 6.0],
            [7.0, 8.0, 9.0]
        ]
        
        Let kernel be [
            [1.0, -1.0],
            [-1.0, 1.0]
        ]
        
        Let stride be 1
        Let padding be 0
        
        Let conv_output be NeuralOps.convolution_2d(input_matrix, kernel, stride, padding)
        
        Note: Expected output size: (3-2+0)/1 + 1 = 2x2
        Note: Expected values:
        Note: Top-left: 1*1 + 2*(-1) + 4*(-1) + 5*1 = 1 - 2 - 4 + 5 = 0
        Note: Top-right: 2*1 + 3*(-1) + 5*(-1) + 6*1 = 2 - 3 - 5 + 6 = 0  
        Note: Bottom-left: 4*1 + 5*(-1) + 7*(-1) + 8*1 = 4 - 5 - 7 + 8 = 0
        Note: Bottom-right: 5*1 + 6*(-1) + 8*(-1) + 9*1 = 5 - 6 - 8 + 9 = 0
        
        Let expected_output be [
            [0.0, 0.0],
            [0.0, 0.0]
        ]
        
        For i in 0 to expected_output.length - 1:
            For j in 0 to expected_output[i].length - 1:
                If not assert_equals(conv_output[i][j], expected_output[i][j], 1e-6):
                    Set result.passed to false
                    Set result.error_message to "Convolution output mismatch at (" + i + ", " + j + ")"
                    Return result
        
        Set result.passed to true
        Set result.error_message to ""
    
    Catch exception as Errors.Error:
        Set result.passed to false
        Set result.error_message to "Exception: " + exception.message
    
    Return result

Note: ===== Main Test Runner =====

Process called "run_neural_ops_tests" that returns TestSuite:
    Let suite be create_test_suite("Neural Operations Tests")
    
    Note: Activation function tests
    add_test_result(suite, test_sigmoid_activation())
    add_test_result(suite, test_relu_activation())
    add_test_result(suite, test_tanh_activation())
    add_test_result(suite, test_softmax_activation())
    
    Note: Weight initialization tests
    add_test_result(suite, test_xavier_initialization())
    add_test_result(suite, test_he_initialization())
    
    Note: Forward propagation tests
    add_test_result(suite, test_linear_forward_pass())
    
    Note: Normalization tests
    add_test_result(suite, test_batch_normalization())
    
    Note: Regularization tests
    add_test_result(suite, test_dropout_training_mode())
    add_test_result(suite, test_dropout_inference_mode())
    
    Note: Convolution tests
    add_test_result(suite, test_convolution_2d())
    
    Return suite

Process called "print_test_results" that takes suite as TestSuite returns Void:
    Print("=== " + suite.suite_name + " ===")
    Print("Passed: " + suite.passed_count + ", Failed: " + suite.failed_count)
    Print("")
    
    For Each test in suite.tests:
        Let status be If test.passed then "PASS" Otherwise "FAIL"
        Print("[" + status + "] " + test.test_name)
        If not test.passed:
            Print("  Error: " + test.error_message)
    
    Print("")
    Let success_rate be (suite.passed_count.to_float() / (suite.passed_count + suite.failed_count).to_float()) * 100.0
    Print("Success Rate: " + success_rate + "%")

Note: Run tests if executed directly
Let test_suite be run_neural_ops_tests()
print_test_results(test_suite)