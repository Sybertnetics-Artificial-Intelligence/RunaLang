Note:
tests/compiler/frontend/lexical/test_lexer_integration.runa
Integration tests for the complete lexer system
:End Note

Import Module "compiler/frontend/lexical/lexer" as Lexer
Import Module "compiler/frontend/lexical/token_stream" as TokenStream
Import Module "compiler/internal/collections" as Collections
Import Module "testing/assert" as Assert

@Reasoning
    While unit tests verify individual components, integration tests ensure
    the entire lexer works correctly when all components interact.
    These tests verify complex tokenization scenarios.
@End Reasoning

Note: =====================================================================
Note: FUNDAMENTAL TOKENIZATION TESTS
Note: =====================================================================

Process called "test_simple_statement_tokenization" returns Nothing:
    Let source be "Let x be 5"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Assert.equals(TokenStream.get_token_count(tokens), 4)
    Assert.equals(TokenStream.get_token_at(tokens, 0).token_type, "KEYWORD")
    Assert.equals(TokenStream.get_token_at(tokens, 0).value, "Let")
    Assert.equals(TokenStream.get_token_at(tokens, 1).token_type, "IDENTIFIER")
    Assert.equals(TokenStream.get_token_at(tokens, 1).value, "x")
    Assert.equals(TokenStream.get_token_at(tokens, 2).token_type, "KEYWORD")
    Assert.equals(TokenStream.get_token_at(tokens, 2).value, "be")
    Assert.equals(TokenStream.get_token_at(tokens, 3).token_type, "NUMBER")
    Assert.equals(TokenStream.get_token_at(tokens, 3).value, "5")
End Process

Process called "test_complex_expression_tokenization" returns Nothing:
    Let source be "Let result be (x + y) * 2 / (z - 1)"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: Verify we have all expected tokens
    Assert.contains_token(tokens, "Let")
    Assert.contains_token(tokens, "result")
    Assert.contains_token(tokens, "be")
    Assert.contains_token(tokens, "(")
    Assert.contains_token(tokens, "x")
    Assert.contains_token(tokens, "+")
    Assert.contains_token(tokens, "y")
    Assert.contains_token(tokens, ")")
    Assert.contains_token(tokens, "*")
    Assert.contains_token(tokens, "2")
    Assert.contains_token(tokens, "/")
    Assert.contains_token(tokens, "z")
    Assert.contains_token(tokens, "-")
    Assert.contains_token(tokens, "1")
End Process

Note: =====================================================================
Note: CANON MODE VS DEVELOPER MODE TESTS
Note: =====================================================================

Process called "test_canon_mode_tokenization" returns Nothing:
    Let source be "Process called \"calculate\" that takes x as Integer returns Integer"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Note: In Canon mode, "called", "that", "takes", "returns", "as" are keywords
    Assert.contains_token_with_type(tokens, "called", "KEYWORD")
    Assert.contains_token_with_type(tokens, "that", "KEYWORD")
    Assert.contains_token_with_type(tokens, "takes", "KEYWORD")
    Assert.contains_token_with_type(tokens, "returns", "KEYWORD")
    Assert.contains_token_with_type(tokens, "as", "KEYWORD")
    Assert.contains_token_with_type(tokens, "calculate", "STRING")  Note: In quotes
    Assert.contains_token_with_type(tokens, "Integer", "IDENTIFIER")
End Process

Process called "test_developer_mode_tokenization" returns Nothing:
    Let source be "function calculate(x: int) -> int"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: In developer mode, function-style syntax
    Assert.contains_token(tokens, "function")
    Assert.contains_token(tokens, "calculate")
    Assert.contains_token(tokens, "(")
    Assert.contains_token(tokens, "x")
    Assert.contains_token(tokens, ":")
    Assert.contains_token(tokens, "int")
    Assert.contains_token(tokens, ")")
    Assert.contains_token(tokens, "->")
End Process

Note: =====================================================================
Note: ENCASING PATTERN INTEGRATION TESTS
Note: =====================================================================

Process called "test_encasing_with_spaces" returns Nothing:
    Let source be "Let my variable name be 42"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Note: Should tokenize as: Let, "my variable name", be, 42
    Assert.equals(TokenStream.get_token_count(tokens), 4)
    Assert.equals(TokenStream.get_token_at(tokens, 0).value, "Let")
    Assert.equals(TokenStream.get_token_at(tokens, 1).value, "my variable name")
    Assert.equals(TokenStream.get_token_at(tokens, 2).value, "be")
    Assert.equals(TokenStream.get_token_at(tokens, 3).value, "42")
End Process

Process called "test_encasing_ambiguity_resolution" returns Nothing:
    Let source be "Let x be y be 5"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Note: Last "be" wins: identifier is "x be y"
    Assert.contains_token(tokens, "Let")
    Assert.contains_token(tokens, "x be y")
    Assert.contains_token(tokens, "be")
    Assert.contains_token(tokens, "5")
End Process

Note: =====================================================================
Note: STATEMENT PATTERN INTEGRATION TESTS
Note: =====================================================================

Process called "test_imperative_statement_pattern" returns Nothing:
    Let source be "Increase counter by 1"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Note: Should recognize as statement pattern
    Assert.contains_token(tokens, "Increase")
    Assert.contains_token(tokens, "counter")
    Assert.contains_token(tokens, "by")
    Assert.contains_token(tokens, "1")
End Process

Process called "test_import_statement_pattern" returns Nothing:
    Let source be "Import Module \"collections/list\" as List"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Assert.contains_token(tokens, "Import")
    Assert.contains_token(tokens, "Module")
    Assert.contains_token(tokens, "\"collections/list\"")  
    Assert.contains_token(tokens, "as")
    Assert.contains_token(tokens, "List")
End Process

Note: =====================================================================
Note: MATH SYMBOL INTEGRATION TESTS
Note: =====================================================================

Process called "test_math_symbols_in_expression" returns Nothing:
    Let source be "Let sum be Σ(i=1, n, i²)"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Assert.contains_token(tokens, "Let")
    Assert.contains_token(tokens, "sum")
    Assert.contains_token(tokens, "be")
    Assert.contains_token(tokens, "Σ")  Note: Summation symbol
    Assert.contains_token(tokens, "i")
    Assert.contains_token(tokens, "=")
    Assert.contains_token(tokens, "1")
    Assert.contains_token(tokens, "n")
    Note: i² might be tokenized as i and ²
End Process

Process called "test_mixed_operators_and_symbols" returns Nothing:
    Let source be "Let result be α + β × γ"
    Let tokens be Lexer.tokenize(source, "canon")
    
    Assert.contains_token(tokens, "α")  Note: Greek alpha
    Assert.contains_token(tokens, "+")
    Assert.contains_token(tokens, "β")  Note: Greek beta
    Assert.contains_token(tokens, "×")  Note: Multiplication symbol
    Assert.contains_token(tokens, "γ")  Note: Greek gamma
End Process

Note: =====================================================================
Note: STRING LITERAL INTEGRATION TESTS
Note: =====================================================================

Process called "test_string_with_escapes" returns Nothing:
    Let source be "Let msg be \"Hello\\nWorld\\t!\""
    Let tokens be Lexer.tokenize(source, "developer")
    
    Let string_token be TokenStream.find_first_token_of_type(tokens, "STRING")
    Assert.is_not_null(string_token)
    Note: The lexer should preserve escape sequences for the parser
    Assert.contains(string_token.value, "\\n")
    Assert.contains(string_token.value, "\\t")
End Process

Process called "test_formatted_string" returns Nothing:
    Let source be "Let greeting be f\"Hello {name}!\""
    Let tokens be Lexer.tokenize(source, "developer")
    
    Let string_token be TokenStream.find_first_token_of_type(tokens, "STRING")
    Assert.is_not_null(string_token)
    Assert.starts_with(string_token.value, "f\"")
    Assert.contains(string_token.value, "{name}")
End Process

Note: =====================================================================
Note: COMMENT HANDLING INTEGRATION TESTS
Note: =====================================================================

Process called "test_comments_are_skipped" returns Nothing:
    Let source be "Let x be 5  Note: Initialize x\nLet y be 10"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: Comments should not appear in token stream
    Assert.not_contains_token(tokens, "Initialize")
    Assert.contains_token(tokens, "x")
    Assert.contains_token(tokens, "y")
    Assert.equals(TokenStream.get_token_count(tokens), 8)  Note: Let x be 5 Let y be 10
End Process

Process called "test_multi_line_comment_blocks" returns Nothing:
    Let source be "Let a be 1\nNote:\nMultiple\nLines\nOf Comments\n:End Note\nLet b be 2"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: Multi-line comment block should be completely skipped
    Assert.not_contains_token(tokens, "Multiple")
    Assert.not_contains_token(tokens, "Lines")
    Assert.not_contains_token(tokens, "Comments")
    Assert.contains_token(tokens, "a")
    Assert.contains_token(tokens, "b")
End Process

Note: =====================================================================
Note: ERROR RECOVERY INTEGRATION TESTS
Note: =====================================================================

Process called "test_unclosed_string_recovery" returns Nothing:
    Let source be "Let x be \"unclosed string\nLet y be 5"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: Lexer should recover and continue tokenizing
    Assert.contains_token(tokens, "Let")
    Assert.contains_token(tokens, "y")
    Assert.contains_token(tokens, "5")
    
    Note: Should have error token for unclosed string
    Let error_tokens be TokenStream.get_tokens_of_type(tokens, "ERROR")
    Assert.is_greater_than(Collections.list_length(error_tokens), 0)
End Process

Process called "test_invalid_character_recovery" returns Nothing:
    Let source be "Let x be 5 @ Let y be 10"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: @ is invalid but lexer should continue
    Assert.contains_token(tokens, "x")
    Assert.contains_token(tokens, "5")
    Assert.contains_token(tokens, "y")
    Assert.contains_token(tokens, "10")
End Process

Note: =====================================================================
Note: POSITION TRACKING INTEGRATION TESTS
Note: =====================================================================

Process called "test_line_column_tracking" returns Nothing:
    Let source be "Let x be 5\nLet y be 10\nLet z be 15"
    Let tokens be Lexer.tokenize(source, "developer")
    
    Note: Find the three "Let" tokens
    Let let_tokens be TokenStream.get_all_tokens_with_value(tokens, "Let")
    Assert.equals(Collections.list_length(let_tokens), 3)
    
    Let first_let be Collections.list_get(let_tokens, 0)
    Let second_let be Collections.list_get(let_tokens, 1)
    Let third_let be Collections.list_get(let_tokens, 2)
    
    Assert.equals(first_let.line, 1)
    Assert.equals(second_let.line, 2)
    Assert.equals(third_let.line, 3)
    
    Assert.equals(first_let.column, 1)
    Assert.equals(second_let.column, 1)
    Assert.equals(third_let.column, 1)
End Process

Note: =====================================================================
Note: COMPLEX INTEGRATION SCENARIOS
Note: =====================================================================

Process called "test_complete_process_definition" returns Nothing:
    Let source be "Process called \"fibonacci\" that takes n as Integer returns Integer:\n    If n is less than or equal to 1:\n        Return n\n    End If\n    Return fibonacci(n minus 1) plus fibonacci(n minus 2)\nEnd Process"
    
    Let tokens be Lexer.tokenize(source, "canon")
    
    Note: Verify major structural tokens
    Assert.contains_token(tokens, "Process")
    Assert.contains_token(tokens, "called")
    Assert.contains_token(tokens, "fibonacci")
    Assert.contains_token(tokens, "that")
    Assert.contains_token(tokens, "takes")
    Assert.contains_token(tokens, "If")
    Assert.contains_token(tokens, "Return")
    Assert.contains_token(tokens, "End")
    Assert.contains_token(tokens, "plus")
    Assert.contains_token(tokens, "minus")
End Process

Note: =====================================================================
Note: HELPER FUNCTIONS
Note: =====================================================================

Process called "contains_token_with_type" that takes tokens as TokenStream.TokenStream, value as String, token_type as String returns Nothing:
    Let found be False
    For i from 0 to TokenStream.get_token_count(tokens) minus 1:
        Let token be TokenStream.get_token_at(tokens, i)
        If token.value equals value And token.token_type equals token_type:
            Set found to True
            Break
        End If
    End For
    Assert.is_true(found)
End Process

Note: =====================================================================
Note: TEST RUNNER
Note: =====================================================================

Process called "run_all_lexer_integration_tests" returns Nothing:
    @TestCases
        - Simple statement tokenization
        - Complex expression tokenization
        - Canon mode vs Developer mode
        - Encasing patterns with spaces in identifiers
        - Statement patterns (imperative, import)
        - Math symbols and Greek letters
        - String literals with escapes and formatting
        - Comment handling (single-line and multi-line)
        - Error recovery from invalid input
        - Line and column position tracking
        - Complete process definitions
    @End TestCases
    
    test_simple_statement_tokenization()
    test_complex_expression_tokenization()
    test_canon_mode_tokenization()
    test_developer_mode_tokenization()
    test_encasing_with_spaces()
    test_encasing_ambiguity_resolution()
    test_imperative_statement_pattern()
    test_import_statement_pattern()
    test_math_symbols_in_expression()
    test_mixed_operators_and_symbols()
    test_string_with_escapes()
    test_formatted_string()
    test_comments_are_skipped()
    test_multi_line_comment_blocks()
    test_unclosed_string_recovery()
    test_invalid_character_recovery()
    test_line_column_tracking()
    test_complete_process_definition()
    
    Print("All lexer integration tests passed!")
End Process