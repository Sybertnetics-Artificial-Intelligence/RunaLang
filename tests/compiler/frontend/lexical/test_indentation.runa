Note:
tests/compiler/frontend/lexical/test_indentation.runa
Indentation Token Generation Tests

This module provides comprehensive tests for the indentation token generation
system, validating Python-style stack-based indentation tracking, Indent and
Dedent token generation, mixed indentation handling, and integration with the
main lexer tokenization process.

Key test scenarios covered:
- Standard indent and dedent token generation
- Nested indentation levels and proper stack management
- Mixed tabs and spaces handling with warnings
- Empty line and comment line skipping
- End-of-file dedent token generation
- Invalid indentation level detection
- Integration with lexer tokenization flow
- Tab width calculation and consistency
:End Note

Import "tests/framework/test_runner" as TestRunner
Import "tests/framework/assertions" as Assert
Import "compiler/frontend/lexical/lexer" as Lexer

Note: =====================================================================
Note: FUNDAMENTAL INDENTATION TESTS
Note: =====================================================================

@TestCases
Tests for indentation token generation covering:
- Simple indent and dedent operations
- Single-level indentation changes
- Proper token positioning and metadata
- Integration with lexer flow
@End TestCases

Process called "test_fundamental_indentation" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: Simple indentation increase
    Let source_with_indent be "Process:\n    Return value\nEnd Process"
    Let tokens be Lexer.lex_source_code(source_with_indent, "canon", "test_file.runa")
    
    Note: Find Indent token
    Let indent_token be find_token_by_type(tokens, "Indent")
    Assert.assert_not_null(indent_token, "Should generate Indent token for increased indentation")
    Assert.assert_equals(indent_token.value, "4", "Indent token should have correct level")
    Assert.assert_equals(indent_token.column, 1, "Indent token should be at column 1")
    
    Note: Test 2: Simple indentation decrease
    Let source_with_dedent be "If condition:\n    statement\nend_statement"
    Let dedent_tokens be Lexer.lex_source_code(source_with_dedent, "canon", "test_file.runa")
    
    Note: Find Dedent token
    Let dedent_token be find_token_by_type(dedent_tokens, "Dedent")
    Assert.assert_not_null(dedent_token, "Should generate Dedent token for decreased indentation")
    
    Note: Test 3: No indentation change
    Let source_no_change be "statement1\nstatement2\nstatement3"
    Let no_change_tokens be Lexer.lex_source_code(source_no_change, "canon", "test_file.runa")
    
    Note: Should not have indent/dedent tokens
    Let has_indent be has_token_type(no_change_tokens, "Indent")
    Let has_dedent be has_token_type(no_change_tokens, "Dedent")
    Assert.assert_false(has_indent, "Should not generate Indent token for same indentation")
    Assert.assert_false(has_dedent, "Should not generate Dedent token for same indentation")
End Process

Note: =====================================================================
Note: NESTED INDENTATION TESTS
Note: =====================================================================

@TestCases
Tests for nested indentation handling covering:
- Multiple indentation levels
- Multiple dedent tokens for unindenting several levels
- Stack-based indentation tracking
- Proper nesting semantics
@End TestCases

Process called "test_nested_indentation" that takes test_runner as TestRunner returns Nothing:
    Note: Test nested indentation with multiple levels
    Let nested_source be string_concat("level0\n", 
        string_concat("    level1\n",
        string_concat("        level2\n",
        string_concat("            level3\n",
        string_concat("        back_to_level2\n",
        string_concat("    back_to_level1\n", "back_to_level0"))))))
    
    Let nested_tokens be Lexer.lex_source_code(nested_source, "canon", "test_file.runa")
    
    Note: Count indent and dedent tokens
    Let indent_tokens be find_all_tokens_by_type(nested_tokens, "Indent")
    Let dedent_tokens be find_all_tokens_by_type(nested_tokens, "Dedent")
    
    Assert.assert_equals(list_length(indent_tokens), 3, "Should generate 3 Indent tokens for 3 indentation increases")
    Assert.assert_equals(list_length(dedent_tokens), 3, "Should generate 3 Dedent tokens for returning to base level")
    
    Note: Verify indent levels are correct
    Assert.assert_equals(get_token_at(indent_tokens, 0).value, "4", "First indent should be level 4")
    Assert.assert_equals(get_token_at(indent_tokens, 1).value, "8", "Second indent should be level 8")
    Assert.assert_equals(get_token_at(indent_tokens, 2).value, "12", "Third indent should be level 12")
    
    Note: Test multiple dedents at once
    Let multi_dedent_source be string_concat("level0\n",
        string_concat("    level1\n",
        string_concat("        level2\n",
        string_concat("            level3\n", "back_to_level0"))))
    
    Let multi_dedent_tokens be Lexer.lex_source_code(multi_dedent_source, "canon", "test_file.runa")
    Let final_dedents be find_all_tokens_by_type(multi_dedent_tokens, "Dedent")
    
    Note: Should generate 3 dedent tokens when jumping from level 3 back to level 0
    Assert.assert_greater_than_or_equal(list_length(final_dedents), 3, "Should generate multiple Dedent tokens for large indentation decrease")
End Process

Note: =====================================================================
Note: TAB AND SPACE HANDLING TESTS
Note: =====================================================================

@TestCases
Tests for tab and space indentation handling covering:
- Tab width calculation (4 spaces per tab)
- Mixed tabs and spaces with warnings
- Consistent indentation level calculation
- Tab expansion to next tab stop
@End TestCases

Process called "test_tab_and_space_handling" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: Pure tab indentation
    Let tab_source be string_concat("function:\n", string_concat("\t", "body"))
    Let tab_tokens be Lexer.lex_source_code(tab_source, "canon", "test_file.runa")
    
    Let tab_indent_token be find_token_by_type(tab_tokens, "Indent")
    Assert.assert_not_null(tab_indent_token, "Should generate Indent token for tab indentation")
    Assert.assert_equals(tab_indent_token.value, "4", "Tab should expand to 4 spaces")
    
    Note: Test 2: Mixed tabs and spaces (should warn)
    Let mixed_source be string_concat("function:\n", string_concat("\t  ", "body"))  Note: Tab + 2 spaces
    Let mixed_tokens be Lexer.lex_source_code(mixed_source, "canon", "test_file.runa")
    
    Note: Should generate warning about mixed indentation
    Assert.assert_true(has_warnings(mixed_tokens), "Should generate warning for mixed tabs and spaces")
    
    Note: Test 3: Tab expansion calculation
    Let complex_tab_source be string_concat("start:\n",
        string_concat("  \t", string_concat("body\n",  Note: 2 spaces + tab = 4 total
        string_concat("    ", "same_level"))))  Note: 4 spaces should be same level
    
    Let complex_tab_tokens be Lexer.lex_source_code(complex_tab_source, "canon", "test_file.runa")
    
    Note: Both lines should be at same indentation level (no extra indent/dedent)
    Let complex_indents be find_all_tokens_by_type(complex_tab_tokens, "Indent")
    Assert.assert_equals(list_length(complex_indents), 1, "Mixed tab/space calculation should produce consistent levels")
End Process

Note: =====================================================================
Note: EMPTY LINE AND COMMENT HANDLING TESTS
Note: =====================================================================

@TestCases
Tests for proper handling of empty lines and comments covering:
- Empty lines ignored for indentation tracking
- Comment-only lines ignored for indentation tracking
- Whitespace-only lines ignored for indentation tracking
- Indentation maintained across empty sections
@End TestCases

Process called "test_empty_line_and_comment_handling" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: Empty lines should be ignored
    Let empty_line_source be string_concat("function:\n",
        string_concat("    statement1\n",
        string_concat("\n",  Note: Empty line
        string_concat("\n",  Note: Another empty line
        string_concat("    statement2\n", "end_function")))))
    
    Let empty_line_tokens be Lexer.lex_source_code(empty_line_source, "canon", "test_file.runa")
    
    Note: Should only have one indent and one dedent (empty lines ignored)
    Let empty_indents be find_all_tokens_by_type(empty_line_tokens, "Indent")
    Let empty_dedents be find_all_tokens_by_type(empty_line_tokens, "Dedent")
    Assert.assert_equals(list_length(empty_indents), 1, "Empty lines should not affect indentation tracking")
    Assert.assert_equals(list_length(empty_dedents), 1, "Empty lines should not affect indentation tracking")
    
    Note: Test 2: Comment-only lines should be ignored
    Let comment_source be string_concat("function:\n",
        string_concat("    statement1\n",
        string_concat("    Note: This is a comment\n",
        string_concat("    statement2\n", "end_function"))))
    
    Let comment_tokens be Lexer.lex_source_code(comment_source, "canon", "test_file.runa")
    
    Note: Comment line should not generate extra indent/dedent tokens
    Let comment_indents be find_all_tokens_by_type(comment_tokens, "Indent")
    Let comment_dedents be find_all_tokens_by_type(comment_tokens, "Dedent")
    Assert.assert_equals(list_length(comment_indents), 1, "Comment lines should not affect indentation tracking")
    Assert.assert_equals(list_length(comment_dedents), 1, "Comment lines should not affect indentation tracking")
    
    Note: Test 3: Whitespace-only lines
    Let whitespace_source be string_concat("function:\n",
        string_concat("    statement1\n",
        string_concat("      \n",  Note: Spaces only
        string_concat("    statement2\n", "end_function"))))
    
    Let whitespace_tokens be Lexer.lex_source_code(whitespace_source, "canon", "test_file.runa")
    
    Note: Whitespace-only lines should be ignored
    Let whitespace_indents be find_all_tokens_by_type(whitespace_tokens, "Indent")
    Assert.assert_equals(list_length(whitespace_indents), 1, "Whitespace-only lines should not affect indentation tracking")
End Process

Note: =====================================================================
Note: END-OF-FILE HANDLING TESTS
Note: =====================================================================

@TestCases
Tests for end-of-file indentation handling covering:
- Final dedent tokens generated at EOF
- All indentation levels properly closed
- Incomplete files handled gracefully
- Token stream consistency maintained
@End TestCases

Process called "test_end_of_file_handling" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: EOF with nested indentation should generate final dedents
    Let eof_nested_source be string_concat("function:\n",
        string_concat("    if condition:\n",
        string_concat("        statement1\n",
        string_concat("            nested_statement\n", "                deep_statement"))))  Note: Ends without dedenting
    
    Let eof_tokens be Lexer.lex_source_code(eof_nested_source, "canon", "test_file.runa")
    
    Note: Should generate dedent tokens to close all levels at EOF
    Let eof_dedents be find_all_tokens_by_type(eof_tokens, "Dedent")
    Assert.assert_greater_than_or_equal(list_length(eof_dedents), 3, "Should generate final dedent tokens at EOF")
    
    Note: Test 2: Properly closed indentation should not generate extra dedents
    Let properly_closed_source be string_concat("function:\n",
        string_concat("    statement\n", "end_function"))
    
    Let closed_tokens be Lexer.lex_source_code(properly_closed_source, "canon", "test_file.runa")
    
    Let closed_indents be find_all_tokens_by_type(closed_tokens, "Indent")
    Let closed_dedents be find_all_tokens_by_type(closed_tokens, "Dedent")
    Assert.assert_equals(list_length(closed_indents), list_length(closed_dedents), "Properly closed indentation should have matching indent/dedent counts")
End Process

Note: =====================================================================
Note: ERROR HANDLING TESTS
Note: =====================================================================

@TestCases
Tests for indentation error handling covering:
- Invalid indentation levels (not matching any outer level)
- Inconsistent indentation detection
- Error reporting with proper location information
- Graceful handling of malformed indentation
@End TestCases

Process called "test_indentation_error_handling" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: Invalid dedent level (not matching any outer level)
    Let invalid_dedent_source be string_concat("function:\n",
        string_concat("    level1\n",
        string_concat("        level2\n",
        string_concat("      invalid_level\n", "end"))))  Note: 6 spaces doesn't match 0, 4, or 8
    
    Let invalid_tokens be Lexer.lex_source_code(invalid_dedent_source, "canon", "test_file.runa")
    
    Note: Should generate error for invalid indentation level
    Assert.assert_true(has_errors(invalid_tokens), "Should generate error for invalid indentation level")
    
    Note: Test 2: Inconsistent indentation should generate warnings
    Let inconsistent_source be string_concat("function:\n",
        string_concat("    spaces\n",
        string_concat("\ttab\n", "end")))
    
    Let inconsistent_tokens be Lexer.lex_source_code(inconsistent_source, "canon", "test_file.runa")
    
    Note: Should warn about inconsistent indentation
    Assert.assert_true(has_warnings(inconsistent_tokens), "Should generate warning for inconsistent indentation")
End Process

Note: =====================================================================
Note: INTEGRATION TESTS
Note: =====================================================================

@TestCases
Tests for indentation integration with other lexer features covering:
- Indentation with import statements
- Indentation with annotations
- Indentation with string literals and comments
- Both Canon and Developer syntax modes
@End TestCases

Process called "test_indentation_integration" that takes test_runner as TestRunner returns Nothing:
    Note: Test 1: Indentation with import statements
    Let import_source be string_concat("Import \"module\" as Mod\n",
        string_concat("Process called \"test\":\n",
        string_concat("    Mod.function()\n", "End Process")))
    
    Let import_tokens be Lexer.lex_source_code(import_source, "canon", "test_file.runa")
    
    Note: Should have both import and indentation tokens
    Assert.assert_true(has_token_type(import_tokens, "Import"), "Should tokenize import statement")
    Assert.assert_true(has_token_type(import_tokens, "Indent"), "Should generate indentation tokens")
    
    Note: Test 2: Indentation with annotations
    Let annotation_source be string_concat("@Reasoning\n",
        string_concat("This explains the logic\n",
        string_concat("@End Reasoning\n",
        string_concat("Process called \"test\":\n",
        string_concat("    statement\n", "End Process")))))
    
    Let annotation_tokens be Lexer.lex_source_code(annotation_source, "canon", "test_file.runa")
    
    Note: Should handle both annotations and indentation
    Assert.assert_true(has_token_type(annotation_tokens, "Annotation"), "Should tokenize annotations")
    Assert.assert_true(has_token_type(annotation_tokens, "Indent"), "Should generate indentation tokens with annotations")
    
    Note: Test 3: Both syntax modes should generate indentation tokens
    Let test_source be string_concat("function:\n", string_concat("    body\n", "end"))
    
    Let canon_tokens be Lexer.lex_source_code(test_source, "canon", "test_file.runa")
    Let dev_tokens be Lexer.lex_source_code(test_source, "developer", "test_file.runa")
    
    Assert.assert_true(has_token_type(canon_tokens, "Indent"), "Canon mode should generate indentation tokens")
    Assert.assert_true(has_token_type(dev_tokens, "Indent"), "Developer mode should generate indentation tokens")
End Process

Note: =====================================================================
Note: TEST UTILITIES
Note: =====================================================================

@Reasoning
Test utilities provide helper functions for token analysis and validation.
These reduce code duplication and improve test readability while providing
comprehensive validation capabilities for indentation testing.
@End Reasoning

Process called "find_token_by_type" that takes tokens as TokenStream, token_type as String returns Token:
    Let token_count be TokenStream.token_count(tokens)
    For i from 0 to token_count minus 1:
        Let token be TokenStream.get_token_at(tokens, i)
        If string_equals(token.token_type, token_type):
            Return token
        End If
    End For
    
    Note: Return null token if not found
    Return Token with
        token_type as "NULL",
        value as "",
        line as 0,
        column as 0,
        length as 0,
        import_statement as None
    End Token
End Process

Process called "find_all_tokens_by_type" that takes tokens as TokenStream, token_type as String returns List[Token]:
    Let result be []
    Let token_count be TokenStream.token_count(tokens)
    
    For i from 0 to token_count minus 1:
        Let token be TokenStream.get_token_at(tokens, i)
        If string_equals(token.token_type, token_type):
            Set result to add_to_list(result, token)
        End If
    End For
    
    Return result
End Process

Process called "has_token_type" that takes tokens as TokenStream, token_type as String returns Boolean:
    Let token be find_token_by_type(tokens, token_type)
    Return Not string_equals(token.token_type, "NULL")
End Process

Process called "has_warnings" that takes tokens as TokenStream returns Boolean:
    Note: Check if token stream has any warning messages
    Return TokenStream.has_warnings(tokens)
End Process

Process called "has_errors" that takes tokens as TokenStream returns Boolean:
    Note: Check if token stream has any error messages
    Return TokenStream.has_errors(tokens)
End Process

Process called "get_token_at" that takes token_list as List[Token], index as Integer returns Token:
    Return list_get(token_list, index)
End Process

Process called "list_length" that takes token_list as List[Token] returns Integer:
    Return list_size(token_list)
End Process

Process called "add_to_list" that takes token_list as List[Token], token as Token returns List[Token]:
    Return list_append(token_list, token)
End Process

Process called "string_concat" that takes str1 as String, str2 as String returns String:
    Return string_concatenate(str1, str2)
End Process

Note: =====================================================================
Note: TEST RUNNER INTEGRATION
Note: =====================================================================

@Reasoning
Test runner integration provides the main entry point for executing
all indentation tests. This enables automated testing and integration
with the continuous integration system.
@End Reasoning

Process called "run_all_indentation_tests" returns TestRunner.TestResult:
    Let test_runner be TestRunner.create_test_runner("Indentation Token Generation Tests")
    
    TestRunner.add_test(test_runner, "Fundamental Indentation", test_fundamental_indentation)
    TestRunner.add_test(test_runner, "Nested Indentation", test_nested_indentation)
    TestRunner.add_test(test_runner, "Tab and Space Handling", test_tab_and_space_handling)
    TestRunner.add_test(test_runner, "Empty Line and Comment Handling", test_empty_line_and_comment_handling)
    TestRunner.add_test(test_runner, "End-of-File Handling", test_end_of_file_handling)
    TestRunner.add_test(test_runner, "Indentation Error Handling", test_indentation_error_handling)
    TestRunner.add_test(test_runner, "Indentation Integration", test_indentation_integration)
    
    Return TestRunner.run_tests(test_runner)
End Process

@Performance_Hints
Indentation testing should focus on:
- Testing with representative code samples for realistic performance
- Measuring indentation processing time for performance regression detection
- Using efficient token stream analysis in test utilities
- Validating memory usage during indentation stack operations
@End Performance_Hints

@Security_Scope
Indentation testing must verify security and robustness:
- Boundary condition testing for deeply nested indentation
- Stack overflow prevention in indentation tracking
- Proper handling of malformed indentation without crashes
- Memory safety during indentation token generation
@End Security_Scope