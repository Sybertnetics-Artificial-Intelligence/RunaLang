Note:
tests/compiler/frontend/lexical/run_all_lexical_tests.runa
Comprehensive Lexical Test Suite Runner

This module orchestrates the execution of all lexical analysis tests,
providing detailed reporting, performance metrics, and failure analysis
for the complete Runa lexer system validation.

Key Features:
- Executes all lexical test modules in sequence
- Provides performance timing and memory usage metrics
- Aggregates test results with detailed failure reporting
- Supports both individual test execution and batch processing
- Includes comprehensive error handling and recovery
- Generates test coverage reports for lexical components
:End Note

Import "tests/compiler/frontend/lexical/test_keywords" as KeywordTests
Import "tests/compiler/frontend/lexical/test_canon_developer_conversion" as ConversionTests
Import "tests/compiler/frontend/lexical/test_lexer_integration" as IntegrationTests
Import "tests/compiler/frontend/lexical/test_indentation" as IndentationTests
Import "tests/compiler/frontend/lexical/test_comments" as CommentTests
Import "tests/compiler/frontend/lexical/test_import_resolution" as ImportTests
Import "tests/compiler/frontend/lexical/test_math_symbols" as MathSymbolTests
Import "tests/compiler/frontend/lexical/test_operators" as OperatorTests
Import "tests/compiler/frontend/lexical/test_literals" as LiteralTests
Import "testing/test_framework" as TestFramework
Import "testing/performance_monitor" as Performance
Import "testing/coverage_analyzer" as Coverage

@Reasoning
The lexical analyzer is the foundational component of the Runa compiler,
responsible for converting raw source text into structured tokens. A
comprehensive test suite ensures reliability across all lexical features
including keywords, operators, literals, comments, and syntax modes.
This test runner provides systematic validation with detailed diagnostics.
@End Reasoning

Note: =====================================================================
Note: TEST SUITE CONFIGURATION
Note: =====================================================================

Type called "TestSuiteConfig":
    enable_performance_monitoring as Boolean
    enable_coverage_analysis as Boolean 
    enable_detailed_reporting as Boolean
    stop_on_first_failure as Boolean
    timeout_per_test as Integer         Note: Seconds
    memory_limit_per_test as Integer    Note: MB
    parallel_execution as Boolean
    test_output_directory as String
End Type

Type called "TestResult":
    test_name as String
    module_name as String
    execution_time as Float            Note: Milliseconds  
    memory_usage as Integer           Note: KB
    total_assertions as Integer
    passed_assertions as Integer
    failed_assertions as Integer
    error_messages as List[String]
    performance_metrics as Dictionary[String, Float]
    coverage_data as Dictionary[String, Integer]
    status as String                  Note: "PASSED", "FAILED", "ERROR", "TIMEOUT"
End Type

Type called "TestSuiteResult":
    total_tests as Integer
    passed_tests as Integer
    failed_tests as Integer
    error_tests as Integer
    total_execution_time as Float     Note: Milliseconds
    total_memory_usage as Integer     Note: KB
    individual_results as List[TestResult]
    failure_summary as List[String]
    performance_summary as Dictionary[String, Float]
    coverage_summary as Dictionary[String, Integer]
End Type

Note: =====================================================================
Note: TEST EXECUTION ENGINE
Note: =====================================================================

@Implementation
The test execution engine provides robust test orchestration with error
recovery, performance monitoring, and detailed result aggregation. Each
test module is executed in isolation with comprehensive metrics collection.
@End Implementation

Process called "create_default_config" returns TestSuiteConfig:
    Return TestSuiteConfig with
        enable_performance_monitoring as True,
        enable_coverage_analysis as True,
        enable_detailed_reporting as True,
        stop_on_first_failure as False,
        timeout_per_test as 300,
        memory_limit_per_test as 512,
        parallel_execution as False,
        test_output_directory as "./test_reports"
    End TestSuiteConfig
End Process

Process called "execute_single_test" that takes test_name as String, test_process as Process, config as TestSuiteConfig returns TestResult:
    Let start_time be Performance.get_current_time_ms()
    Let start_memory be Performance.get_memory_usage_kb()
    
    Let result be TestResult with
        test_name as test_name,
        module_name as "unknown",
        execution_time as 0.0,
        memory_usage as 0,
        total_assertions as 0,
        passed_assertions as 0,
        failed_assertions as 0,
        error_messages as [],
        performance_metrics as create_dictionary(),
        coverage_data as create_dictionary(),
        status as "RUNNING"
    End TestResult
    
    Note: Set up test isolation environment
    Let test_context be TestFramework.create_isolated_context(config.memory_limit_per_test)
    
    Note: Enable coverage monitoring if requested
    If config.enable_coverage_analysis:
        Coverage.start_monitoring(test_context)
    End If
    
    Note: Execute test with timeout protection
    Let execution_result be TestFramework.execute_with_timeout(
        test_process,
        config.timeout_per_test,
        test_context
    )
    
    Note: Calculate execution metrics
    Let end_time be Performance.get_current_time_ms()
    Let end_memory be Performance.get_memory_usage_kb()
    Set result.execution_time to end_time minus start_time
    Set result.memory_usage to end_memory minus start_memory
    
    Note: Process execution results
    Match execution_result:
        When Success as test_outcome:
            Set result.status to "PASSED"
            Set result.total_assertions to test_outcome.assertion_count
            Set result.passed_assertions to test_outcome.passed_assertions
            Set result.failed_assertions to test_outcome.failed_assertions
            
            If result.failed_assertions is greater than 0:
                Set result.status to "FAILED"
                Set result.error_messages to test_outcome.failure_messages
            End If
            
        When Timeout as timeout_info:
            Set result.status to "TIMEOUT"
            Set result.error_messages to ["Test execution timed out after " + 
                string_from_integer(config.timeout_per_test) + " seconds"]
                
        When Error as error_info:
            Set result.status to "ERROR"
            Set result.error_messages to [error_info.error_message]
            
        When MemoryExceeded as memory_info:
            Set result.status to "ERROR"
            Set result.error_messages to ["Memory limit exceeded: " + 
                string_from_integer(memory_info.memory_used) + "KB > " +
                string_from_integer(config.memory_limit_per_test) + "KB"]
    End Match
    
    Note: Collect performance metrics if enabled
    If config.enable_performance_monitoring:
        Set result.performance_metrics["cpu_time"] to Performance.get_cpu_time_ms(test_context)
        Set result.performance_metrics["gc_time"] to Performance.get_gc_time_ms(test_context)
        Set result.performance_metrics["io_time"] to Performance.get_io_time_ms(test_context)
    End If
    
    Note: Collect coverage data if enabled
    If config.enable_coverage_analysis:
        Set result.coverage_data to Coverage.get_coverage_data(test_context)
        Coverage.stop_monitoring(test_context)
    End If
    
    Note: Clean up test context
    TestFramework.destroy_context(test_context)
    
    Return result
End Process

Note: =====================================================================
Note: TEST SUITE ORCHESTRATION
Note: =====================================================================

@Implementation
The orchestration system manages the execution of all lexical test modules,
collecting comprehensive results and generating detailed reports. Tests are
executed in dependency order with proper error handling and recovery.
@End Implementation

Process called "run_all_lexical_tests" returns TestSuiteResult:
    Return run_all_lexical_tests_with_config(create_default_config())
End Process

Process called "run_all_lexical_tests_with_config" that takes config as TestSuiteConfig returns TestSuiteResult:
    Let suite_result be TestSuiteResult with
        total_tests as 0,
        passed_tests as 0,
        failed_tests as 0,
        error_tests as 0,
        total_execution_time as 0.0,
        total_memory_usage as 0,
        individual_results as [],
        failure_summary as [],
        performance_summary as create_dictionary(),
        coverage_summary as create_dictionary()
    End TestSuiteResult
    
    Let suite_start_time be Performance.get_current_time_ms()
    
    Print("========================================")
    Print("Runa Lexical Analysis Test Suite")
    Print("========================================")
    Print("")
    
    Note: Define test execution order (dependency-aware)
    Let test_modules be [
        {"name": "Keywords", "process": KeywordTests.run_all_keyword_tests},
        {"name": "Canon/Developer Conversion", "process": ConversionTests.run_all_tests},
        {"name": "Math Symbols", "process": MathSymbolTests.run_all_math_symbol_tests},
        {"name": "Operators", "process": OperatorTests.run_all_operator_tests},
        {"name": "Literals", "process": LiteralTests.run_all_literal_tests},
        {"name": "Comments", "process": CommentTests.run_all_comment_tests},
        {"name": "Indentation", "process": IndentationTests.run_all_indentation_tests},
        {"name": "Import Resolution", "process": ImportTests.run_all_import_resolution_tests},
        {"name": "Lexer Integration", "process": IntegrationTests.run_all_lexer_integration_tests}
    ]
    
    Note: Execute each test module
    For Each test_module in test_modules:
        Set suite_result.total_tests to suite_result.total_tests plus 1
        
        Print("Running: " + test_module["name"] + " Tests...")
        
        Let test_result be execute_single_test(
            test_module["name"],
            test_module["process"],
            config
        )
        
        Set test_result.module_name to test_module["name"]
        Set suite_result.individual_results to add_to_list(suite_result.individual_results, test_result)
        
        Note: Update suite statistics
        Set suite_result.total_execution_time to suite_result.total_execution_time plus test_result.execution_time
        Set suite_result.total_memory_usage to suite_result.total_memory_usage plus test_result.memory_usage
        
        Match test_result.status:
            When "PASSED":
                Set suite_result.passed_tests to suite_result.passed_tests plus 1
                Print("‚úì " + test_module["name"] + " - PASSED (" + 
                      format_time_ms(test_result.execution_time) + ")")
                      
            When "FAILED":
                Set suite_result.failed_tests to suite_result.failed_tests plus 1
                Print("‚úó " + test_module["name"] + " - FAILED (" + 
                      format_time_ms(test_result.execution_time) + ")")
                      
                For Each error in test_result.error_messages:
                    Print("  Error: " + error)
                    Set suite_result.failure_summary to add_to_list(suite_result.failure_summary, 
                        test_module["name"] + ": " + error)
                End For
                
                If config.stop_on_first_failure:
                    Print("Stopping execution due to failure (stop_on_first_failure=True)")
                    Break
                End If
                
            When "ERROR":
                Set suite_result.error_tests to suite_result.error_tests plus 1
                Print("‚ö† " + test_module["name"] + " - ERROR")
                
                For Each error in test_result.error_messages:
                    Print("  Error: " + error)
                    Set suite_result.failure_summary to add_to_list(suite_result.failure_summary, 
                        test_module["name"] + " (ERROR): " + error)
                End For
                
                If config.stop_on_first_failure:
                    Print("Stopping execution due to error (stop_on_first_failure=True)")
                    Break
                End If
                
            When "TIMEOUT":
                Set suite_result.error_tests to suite_result.error_tests plus 1
                Print("‚è± " + test_module["name"] + " - TIMEOUT")
                
                For Each error in test_result.error_messages:
                    Print("  " + error)
                    Set suite_result.failure_summary to add_to_list(suite_result.failure_summary, 
                        test_module["name"] + " (TIMEOUT): " + error)
                End For
        End Match
        
        Print("")
    End For
    
    Let suite_end_time be Performance.get_current_time_ms()
    Set suite_result.total_execution_time to suite_end_time minus suite_start_time
    
    Note: Generate performance summary
    If config.enable_performance_monitoring:
        generate_performance_summary(suite_result)
    End If
    
    Note: Generate coverage summary  
    If config.enable_coverage_analysis:
        generate_coverage_summary(suite_result)
    End If
    
    Note: Generate detailed report if requested
    If config.enable_detailed_reporting:
        generate_detailed_report(suite_result, config)
    End If
    
    print_final_summary(suite_result)
    
    Return suite_result
End Process

Note: =====================================================================
Note: REPORTING AND ANALYSIS
Note: =====================================================================

@Implementation
The reporting system provides comprehensive analysis of test results,
including performance metrics, coverage analysis, and failure diagnostics.
Reports are generated in multiple formats for different consumption needs.
@End Implementation

Process called "generate_performance_summary" that takes suite_result as TestSuiteResult returns Nothing:
    Let total_cpu_time be 0.0
    Let total_gc_time be 0.0
    Let total_io_time be 0.0
    
    For Each result in suite_result.individual_results:
        If dictionary_has_key(result.performance_metrics, "cpu_time"):
            Set total_cpu_time to total_cpu_time plus result.performance_metrics["cpu_time"]
        End If
        If dictionary_has_key(result.performance_metrics, "gc_time"):
            Set total_gc_time to total_gc_time plus result.performance_metrics["gc_time"]
        End If
        If dictionary_has_key(result.performance_metrics, "io_time"):
            Set total_io_time to total_io_time plus result.performance_metrics["io_time"]
        End If
    End For
    
    Set suite_result.performance_summary["total_cpu_time"] to total_cpu_time
    Set suite_result.performance_summary["total_gc_time"] to total_gc_time
    Set suite_result.performance_summary["total_io_time"] to total_io_time
    Set suite_result.performance_summary["average_test_time"] to 
        suite_result.total_execution_time divided by suite_result.total_tests
End Process

Process called "generate_coverage_summary" that takes suite_result as TestSuiteResult returns Nothing:
    Let covered_lines be 0
    Let total_lines be 0
    Let covered_functions be 0
    Let total_functions be 0
    
    For Each result in suite_result.individual_results:
        If dictionary_has_key(result.coverage_data, "covered_lines"):
            Set covered_lines to covered_lines plus result.coverage_data["covered_lines"]
        End If
        If dictionary_has_key(result.coverage_data, "total_lines"):
            Set total_lines to total_lines plus result.coverage_data["total_lines"]
        End If
        If dictionary_has_key(result.coverage_data, "covered_functions"):
            Set covered_functions to covered_functions plus result.coverage_data["covered_functions"]
        End If
        If dictionary_has_key(result.coverage_data, "total_functions"):
            Set total_functions to total_functions plus result.coverage_data["total_functions"]
        End If
    End For
    
    Set suite_result.coverage_summary["covered_lines"] to covered_lines
    Set suite_result.coverage_summary["total_lines"] to total_lines
    Set suite_result.coverage_summary["covered_functions"] to covered_functions
    Set suite_result.coverage_summary["total_functions"] to total_functions
    
    If total_lines is greater than 0:
        Set suite_result.coverage_summary["line_coverage_percent"] to 
            (covered_lines multiplied by 100) divided by total_lines
    End If
    
    If total_functions is greater than 0:
        Set suite_result.coverage_summary["function_coverage_percent"] to 
            (covered_functions multiplied by 100) divided by total_functions
    End If
End Process

Process called "generate_detailed_report" that takes suite_result as TestSuiteResult, config as TestSuiteConfig returns Nothing:
    Let report_file be config.test_output_directory + "/lexical_test_report.txt"
    
    Let report_content be "Runa Lexical Analysis Test Suite - Detailed Report\n"
    Set report_content to report_content + "Generated: " + get_current_timestamp() + "\n\n"
    
    Set report_content to report_content + "SUMMARY:\n"
    Set report_content to report_content + "Total Tests: " + string_from_integer(suite_result.total_tests) + "\n"
    Set report_content to report_content + "Passed: " + string_from_integer(suite_result.passed_tests) + "\n"
    Set report_content to report_content + "Failed: " + string_from_integer(suite_result.failed_tests) + "\n"
    Set report_content to report_content + "Errors: " + string_from_integer(suite_result.error_tests) + "\n"
    Set report_content to report_content + "Total Time: " + format_time_ms(suite_result.total_execution_time) + "\n"
    Set report_content to report_content + "Total Memory: " + format_memory_kb(suite_result.total_memory_usage) + "\n\n"
    
    If config.enable_performance_monitoring:
        Set report_content to report_content + "PERFORMANCE METRICS:\n"
        For Each key in get_dictionary_keys(suite_result.performance_summary):
            Set report_content to report_content + key + ": " + 
                string_from_float(suite_result.performance_summary[key]) + "\n"
        End For
        Set report_content to report_content + "\n"
    End If
    
    If config.enable_coverage_analysis:
        Set report_content to report_content + "COVERAGE ANALYSIS:\n"
        For Each key in get_dictionary_keys(suite_result.coverage_summary):
            Set report_content to report_content + key + ": " + 
                string_from_integer(suite_result.coverage_summary[key]) + "\n"
        End For
        Set report_content to report_content + "\n"
    End If
    
    Set report_content to report_content + "INDIVIDUAL TEST RESULTS:\n"
    For Each result in suite_result.individual_results:
        Set report_content to report_content + "Test: " + result.test_name + "\n"
        Set report_content to report_content + "  Status: " + result.status + "\n"
        Set report_content to report_content + "  Time: " + format_time_ms(result.execution_time) + "\n"
        Set report_content to report_content + "  Memory: " + format_memory_kb(result.memory_usage) + "\n"
        Set report_content to report_content + "  Assertions: " + 
            string_from_integer(result.passed_assertions) + "/" + 
            string_from_integer(result.total_assertions) + "\n"
            
        If list_length(result.error_messages) is greater than 0:
            Set report_content to report_content + "  Errors:\n"
            For Each error in result.error_messages:
                Set report_content to report_content + "    - " + error + "\n"
            End For
        End If
        Set report_content to report_content + "\n"
    End For
    
    write_file(report_file, report_content)
    Print("Detailed report written to: " + report_file)
End Process

Process called "print_final_summary" that takes suite_result as TestSuiteResult returns Nothing:
    Print("========================================")
    Print("FINAL RESULTS")
    Print("========================================")
    Print("Total Tests:     " + string_from_integer(suite_result.total_tests))
    Print("Passed:          " + string_from_integer(suite_result.passed_tests) + 
          " (" + format_percentage(suite_result.passed_tests, suite_result.total_tests) + ")")
    Print("Failed:          " + string_from_integer(suite_result.failed_tests) +
          " (" + format_percentage(suite_result.failed_tests, suite_result.total_tests) + ")")
    Print("Errors:          " + string_from_integer(suite_result.error_tests) +
          " (" + format_percentage(suite_result.error_tests, suite_result.total_tests) + ")")
    Print("Total Time:      " + format_time_ms(suite_result.total_execution_time))
    Print("Average per Test: " + format_time_ms(suite_result.total_execution_time divided by suite_result.total_tests))
    Print("Total Memory:    " + format_memory_kb(suite_result.total_memory_usage))
    
    If dictionary_has_key(suite_result.coverage_summary, "line_coverage_percent"):
        Print("Line Coverage:   " + format_float_2(suite_result.coverage_summary["line_coverage_percent"]) + "%")
    End If
    
    If dictionary_has_key(suite_result.coverage_summary, "function_coverage_percent"):
        Print("Func Coverage:   " + format_float_2(suite_result.coverage_summary["function_coverage_percent"]) + "%")
    End If
    
    If list_length(suite_result.failure_summary) is greater than 0:
        Print("")
        Print("FAILURE SUMMARY:")
        For Each failure in suite_result.failure_summary:
            Print("  ‚Ä¢ " + failure)
        End For
    End If
    
    Print("========================================")
    
    If suite_result.failed_tests equals 0 And suite_result.error_tests equals 0:
        Print("üéâ All lexical tests PASSED!")
        Print("The Runa lexer is ready for integration.")
    Otherwise:
        Print("‚ùå Some tests FAILED or had ERRORS.")
        Print("Review the failures above before proceeding.")
    End If
    
    Print("")
End Process

Note: =====================================================================
Note: UTILITY FUNCTIONS
Note: =====================================================================

@Implementation
Utility functions provide formatting, file operations, and common
calculations needed throughout the test suite execution and reporting.
@End Implementation

Process called "format_time_ms" that takes milliseconds as Float returns String:
    If milliseconds is less than 1000.0:
        Return format_float_1(milliseconds) + "ms"
    Otherwise:
        Return format_float_2(milliseconds divided by 1000.0) + "s"
    End If
End Process

Process called "format_memory_kb" that takes kilobytes as Integer returns String:
    If kilobytes is less than 1024:
        Return string_from_integer(kilobytes) + "KB"
    Otherwise:
        Let megabytes be kilobytes divided by 1024
        Return format_float_1(megabytes) + "MB"
    End If
End Process

Process called "format_percentage" that takes numerator as Integer, denominator as Integer returns String:
    If denominator equals 0:
        Return "0.0%"
    End If
    Let percentage be (numerator multiplied by 100) divided by denominator
    Return format_float_1(percentage) + "%"
End Process

Process called "format_float_1" that takes value as Float returns String:
    Return string_from_float_precision(value, 1)
End Process

Process called "format_float_2" that takes value as Float returns String:
    Return string_from_float_precision(value, 2)
End Process

Process called "get_current_timestamp" returns String:
    Let current_time be get_system_time()
    Return format_timestamp(current_time, "YYYY-MM-DD HH:MM:SS")
End Process

Process called "create_dictionary" returns Dictionary[String, Float]:
    Return new_dictionary()
End Process

Process called "dictionary_has_key" that takes dict as Dictionary[String, Float], key as String returns Boolean:
    Return contains_key(dict, key)
End Process

Process called "get_dictionary_keys" that takes dict as Dictionary[String, Float] returns List[String]:
    Return get_keys(dict)
End Process

Process called "write_file" that takes filename as String, content as String returns Nothing:
    Let file_handle be create_file_writer(filename)
    write_string(file_handle, content)
    close_file_writer(file_handle)
End Process

Note: =====================================================================
Note: MAIN ENTRY POINT
Note: =====================================================================

@Implementation
The main entry point provides both programmatic and command-line interfaces
for executing the lexical test suite with various configuration options.
@End Implementation

Process called "main" returns Nothing:
    @Performance_Hints
    The test suite runner should be optimized for:
    - Minimizing test setup/teardown overhead
    - Efficient memory management during parallel execution
    - Fast failure detection and reporting
    - Comprehensive metrics collection without impacting test performance
    @End Performance_Hints
    
    Let result be run_all_lexical_tests()
    
    Note: Exit with appropriate code for CI/CD systems
    If result.failed_tests equals 0 And result.error_tests equals 0:
        exit_with_code(0)
    Otherwise:
        exit_with_code(1)
    End If
End Process

@Security_Scope
The lexical test suite runner operates with:
- Read access to all lexical test modules and source files
- Write access to test report output directory
- System process control for timeout and memory limit enforcement
- No network access or external system modification capabilities
- Isolated test execution environments for security and reliability
@End Security_Scope

Note: Entry point for test execution
main()