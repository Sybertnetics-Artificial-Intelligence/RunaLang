Note: Apache Spark compatibility layer for Runa
Note: Provides Apache Spark-compatible distributed big data processing and analytics

Import "collections" as Collections
Import "errors" as Errors

Note: Spark session and context types
Type called "SparkSession":
    app_name as String
    master as String
    config as Dictionary[String, String]
    spark_context as SparkContext
    sql_context as SparkSQLContext
    catalog as SparkCatalog
    conf as SparkConf
    sparkContext as SparkContext
    version as String
    active_session as Boolean

Type called "SparkContext":
    app_name as String
    master as String
    spark_home as String
    pyFiles as Array[String]
    environment as Dictionary[String, String]
    batchSize as Integer
    serializer as String
    conf as SparkConf
    gateway as String
    jsc as String
    profiler_collector as String
    python_includes as Array[String]
    python_exec as String
    default_parallelism as Integer
    default_min_partitions as Integer
    start_time as Integer
    status_tracker as SparkStatusTracker
    ui_web_url as String
    application_id as String
    application_attempt_id as String

Type called "SparkConf":
    app_name as String
    master as String
    spark_home as String
    executor_env as Dictionary[String, String]
    configurations as Dictionary[String, String]

Type called "SparkSQLContext":
    spark_session as SparkSession
    spark_context as SparkContext
    default_session as SparkSession

Note: Spark DataFrame and Dataset types
Type called "SparkDataFrame":
    sql_ctx as SparkSQLContext
    schema as SparkStructType
    is_cached as Boolean
    is_local as Boolean
    rdd as SparkRDD
    write as SparkDataFrameWriter
    na as SparkDataFrameNaFunctions
    stat as SparkDataFrameStatFunctions
    columns as Array[String]
    dtypes as Array[Array[String]]
    count as Integer

Type called "SparkDataFrameWriter":
    df as SparkDataFrame
    format as String
    mode as String
    options as Dictionary[String, String]
    partition_by as Array[String]
    bucket_by as Dictionary[String, Any]
    sort_by as Array[String]

Type called "SparkDataFrameReader":
    spark as SparkSession
    format as String
    schema as SparkStructType
    options as Dictionary[String, String]

Type called "SparkColumn":
    jc as String
    name as String
    alias as String

Type called "SparkRow":
    fields as Dictionary[String, Any]
    schema as SparkStructType

Note: Spark RDD types
Type called "SparkRDD":
    ctx as SparkContext
    jrdd as String
    jrdd_deserializer as String
    partitioner as String
    is_cached as Boolean
    is_checkpointed as Boolean
    name as String
    id as Integer
    glom as Function

Type called "SparkPairRDD":
    ctx as SparkContext
    jrdd as String
    jrdd_deserializer as String
    partitioner as String
    is_cached as Boolean
    is_checkpointed as Boolean

Note: Spark schema types
Type called "SparkStructType":
    fields as Array[SparkStructField]
    field_names as Array[String]

Type called "SparkStructField":
    name as String
    data_type as SparkDataType
    nullable as Boolean
    metadata as Dictionary[String, Any]

Type called "SparkDataType":
    type_name as String
    json_value as String
    simple_string as String

Type called "SparkArrayType":
    element_type as SparkDataType
    contains_null as Boolean

Type called "SparkMapType":
    key_type as SparkDataType
    value_type as SparkDataType
    value_contains_null as Boolean

Note: Spark ML types
Type called "SparkMLPipeline":
    stages as Array[SparkMLPipelineStage]
    uid as String

Type called "SparkMLPipelineStage":
    uid as String
    params as Dictionary[String, Any]

Type called "SparkMLModel":
    uid as String
    parent as SparkMLPipelineStage
    params as Dictionary[String, Any]

Type called "SparkMLFeature":
    input_col as String
    output_col as String
    uid as String

Type called "SparkMLTransformer":
    uid as String
    input_col as String
    output_col as String
    params as Dictionary[String, Any]

Note: Spark streaming types
Type called "SparkStreamingContext":
    spark_context as SparkContext
    batch_duration as Float
    checkpoint_directory as String

Type called "SparkDStream":
    ssc as SparkStreamingContext
    jdstream as String
    transformations as Array[Function]

Type called "SparkStructuredStreaming":
    spark as SparkSession
    source as String
    format as String
    schema as SparkStructType
    options as Dictionary[String, String]

Note: Spark monitoring types
Type called "SparkStatusTracker":
    sc as SparkContext

Type called "SparkJobInfo":
    job_id as Integer
    stage_ids as Array[Integer]
    status as String
    num_tasks as Integer
    num_active_tasks as Integer
    num_completed_tasks as Integer
    num_skipped_tasks as Integer
    num_failed_tasks as Integer

Type called "SparkStageInfo":
    stage_id as Integer
    attempt_id as Integer
    name as String
    num_tasks as Integer
    num_active_tasks as Integer
    num_complete_tasks as Integer
    num_failed_tasks as Integer

Type called "SparkCatalog":
    current_database as String
    databases as Array[String]
    tables as Array[String]
    functions as Array[String]

Note: Create Spark session
Process called "spark_session" that takes app_name as String and master as String and config as Dictionary[String, String] returns SparkSession:
    Throw Errors.NotImplemented with "Spark session creation not implemented"

Note: Stop Spark session
Process called "spark_stop_session" that takes session as SparkSession returns Boolean:
    Throw Errors.NotImplemented with "Spark session stop not implemented"

Note: Create DataFrame from file
Process called "spark_read_parquet" that takes spark as SparkSession and path as String and options as Dictionary[String, String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark Parquet reading not implemented"

Note: Read CSV file
Process called "spark_read_csv" that takes spark as SparkSession and path as String and header as Boolean and infer_schema as Boolean and sep as String and options as Dictionary[String, String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark CSV reading not implemented"

Note: Read JSON file
Process called "spark_read_json" that takes spark as SparkSession and path as String and schema as SparkStructType and options as Dictionary[String, String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark JSON reading not implemented"

Note: Read from JDBC
Process called "spark_read_jdbc" that takes spark as SparkSession and url as String and table as String and properties as Dictionary[String, String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark JDBC reading not implemented"

Note: Create DataFrame from RDD
Process called "spark_dataframe_from_rdd" that takes spark as SparkSession and rdd as SparkRDD and schema as SparkStructType returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark DataFrame from RDD not implemented"

Note: Write DataFrame to file
Process called "spark_write_parquet" that takes df as SparkDataFrame and path as String and mode as String and options as Dictionary[String, String] returns Boolean:
    Throw Errors.NotImplemented with "Spark Parquet writing not implemented"

Note: Write DataFrame to CSV
Process called "spark_write_csv" that takes df as SparkDataFrame and path as String and header as Boolean and mode as String and options as Dictionary[String, String] returns Boolean:
    Throw Errors.NotImplemented with "Spark CSV writing not implemented"

Note: Write DataFrame to JSON
Process called "spark_write_json" that takes df as SparkDataFrame and path as String and mode as String and options as Dictionary[String, String] returns Boolean:
    Throw Errors.NotImplemented with "Spark JSON writing not implemented"

Note: Write to JDBC
Process called "spark_write_jdbc" that takes df as SparkDataFrame and url as String and table as String and mode as String and properties as Dictionary[String, String] returns Boolean:
    Throw Errors.NotImplemented with "Spark JDBC writing not implemented"

Note: Execute SQL query
Process called "spark_sql" that takes spark as SparkSession and sql_query as String returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark SQL execution not implemented"

Note: Register DataFrame as table
Process called "spark_create_temp_view" that takes df as SparkDataFrame and name as String and replace as Boolean returns Boolean:
    Throw Errors.NotImplemented with "Spark temp view creation not implemented"

Note: Select columns
Process called "spark_select" that takes df as SparkDataFrame and columns as Array[String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark column selection not implemented"

Note: Filter DataFrame
Process called "spark_filter" that takes df as SparkDataFrame and condition as String returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark filtering not implemented"

Note: Group by columns
Process called "spark_groupby" that takes df as SparkDataFrame and columns as Array[String] returns Dictionary[String, Any]:
    Throw Errors.NotImplemented with "Spark groupby not implemented"

Note: Join DataFrames
Process called "spark_join" that takes left as SparkDataFrame and right as SparkDataFrame and on as String and how as String returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark join not implemented"

Note: Sort DataFrame
Process called "spark_sort" that takes df as SparkDataFrame and columns as Array[String] and ascending as Array[Boolean] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark sorting not implemented"

Note: Add column
Process called "spark_with_column" that takes df as SparkDataFrame and col_name as String and column as SparkColumn returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark column addition not implemented"

Note: Drop columns
Process called "spark_drop" that takes df as SparkDataFrame and columns as Array[String] returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark column dropping not implemented"

Note: Show DataFrame
Process called "spark_show" that takes df as SparkDataFrame and num_rows as Integer and truncate as Boolean and vertical as Boolean returns String:
    Throw Errors.NotImplemented with "Spark show not implemented"

Note: Count rows
Process called "spark_count" that takes df as SparkDataFrame returns Integer:
    Throw Errors.NotImplemented with "Spark count not implemented"

Note: Collect DataFrame
Process called "spark_collect" that takes df as SparkDataFrame returns Array[SparkRow]:
    Throw Errors.NotImplemented with "Spark collect not implemented"

Note: Take rows
Process called "spark_take" that takes df as SparkDataFrame and num as Integer returns Array[SparkRow]:
    Throw Errors.NotImplemented with "Spark take not implemented"

Note: Create RDD from collection
Process called "spark_parallelize" that takes sc as SparkContext and collection as Array[Any] and num_slices as Integer returns SparkRDD:
    Throw Errors.NotImplemented with "Spark RDD parallelization not implemented"

Note: Read text file to RDD
Process called "spark_text_file" that takes sc as SparkContext and name as String and min_partitions as Integer and use_unicode as Boolean returns SparkRDD:
    Throw Errors.NotImplemented with "Spark text file reading not implemented"

Note: Map RDD
Process called "spark_rdd_map" that takes rdd as SparkRDD and func as Function and preserves_partitioning as Boolean returns SparkRDD:
    Throw Errors.NotImplemented with "Spark RDD map not implemented"

Note: Filter RDD
Process called "spark_rdd_filter" that takes rdd as SparkRDD and func as Function returns SparkRDD:
    Throw Errors.NotImplemented with "Spark RDD filter not implemented"

Note: Reduce RDD
Process called "spark_rdd_reduce" that takes rdd as SparkRDD and func as Function returns Any:
    Throw Errors.NotImplemented with "Spark RDD reduce not implemented"

Note: Collect RDD
Process called "spark_rdd_collect" that takes rdd as SparkRDD returns Array[Any]:
    Throw Errors.NotImplemented with "Spark RDD collect not implemented"

Note: Cache DataFrame
Process called "spark_cache" that takes df as SparkDataFrame returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark caching not implemented"

Note: Persist DataFrame
Process called "spark_persist" that takes df as SparkDataFrame and storage_level as String returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark persistence not implemented"

Note: Unpersist DataFrame
Process called "spark_unpersist" that takes df as SparkDataFrame and blocking as Boolean returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark unpersisting not implemented"

Note: Create ML Pipeline
Process called "spark_ml_pipeline" that takes stages as Array[SparkMLPipelineStage] returns SparkMLPipeline:
    Throw Errors.NotImplemented with "Spark ML Pipeline not implemented"

Note: Fit ML Pipeline
Process called "spark_ml_fit" that takes pipeline as SparkMLPipeline and dataset as SparkDataFrame returns SparkMLModel:
    Throw Errors.NotImplemented with "Spark ML fit not implemented"

Note: Transform with model
Process called "spark_ml_transform" that takes model as SparkMLModel and dataset as SparkDataFrame returns SparkDataFrame:
    Throw Errors.NotImplemented with "Spark ML transform not implemented"

Note: Create column expression
Process called "spark_col" that takes col_name as String returns SparkColumn:
    Throw Errors.NotImplemented with "Spark column expression not implemented"

Note: Create literal expression
Process called "spark_lit" that takes value as Any returns SparkColumn:
    Throw Errors.NotImplemented with "Spark literal expression not implemented"

Note: Get Spark version
Process called "spark_version" that takes spark as SparkSession returns String:
    Throw Errors.NotImplemented with "Spark version not implemented"

Note: Get application ID
Process called "spark_application_id" that takes spark as SparkSession returns String:
    Throw Errors.NotImplemented with "Spark application ID not implemented"