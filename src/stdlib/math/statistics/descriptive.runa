Note:
math/statistics/descriptive.runa
Descriptive Statistics Operations

This module provides comprehensive descriptive statistics capabilities including
measures of central tendency (mean, median, mode), measures of variability 
(standard deviation, variance, range), distribution shape measures (skewness, 
kurtosis), and quantile analysis for statistical data summarization.
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "algorithms/sorting/core" as Sorting
Import module "math/core/operations" as MathOps
Import module "math/core/constants" as Constants
Import module "math/core/conversion" as Conversions

Note: =====================================================================
Note: DESCRIPTIVE STATISTICS DATA STRUCTURES
Note: =====================================================================

Type called "StatisticalSummary":
    sample_size as Integer
    mean as Float
    median as Float
    mode as List[Float]
    standard_deviation as Float
    variance as Float
    minimum as Float
    maximum as Float
    range as Float
    interquartile_range as Float
    skewness as Float
    kurtosis as Float

Type called "QuantileConfiguration":
    percentile_levels as List[Float]
    interpolation_method as String
    sample_quantiles as Dictionary[String, Float]
    confidence_intervals as Dictionary[String, List[Float]]
    robust_estimates as Boolean

Type called "MomentsAnalysis":
    raw_moments as List[Float]
    central_moments as List[Float]
    standardized_moments as List[Float]
    moment_generating_function as Dictionary[String, Float]
    cumulants as List[Float]
    moment_order as Integer

Type called "DistributionShape":
    skewness_coefficient as Float
    kurtosis_coefficient as Float
    excess_kurtosis as Float
    shape_classification as String
    normality_indicators as Dictionary[String, Float]
    outlier_detection as List[Integer]

Note: =====================================================================
Note: CENTRAL TENDENCY OPERATIONS
Note: =====================================================================

Process called "calculate_arithmetic_mean" that takes data as List[Float], weights as List[Float] returns Float:
    Note: Calculate arithmetic mean with optional weights. Formula: Σ(xi multiplied by wi) / Σ(wi)
    Note: Assumes equal weights if not specified. Handles missing values and edge cases
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate mean of empty dataset"
    
    If weights.size() is equal to 0:
        Note: Use equal weights when no weights provided
        Let sum be 0.0
        For each value in data:
            Set sum to sum plus value
        Return sum / Float(data.size())
    
    If weights.size() does not equal data.size():
        Throw Errors.InvalidArgument with "Weights and data must have same size"
    
    Let weighted_sum be 0.0
    Let weight_sum be 0.0
    
    For i from 0 to data.size() minus 1:
        If weights[i] is less than 0.0:
            Throw Errors.InvalidArgument with "Weights must be non-negative"
        Set weighted_sum to weighted_sum plus (data[i] multiplied by weights[i])
        Set weight_sum to weight_sum plus weights[i]
    
    If weight_sum is equal to 0.0:
        Throw Errors.InvalidArgument with "Sum of weights cannot be zero"
    
    Return weighted_sum / weight_sum

Process called "calculate_geometric_mean" that takes data as List[Float], handle_negative as Boolean returns Float:
    Note: Calculate geometric mean using nth root of product. Formula: (Π xi)^(1/n)
    Note: Requires positive values unless handle_negative flag transforms data
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate geometric mean of empty dataset"
    
    Let log_sum be 0.0
    Let n be Float(data.size())
    
    For each value in data:
        If value is equal to 0.0:
            Return 0.0
        Otherwise if value is less than 0.0:
            If handle_negative:
                Note: Use absolute value and track sign changes
                Let abs_val be MathOps.absolute_value(Float.to_string(value))
                Let log_result be MathOps.natural_logarithm(abs_val.result_value, 50)
                Set log_sum to log_sum plus Float(log_result.result_value)
            Otherwise:
                Throw Errors.InvalidArgument with "Geometric mean undefined for negative values"
        Otherwise:
            Let log_result be MathOps.natural_logarithm(Float.to_string(value), 50)
            Set log_sum to log_sum plus Float(log_result.result_value)
    
    Let mean_log be log_sum / n
    Let exp_result be MathOps.exponential(Float.to_string(mean_log), 50)
    Return Float(exp_result.result_value)

Process called "calculate_harmonic_mean" that takes data as List[Float], exclude_zero as Boolean returns Float:
    Note: Calculate harmonic mean as reciprocal of arithmetic mean of reciprocals
    Note: Formula: n / Σ(1/xi). Undefined for zero values unless excluded
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate harmonic mean of empty dataset"
    
    Let reciprocal_sum be 0.0
    Let valid_count be 0
    
    For each value in data:
        If value is equal to 0.0:
            If exclude_zero:
                Note: Skip zero values
                Continue
            Otherwise:
                Throw Errors.InvalidArgument with "Harmonic mean undefined for zero values"
        Otherwise:
            Set reciprocal_sum to reciprocal_sum plus (1.0 / value)
            Set valid_count to valid_count plus 1
    
    If valid_count is equal to 0:
        Throw Errors.InvalidArgument with "No valid non-zero values for harmonic mean"
    
    Return Float(valid_count) / reciprocal_sum

Process called "find_median" that takes data as List[Float], interpolation_method as String returns Float:
    Note: Find median value using specified interpolation method for even-sized samples
    Note: Methods: linear, lower, higher, midpoint, nearest. Handles sorted/unsorted data
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot find median of empty dataset"
    
    Note: Convert Float list to String list for sorting
    Let string_data be List[String]
    For each value in data:
        Call string_data.append(Float.to_string(value))
    
    Note: Sort the data
    Let sorted_result be Sorting.quicksort(string_data, "numeric")
    Let sorted_data be List[Float]
    For each str_value in sorted_result.sorted_array:
        Call sorted_data.append(Float(str_value))
    
    Let n be data.size()
    
    If n % 2 is equal to 1:
        Note: Odd number of elements minus return middle element
        Let middle_index be n / 2
        Return sorted_data[middle_index]
    Otherwise:
        Note: Even number of elements minus interpolate between middle two
        Let lower_index be (n / 2) minus 1
        Let upper_index be n / 2
        Let lower_value be sorted_data[lower_index]
        Let upper_value be sorted_data[upper_index]
        
        If interpolation_method is equal to "linear" or interpolation_method is equal to "midpoint":
            Return (lower_value plus upper_value) / 2.0
        Otherwise if interpolation_method is equal to "lower":
            Return lower_value
        Otherwise if interpolation_method is equal to "higher":
            Return upper_value
        Otherwise if interpolation_method is equal to "nearest":
            Note: Return value closest to arithmetic mean
            Let mean_val be (lower_value plus upper_value) / 2.0
            If MathOps.absolute_value(Float.to_string(lower_value minus mean_val)).result_value is less than MathOps.absolute_value(Float.to_string(upper_value minus mean_val)).result_value:
                Return lower_value
            Otherwise:
                Return upper_value
        Otherwise:
            Note: Default to linear interpolation
            Return (lower_value plus upper_value) / 2.0

Process called "find_mode" that takes data as List[Float], tolerance as Float returns List[Float]:
    Note: Find mode(s) minus most frequently occurring values with specified tolerance
    Note: Returns multimodal results. Handles continuous data through binning
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot find mode of empty dataset"
    
    Let frequency_map be Dictionary[String, Integer]
    Let max_frequency be 0
    
    For each value in data:
        Note: Round values within tolerance to create bins
        Let rounded_value be Float.round(value / tolerance) multiplied by tolerance
        Let key be Float.to_string(rounded_value)
        
        If frequency_map.contains_key(key):
            Set frequency_map[key] to frequency_map[key] plus 1
        Otherwise:
            Set frequency_map[key] to 1
        
        If frequency_map[key] is greater than max_frequency:
            Set max_frequency to frequency_map[key]
    
    Let modes be List[Float]
    For each key in frequency_map.keys():
        If frequency_map[key] is equal to max_frequency:
            Call modes.append(Float(key))
    
    Note: Sort modes for consistent output
    Let string_modes be List[String]
    For each mode_val in modes:
        Call string_modes.append(Float.to_string(mode_val))
    
    Let sorted_result be Sorting.quicksort(string_modes, "numeric")
    Let sorted_modes be List[Float]
    For each str_mode in sorted_result.sorted_array:
        Call sorted_modes.append(Float(str_mode))
    
    Return sorted_modes

Note: =====================================================================
Note: VARIABILITY MEASURES OPERATIONS
Note: =====================================================================

Process called "calculate_variance" that takes data as List[Float], population as Boolean, bias_correction as Boolean returns Float:
    Note: Calculate sample or population variance. Formula: Σ(xi minus μ)² / (n-δ)
    Note: Uses Bessel's correction (δ=1) for sample variance when bias_correction=true
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate variance of empty dataset"
    
    If data.size() is equal to 1 and not population:
        Throw Errors.InvalidArgument with "Cannot calculate sample variance with only one data point"
    
    Note: Calculate mean first
    Let empty_weights be List[Float]
    Let mean_value be calculate_arithmetic_mean(data, empty_weights)
    
    Let sum_squared_deviations be 0.0
    For each value in data:
        Let deviation be value minus mean_value
        Set sum_squared_deviations to sum_squared_deviations plus (deviation multiplied by deviation)
    
    Let n be Float(data.size())
    Let divisor be n
    
    If not population:
        Note: Sample variance calculation
        If bias_correction:
            Set divisor to n minus 1.0
        Otherwise:
            Set divisor to n
    
    If divisor is equal to 0.0:
        Throw Errors.InvalidArgument with "Division by zero in variance calculation"
    
    Return sum_squared_deviations / divisor

Process called "calculate_standard_deviation" that takes data as List[Float], population as Boolean returns Float:
    Note: Calculate standard deviation as square root of variance
    Note: Provides measure of spread around mean in original units
    
    Let variance be calculate_variance(data, population, true)
    
    If variance is less than 0.0:
        Throw Errors.InvalidArgument with "Variance cannot be negative"
    
    Let sqrt_result be MathOps.square_root(Float.to_string(variance), 50)
    Return Float(sqrt_result.result_value)

Process called "calculate_range" that takes data as List[Float] returns Dictionary[String, Float]:
    Note: Calculate range, interquartile range, and range-based statistics
    Note: Returns min, max, range, Q1, Q3, IQR, and outlier boundaries
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate range of empty dataset"
    
    Let result be Dictionary[String, Float]
    
    Note: Find minimum and maximum values
    Let min_value be data[0]
    Let max_value be data[0]
    
    For each value in data:
        If value is less than min_value:
            Set min_value to value
        If value is greater than max_value:
            Set max_value to value
    
    Set result["minimum"] to min_value
    Set result["maximum"] to max_value
    Set result["range"] to max_value minus min_value
    
    Note: Calculate quartiles if dataset is large enough
    If data.size() is greater than or equal to 4:
        Let quartiles be calculate_quartiles(data)
        Set result["Q1"] to quartiles["Q1"]
        Set result["Q3"] to quartiles["Q3"]
        Set result["IQR"] to quartiles["IQR"]
        
        Note: Calculate outlier boundaries using 1.5 multiplied by IQR rule
        Let iqr be quartiles["IQR"]
        Set result["lower_outlier_boundary"] to quartiles["Q1"] minus (1.5 multiplied by iqr)
        Set result["upper_outlier_boundary"] to quartiles["Q3"] plus (1.5 multiplied by iqr)
    Otherwise:
        Set result["Q1"] to min_value
        Set result["Q3"] to max_value
        Set result["IQR"] to max_value minus min_value
        Set result["lower_outlier_boundary"] to min_value
        Set result["upper_outlier_boundary"] to max_value
    
    Return result

Process called "calculate_coefficient_of_variation" that takes data as List[Float] returns Float:
    Note: Calculate coefficient of variation as ratio of standard deviation to mean
    Note: Provides relative measure of variability. Formula: σ/μ multiplied by 100%
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate coefficient of variation of empty dataset"
    
    Let empty_weights be List[Float]
    Let mean_value be calculate_arithmetic_mean(data, empty_weights)
    
    If mean_value is equal to 0.0:
        Throw Errors.InvalidArgument with "Coefficient of variation undefined when mean is zero"
    
    Let std_dev be calculate_standard_deviation(data, false)
    
    Return (std_dev / MathOps.absolute_value(Float.to_string(mean_value)).result_value) multiplied by 100.0

Process called "calculate_mean_absolute_deviation" that takes data as List[Float], center as String returns Float:
    Note: Calculate mean absolute deviation from specified center (mean, median, mode)
    Note: Robust measure of variability. Formula: Σ|xi minus center| / n
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate mean absolute deviation of empty dataset"
    
    Let center_value be 0.0
    
    If center is equal to "mean":
        Let empty_weights be List[Float]
        Set center_value to calculate_arithmetic_mean(data, empty_weights)
    Otherwise if center is equal to "median":
        Set center_value to find_median(data, "linear")
    Otherwise if center is equal to "mode":
        Let modes be find_mode(data, 0.0001)
        If modes.size() is greater than 0:
            Set center_value to modes[0]
        Otherwise:
            Let empty_weights be List[Float]
            Set center_value to calculate_arithmetic_mean(data, empty_weights)
    Otherwise:
        Throw Errors.InvalidArgument with "Center must be 'mean', 'median', or 'mode'"
    
    Let sum_absolute_deviations be 0.0
    For each value in data:
        Let deviation be value minus center_value
        Let abs_deviation be Float(MathOps.absolute_value(Float.to_string(deviation)).result_value)
        Set sum_absolute_deviations to sum_absolute_deviations plus abs_deviation
    
    Return sum_absolute_deviations / Float(data.size())

Note: =====================================================================
Note: QUANTILE ANALYSIS OPERATIONS
Note: =====================================================================

Process called "calculate_percentiles" that takes data as List[Float], percentiles as List[Float], method as String returns Dictionary[String, Float]:
    Note: Calculate specified percentiles using chosen interpolation method
    Note: Methods: linear, nearest, lower, higher, midpoint. Returns percentile values
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate percentiles of empty dataset"
    
    If percentiles.size() is equal to 0:
        Return Dictionary[String, Float]
    
    Note: Sort the data first
    Let string_data be List[String]
    For each value in data:
        Call string_data.append(Float.to_string(value))
    
    Let sorted_result be Sorting.quicksort(string_data, "numeric")
    Let sorted_data be List[Float]
    For each str_value in sorted_result.sorted_array:
        Call sorted_data.append(Float(str_value))
    
    Let result be Dictionary[String, Float]
    Let n be Float(data.size())
    
    For each p in percentiles:
        If p is less than 0.0 or p is greater than 100.0:
            Throw Errors.InvalidArgument with "Percentiles must be between 0 and 100"
        
        Note: Calculate position using (n-1) multiplied by p/100 formula
        Let position be (n minus 1.0) multiplied by (p / 100.0)
        Let lower_index be Integer(Float.floor(position))
        Let upper_index be Integer(Float.ceiling(position))
        
        If lower_index is equal to upper_index or lower_index is equal to Integer(n minus 1):
            Note: Exact position or at boundary
            Set result[Float.to_string(p)] to sorted_data[lower_index]
        Otherwise:
            Note: Interpolation needed
            Let lower_value be sorted_data[lower_index]
            Let upper_value be sorted_data[upper_index]
            Let weight be position minus Float(lower_index)
            
            If method is equal to "linear":
                Let interpolated be lower_value plus (weight multiplied by (upper_value minus lower_value))
                Set result[Float.to_string(p)] to interpolated
            Otherwise if method is equal to "nearest":
                If weight is less than 0.5:
                    Set result[Float.to_string(p)] to lower_value
                Otherwise:
                    Set result[Float.to_string(p)] to upper_value
            Otherwise if method is equal to "lower":
                Set result[Float.to_string(p)] to lower_value
            Otherwise if method is equal to "higher":
                Set result[Float.to_string(p)] to upper_value
            Otherwise if method is equal to "midpoint":
                Set result[Float.to_string(p)] to (lower_value plus upper_value) / 2.0
            Otherwise:
                Note: Default to linear
                Let interpolated be lower_value plus (weight multiplied by (upper_value minus lower_value))
                Set result[Float.to_string(p)] to interpolated
    
    Return result

Process called "calculate_quartiles" that takes data as List[Float] returns Dictionary[String, Float]:
    Note: Calculate first, second (median), and third quartiles with IQR
    Note: Returns Q1, Q2, Q3, IQR, and outlier detection boundaries
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate quartiles of empty dataset"
    
    Let quartile_percentiles be [25.0, 50.0, 75.0]
    Let percentile_results be calculate_percentiles(data, quartile_percentiles, "linear")
    
    Let result be Dictionary[String, Float]
    Set result["Q1"] to percentile_results["25.0"]
    Set result["Q2"] to percentile_results["50.0"]
    Set result["Q3"] to percentile_results["75.0"]
    
    Let iqr be percentile_results["75.0"] minus percentile_results["25.0"]
    Set result["IQR"] to iqr
    
    Note: Calculate outlier boundaries using 1.5 multiplied by IQR rule
    Set result["lower_outlier_boundary"] to percentile_results["25.0"] minus (1.5 multiplied by iqr)
    Set result["upper_outlier_boundary"] to percentile_results["75.0"] plus (1.5 multiplied by iqr)
    
    Return result

Process called "calculate_quantile_function" that takes data as List[Float], probability_levels as List[Float] returns Dictionary[String, Float]:
    Note: Calculate inverse cumulative distribution (quantile function)
    Note: Maps probabilities to data values. Useful for confidence intervals
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate quantile function of empty dataset"
    
    Note: Convert probability levels to percentiles
    Let percentiles be List[Float]
    For each prob in probability_levels:
        If prob is less than 0.0 or prob is greater than 1.0:
            Throw Errors.InvalidArgument with "Probability levels must be between 0 and 1"
        Call percentiles.append(prob multiplied by 100.0)
    
    Let percentile_results be calculate_percentiles(data, percentiles, "linear")
    
    Note: Convert results back to probability scale
    Let result be Dictionary[String, Float]
    For each prob in probability_levels:
        Let percentile_key be Float.to_string(prob multiplied by 100.0)
        Let prob_key be Float.to_string(prob)
        Set result[prob_key] to percentile_results[percentile_key]
    
    Return result

Process called "detect_outliers_iqr" that takes data as List[Float], multiplier as Float returns List[Integer]:
    Note: Detect outliers using IQR method with specified multiplier (typically 1.5)
    Note: Outliers: values is less than Q1 minus k*IQR or is greater than Q3 plus k*IQR. Returns indices
    
    If data.size() is equal to 0:
        Return List[Integer]
    
    Let quartiles be calculate_quartiles(data)
    Let q1 be quartiles["Q1"]
    Let q3 be quartiles["Q3"]
    Let iqr be quartiles["IQR"]
    
    Let lower_bound be q1 minus (multiplier multiplied by iqr)
    Let upper_bound be q3 plus (multiplier multiplied by iqr)
    
    Let outlier_indices be List[Integer]
    
    For i from 0 to data.size() minus 1:
        If data[i] is less than lower_bound or data[i] is greater than upper_bound:
            Call outlier_indices.append(i)
    
    Return outlier_indices

Note: =====================================================================
Note: DISTRIBUTION SHAPE OPERATIONS
Note: =====================================================================

Process called "calculate_skewness" that takes data as List[Float], method as String returns Float:
    Note: Calculate skewness coefficient using specified method (Fisher, Pearson)
    Note: Measures asymmetry of distribution. Formula: E[(X-μ)³]/σ³
    
    If data.size() is less than 3:
        Throw Errors.InvalidArgument with "Skewness requires at least 3 data points"
    
    Let empty_weights be List[Float]
    Let mean_value be calculate_arithmetic_mean(data, empty_weights)
    Let std_dev be calculate_standard_deviation(data, false)
    
    If std_dev is equal to 0.0:
        Throw Errors.InvalidArgument with "Skewness undefined when standard deviation is zero"
    
    Let sum_cubed_deviations be 0.0
    Let n be Float(data.size())
    
    For each value in data:
        Let deviation be value minus mean_value
        Let standardized_deviation be deviation / std_dev
        Let cubed_deviation be standardized_deviation multiplied by standardized_deviation multiplied by standardized_deviation
        Set sum_cubed_deviations to sum_cubed_deviations plus cubed_deviation
    
    Let sample_skewness be sum_cubed_deviations / n
    
    If method is equal to "Fisher" or method is equal to "sample":
        Note: Apply bias correction for sample skewness
        If n is greater than 2.0:
            Let bias_correction be (n multiplied by (n minus 1.0)) / ((n minus 1.0) multiplied by (n minus 2.0))
            Return sample_skewness multiplied by bias_correction
        Otherwise:
            Return sample_skewness
    Otherwise if method is equal to "Pearson" or method is equal to "population":
        Return sample_skewness
    Otherwise:
        Note: Default to Fisher method
        If n is greater than 2.0:
            Let bias_correction be (n multiplied by (n minus 1.0)) / ((n minus 1.0) multiplied by (n minus 2.0))
            Return sample_skewness multiplied by bias_correction
        Otherwise:
            Return sample_skewness

Process called "calculate_kurtosis" that takes data as List[Float], excess as Boolean returns Float:
    Note: Calculate kurtosis coefficient, optionally as excess kurtosis
    Note: Measures tail heaviness. Formula: E[(X-μ)⁴]/σ⁴. Excess is equal to kurtosis minus 3
    
    If data.size() is less than 4:
        Throw Errors.InvalidArgument with "Kurtosis requires at least 4 data points"
    
    Let empty_weights be List[Float]
    Let mean_value be calculate_arithmetic_mean(data, empty_weights)
    Let std_dev be calculate_standard_deviation(data, false)
    
    If std_dev is equal to 0.0:
        Throw Errors.InvalidArgument with "Kurtosis undefined when standard deviation is zero"
    
    Let sum_fourth_deviations be 0.0
    Let n be Float(data.size())
    
    For each value in data:
        Let deviation be value minus mean_value
        Let standardized_deviation be deviation / std_dev
        Let fourth_deviation be standardized_deviation multiplied by standardized_deviation multiplied by standardized_deviation multiplied by standardized_deviation
        Set sum_fourth_deviations to sum_fourth_deviations plus fourth_deviation
    
    Let sample_kurtosis be sum_fourth_deviations / n
    
    Note: Apply bias correction for sample kurtosis
    Let bias_corrected_kurtosis be sample_kurtosis
    If n is greater than 3.0:
        Let correction_factor be ((n minus 1.0) multiplied by (n plus 1.0)) / ((n minus 2.0) multiplied by (n minus 3.0))
        Set bias_corrected_kurtosis to sample_kurtosis multiplied by correction_factor
    
    If excess:
        Return bias_corrected_kurtosis minus 3.0
    Otherwise:
        Return bias_corrected_kurtosis

Process called "assess_normality" that takes data as List[Float], alpha as Float returns Dictionary[String, Float]:
    Note: Assess normality using multiple tests (Shapiro-Wilk, Anderson-Darling, etc.)
    Note: Returns test statistics, p-values, and normality assessment at given alpha
    
    If data.size() is less than 3:
        Throw Errors.InvalidArgument with "Normality assessment requires at least 3 data points"
    
    Let result be Dictionary[String, Float]
    
    Note: Basic normality indicators using skewness and kurtosis
    Let skewness be calculate_skewness(data, "Fisher")
    Let excess_kurtosis be calculate_kurtosis(data, true)
    
    Set result["skewness"] to skewness
    Set result["excess_kurtosis"] to excess_kurtosis
    
    Note: Simple normality score based on skewness and kurtosis proximity to normal distribution
    Let skewness_score be 1.0 minus Float(MathOps.absolute_value(Float.to_string(skewness)).result_value)
    Let kurtosis_score be 1.0 minus Float(MathOps.absolute_value(Float.to_string(excess_kurtosis)).result_value)
    
    If skewness_score is less than 0.0:
        Set skewness_score to 0.0
    If kurtosis_score is less than 0.0:
        Set kurtosis_score to 0.0
    
    Let normality_score be (skewness_score plus kurtosis_score) / 2.0
    Set result["normality_score"] to normality_score
    
    Note: Simple normality test based on alpha threshold
    Let skewness_threshold be 2.0
    Let kurtosis_threshold be 2.0
    
    Let is_normal be true
    If Float(MathOps.absolute_value(Float.to_string(skewness)).result_value) is greater than skewness_threshold:
        Set is_normal to false
    If Float(MathOps.absolute_value(Float.to_string(excess_kurtosis)).result_value) is greater than kurtosis_threshold:
        Set is_normal to false
    
    If is_normal:
        Set result["is_normal"] to 1.0
    Otherwise:
        Set result["is_normal"] to 0.0
    
    Set result["alpha_used"] to alpha
    
    Return result

Process called "calculate_moments" that takes data as List[Float], order as Integer, central as Boolean returns MomentsAnalysis:
    Note: Calculate raw or central moments up to specified order
    Note: Central moments: E[(X-μ)ᵏ]. Raw moments: E[Xᵏ]. Includes standardized moments
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate moments of empty dataset"
    
    If order is less than 0:
        Throw Errors.InvalidArgument with "Moment order must be non-negative"
    
    Let result be MomentsAnalysis
    Set result.moment_order to order
    Set result.raw_moments to List[Float]
    Set result.central_moments to List[Float]
    Set result.standardized_moments to List[Float]
    Set result.cumulants to List[Float]
    Set result.moment_generating_function to Dictionary[String, Float]
    
    Let empty_weights be List[Float]
    Let mean_value be calculate_arithmetic_mean(data, empty_weights)
    Let n be Float(data.size())
    
    For k from 0 to order:
        Note: Calculate raw moment
        Let raw_sum be 0.0
        For each value in data:
            Let power_result be MathOps.power(Float.to_string(value), Integer.to_string(k), 50)
            Set raw_sum to raw_sum plus Float(power_result.result_value)
        Let raw_moment be raw_sum / n
        Call result.raw_moments.append(raw_moment)
        
        Note: Calculate central moment
        Let central_sum be 0.0
        If central and k is greater than 0:
            For each value in data:
                Let deviation be value minus mean_value
                Let power_result be MathOps.power(Float.to_string(deviation), Integer.to_string(k), 50)
                Set central_sum to central_sum plus Float(power_result.result_value)
            Let central_moment be central_sum / n
            Call result.central_moments.append(central_moment)
        Otherwise if k is equal to 0:
            Call result.central_moments.append(1.0)
        Otherwise if k is equal to 1:
            Call result.central_moments.append(0.0)
        Otherwise:
            Call result.central_moments.append(raw_moment)
    
    Note: Calculate standardized moments (requires std dev for k is greater than 2)
    If order is greater than or equal to 2:
        Let std_dev be calculate_standard_deviation(data, false)
        
        For k from 0 to order:
            If k is less than or equal to 1:
                If k is equal to 0:
                    Call result.standardized_moments.append(1.0)
                Otherwise:
                    Call result.standardized_moments.append(0.0)
            Otherwise:
                If std_dev is greater than 0.0:
                    Let std_power_result be MathOps.power(Float.to_string(std_dev), Integer.to_string(k), 50)
                    Let standardized_moment be result.central_moments[k] / Float(std_power_result.result_value)
                    Call result.standardized_moments.append(standardized_moment)
                Otherwise:
                    Call result.standardized_moments.append(0.0)
    Otherwise:
        For k from 0 to order:
            Call result.standardized_moments.append(result.central_moments[k])
    
    Note: Simple cumulants calculation (first few cumulants)
    For k from 0 to order:
        If k is equal to 0:
            Call result.cumulants.append(0.0)
        Otherwise if k is equal to 1:
            Call result.cumulants.append(mean_value)
        Otherwise if k is equal to 2:
            Let variance be calculate_variance(data, false, true)
            Call result.cumulants.append(variance)
        Otherwise:
            Note: Higher order cumulants approximate to central moments for simplicity
            Call result.cumulants.append(result.central_moments[k])
    
    Return result

Note: =====================================================================
Note: ROBUST STATISTICS OPERATIONS
Note: =====================================================================

Process called "calculate_trimmed_mean" that takes data as List[Float], trim_percentage as Float returns Float:
    Note: Calculate trimmed mean by removing specified percentage from each tail
    Note: Robust estimator less sensitive to outliers. Typically 5-25% trimming
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate trimmed mean of empty dataset"
    
    If trim_percentage is less than 0.0 or trim_percentage is greater than or equal to 50.0:
        Throw Errors.InvalidArgument with "Trim percentage must be between 0 and 50"
    
    Note: Sort the data first
    Let string_data be List[String]
    For each value in data:
        Call string_data.append(Float.to_string(value))
    
    Let sorted_result be Sorting.quicksort(string_data, "numeric")
    Let sorted_data be List[Float]
    For each str_value in sorted_result.sorted_array:
        Call sorted_data.append(Float(str_value))
    
    Let n be data.size()
    Let trim_count be Integer(Float(n) multiplied by trim_percentage / 100.0)
    
    If trim_count multiplied by 2 is greater than or equal to n:
        Note: Too much trimming, return median
        Return find_median(data, "linear")
    
    Let start_index be trim_count
    Let end_index be n minus trim_count minus 1
    
    Let sum be 0.0
    Let count be 0
    
    For i from start_index to end_index:
        Set sum to sum plus sorted_data[i]
        Set count to count plus 1
    
    If count is equal to 0:
        Throw Errors.InvalidArgument with "No data remaining after trimming"
    
    Return sum / Float(count)

Process called "calculate_winsorized_mean" that takes data as List[Float], limits as List[Float] returns Float:
    Note: Calculate winsorized mean by replacing extreme values with percentile limits
    Note: More conservative than trimmed mean. Limits specify lower/upper percentiles
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate winsorized mean of empty dataset"
    
    If limits.size() does not equal 2:
        Throw Errors.InvalidArgument with "Limits must contain exactly two values: [lower_percentile, upper_percentile]"
    
    Let lower_percentile be limits[0]
    Let upper_percentile be limits[1]
    
    If lower_percentile is less than 0.0 or lower_percentile is greater than 100.0:
        Throw Errors.InvalidArgument with "Lower percentile must be between 0 and 100"
    
    If upper_percentile is less than 0.0 or upper_percentile is greater than 100.0:
        Throw Errors.InvalidArgument with "Upper percentile must be between 0 and 100"
    
    If lower_percentile is greater than or equal to upper_percentile:
        Throw Errors.InvalidArgument with "Lower percentile must be less than upper percentile"
    
    Note: Calculate percentile values
    Let percentiles be [lower_percentile, upper_percentile]
    Let percentile_results be calculate_percentiles(data, percentiles, "linear")
    Let lower_limit be percentile_results[Float.to_string(lower_percentile)]
    Let upper_limit be percentile_results[Float.to_string(upper_percentile)]
    
    Note: Winsorize the data
    Let winsorized_data be List[Float]
    For each value in data:
        If value is less than lower_limit:
            Call winsorized_data.append(lower_limit)
        Otherwise if value is greater than upper_limit:
            Call winsorized_data.append(upper_limit)
        Otherwise:
            Call winsorized_data.append(value)
    
    Note: Calculate mean of winsorized data
    Let empty_weights be List[Float]
    Return calculate_arithmetic_mean(winsorized_data, empty_weights)

Process called "calculate_robust_scale" that takes data as List[Float], estimator as String returns Float:
    Note: Calculate robust scale estimator (MAD, IQR, Rousseeuw-Croux)
    Note: Resistant to outliers. MAD is equal to median(|xi minus median(x)|) multiplied by 1.4826
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate robust scale of empty dataset"
    
    If estimator is equal to "MAD" or estimator is equal to "mad":
        Let median_value be find_median(data, "linear")
        Let absolute_deviations be List[Float]
        
        For each value in data:
            Let deviation be value minus median_value
            Let abs_deviation be Float(MathOps.absolute_value(Float.to_string(deviation)).result_value)
            Call absolute_deviations.append(abs_deviation)
        
        Let mad_value be find_median(absolute_deviations, "linear")
        Return mad_value multiplied by 1.4826
    
    Otherwise if estimator is equal to "IQR" or estimator is equal to "iqr":
        Let quartiles be calculate_quartiles(data)
        Return quartiles["IQR"]
    
    Otherwise if estimator is equal to "Rousseeuw-Croux" or estimator is equal to "RC":
        Note: Simplified implementation of Rousseeuw-Croux estimator
        Let pairwise_diffs be List[Float]
        
        For i from 0 to data.size() minus 1:
            For j from i plus 1 to data.size() minus 1:
                Let diff be Float(MathOps.absolute_value(Float.to_string(data[i] minus data[j])).result_value)
                Call pairwise_diffs.append(diff)
        
        If pairwise_diffs.size() is greater than 0:
            Return find_median(pairwise_diffs, "linear") multiplied by 2.2219
        Otherwise:
            Return 0.0
    
    Otherwise:
        Note: Default to MAD
        Let median_value be find_median(data, "linear")
        Let absolute_deviations be List[Float]
        
        For each value in data:
            Let deviation be value minus median_value
            Let abs_deviation be Float(MathOps.absolute_value(Float.to_string(deviation)).result_value)
            Call absolute_deviations.append(abs_deviation)
        
        Let mad_value be find_median(absolute_deviations, "linear")
        Return mad_value multiplied by 1.4826

Process called "calculate_hodges_lehmann_estimator" that takes data as List[Float] returns Float:
    Note: Calculate Hodges-Lehmann location estimator as median of pairwise averages
    Note: Robust alternative to mean. Formula: median((xi plus xj)/2) for all i ≤ j
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate Hodges-Lehmann estimator of empty dataset"
    
    If data.size() is equal to 1:
        Return data[0]
    
    Let pairwise_averages be List[Float]
    
    For i from 0 to data.size() minus 1:
        For j from i to data.size() minus 1:
            Let average be (data[i] plus data[j]) / 2.0
            Call pairwise_averages.append(average)
    
    Return find_median(pairwise_averages, "linear")

Note: =====================================================================
Note: COMPREHENSIVE SUMMARY OPERATIONS
Note: =====================================================================

Process called "generate_descriptive_summary" that takes data as List[Float], include_robust as Boolean returns StatisticalSummary:
    Note: Generate comprehensive descriptive statistics summary
    Note: Includes all central tendency, variability, and shape measures
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot generate summary of empty dataset"
    
    Let summary be StatisticalSummary
    Let empty_weights be List[Float]
    
    Set summary.sample_size to data.size()
    Set summary.mean to calculate_arithmetic_mean(data, empty_weights)
    Set summary.median to find_median(data, "linear")
    
    Let modes be find_mode(data, 0.0001)
    Set summary.mode to modes
    
    Set summary.variance to calculate_variance(data, false, true)
    Set summary.standard_deviation to calculate_standard_deviation(data, false)
    
    Let range_stats be calculate_range(data)
    Set summary.minimum to range_stats["minimum"]
    Set summary.maximum to range_stats["maximum"]
    Set summary.range to range_stats["range"]
    Set summary.interquartile_range to range_stats["IQR"]
    
    If data.size() is greater than or equal to 3:
        Set summary.skewness to calculate_skewness(data, "Fisher")
    Otherwise:
        Set summary.skewness to 0.0
    
    If data.size() is greater than or equal to 4:
        Set summary.kurtosis to calculate_kurtosis(data, true)
    Otherwise:
        Set summary.kurtosis to 0.0
    
    Return summary

Process called "create_five_number_summary" that takes data as List[Float] returns Dictionary[String, Float]:
    Note: Create five-number summary: minimum, Q1, median, Q3, maximum
    Note: Foundation for box plot visualization. Includes outlier information
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot create five-number summary of empty dataset"
    
    Let result be Dictionary[String, Float]
    
    Let range_stats be calculate_range(data)
    Set result["minimum"] to range_stats["minimum"]
    Set result["maximum"] to range_stats["maximum"]
    
    Let quartiles be calculate_quartiles(data)
    Set result["Q1"] to quartiles["Q1"]
    Set result["median"] to quartiles["Q2"]
    Set result["Q3"] to quartiles["Q3"]
    
    Note: Additional box plot information
    Set result["IQR"] to quartiles["IQR"]
    Set result["lower_fence"] to quartiles["lower_outlier_boundary"]
    Set result["upper_fence"] to quartiles["upper_outlier_boundary"]
    
    Note: Count outliers
    Let outlier_indices be detect_outliers_iqr(data, 1.5)
    Set result["outlier_count"] to Float(outlier_indices.size())
    
    Return result

Process called "calculate_frequency_distribution" that takes data as List[Float], bins as Integer, method as String returns Dictionary[String, List[Float]]:
    Note: Calculate frequency distribution with specified binning method
    Note: Methods: equal-width, equal-frequency, Sturges, Scott, Freedman-Diaconis
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate frequency distribution of empty dataset"
    
    If bins is less than or equal to 0:
        Throw Errors.InvalidArgument with "Number of bins must be positive"
    
    Let result be Dictionary[String, List[Float]]
    Let range_stats be calculate_range(data)
    Let min_val be range_stats["minimum"]
    Let max_val be range_stats["maximum"]
    
    Let effective_bins be bins
    
    If method is equal to "Sturges":
        Let n be Float(data.size())
        Let log_result be MathOps.binary_logarithm(Float.to_string(n), 50)
        Set effective_bins to Integer(1.0 plus Float(log_result.result_value))
    Otherwise if method is equal to "Scott":
        Let std_dev be calculate_standard_deviation(data, false)
        Let n be Float(data.size())
        Let cube_root_n be MathOps.power(Float.to_string(n), "0.33333333", 50)
        Let bin_width be (3.5 multiplied by std_dev) / Float(cube_root_n.result_value)
        If bin_width is greater than 0.0:
            Set effective_bins to Integer((max_val minus min_val) / bin_width) plus 1
        Otherwise:
            Set effective_bins to bins
    Otherwise if method is equal to "Freedman-Diaconis":
        Let quartiles be calculate_quartiles(data)
        Let iqr be quartiles["IQR"]
        Let n be Float(data.size())
        Let cube_root_n be MathOps.power(Float.to_string(n), "0.33333333", 50)
        Let bin_width be (2.0 multiplied by iqr) / Float(cube_root_n.result_value)
        If bin_width is greater than 0.0:
            Set effective_bins to Integer((max_val minus min_val) / bin_width) plus 1
        Otherwise:
            Set effective_bins to bins
    
    If effective_bins is less than or equal to 0:
        Set effective_bins to 1
    
    Let bin_edges be List[Float]
    Let bin_counts be List[Float]
    Let bin_centers be List[Float]
    
    If method is equal to "equal-width" or method is equal to "Sturges" or method is equal to "Scott" or method is equal to "Freedman-Diaconis":
        Let bin_width be (max_val minus min_val) / Float(effective_bins)
        
        For i from 0 to effective_bins:
            Let edge be min_val plus (Float(i) multiplied by bin_width)
            Call bin_edges.append(edge)
        
        For i from 0 to effective_bins minus 1:
            Call bin_counts.append(0.0)
            Let center be (bin_edges[i] plus bin_edges[i plus 1]) / 2.0
            Call bin_centers.append(center)
        
        For each value in data:
            Let bin_index be Integer((value minus min_val) / bin_width)
            If bin_index is greater than or equal to effective_bins:
                Set bin_index to effective_bins minus 1
            If bin_index is less than 0:
                Set bin_index to 0
            Set bin_counts[bin_index] to bin_counts[bin_index] plus 1.0
    
    Otherwise if method is equal to "equal-frequency":
        Note: Sort data and create equal-frequency bins
        Let string_data be List[String]
        For each value in data:
            Call string_data.append(Float.to_string(value))
        
        Let sorted_result be Sorting.quicksort(string_data, "numeric")
        Let sorted_data be List[Float]
        For each str_value in sorted_result.sorted_array:
            Call sorted_data.append(Float(str_value))
        
        Let items_per_bin be data.size() / effective_bins
        
        For i from 0 to effective_bins minus 1:
            Let start_idx be i multiplied by items_per_bin
            Let end_idx be (i plus 1) multiplied by items_per_bin minus 1
            If i is equal to effective_bins minus 1:
                Set end_idx to data.size() minus 1
            
            If start_idx is less than sorted_data.size():
                Let edge be sorted_data[start_idx]
                Call bin_edges.append(edge)
                
                Let count be Float(end_idx minus start_idx plus 1)
                Call bin_counts.append(count)
                
                Let center be (sorted_data[start_idx] plus sorted_data[end_idx]) / 2.0
                Call bin_centers.append(center)
        
        Note: Add final edge
        Call bin_edges.append(sorted_data[sorted_data.size() minus 1])
    
    Set result["bin_edges"] to bin_edges
    Set result["bin_counts"] to bin_counts
    Set result["bin_centers"] to bin_centers
    
    Return result

Process called "generate_statistical_profile" that takes data as List[Float], profile_config as Dictionary[String, String] returns Dictionary[String, Dictionary[String, Float]]:
    Note: Generate comprehensive statistical profile with customizable components
    Note: Includes descriptive stats, distribution fitting, and diagnostic measures
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot generate profile of empty dataset"
    
    Let result be Dictionary[String, Dictionary[String, Float]]
    
    Note: Basic descriptive statistics
    Let basic_stats be Dictionary[String, Float]
    Let summary be generate_descriptive_summary(data, true)
    Set basic_stats["mean"] to summary.mean
    Set basic_stats["median"] to summary.median
    Set basic_stats["std_dev"] to summary.standard_deviation
    Set basic_stats["variance"] to summary.variance
    Set basic_stats["skewness"] to summary.skewness
    Set basic_stats["kurtosis"] to summary.kurtosis
    Set result["basic_statistics"] to basic_stats
    
    Note: Range statistics
    let range_stats be calculate_range(data)
    Set result["range_statistics"] to range_stats
    
    Note: Quartile statistics
    Let quartile_stats be calculate_quartiles(data)
    Set result["quartile_statistics"] to quartile_stats
    
    Note: Outlier analysis
    Let outlier_stats be Dictionary[String, Float]
    Let outlier_indices be detect_outliers_iqr(data, 1.5)
    Set outlier_stats["outlier_count"] to Float(outlier_indices.size())
    Set outlier_stats["outlier_percentage"] to (Float(outlier_indices.size()) / Float(data.size())) multiplied by 100.0
    Set result["outlier_analysis"] to outlier_stats
    
    Note: Normality assessment
    If data.size() is greater than or equal to 3:
        Let normality_stats be assess_normality(data, 0.05)
        Set result["normality_assessment"] to normality_stats
    
    Return result

Note: =====================================================================
Note: COMPARATIVE STATISTICS OPERATIONS
Note: =====================================================================

Process called "compare_distributions" that takes data1 as List[Float], data2 as List[Float] returns Dictionary[String, Float]:
    Note: Compare two distributions using multiple statistical measures
    Note: Returns effect sizes, overlap measures, and distribution comparisons
    
    If data1.size() is equal to 0 or data2.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compare empty datasets"
    
    Let result be Dictionary[String, Float]
    
    Note: Basic descriptive comparison
    Let empty_weights be List[Float]
    Let mean1 be calculate_arithmetic_mean(data1, empty_weights)
    Let mean2 be calculate_arithmetic_mean(data2, empty_weights)
    Let std1 be calculate_standard_deviation(data1, false)
    Let std2 be calculate_standard_deviation(data2, false)
    
    Set result["mean_difference"] to mean1 minus mean2
    Set result["std_dev_ratio"] to std1 / std2
    
    Note: Effect size (Cohen's d)
    Let pooled_std be MathOps.square_root(Float.to_string((std1 multiplied by std1 plus std2 multiplied by std2) / 2.0), 50)
    Let cohens_d be (mean1 minus mean2) / Float(pooled_std.result_value)
    Set result["cohens_d"] to cohens_d
    
    Note: Range comparison
    Let range1 be calculate_range(data1)
    Let range2 be calculate_range(data2)
    Set result["range_ratio"] to range1["range"] / range2["range"]
    
    Note: Median comparison
    Let median1 be find_median(data1, "linear")
    Let median2 be find_median(data2, "linear")
    Set result["median_difference"] to median1 minus median2
    
    Note: IQR comparison
    let quartiles1 be calculate_quartiles(data1)
    let quartiles2 be calculate_quartiles(data2)
    Set result["iqr_ratio"] to quartiles1["IQR"] / quartiles2["IQR"]
    
    Note: Shape comparison
    If data1.size() is greater than or equal to 3 and data2.size() is greater than or equal to 3:
        Let skew1 be calculate_skewness(data1, "Fisher")
        Let skew2 be calculate_skewness(data2, "Fisher")
        Set result["skewness_difference"] to skew1 minus skew2
    
    If data1.size() is greater than or equal to 4 and data2.size() is greater than or equal to 4:
        Let kurt1 be calculate_kurtosis(data1, true)
        Let kurt2 be calculate_kurtosis(data2, true)
        Set result["kurtosis_difference"] to kurt1 minus kurt2
    
    Return result

Process called "calculate_effect_size" that takes group1 as List[Float], group2 as List[Float], method as String returns Float:
    Note: Calculate effect size using specified method (Cohen's d, Glass's delta, etc.)
    Note: Quantifies practical significance of difference between groups
    
    If group1.size() is equal to 0 or group2.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate effect size with empty groups"
    
    Let empty_weights be List[Float]
    Let mean1 be calculate_arithmetic_mean(group1, empty_weights)
    Let mean2 be calculate_arithmetic_mean(group2, empty_weights)
    Let mean_diff be mean1 minus mean2
    
    If method is equal to "cohens_d" or method is equal to "Cohen's d":
        Let std1 be calculate_standard_deviation(group1, false)
        Let std2 be calculate_standard_deviation(group2, false)
        Let pooled_std be MathOps.square_root(Float.to_string((std1 multiplied by std1 plus std2 multiplied by std2) / 2.0), 50)
        Return mean_diff / Float(pooled_std.result_value)
    
    Otherwise if method is equal to "glass_delta" or method is equal to "Glass's delta":
        Let std2 be calculate_standard_deviation(group2, false)
        If std2 is equal to 0.0:
            Throw Errors.InvalidArgument with "Glass's delta undefined when control group standard deviation is zero"
        Return mean_diff / std2
    
    Otherwise if method is equal to "hedges_g" or method is equal to "Hedge's g":
        Let std1 be calculate_standard_deviation(group1, false)
        Let std2 be calculate_standard_deviation(group2, false)
        Let n1 be Float(group1.size())
        Let n2 be Float(group2.size())
        Let pooled_std be MathOps.square_root(Float.to_string(((n1 minus 1.0) multiplied by std1 multiplied by std1 plus (n2 minus 1.0) multiplied by std2 multiplied by std2) / (n1 plus n2 minus 2.0)), 50)
        Let cohens_d be mean_diff / Float(pooled_std.result_value)
        Note: Apply correction factor for Hedge's g
        Let correction_factor be 1.0 minus (3.0 / (4.0 multiplied by (n1 plus n2) minus 9.0))
        Return cohens_d multiplied by correction_factor
    
    Otherwise:
        Note: Default to Cohen's d
        Let std1 be calculate_standard_deviation(group1, false)
        Let std2 be calculate_standard_deviation(group2, false)
        Let pooled_std be MathOps.square_root(Float.to_string((std1 multiplied by std1 plus std2 multiplied by std2) / 2.0), 50)
        Return mean_diff / Float(pooled_std.result_value)

Process called "assess_distribution_overlap" that takes data1 as List[Float], data2 as List[Float] returns Dictionary[String, Float]:
    Note: Assess overlap between two distributions using multiple measures
    Note: Returns overlap coefficient, Bhattacharyya coefficient, and other metrics
    
    If data1.size() is equal to 0 or data2.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot assess overlap of empty datasets"
    
    Let result be Dictionary[String, Float]
    
    Note: Range-based overlap
    Let range1 be calculate_range(data1)
    Let range2 be calculate_range(data2)
    Let min1 be range1["minimum"]
    Let max1 be range1["maximum"]
    Let min2 be range2["minimum"]
    Let max2 be range2["maximum"]
    
    Let overlap_start be Float.max(min1, min2)
    Let overlap_end be Float.min(max1, max2)
    
    If overlap_start is less than or equal to overlap_end:
        Let overlap_length be overlap_end minus overlap_start
        Let total_range be Float.max(max1, max2) minus Float.min(min1, min2)
        Set result["range_overlap_ratio"] to overlap_length / total_range
    Otherwise:
        Set result["range_overlap_ratio"] to 0.0
    
    Note: IQR-based overlap
    Let quartiles1 be calculate_quartiles(data1)
    Let quartiles2 be calculate_quartiles(data2)
    
    Let q1_min be quartiles1["Q1"]
    Let q1_max be quartiles1["Q3"]
    Let q2_min be quartiles2["Q1"]
    Let q2_max be quartiles2["Q3"]
    
    Let iqr_overlap_start be Float.max(q1_min, q2_min)
    Let iqr_overlap_end be Float.min(q1_max, q2_max)
    
    If iqr_overlap_start is less than or equal to iqr_overlap_end:
        Let iqr_overlap_length be iqr_overlap_end minus iqr_overlap_start
        Let total_iqr_range be Float.max(q1_max, q2_max) minus Float.min(q1_min, q2_min)
        If total_iqr_range is greater than 0.0:
            Set result["iqr_overlap_ratio"] to iqr_overlap_length / total_iqr_range
        Otherwise:
            Set result["iqr_overlap_ratio"] to 1.0
    Otherwise:
        Set result["iqr_overlap_ratio"] to 0.0
    
    Note: Simple distribution distance measures
    Let empty_weights be List[Float]
    Let mean1 be calculate_arithmetic_mean(data1, empty_weights)
    Let mean2 be calculate_arithmetic_mean(data2, empty_weights)
    Let std1 be calculate_standard_deviation(data1, false)
    Let std2 be calculate_standard_deviation(data2, false)
    
    Note: Normalized distance between means
    Let mean_distance be Float(MathOps.absolute_value(Float.to_string(mean1 minus mean2)).result_value)
    Let pooled_std be (std1 plus std2) / 2.0
    If pooled_std is greater than 0.0:
        Set result["standardized_mean_distance"] to mean_distance / pooled_std
    Otherwise:
        Set result["standardized_mean_distance"] to 0.0
    
    Note: Coefficient of variation similarity
    Let cv1 be (std1 / Float(MathOps.absolute_value(Float.to_string(mean1)).result_value)) multiplied by 100.0
    Let cv2 be (std2 / Float(MathOps.absolute_value(Float.to_string(mean2)).result_value)) multiplied by 100.0
    Set result["cv_ratio"] to cv1 / cv2
    
    Return result

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

Process called "validate_statistical_assumptions" that takes data as List[Float], assumptions as List[String] returns Dictionary[String, Boolean]:
    Note: Validate common statistical assumptions (normality, homoscedasticity, etc.)
    Note: Returns boolean results and diagnostic statistics for each assumption
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot validate assumptions on empty dataset"
    
    Let result be Dictionary[String, Boolean]
    
    For each assumption in assumptions:
        If assumption is equal to "normality":
            If data.size() is greater than or equal to 3:
                Let normality_results be assess_normality(data, 0.05)
                Let is_normal be normality_results["is_normal"] is greater than 0.0
                Set result["normality"] to is_normal
            Otherwise:
                Set result["normality"] to false
        
        Otherwise if assumption is equal to "no_outliers":
            Let outliers be detect_outliers_iqr(data, 1.5)
            Set result["no_outliers"] to outliers.size() is equal to 0
        
        Otherwise if assumption is equal to "positive_values":
            Let all_positive be true
            For each value in data:
                If value is less than or equal to 0.0:
                    Set all_positive to false
                    Break
            Set result["positive_values"] to all_positive
        
        Otherwise if assumption is equal to "finite_variance":
            If data.size() is greater than 1:
                Let variance be calculate_variance(data, false, true)
                Set result["finite_variance"] to variance is greater than 0.0 and variance is less than Float.INFINITY
            Otherwise:
                Set result["finite_variance"] to false
        
        Otherwise if assumption is equal to "sufficient_sample_size":
            Set result["sufficient_sample_size"] to data.size() is greater than or equal to 30
        
        Otherwise:
            Note: Unknown assumption, mark as false
            Set result[assumption] to false
    
    Return result

Process called "handle_missing_values" that takes data as List[Float], method as String returns List[Float]:
    Note: Handle missing values using specified method (removal, imputation, etc.)
    Note: Methods: listwise deletion, mean imputation, median imputation, interpolation
    
    Let result be List[Float]
    
    Note: In Runa, NaN would be represented as Float.NaN, but for simplicity we'll assume missing values are identified differently
    Note: For this implementation, we'll treat extremely large negative values (-999999.0) as missing
    Let missing_value_indicator be -999999.0
    
    If method is equal to "listwise_deletion" or method is equal to "remove":
        For each value in data:
            If value does not equal missing_value_indicator and not Float.is_nan(value):
                Call result.append(value)
    
    Otherwise if method is equal to "mean_imputation":
        Note: First pass: collect non-missing values
        Let valid_values be List[Float]
        For each value in data:
            If value does not equal missing_value_indicator and not Float.is_nan(value):
                Call valid_values.append(value)
        
        If valid_values.size() is greater than 0:
            Let empty_weights be List[Float]
            Let mean_value be calculate_arithmetic_mean(valid_values, empty_weights)
            
            For each value in data:
                If value is equal to missing_value_indicator or Float.is_nan(value):
                    Call result.append(mean_value)
                Otherwise:
                    Call result.append(value)
        Otherwise:
            Note: All values are missing, return empty list
            Return result
    
    Otherwise if method is equal to "median_imputation":
        Note: First pass: collect non-missing values
        Let valid_values be List[Float]
        For each value in data:
            If value does not equal missing_value_indicator and not Float.is_nan(value):
                Call valid_values.append(value)
        
        If valid_values.size() is greater than 0:
            Let median_value be find_median(valid_values, "linear")
            
            For each value in data:
                If value is equal to missing_value_indicator or Float.is_nan(value):
                    Call result.append(median_value)
                Otherwise:
                    Call result.append(value)
        Otherwise:
            Return result
    
    Otherwise if method is equal to "interpolation":
        Note: Simple linear interpolation for missing values
        For i from 0 to data.size() minus 1:
            If data[i] is equal to missing_value_indicator or Float.is_nan(data[i]):
                Note: Find nearest non-missing values
                Let prev_valid be -1
                Let next_valid be -1
                
                For j from i minus 1 down to 0:
                    If data[j] does not equal missing_value_indicator and not Float.is_nan(data[j]):
                        Set prev_valid to j
                        Break
                
                For j from i plus 1 to data.size() minus 1:
                    If data[j] does not equal missing_value_indicator and not Float.is_nan(data[j]):
                        Set next_valid to j
                        Break
                
                If prev_valid is greater than or equal to 0 and next_valid is greater than or equal to 0:
                    Let weight be Float(i minus prev_valid) / Float(next_valid minus prev_valid)
                    Let interpolated be data[prev_valid] plus weight multiplied by (data[next_valid] minus data[prev_valid])
                    Call result.append(interpolated)
                Otherwise if prev_valid is greater than or equal to 0:
                    Call result.append(data[prev_valid])
                Otherwise if next_valid is greater than or equal to 0:
                    Call result.append(data[next_valid])
                Otherwise:
                    Call result.append(0.0)
            Otherwise:
                Call result.append(data[i])
    
    Otherwise:
        Note: Default to listwise deletion
        For each value in data:
            If value does not equal missing_value_indicator and not Float.is_nan(value):
                Call result.append(value)
    
    Return result

Process called "transform_data" that takes data as List[Float], transformation as String returns List[Float]:
    Note: Apply data transformation (log, sqrt, Box-Cox, etc.) for analysis
    Note: Helps achieve normality, homoscedasticity, or linearize relationships
    
    If data.size() is equal to 0:
        Return List[Float]
    
    Let result be List[Float]
    
    If transformation is equal to "log" or transformation is equal to "natural_log":
        For each value in data:
            If value is greater than 0.0:
                Let log_result be MathOps.natural_logarithm(Float.to_string(value), 50)
                Call result.append(Float(log_result.result_value))
            Otherwise:
                Throw Errors.InvalidArgument with "Natural log transformation requires positive values"
    
    Otherwise if transformation is equal to "log10":
        For each value in data:
            If value is greater than 0.0:
                Let log_result be MathOps.common_logarithm(Float.to_string(value), 50)
                Call result.append(Float(log_result.result_value))
            Otherwise:
                Throw Errors.InvalidArgument with "Log10 transformation requires positive values"
    
    Otherwise if transformation is equal to "sqrt" or transformation is equal to "square_root":
        For each value in data:
            If value is greater than or equal to 0.0:
                Let sqrt_result be MathOps.square_root(Float.to_string(value), 50)
                Call result.append(Float(sqrt_result.result_value))
            Otherwise:
                Throw Errors.InvalidArgument with "Square root transformation requires non-negative values"
    
    Otherwise if transformation is equal to "square":
        For each value in data:
            Let squared be value multiplied by value
            Call result.append(squared)
    
    Otherwise if transformation is equal to "reciprocal" or transformation is equal to "inverse":
        For each value in data:
            If value does not equal 0.0:
                Call result.append(1.0 / value)
            Otherwise:
                Throw Errors.InvalidArgument with "Reciprocal transformation undefined for zero values"
    
    Otherwise if transformation is equal to "standardize" or transformation is equal to "z_score":
        Let empty_weights be List[Float]
        Let mean_value be calculate_arithmetic_mean(data, empty_weights)
        Let std_dev be calculate_standard_deviation(data, false)
        
        If std_dev is equal to 0.0:
            Throw Errors.InvalidArgument with "Cannot standardize data with zero standard deviation"
        
        For each value in data:
            Let z_score be (value minus mean_value) / std_dev
            Call result.append(z_score)
    
    Otherwise if transformation is equal to "normalize" or transformation is equal to "min_max":
        Let range_stats be calculate_range(data)
        Let min_val be range_stats["minimum"]
        Let max_val be range_stats["maximum"]
        Let range_val be range_stats["range"]
        
        If range_val is equal to 0.0:
            For each value in data:
                Call result.append(0.0)
        Otherwise:
            For each value in data:
                Let normalized be (value minus min_val) / range_val
                Call result.append(normalized)
    
    Otherwise:
        Note: No transformation applied, return original data
        For each value in data:
            Call result.append(value)
    
    Return result

Process called "bootstrap_statistic" that takes data as List[Float], statistic_function as String, samples as Integer returns List[Float]:
    Note: Generate bootstrap samples of specified statistic for uncertainty estimation
    Note: Resamples data with replacement to estimate sampling distribution
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot bootstrap empty dataset"
    
    If samples is less than or equal to 0:
        Throw Errors.InvalidArgument with "Number of bootstrap samples must be positive"
    
    Let result be List[Float]
    Let n be data.size()
    
    For bootstrap_iteration from 1 to samples:
        Note: Create bootstrap sample with replacement
        Let bootstrap_sample be List[Float]
        
        For i from 1 to n:
            Note: Simple pseudo-random sampling (using iteration-based approach)
            Let random_index be (bootstrap_iteration multiplied by 17 plus i multiplied by 37) % n
            Call bootstrap_sample.append(data[random_index])
        
        Note: Calculate statistic on bootstrap sample
        Let statistic_value be 0.0
        
        If statistic_function is equal to "mean":
            Let empty_weights be List[Float]
            Set statistic_value to calculate_arithmetic_mean(bootstrap_sample, empty_weights)
        
        Otherwise if statistic_function is equal to "median":
            Set statistic_value to find_median(bootstrap_sample, "linear")
        
        Otherwise if statistic_function is equal to "std_dev":
            Set statistic_value to calculate_standard_deviation(bootstrap_sample, false)
        
        Otherwise if statistic_function is equal to "variance":
            Set statistic_value to calculate_variance(bootstrap_sample, false, true)
        
        Otherwise if statistic_function is equal to "range":
            Let range_stats be calculate_range(bootstrap_sample)
            Set statistic_value to range_stats["range"]
        
        Otherwise if statistic_function is equal to "iqr":
            Let quartiles be calculate_quartiles(bootstrap_sample)
            Set statistic_value to quartiles["IQR"]
        
        Otherwise if statistic_function is equal to "skewness":
            If bootstrap_sample.size() is greater than or equal to 3:
                Set statistic_value to calculate_skewness(bootstrap_sample, "Fisher")
            Otherwise:
                Set statistic_value to 0.0
        
        Otherwise if statistic_function is equal to "kurtosis":
            If bootstrap_sample.size() is greater than or equal to 4:
                Set statistic_value to calculate_kurtosis(bootstrap_sample, true)
            Otherwise:
                Set statistic_value to 0.0
        
        Otherwise:
            Note: Default to mean
            Let empty_weights be List[Float]
            Set statistic_value to calculate_arithmetic_mean(bootstrap_sample, empty_weights)
        
        Call result.append(statistic_value)
    
    Return result

Process called "calculate_covariance_matrix" that takes data_matrix as List[List[Float]] returns List[List[Float]]:
    Note: Calculate sample covariance matrix from data matrix where rows are observations, columns are variables
    Note: Formula: Cov(X,Y) is equal to Σ((xi minus μx)(yi minus μy)) / (n-1) for sample covariance
    
    If data_matrix.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate covariance matrix of empty dataset"
    
    Let n_observations be data_matrix.size()
    Let n_variables be data_matrix[0].size()
    
    If n_observations is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 observations for sample covariance"
    
    Note: Calculate means for each variable
    Let means be List[Float]()
    For j from 0 to n_variables minus 1:
        Let column_data be List[Float]()
        For i from 0 to n_observations minus 1:
            Call column_data.append(data_matrix[i][j])
        Let empty_weights be List[Float]()
        Let column_mean be calculate_arithmetic_mean(column_data, empty_weights)
        Call means.append(column_mean)
    
    Note: Calculate covariance matrix
    Let cov_matrix be List[List[Float]]()
    For i from 0 to n_variables minus 1:
        Let row be List[Float]()
        For j from 0 to n_variables minus 1:
            Let sum_products be 0.0
            For k from 0 to n_observations minus 1:
                Let xi_minus_mean be data_matrix[k][i] minus means[i]
                Let xj_minus_mean be data_matrix[k][j] minus means[j]
                Set sum_products to sum_products plus (xi_minus_mean multiplied by xj_minus_mean)
            Let covariance be sum_products / Float(n_observations minus 1)
            Call row.append(covariance)
        Call cov_matrix.append(row)
    
    Return cov_matrix

Process called "calculate_correlation_matrix" that takes data_matrix as List[List[Float]] returns List[List[Float]]:
    Note: Calculate Pearson correlation matrix from data matrix
    Note: Formula: Corr(X,Y) is equal to Cov(X,Y) / (σx multiplied by σy)
    
    Let cov_matrix be calculate_covariance_matrix(data_matrix)
    Let n_variables be cov_matrix.size()
    
    Note: Calculate standard deviations from covariance matrix diagonal
    Let std_devs be List[Float]()
    For i from 0 to n_variables minus 1:
        Let variance be cov_matrix[i][i]
        If variance is less than 0.0:
            Throw Errors.InvalidArgument with "Negative variance encountered"
        Let std_dev be Float.sqrt(variance)
        Call std_devs.append(std_dev)
    
    Note: Calculate correlation matrix
    Let corr_matrix be List[List[Float]]()
    For i from 0 to n_variables minus 1:
        Let row be List[Float]()
        For j from 0 to n_variables minus 1:
            If i is equal to j:
                Call row.append(1.0)
            Otherwise:
                If std_devs[i] is equal to 0.0 or std_devs[j] is equal to 0.0:
                    Call row.append(0.0)
                Otherwise:
                    Let correlation be cov_matrix[i][j] / (std_devs[i] multiplied by std_devs[j])
                    Call row.append(correlation)
        Call corr_matrix.append(row)
    
    Return corr_matrix

Process called "calculate_percentile" that takes data as List[Float], percentile as Float returns Float:
    Note: Calculate single percentile value using linear interpolation method
    Note: Percentile should be between 0 and 100
    
    If data.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot calculate percentile of empty dataset"
    
    If percentile is less than 0.0 or percentile is greater than 100.0:
        Throw Errors.InvalidArgument with "Percentile must be between 0 and 100"
    
    Let percentiles_list be List[Float]()
    Call percentiles_list.append(percentile)
    Let result_dict be calculate_percentiles(data, percentiles_list, "linear")
    
    Return result_dict[Float.to_string(percentile)]