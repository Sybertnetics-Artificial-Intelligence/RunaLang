Note:
math/statistics/regression.runa
Regression Analysis Operations

This module provides comprehensive regression analysis capabilities including
linear and nonlinear regression, model selection, diagnostic testing,
regularization methods, and predictive modeling for statistical relationship
analysis between variables.
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/engine/linalg/core" as LinAlg
Import module "math/engine/linalg/decomposition" as LinAlgDecomp
Import module "math/engine/optimization/core" as OptCore
Import module "math/engine/optimization/solvers" as OptSolvers
Import module "math/probability/distributions" as Distributions
Import module "math/statistics/descriptive" as Stats
Import module "math/statistics/inferential" as InfStats
Import module "math/core/operations" as MathOps

Note: =====================================================================
Note: REGRESSION ANALYSIS DATA STRUCTURES
Note: =====================================================================

Type called "RegressionModel":
    model_type as String
    coefficients as List[Float]
    intercept as Float
    r_squared as Float
    adjusted_r_squared as Float
    f_statistic as Float
    p_value as Float
    standard_errors as List[Float]
    t_statistics as List[Float]
    coefficient_p_values as List[Float]
    residuals as List[Float]
    fitted_values as List[Float]

Type called "ModelDiagnostics":
    residual_statistics as Dictionary[String, Float]
    normality_tests as Dictionary[String, Float]
    homoscedasticity_tests as Dictionary[String, Float]
    autocorrelation_tests as Dictionary[String, Float]
    outlier_detection as List[Integer]
    influential_points as List[Integer]
    multicollinearity_metrics as Dictionary[String, Float]

Type called "ModelSelection":
    selection_criterion as String
    candidate_models as List[Dictionary[String, String]]
    criterion_values as List[Float]
    best_model as Dictionary[String, String]
    model_rankings as List[Integer]
    cross_validation_scores as List[Float]

Type called "PredictionResult":
    predicted_values as List[Float]
    prediction_intervals as List[List[Float]]
    confidence_intervals as List[List[Float]]
    standard_errors as List[Float]
    residuals as List[Float]
    prediction_metrics as Dictionary[String, Float]

Note: =====================================================================
Note: LINEAR REGRESSION OPERATIONS
Note: =====================================================================

Process called "simple_linear_regression" that takes x as List[Float], y as List[Float] returns RegressionModel:
    Note: Fit simple linear regression model: y is equal to β₀ plus β₁x plus ε
    Note: Uses ordinary least squares estimation. Returns complete model diagnostics
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    If Length(x) is less than 2:
        Throw Errors.InvalidInput with "At least 2 data points required for regression"
    
    Let n be Length(x) as Float
    
    Note: Compute means
    Let x_mean be Stats.calculate_arithmetic_mean(x, [])
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    
    Note: Compute sums of squares and cross products
    Let sxx be 0.0
    Let sxy be 0.0
    Let syy be 0.0
    
    For i from 0 to Length(x) minus 1:
        Let x_dev be x[i] minus x_mean
        Let y_dev be y[i] minus y_mean
        Set sxx to sxx plus x_dev multiplied by x_dev
        Set sxy to sxy plus x_dev multiplied by y_dev
        Set syy to syy plus y_dev multiplied by y_dev
    
    Note: Calculate regression coefficients using normal equations
    If sxx is less than 1e-15:
        Throw Errors.InvalidInput with "All x values are identical minus cannot fit regression"
    
    Let beta1 be sxy / sxx  Note: Slope
    Let beta0 be y_mean minus beta1 multiplied by x_mean  Note: Intercept
    
    Note: Calculate fitted values and residuals
    Let fitted_values be []
    Let residuals be []
    Let rss be 0.0  Note: Residual sum of squares
    
    For i from 0 to Length(x) minus 1:
        Let fitted be beta0 plus beta1 multiplied by x[i]
        Append fitted to fitted_values
        Let residual be y[i] minus fitted
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared and adjusted R-squared
    Let tss be syy  Note: Total sum of squares
    Let r_squared be 1.0 minus rss / tss
    Let adjusted_r_squared be 1.0 minus (rss / (n minus 2.0)) / (tss / (n minus 1.0))
    
    Note: Calculate standard errors and test statistics
    Let mse be rss / (n minus 2.0)  Note: Mean squared error
    Let se_beta1 be MathOps.square_root(mse / sxx, 15).result
    Let se_beta0 be MathOps.square_root(mse multiplied by (1.0 / n plus x_mean multiplied by x_mean / sxx), 15).result
    
    Note: T-statistics for coefficients
    Let t_beta0 be beta0 / se_beta0
    Let t_beta1 be beta1 / se_beta1
    
    Note: Calculate p-values using t-distribution (simplified approximation)
    Let df be n minus 2.0
    Let p_value_beta0 be 2.0 multiplied by (1.0 minus Distributions.t_distribution_cdf(MathOps.abs(t_beta0), df as Integer))
    Let p_value_beta1 be 2.0 multiplied by (1.0 minus Distributions.t_distribution_cdf(MathOps.abs(t_beta1), df as Integer))
    
    Note: F-statistic for overall model
    Let ess be tss minus rss  Note: Explained sum of squares
    Let f_statistic be (ess / 1.0) / (rss / df)
    Let f_p_value be 1.0 minus Distributions.f_distribution_cdf(f_statistic, 1, df as Integer)
    
    Note: Create and return regression model
    Let model be RegressionModel
    Set model.model_type to "Simple Linear Regression"
    Set model.coefficients to [beta1]
    Set model.intercept to beta0
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to f_statistic
    Set model.p_value to f_p_value
    Set model.standard_errors to [se_beta1]
    Set model.t_statistics to [t_beta1]
    Set model.coefficient_p_values to [p_value_beta1]
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "multiple_linear_regression" that takes X as List[List[Float]], y as List[Float], include_intercept as Boolean returns RegressionModel:
    Note: Fit multiple linear regression: y is equal to β₀ plus β₁x₁ plus ... plus βₚxₚ plus ε
    Note: Solves normal equations: β is equal to (X'X)⁻¹X'y. Includes statistical tests
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 2:
        Throw Errors.InvalidInput with "At least 2 observations required for regression"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Add intercept column if requested
    Let design_matrix be []
    If include_intercept:
        For i from 0 to Length(X) minus 1:
            Let row be [1.0]  Note: Intercept column
            For j from 0 to p minus 1:
                Append X[i][j] to row
            Append row to design_matrix
        Set p to p plus 1
    Otherwise:
        Set design_matrix to X
    
    Note: Convert to matrix format for linear algebra operations
    Let X_matrix be LinAlg.create_matrix(design_matrix, "Float")
    Let y_vector be LinAlg.create_vector(y)
    
    Note: Compute X'X and X'y
    Let X_transpose be LinAlg.matrix_transpose(X_matrix)
    Let XtX be LinAlg.multiply_matrices(X_transpose, X_matrix)
    Let Xty be LinAlg.matrix_vector_multiply(design_matrix, y)
    
    Note: Solve normal equations β is equal to (X'X)⁻¹X'y
    Let XtX_inv be LinAlg.matrix_inverse(XtX, "LU")
    Let coefficients_vector be LinAlg.matrix_vector_multiply(XtX_inv.entries, Xty)
    
    Note: Calculate fitted values and residuals
    Let fitted_values be LinAlg.matrix_vector_multiply(design_matrix, coefficients_vector)
    Let residuals be []
    Let rss be 0.0
    
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let adjusted_r_squared be 1.0 minus (rss / (n minus p as Float)) / (tss / (n minus 1.0))
    
    Note: Calculate standard errors using diagonal of (X'X)⁻¹
    Let mse be rss / (n minus p as Float)
    Let standard_errors be []
    For i from 0 to p minus 1:
        Let var_i be XtX_inv.entries[i][i] multiplied by mse
        Let se_i be MathOps.square_root(var_i, 15).result
        Append se_i to standard_errors
    
    Note: Calculate t-statistics and p-values
    Let t_statistics be []
    Let p_values be []
    Let df be (n minus p as Float) as Integer
    
    For i from 0 to p minus 1:
        Let t_stat be coefficients_vector[i] / standard_errors[i]
        Append t_stat to t_statistics
        Let p_val be 2.0 multiplied by (1.0 minus Distributions.t_distribution_cdf(MathOps.abs(t_stat), df))
        Append p_val to p_values
    
    Note: F-statistic for overall model
    Let ess be tss minus rss
    Let f_statistic be (ess / (p as Float minus 1.0)) / (rss / (n minus p as Float))
    Let f_p_value be 1.0 minus Distributions.f_distribution_cdf(f_statistic, p minus 1, df)
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Multiple Linear Regression"
    
    If include_intercept:
        Set model.intercept to coefficients_vector[0]
        Set model.coefficients to coefficients_vector[1:]
        Set model.standard_errors to standard_errors[1:]
        Set model.t_statistics to t_statistics[1:]
        Set model.coefficient_p_values to p_values[1:]
    Otherwise:
        Set model.intercept to 0.0
        Set model.coefficients to coefficients_vector
        Set model.standard_errors to standard_errors
        Set model.t_statistics to t_statistics
        Set model.coefficient_p_values to p_values
    
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to f_statistic
    Set model.p_value to f_p_value
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "weighted_least_squares" that takes X as List[List[Float]], y as List[Float], weights as List[Float] returns RegressionModel:
    Note: Fit weighted least squares regression for heteroscedastic errors
    Note: Minimizes Σwᵢ(yᵢ minus ŷᵢ)². Accounts for unequal error variances
    
    If Length(X) does not equal Length(y) || Length(y) does not equal Length(weights):
        Throw Errors.InvalidInput with "X, y, and weights must have the same number of observations"
    
    If Length(X) is less than 2:
        Throw Errors.InvalidInput with "At least 2 observations required for regression"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Check that all weights are positive
    For i from 0 to Length(weights) minus 1:
        If weights[i] is less than or equal to 0.0:
            Throw Errors.InvalidInput with "All weights must be positive"
    
    Note: Create design matrix with intercept column
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]  Note: Intercept column
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Create weight matrix W (diagonal)
    Let W be LinAlg.create_zero_matrix(n as Integer, n as Integer)
    For i from 0 to n as Integer minus 1:
        Set W.entries[i][i] to weights[i]
    
    Note: Convert to matrix format
    Let X_matrix be LinAlg.create_matrix(design_matrix, "Float")
    Let y_vector be LinAlg.create_vector(y)
    
    Note: Compute X'WX and X'Wy for weighted normal equations
    Let X_transpose be LinAlg.matrix_transpose(X_matrix)
    Let WX be LinAlg.multiply_matrices(W, X_matrix)
    Let XtWX be LinAlg.multiply_matrices(X_transpose, WX)
    
    Note: Compute X'Wy
    Let Wy be LinAlg.matrix_vector_multiply(W.entries, y)
    Let XtWy be LinAlg.matrix_vector_multiply(X_transpose.entries, Wy)
    
    Note: Solve weighted normal equations: β is equal to (X'WX)⁻¹X'Wy
    Let XtWX_inv be LinAlg.matrix_inverse(XtWX, "LU")
    Let coefficients_vector be LinAlg.matrix_vector_multiply(XtWX_inv.entries, XtWy)
    
    Note: Calculate weighted fitted values and residuals
    Let fitted_values be LinAlg.matrix_vector_multiply(design_matrix, coefficients_vector)
    Let residuals be []
    Let weighted_rss be 0.0
    
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set weighted_rss to weighted_rss plus weights[i] multiplied by residual multiplied by residual
    
    Note: Calculate weighted R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, weights)
    Let weighted_tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set weighted_tss to weighted_tss plus weights[i] multiplied by dev multiplied by dev
    
    Let r_squared be 1.0 minus weighted_rss / weighted_tss
    Let adjusted_r_squared be 1.0 minus (weighted_rss / (n minus (p plus 1) as Float)) / (weighted_tss / (n minus 1.0))
    
    Note: Calculate standard errors using (X'WX)⁻¹
    Let mse be weighted_rss / (n minus (p plus 1) as Float)
    Let standard_errors be []
    For i from 0 to p:
        Let se_i be MathOps.square_root(mse multiplied by XtWX_inv.entries[i][i], 15).result
        Append se_i to standard_errors
    
    Note: Calculate t-statistics and p-values
    Let t_statistics be []
    Let p_values be []
    Let df be (n minus (p plus 1) as Float) as Integer
    
    For i from 0 to p:
        Let t_stat be coefficients_vector[i] / standard_errors[i]
        Append t_stat to t_statistics
        Let p_val be 2.0 multiplied by (1.0 minus Distributions.t_distribution_cdf(MathOps.abs(t_stat), df))
        Append p_val to p_values
    
    Note: F-statistic for overall model
    Let ess be weighted_tss minus weighted_rss
    Let f_statistic be (ess / p as Float) / (weighted_rss / df as Float)
    Let f_p_value be 1.0 minus Distributions.f_distribution_cdf(f_statistic, p, df)
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Weighted Least Squares"
    Set model.intercept to coefficients_vector[0]
    Set model.coefficients to coefficients_vector[1:]
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to f_statistic
    Set model.p_value to f_p_value
    Set model.standard_errors to standard_errors[1:]
    Set model.t_statistics to t_statistics[1:]
    Set model.coefficient_p_values to p_values[1:]
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "generalized_least_squares" that takes X as List[List[Float]], y as List[Float], covariance_matrix as List[List[Float]] returns RegressionModel:
    Note: Fit GLS regression for correlated errors with known covariance structure
    Note: Optimal estimator when Var(ε) is equal to σ²Ω where Ω is known
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(covariance_matrix) does not equal Length(y):
        Throw Errors.InvalidInput with "Covariance matrix must match number of observations"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix with intercept
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Convert to matrix format
    Let X_matrix be LinAlg.create_matrix(design_matrix, "Float")
    Let Omega be LinAlg.create_matrix(covariance_matrix, "Float")
    
    Note: Compute Omega^(-1) for transformation
    Let Omega_inv be LinAlg.matrix_inverse(Omega, "LU")
    
    Note: GLS normal equations: β is equal to (X'Ω⁻¹X)⁻¹X'Ω⁻¹y
    Let X_transpose be LinAlg.matrix_transpose(X_matrix)
    Let Omega_inv_X be LinAlg.multiply_matrices(Omega_inv, X_matrix)
    Let XtOmegaInvX be LinAlg.multiply_matrices(X_transpose, Omega_inv_X)
    
    Let Omega_inv_y be LinAlg.matrix_vector_multiply(Omega_inv.entries, y)
    Let XtOmegaInvy be LinAlg.matrix_vector_multiply(X_transpose.entries, Omega_inv_y)
    
    Let XtOmegaInvX_inv be LinAlg.matrix_inverse(XtOmegaInvX, "LU")
    Let coefficients_vector be LinAlg.matrix_vector_multiply(XtOmegaInvX_inv.entries, XtOmegaInvy)
    
    Note: Calculate fitted values and residuals
    Let fitted_values be LinAlg.matrix_vector_multiply(design_matrix, coefficients_vector)
    Let residuals be []
    Let rss be 0.0
    
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let adjusted_r_squared be 1.0 minus (rss / (n minus (p plus 1) as Float)) / (tss / (n minus 1.0))
    
    Note: Standard errors using (X'Ω⁻¹X)⁻¹
    Let standard_errors be []
    For i from 0 to p:
        Let se_i be MathOps.square_root(XtOmegaInvX_inv.entries[i][i], 15).result
        Append se_i to standard_errors
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Generalized Least Squares"
    Set model.intercept to coefficients_vector[0]
    Set model.coefficients to coefficients_vector[1:]
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0
    Set model.p_value to 1.0
    Set model.standard_errors to standard_errors[1:]
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Note: =====================================================================
Note: POLYNOMIAL REGRESSION OPERATIONS
Note: =====================================================================

Process called "polynomial_regression" that takes x as List[Float], y as List[Float], degree as Integer returns RegressionModel:
    Note: Fit polynomial regression: y is equal to β₀ plus β₁x plus β₂x² plus ... plus βₚx^p plus ε
    Note: Creates polynomial features automatically. Watch for overfitting
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    If degree is less than 1:
        Throw Errors.InvalidInput with "Polynomial degree must be at least 1"
    
    If Length(x) is less than or equal to degree:
        Throw Errors.InvalidInput with "Need more observations than polynomial degree"
    
    Note: Create polynomial design matrix
    Let X_poly be []
    For i from 0 to Length(x) minus 1:
        Let row be []
        For deg from 0 to degree:
            If deg is equal to 0:
                Append 1.0 to row  Note: Intercept term
            Otherwise:
                Append MathOps.power(x[i], deg as Float, 15).result to row
        Append row to X_poly
    
    Note: Use multiple linear regression with polynomial features
    Return multiple_linear_regression(X_poly, y, false)  Note: Intercept already included

Process called "orthogonal_polynomial_regression" that takes x as List[Float], y as List[Float], degree as Integer returns RegressionModel:
    Note: Fit orthogonal polynomial regression using orthogonal basis functions
    Note: Reduces multicollinearity issues in high-degree polynomials
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    If degree is less than 1:
        Throw Errors.InvalidInput with "Polynomial degree must be at least 1"
    
    If Length(x) is less than or equal to degree:
        Throw Errors.InvalidInput with "Need more observations than polynomial degree"
    
    Note: Create orthogonal polynomial basis using Gram-Schmidt process
    Let X_orthogonal be []
    Let basis_polynomials be []
    
    Note: Start with constant and linear terms
    For i from 0 to Length(x) minus 1:
        Append [1.0, x[i]] to X_orthogonal
    
    Note: Apply Gram-Schmidt to create orthogonal basis for higher degree terms
    For deg from 2 to degree:
        Let new_column be []
        For i from 0 to Length(x) minus 1:
            Append MathOps.power(x[i], deg as Float, 15).result to new_column
        
        Note: Orthogonalize against previous columns
        For j from 0 to deg minus 1:
            Let dot_product be 0.0
            Let norm_squared be 0.0
            
            For k from 0 to Length(x) minus 1:
                Set dot_product to dot_product plus new_column[k] multiplied by X_orthogonal[k][j]
                Set norm_squared to norm_squared plus X_orthogonal[k][j] multiplied by X_orthogonal[k][j]
            
            If norm_squared is greater than 1e-12:
                Let projection_coeff be dot_product / norm_squared
                For k from 0 to Length(x) minus 1:
                    Set new_column[k] to new_column[k] minus projection_coeff multiplied by X_orthogonal[k][j]
        
        Note: Add orthogonalized column to design matrix
        For i from 0 to Length(x) minus 1:
            Append new_column[i] to X_orthogonal[i]
    
    Note: Use multiple linear regression with orthogonal features
    Return multiple_linear_regression(X_orthogonal, y, false)

Process called "piecewise_regression" that takes x as List[Float], y as List[Float], breakpoints as List[Float] returns RegressionModel:
    Note: Fit piecewise linear regression with specified breakpoints
    Note: Models different linear relationships across data segments
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    If Length(breakpoints) is equal to 0:
        Throw Errors.InvalidInput with "At least one breakpoint must be specified"
    
    Note: Sort breakpoints
    Let sorted_breakpoints be breakpoints
    Note: Simple bubble sort for breakpoints
    For i from 0 to Length(sorted_breakpoints) minus 1:
        For j from 0 to Length(sorted_breakpoints) minus 2 minus i:
            If sorted_breakpoints[j] is greater than sorted_breakpoints[j plus 1]:
                Let temp be sorted_breakpoints[j]
                Set sorted_breakpoints[j] to sorted_breakpoints[j plus 1]
                Set sorted_breakpoints[j plus 1] to temp
    
    Note: Create piecewise design matrix
    Let X_piecewise be []
    Let num_segments be Length(sorted_breakpoints) plus 1
    
    For i from 0 to Length(x) minus 1:
        Let row be [1.0, x[i]]  Note: Intercept and linear term
        
        Note: Add indicator variables and interactions for each segment
        For seg from 0 to Length(sorted_breakpoints) minus 1:
            If x[i] is greater than sorted_breakpoints[seg]:
                Append 1.0 to row  Note: Indicator for segment
                Append x[i] minus sorted_breakpoints[seg] to row  Note: Interaction term
            Otherwise:
                Append 0.0 to row
                Append 0.0 to row
        
        Append row to X_piecewise
    
    Note: Fit using multiple linear regression
    Let base_model be multiple_linear_regression(X_piecewise, y, false)
    
    Note: Update model type
    Let model be base_model
    Set model.model_type to "Piecewise Linear Regression"
    
    Return model

Process called "spline_regression" that takes x as List[Float], y as List[Float], knots as List[Float], degree as Integer returns RegressionModel:
    Note: Fit regression splines with specified knots and degree
    Note: Smooth piecewise polynomials with continuous derivatives
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    If degree is less than 1:
        Throw Errors.InvalidInput with "Spline degree must be at least 1"
    
    Note: For simplicity, implement linear splines (degree is equal to 1) with continuous basis
    Let X_spline be []
    
    For i from 0 to Length(x) minus 1:
        Let row be [1.0, x[i]]  Note: Linear base
        
        Note: Add truncated power basis functions for each knot
        For k from 0 to Length(knots) minus 1:
            If x[i] is greater than knots[k]:
                Let basis_val be MathOps.power(x[i] minus knots[k], degree as Float, 15).result
                Append basis_val to row
            Otherwise:
                Append 0.0 to row
        
        Append row to X_spline
    
    Note: Fit using multiple linear regression
    Let base_model be multiple_linear_regression(X_spline, y, false)
    
    Note: Update model type
    Let model be base_model
    Set model.model_type to "Spline Regression"
    
    Return model

Note: =====================================================================
Note: LOGISTIC REGRESSION OPERATIONS
Note: =====================================================================

Process called "binary_logistic_regression" that takes X as List[List[Float]], y as List[Integer], method as String returns RegressionModel:
    Note: Fit binary logistic regression using maximum likelihood estimation
    Note: Models P(Y=1|X) is equal to 1/(1 plus e^(-X'β)). Methods: Newton-Raphson, IRLS
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 2:
        Throw Errors.InvalidInput with "At least 2 observations required for logistic regression"
    
    Note: Check that y contains only 0 and 1
    For i from 0 to Length(y) minus 1:
        If y[i] does not equal 0 && y[i] does not equal 1:
            Throw Errors.InvalidInput with "Binary logistic regression requires y values to be 0 or 1"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix with intercept column
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]  Note: Intercept column
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Convert y to float for calculations
    Let y_float be []
    For i from 0 to Length(y) minus 1:
        Append y[i] as Float to y_float
    
    Note: Create constrained optimization problem for IRLS
    Let problem be OptSolvers.ConstrainedProblem
    Set problem.initial_point to List.repeat(0.0, p plus 1)  Note: Initialize coefficients to zero
    Set problem.objective_function to "logistic_log_likelihood"
    Set problem.residual_function to "logistic_residuals"
    Set problem.jacobian_function to "logistic_jacobian"
    
    Note: Configure IRLS algorithm
    Let irls_config be Dictionary[String, String]
    Set irls_config["max_iterations"] to "100"
    Set irls_config["tolerance"] to "1e-8"
    
    Let convergence be OptCore.create_convergence_criteria()
    Set convergence.gradient_tolerance to 1e-8
    Set convergence.max_iterations to 100
    
    Note: Run IRLS optimization
    Let logistic_result be OptCore.iteratively_reweighted_least_squares(problem, irls_config, convergence)
    Let coefficients be logistic_result.solution
    
    Note: Calculate fitted probabilities using logistic function
    Let fitted_probabilities be []
    Let fitted_values be []
    For i from 0 to Length(X) minus 1:
        Let linear_predictor be coefficients[0]  Note: Intercept
        For j from 0 to p minus 1:
            Set linear_predictor to linear_predictor plus coefficients[j plus 1] multiplied by X[i][j]
        
        Let probability be 1.0 / (1.0 plus MathOps.exponential(-linear_predictor, 15).result)
        Append probability to fitted_probabilities
        
        Note: Convert to binary prediction (0.5 threshold)
        If probability is greater than or equal to 0.5:
            Append 1.0 to fitted_values
        Otherwise:
            Append 0.0 to fitted_values
    
    Note: Calculate deviance residuals
    Let residuals be []
    Let deviance be 0.0
    For i from 0 to Length(y) minus 1:
        Let pi be fitted_probabilities[i]
        If pi is less than 1e-15:
            Set pi to 1e-15  Note: Prevent log(0)
        If pi is greater than 1.0 minus 1e-15:
            Set pi to 1.0 minus 1e-15  Note: Prevent log(0)
        
        Let dev_resid be 0.0
        If y[i] is equal to 1:
            Set dev_resid to MathOps.square_root(2.0 multiplied by MathOps.natural_logarithm(1.0 / pi, 15).result, 15).result
        Otherwise:
            Set dev_resid to -MathOps.square_root(2.0 multiplied by MathOps.natural_logarithm(1.0 / (1.0 minus pi), 15).result, 15).result
        
        Append dev_resid to residuals
        Set deviance to deviance plus dev_resid multiplied by dev_resid
    
    Note: Calculate McFadden's pseudo R-squared
    Let null_log_likelihood be calculate_null_log_likelihood(y_float)
    Let full_log_likelihood be calculate_logistic_log_likelihood(design_matrix, y_float, coefficients)
    Let pseudo_r_squared be 1.0 minus full_log_likelihood / null_log_likelihood
    
    Note: Calculate approximate standard errors using Fisher Information Matrix
    Let information_matrix be calculate_fisher_information(design_matrix, fitted_probabilities)
    Let covariance_matrix be LinAlg.matrix_inverse(information_matrix, "LU")
    
    Let standard_errors be []
    For i from 0 to p:
        Let se_i be MathOps.square_root(covariance_matrix.entries[i][i], 15).result
        Append se_i to standard_errors
    
    Note: Calculate Wald statistics and p-values
    Let z_statistics be []
    Let p_values be []
    For i from 0 to p:
        Let z_stat be coefficients[i] / standard_errors[i]
        Append z_stat to z_statistics
        Let p_val be 2.0 multiplied by (1.0 minus Distributions.normal_distribution_cdf(MathOps.abs(z_stat), 0.0, 1.0))
        Append p_val to p_values
    
    Note: Likelihood ratio test for overall model
    Let lr_statistic be 2.0 multiplied by (full_log_likelihood minus null_log_likelihood)
    Let lr_p_value be 1.0 minus Distributions.chi_squared_cdf(lr_statistic, p)
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Binary Logistic Regression"
    Set model.intercept to coefficients[0]
    Set model.coefficients to coefficients[1:]
    Set model.r_squared to pseudo_r_squared  Note: McFadden's pseudo R-squared
    Set model.adjusted_r_squared to pseudo_r_squared  Note: No standard adjustment for logistic
    Set model.f_statistic to lr_statistic  Note: Use LR statistic instead of F
    Set model.p_value to lr_p_value
    Set model.standard_errors to standard_errors[1:]
    Set model.t_statistics to z_statistics[1:]  Note: Z-statistics for logistic regression
    Set model.coefficient_p_values to p_values[1:]
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "multinomial_logistic_regression" that takes X as List[List[Float]], y as List[Integer], num_classes as Integer returns RegressionModel:
    Note: Fit multinomial logistic regression for multi-class outcomes
    Note: Uses softmax function for probability modeling across categories
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than num_classes:
        Throw Errors.InvalidInput with "Need at least as many observations as classes"
    
    Note: Validate y values are in range [0, num_classes-1]
    For i from 0 to Length(y) minus 1:
        If y[i] is less than 0 || y[i] is greater than or equal to num_classes:
            Throw Errors.InvalidInput with "y values must be in range [0, num_classes-1]"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix with intercept
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Initialize coefficient matrix (num_classes-1) x (p+1)
    Note: Last class is reference category
    Let coefficients be []
    For class_idx from 0 to num_classes minus 2:
        Let class_coef be []
        For j from 0 to p:
            Append 0.0 to class_coef
        Append class_coef to coefficients
    
    Note: Newton-Raphson optimization for maximum likelihood
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate probabilities using softmax
        Let probabilities be []
        For i from 0 to Length(design_matrix) minus 1:
            Let linear_combos be []
            For class_idx from 0 to num_classes minus 2:
                Let linear_combo be 0.0
                For j from 0 to p:
                    Set linear_combo to linear_combo plus coefficients[class_idx][j] multiplied by design_matrix[i][j]
                Append linear_combo to linear_combos
            Append 0.0 to linear_combos  Note: Reference class
            
            Note: Softmax transformation
            Let max_combo be linear_combos[0]
            For k from 1 to Length(linear_combos) minus 1:
                If linear_combos[k] is greater than max_combo:
                    Set max_combo to linear_combos[k]
            
            Let exp_sum be 0.0
            Let row_probs be []
            For k from 0 to Length(linear_combos) minus 1:
                Let exp_val be MathOps.exp(linear_combos[k] minus max_combo)
                Append exp_val to row_probs
                Set exp_sum to exp_sum plus exp_val
            
            For k from 0 to Length(row_probs) minus 1:
                Set row_probs[k] to row_probs[k] / exp_sum
            
            Append row_probs to probabilities
        
        Note: Calculate gradient and Hessian
        Let gradient be []
        For class_idx from 0 to num_classes minus 2:
            Let class_grad be []
            For j from 0 to p:
                Let grad_val be 0.0
                For i from 0 to Length(design_matrix) minus 1:
                    Let indicator be 0.0
                    If y[i] is equal to class_idx:
                        Set indicator to 1.0
                    Set grad_val to grad_val plus design_matrix[i][j] multiplied by (indicator minus probabilities[i][class_idx])
                Append grad_val to class_grad
            Append class_grad to gradient
        
        Note: Simplified Hessian approximation (diagonal)
        Let hessian_diag be []
        For class_idx from 0 to num_classes minus 2:
            Let class_hess be []
            For j from 0 to p:
                Let hess_val be 0.0
                For i from 0 to Length(design_matrix) minus 1:
                    Let prob_val be probabilities[i][class_idx]
                    Set hess_val to hess_val plus design_matrix[i][j] multiplied by design_matrix[i][j] multiplied by prob_val multiplied by (1.0 minus prob_val)
                If hess_val is greater than 1e-8:
                    Append hess_val to class_hess
                Otherwise:
                    Append 1e-8 to class_hess
            Append class_hess to hessian_diag
        
        Note: Update coefficients
        Let max_change be 0.0
        For class_idx from 0 to num_classes minus 2:
            For j from 0 to p:
                Let update be gradient[class_idx][j] / hessian_diag[class_idx][j]
                Set coefficients[class_idx][j] to coefficients[class_idx][j] plus update
                If MathOps.abs(update) is greater than max_change:
                    Set max_change to MathOps.abs(update)
        
        If max_change is less than tolerance:
            Break
    
    Note: Flatten coefficients for storage
    Let flat_coef be []
    For class_idx from 0 to num_classes minus 2:
        For j from 0 to p:
            Append coefficients[class_idx][j] to flat_coef
    
    Note: Calculate final log-likelihood
    Let log_likelihood be 0.0
    For i from 0 to Length(design_matrix) minus 1:
        Let prob_y be probabilities[i][y[i]]
        If prob_y is greater than 1e-15:
            Set log_likelihood to log_likelihood plus MathOps.log(prob_y)
    
    Note: Create model structure
    Let model be RegressionModel with:
        coefficients is equal to flat_coef,
        intercept is equal to 0.0,
        model_type is equal to "Multinomial Logistic Regression",
        n_observations is equal to Length(X) as Integer,
        n_parameters is equal to (num_classes minus 1) multiplied by (p plus 1),
        r_squared is equal to 0.0,
        adjusted_r_squared is equal to 0.0,
        log_likelihood is equal to log_likelihood,
        aic is equal to -2.0 multiplied by log_likelihood plus 2.0 multiplied by ((num_classes minus 1) multiplied by (p plus 1)) as Float,
        bic is equal to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by ((num_classes minus 1) multiplied by (p plus 1)) as Float,
        residual_standard_error is equal to 0.0,
        f_statistic is equal to 0.0,
        p_value_f is equal to 0.0,
        has_intercept is equal to true
    
    Return model

Process called "ordinal_logistic_regression" that takes X as List[List[Float]], y as List[Integer] returns RegressionModel:
    Note: Fit ordinal logistic regression for ordered categorical outcomes
    Note: Models cumulative odds using proportional odds assumption
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 10:
        Throw Errors.InvalidInput with "At least 10 observations required for ordinal logistic regression"
    
    Note: Find unique y values and sort them
    Let unique_y be []
    For i from 0 to Length(y) minus 1:
        Let found be false
        For j from 0 to Length(unique_y) minus 1:
            If y[i] is equal to unique_y[j]:
                Set found to true
                Break
        If !found:
            Append y[i] to unique_y
    
    Note: Sort unique_y
    For i from 0 to Length(unique_y) minus 1:
        For j from i plus 1 to Length(unique_y) minus 1:
            If unique_y[i] is greater than unique_y[j]:
                Let temp be unique_y[i]
                Set unique_y[i] to unique_y[j]
                Set unique_y[j] to temp
    
    Let num_categories be Length(unique_y)
    If num_categories is less than 3:
        Throw Errors.InvalidInput with "Need at least 3 ordinal categories"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix without intercept (cutpoints serve as intercepts)
    Let design_matrix be X
    
    Note: Initialize parameters: (num_categories-1) cutpoints plus p slopes
    Let cutpoints be []
    For k from 0 to num_categories minus 2:
        Append (k as Float) minus (num_categories as Float) / 2.0 to cutpoints
    
    Let slopes be []
    For j from 0 to p minus 1:
        Append 0.0 to slopes
    
    Note: Newton-Raphson optimization
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate cumulative probabilities and individual probabilities
        Let cum_probs be []
        Let ind_probs be []
        
        For i from 0 to Length(design_matrix) minus 1:
            Let linear_pred be 0.0
            For j from 0 to p minus 1:
                Set linear_pred to linear_pred plus slopes[j] multiplied by design_matrix[i][j]
            
            Let row_cum_probs be []
            For k from 0 to num_categories minus 2:
                Let logit be cutpoints[k] minus linear_pred
                Let cum_prob be 1.0 / (1.0 plus MathOps.exp(-logit))
                Append cum_prob to row_cum_probs
            
            Let row_ind_probs be []
            For k from 0 to num_categories minus 1:
                If k is equal to 0:
                    Append row_cum_probs[0] to row_ind_probs
                Otherwise if k is equal to num_categories minus 1:
                    Append 1.0 minus row_cum_probs[k minus 1] to row_ind_probs
                Otherwise:
                    Append row_cum_probs[k] minus row_cum_probs[k minus 1] to row_ind_probs
            
            Append row_cum_probs to cum_probs
            Append row_ind_probs to ind_probs
        
        Note: Calculate gradient for cutpoints
        Let cutpoint_gradient be []
        For k from 0 to num_categories minus 2:
            Let grad_val be 0.0
            For i from 0 to Length(design_matrix) minus 1:
                Let y_category be -1
                For cat_idx from 0 to Length(unique_y) minus 1:
                    If y[i] is equal to unique_y[cat_idx]:
                        Set y_category to cat_idx
                        Break
                
                Let contribution be 0.0
                If y_category is less than or equal to k:
                    Set contribution to contribution plus (1.0 minus cum_probs[i][k])
                If y_category is greater than k:
                    Set contribution to contribution minus cum_probs[i][k]
                
                Set grad_val to grad_val plus contribution
            Append grad_val to cutpoint_gradient
        
        Note: Calculate gradient for slopes
        Let slope_gradient be []
        For j from 0 to p minus 1:
            Let grad_val be 0.0
            For i from 0 to Length(design_matrix) minus 1:
                Let y_category be -1
                For cat_idx from 0 to Length(unique_y) minus 1:
                    If y[i] is equal to unique_y[cat_idx]:
                        Set y_category to cat_idx
                        Break
                
                Let contribution be 0.0
                For k from 0 to num_categories minus 2:
                    Let weight be 0.0
                    If y_category is less than or equal to k:
                        Set weight to weight plus (cum_probs[i][k] minus 1.0)
                    If y_category is greater than k:
                        Set weight to weight plus cum_probs[i][k]
                    Set contribution to contribution plus weight
                
                Set grad_val to grad_val plus contribution multiplied by design_matrix[i][j]
            Append grad_val to slope_gradient
        
        Note: Simple step size for updates
        Let step_size be 0.01
        
        Note: Update cutpoints
        Let max_change be 0.0
        For k from 0 to num_categories minus 2:
            Let update be step_size multiplied by cutpoint_gradient[k]
            Set cutpoints[k] to cutpoints[k] plus update
            If MathOps.abs(update) is greater than max_change:
                Set max_change to MathOps.abs(update)
        
        Note: Update slopes
        For j from 0 to p minus 1:
            Let update be step_size multiplied by slope_gradient[j]
            Set slopes[j] to slopes[j] plus update
            If MathOps.abs(update) is greater than max_change:
                Set max_change to MathOps.abs(update)
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate log-likelihood
    Let log_likelihood be 0.0
    For i from 0 to Length(design_matrix) minus 1:
        Let y_category be -1
        For cat_idx from 0 to Length(unique_y) minus 1:
            If y[i] is equal to unique_y[cat_idx]:
                Set y_category to cat_idx
                Break
        
        Let prob_y be ind_probs[i][y_category]
        If prob_y is greater than 1e-15:
            Set log_likelihood to log_likelihood plus MathOps.log(prob_y)
    
    Note: Combine parameters for storage
    Let all_coef be []
    For k from 0 to num_categories minus 2:
        Append cutpoints[k] to all_coef
    For j from 0 to p minus 1:
        Append slopes[j] to all_coef
    
    Note: Create model structure
    Let model be RegressionModel with:
        coefficients is equal to all_coef,
        intercept is equal to 0.0,
        model_type is equal to "Ordinal Logistic Regression",
        n_observations is equal to Length(X) as Integer,
        n_parameters is equal to (num_categories minus 1) plus p,
        r_squared is equal to 0.0,
        adjusted_r_squared is equal to 0.0,
        log_likelihood is equal to log_likelihood,
        aic is equal to -2.0 multiplied by log_likelihood plus 2.0 multiplied by ((num_categories minus 1) plus p) as Float,
        bic is equal to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by ((num_categories minus 1) plus p) as Float,
        residual_standard_error is equal to 0.0,
        f_statistic is equal to 0.0,
        p_value_f is equal to 0.0,
        has_intercept is equal to false
    
    Return model

Process called "conditional_logistic_regression" that takes X as List[List[Float]], y as List[Integer], strata as List[Integer] returns RegressionModel:
    Note: Fit conditional logistic regression for matched case-control data
    Note: Controls for stratum-specific effects in matched designs
    
    If Length(X) does not equal Length(y) || Length(X) does not equal Length(strata):
        Throw Errors.InvalidInput with "X, y, and strata must have the same number of observations"
    
    If Length(X) is less than 4:
        Throw Errors.InvalidInput with "At least 4 observations required for conditional logistic regression"
    
    Note: Check that y contains only 0 and 1
    For i from 0 to Length(y) minus 1:
        If y[i] does not equal 0 && y[i] does not equal 1:
            Throw Errors.InvalidInput with "y values must be 0 or 1 for conditional logistic regression"
    
    Let p be Length(X[0]) as Integer
    
    Note: Group observations by strata
    Let strata_groups be Dictionary[Integer, List[Integer]]
    For i from 0 to Length(strata) minus 1:
        Let stratum_id be strata[i]
        If !Contains(strata_groups, stratum_id):
            Set strata_groups[stratum_id] to []
        Append i to strata_groups[stratum_id]
    
    Note: Filter strata that have both cases and controls
    Let valid_strata be []
    For Each stratum_id in Keys(strata_groups):
        Let indices be strata_groups[stratum_id]
        Let has_case be false
        Let has_control be false
        For Each idx in indices:
            If y[idx] is equal to 1:
                Set has_case to true
            If y[idx] is equal to 0:
                Set has_control to true
        If has_case && has_control:
            Append stratum_id to valid_strata
    
    If Length(valid_strata) is less than 2:
        Throw Errors.InvalidInput with "Need at least 2 strata with both cases and controls"
    
    Note: Initialize coefficients
    Let coefficients be []
    For j from 0 to p minus 1:
        Append 0.0 to coefficients
    
    Note: Newton-Raphson optimization for conditional likelihood
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Let gradient be []
        For j from 0 to p minus 1:
            Append 0.0 to gradient
        
        Let hessian_diag be []
        For j from 0 to p minus 1:
            Append 0.0 to hessian_diag
        
        Note: Calculate gradient and Hessian contributions from each stratum
        For Each stratum_id in valid_strata:
            Let indices be strata_groups[stratum_id]
            Let stratum_size be Length(indices)
            
            Note: Calculate linear predictors for this stratum
            Let linear_preds be []
            For Each idx in indices:
                Let linear_pred be 0.0
                For j from 0 to p minus 1:
                    Set linear_pred to linear_pred plus coefficients[j] multiplied by X[idx][j]
                Append linear_pred to linear_preds
            
            Note: Calculate probabilities using softmax within stratum
            Let max_pred be linear_preds[0]
            For k from 1 to Length(linear_preds) minus 1:
                If linear_preds[k] is greater than max_pred:
                    Set max_pred to linear_preds[k]
            
            Let exp_sum be 0.0
            Let exp_preds be []
            For k from 0 to Length(linear_preds) minus 1:
                Let exp_pred be MathOps.exp(linear_preds[k] minus max_pred)
                Append exp_pred to exp_preds
                Set exp_sum to exp_sum plus exp_pred
            
            Let probs be []
            For k from 0 to Length(exp_preds) minus 1:
                Append exp_preds[k] / exp_sum to probs
            
            Note: Add gradient contribution from this stratum
            For k from 0 to stratum_size minus 1:
                Let idx be indices[k]
                For j from 0 to p minus 1:
                    Let contribution be X[idx][j] multiplied by ((y[idx] as Float) minus probs[k])
                    Set gradient[j] to gradient[j] plus contribution
            
            Note: Add Hessian diagonal approximation
            For k from 0 to stratum_size minus 1:
                Let idx be indices[k]
                For j from 0 to p minus 1:
                    Let variance be probs[k] multiplied by (1.0 minus probs[k])
                    Set hessian_diag[j] to hessian_diag[j] plus X[idx][j] multiplied by X[idx][j] multiplied by variance
        
        Note: Update coefficients
        Let max_change be 0.0
        For j from 0 to p minus 1:
            If hessian_diag[j] is greater than 1e-8:
                Let update be gradient[j] / hessian_diag[j]
                Set coefficients[j] to coefficients[j] plus update
                If MathOps.abs(update) is greater than max_change:
                    Set max_change to MathOps.abs(update)
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate conditional log-likelihood
    Let log_likelihood be 0.0
    For Each stratum_id in valid_strata:
        Let indices be strata_groups[stratum_id]
        
        Let linear_preds be []
        For Each idx in indices:
            Let linear_pred be 0.0
            For j from 0 to p minus 1:
                Set linear_pred to linear_pred plus coefficients[j] multiplied by X[idx][j]
            Append linear_pred to linear_preds
        
        Note: Calculate softmax probabilities
        Let max_pred be linear_preds[0]
        For k from 1 to Length(linear_preds) minus 1:
            If linear_preds[k] is greater than max_pred:
                Set max_pred to linear_preds[k]
        
        Let exp_sum be 0.0
        For k from 0 to Length(linear_preds) minus 1:
            Let exp_pred be MathOps.exp(linear_preds[k] minus max_pred)
            Set exp_sum to exp_sum plus exp_pred
        
        Note: Add log probability of observed outcomes in this stratum
        For k from 0 to Length(indices) minus 1:
            If y[indices[k]] is equal to 1:
                Let prob be MathOps.exp(linear_preds[k] minus max_pred) / exp_sum
                If prob is greater than 1e-15:
                    Set log_likelihood to log_likelihood plus MathOps.log(prob)
    
    Let n be Length(X) as Float
    
    Note: Create model structure
    Let model be RegressionModel with:
        coefficients is equal to coefficients,
        intercept is equal to 0.0,
        model_type is equal to "Conditional Logistic Regression",
        n_observations is equal to Length(X) as Integer,
        n_parameters is equal to p,
        r_squared is equal to 0.0,
        adjusted_r_squared is equal to 0.0,
        log_likelihood is equal to log_likelihood,
        aic is equal to -2.0 multiplied by log_likelihood plus 2.0 multiplied by (p as Float),
        bic is equal to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by (p as Float),
        residual_standard_error is equal to 0.0,
        f_statistic is equal to 0.0,
        p_value_f is equal to 0.0,
        has_intercept is equal to false
    
    Return model

Note: =====================================================================
Note: REGULARIZATION METHODS OPERATIONS
Note: =====================================================================

Process called "ridge_regression" that takes X as List[List[Float]], y as List[Float], lambda as Float returns RegressionModel:
    Note: Fit ridge regression with L2 penalty: ||y minus Xβ||² plus λ||β||²
    Note: Reduces overfitting by shrinking coefficients. Good for multicollinearity
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If lambda is less than 0.0:
        Throw Errors.InvalidInput with "Lambda penalty parameter must be non-negative"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]  Note: Intercept column
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Convert to matrix operations
    Let X_matrix be LinAlg.create_matrix(design_matrix, "Float")
    
    Note: Compute X'X and X'y
    Let X_transpose be LinAlg.matrix_transpose(X_matrix)
    Let XtX be LinAlg.multiply_matrices(X_transpose, X_matrix)
    Let Xty be LinAlg.matrix_vector_multiply(design_matrix, y)
    
    Note: Add ridge penalty to diagonal: X'X plus λI
    Let identity_size be p plus 1
    Let lambda_I be LinAlg.create_identity_matrix(identity_size)
    
    Note: Scale identity matrix by lambda (skip intercept term)
    For i from 1 to identity_size minus 1:  Note: Start from 1 to skip intercept
        Set lambda_I.entries[i][i] to lambda
    Set lambda_I.entries[0][0] to 0.0  Note: Don't penalize intercept
    
    Note: Solve ridge normal equations: β is equal to (X'X plus λI)⁻¹X'y
    Let regularized_XtX be LinAlg.add_matrices(XtX, lambda_I)
    Let regularized_inv be LinAlg.matrix_inverse(regularized_XtX, "LU")
    Let coefficients_vector be LinAlg.matrix_vector_multiply(regularized_inv.entries, Xty)
    
    Note: Calculate fitted values and residuals
    Let fitted_values be LinAlg.matrix_vector_multiply(design_matrix, coefficients_vector)
    Let residuals be []
    Let rss be 0.0
    
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let adjusted_r_squared be 1.0 minus (rss / (n minus (p plus 1) as Float)) / (tss / (n minus 1.0))
    
    Note: Calculate approximate standard errors (more complex for ridge)
    Let mse be rss / (n minus (p plus 1) as Float)
    Let standard_errors be []
    For i from 0 to p:
        Let se_approx be MathOps.square_root(mse multiplied by regularized_inv.entries[i][i], 15).result
        Append se_approx to standard_errors
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Ridge Regression"
    Set model.intercept to coefficients_vector[0]
    Set model.coefficients to coefficients_vector[1:]
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable for ridge
    Set model.p_value to 1.0
    Set model.standard_errors to standard_errors[1:]
    Set model.t_statistics to []  Note: T-tests not applicable for ridge
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "lasso_regression" that takes X as List[List[Float]], y as List[Float], lambda as Float returns RegressionModel:
    Note: Fit LASSO regression with L1 penalty: ||y minus Xβ||² plus λ||β||₁
    Note: Performs feature selection by setting some coefficients to zero
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If lambda is less than 0.0:
        Throw Errors.InvalidInput with "Lambda penalty parameter must be non-negative"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Standardize features for LASSO
    Let X_standardized be []
    Let feature_means be []
    Let feature_stds be []
    
    For j from 0 to p minus 1:
        Let column be []
        For i from 0 to Length(X) minus 1:
            Append X[i][j] to column
        
        Let col_mean be Stats.calculate_arithmetic_mean(column, [])
        Let col_std be Stats.calculate_standard_deviation(column, false)
        Append col_mean to feature_means
        Append col_std to feature_stds
    
    Note: Create standardized design matrix with intercept
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]  Note: Intercept column (not standardized)
        For j from 0 to p minus 1:
            If feature_stds[j] is greater than 1e-10:
                Let standardized_val be (X[i][j] minus feature_means[j]) / feature_stds[j]
                Append standardized_val to row
            Otherwise:
                Append 0.0 to row  Note: Constant feature
        Append row to X_standardized
    
    Note: Create optimization problem for coordinate descent
    Let problem be OptCore.OptimizationProblem
    Set problem.initial_point to List.repeat(0.0, p plus 1)  Note: Initialize coefficients to zero
    Set problem.objective_function to "lasso_objective"  Note: Placeholder
    
    Note: Configure coordinate descent
    Let cd_config be Dictionary[String, String]
    Set cd_config["lambda_l1"] to lambda
    Set cd_config["lambda_l2"] to 0.0  Note: Pure LASSO (no ridge component)
    
    Let convergence be OptCore.ConvergenceCriteria
    Set convergence.gradient_tolerance to 1e-6
    Set convergence.max_iterations to 1000
    
    Note: Run coordinate descent optimization
    Let lasso_result be OptCore.coordinate_descent(problem, cd_config, convergence)
    Let coefficients_standardized be lasso_result.solution
    
    Note: Transform coefficients back to original scale
    Let coefficients_original be []
    Let intercept_original be coefficients_standardized[0]
    
    For j from 1 to p:  Note: Skip intercept
        Let original_coef be coefficients_standardized[j] / feature_stds[j minus 1]
        Append original_coef to coefficients_original
        Set intercept_original to intercept_original minus original_coef multiplied by feature_means[j minus 1]
    
    Note: Calculate fitted values using original coefficients
    Let fitted_values be []
    For i from 0 to Length(X) minus 1:
        Let fitted be intercept_original
        For j from 0 to p minus 1:
            Set fitted to fitted plus coefficients_original[j] multiplied by X[i][j]
        Append fitted to fitted_values
    
    Note: Calculate residuals and R-squared
    Let residuals be []
    Let rss be 0.0
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    
    Note: Count non-zero coefficients for effective degrees of freedom
    Let nonzero_count be 1  Note: Always count intercept
    For j from 0 to p minus 1:
        If MathOps.abs(coefficients_original[j]) is greater than 1e-8:
            Set nonzero_count to nonzero_count plus 1
    
    Let adjusted_r_squared be 1.0 minus (rss / (n minus nonzero_count as Float)) / (tss / (n minus 1.0))
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "LASSO Regression"
    Set model.intercept to intercept_original
    Set model.coefficients to coefficients_original
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable for LASSO
    Set model.p_value to 1.0
    Set model.standard_errors to []  Note: Bootstrap required for SE in LASSO
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "elastic_net_regression" that takes X as List[List[Float]], y as List[Float], alpha as Float, lambda as Float returns RegressionModel:
    Note: Fit elastic net combining L1 and L2 penalties: (1-α)L2 plus αL1
    Note: Balances ridge and LASSO properties. α=0 is ridge, α=1 is LASSO
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If alpha is less than 0.0 || alpha is greater than 1.0:
        Throw Errors.InvalidInput with "Alpha must be between 0 and 1"
    
    If lambda is less than 0.0:
        Throw Errors.InvalidInput with "Lambda penalty parameter must be non-negative"
    
    Note: Use coordinate descent with both L1 and L2 penalties
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Standardize features
    Let X_standardized be []
    Let feature_means be []
    Let feature_stds be []
    
    For j from 0 to p minus 1:
        Let column be []
        For i from 0 to Length(X) minus 1:
            Append X[i][j] to column
        
        Let col_mean be Stats.calculate_arithmetic_mean(column, [])
        Let col_std be Stats.calculate_standard_deviation(column, false)
        Append col_mean to feature_means
        Append col_std to feature_stds
    
    Note: Create standardized design matrix with intercept
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]  Note: Intercept column
        For j from 0 to p minus 1:
            If feature_stds[j] is greater than 1e-10:
                Let standardized_val be (X[i][j] minus feature_means[j]) / feature_stds[j]
                Append standardized_val to row
            Otherwise:
                Append 0.0 to row
        Append row to X_standardized
    
    Note: Create optimization problem for coordinate descent
    Let problem be OptCore.OptimizationProblem
    Set problem.initial_point to List.repeat(0.0, p plus 1)
    Set problem.objective_function to "elastic_net_objective"
    
    Note: Configure coordinate descent with both penalties
    Let cd_config be Dictionary[String, String]
    Set cd_config["lambda_l1"] to (alpha multiplied by lambda) as String
    Set cd_config["lambda_l2"] to ((1.0 minus alpha) multiplied by lambda) as String
    
    Let convergence be OptCore.ConvergenceCriteria
    Set convergence.gradient_tolerance to 1e-6
    Set convergence.max_iterations to 1000
    
    Note: Run coordinate descent optimization
    Let elastic_result be OptCore.coordinate_descent(problem, cd_config, convergence)
    Let coefficients_standardized be elastic_result.solution
    
    Note: Transform coefficients back to original scale
    Let coefficients_original be []
    Let intercept_original be coefficients_standardized[0]
    
    For j from 1 to p:
        Let original_coef be coefficients_standardized[j] / feature_stds[j minus 1]
        Append original_coef to coefficients_original
        Set intercept_original to intercept_original minus original_coef multiplied by feature_means[j minus 1]
    
    Note: Calculate fitted values and residuals
    Let fitted_values be []
    For i from 0 to Length(X) minus 1:
        Let fitted be intercept_original
        For j from 0 to p minus 1:
            Set fitted to fitted plus coefficients_original[j] multiplied by X[i][j]
        Append fitted to fitted_values
    
    Let residuals be []
    Let rss be 0.0
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    
    Note: Count non-zero coefficients for effective degrees of freedom
    Let nonzero_count be 1
    For j from 0 to p minus 1:
        If MathOps.abs(coefficients_original[j]) is greater than 1e-8:
            Set nonzero_count to nonzero_count plus 1
    
    Let adjusted_r_squared be 1.0 minus (rss / (n minus nonzero_count as Float)) / (tss / (n minus 1.0))
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Elastic Net Regression"
    Set model.intercept to intercept_original
    Set model.coefficients to coefficients_original
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable
    Set model.p_value to 1.0
    Set model.standard_errors to []
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "adaptive_lasso" that takes X as List[List[Float]], y as List[Float], weights as List[Float], lambda as Float returns RegressionModel:
    Note: Fit adaptive LASSO with weighted L1 penalty using adaptive weights
    Note: Oracle properties under regularity conditions. Better feature selection
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(weights) does not equal Length(X[0]):
        Throw Errors.InvalidInput with "weights must have same length as number of features"
    
    If lambda is less than 0.0:
        Throw Errors.InvalidInput with "Lambda penalty parameter must be non-negative"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Check for non-negative weights
    For j from 0 to p minus 1:
        If weights[j] is less than 0.0:
            Throw Errors.InvalidInput with "All adaptive weights must be non-negative"
    
    Note: Standardize features for adaptive LASSO
    Let X_standardized be []
    Let feature_means be []
    Let feature_stds be []
    
    For j from 0 to p minus 1:
        Let column be []
        For i from 0 to Length(X) minus 1:
            Append X[i][j] to column
        
        Let col_mean be Stats.calculate_arithmetic_mean(column, [])
        Let col_std be Stats.calculate_standard_deviation(column, false)
        Append col_mean to feature_means
        Append col_std to feature_stds
    
    For i from 0 to Length(X) minus 1:
        Let standardized_row be []
        For j from 0 to p minus 1:
            If feature_stds[j] is greater than 1e-8:
                Append (X[i][j] minus feature_means[j]) / feature_stds[j] to standardized_row
            Otherwise:
                Append 0.0 to standardized_row
        Append standardized_row to X_standardized
    
    Note: Center y
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let y_centered be []
    For i from 0 to Length(y) minus 1:
        Append y[i] minus y_mean to y_centered
    
    Note: Initialize coefficients
    Let coefficients be []
    For j from 0 to p minus 1:
        Append 0.0 to coefficients
    
    Note: Coordinate descent with adaptive weights
    Let max_iterations be 1000
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Let max_change be 0.0
        
        For j from 0 to p minus 1:
            Note: Skip if weight is too large (effectively removes feature)
            If weights[j] is greater than 1e8:
                Set coefficients[j] to 0.0
                Continue
            
            Note: Calculate partial residual
            Let partial_residual be []
            For i from 0 to Length(X_standardized) minus 1:
                Let residual be y_centered[i]
                For k from 0 to p minus 1:
                    If k does not equal j:
                        Set residual to residual minus coefficients[k] multiplied by X_standardized[i][k]
                Append residual to partial_residual
            
            Note: Calculate correlation with feature j
            Let correlation be 0.0
            For i from 0 to Length(X_standardized) minus 1:
                Set correlation to correlation plus X_standardized[i][j] multiplied by partial_residual[i]
            
            Note: Apply soft thresholding with adaptive weight
            Let threshold be lambda multiplied by weights[j] multiplied by n
            Let old_coef be coefficients[j]
            
            If correlation is greater than threshold:
                Set coefficients[j] to correlation minus threshold
            Otherwise if correlation is less than -threshold:
                Set coefficients[j] to correlation plus threshold
            Otherwise:
                Set coefficients[j] to 0.0
            
            Let change be MathOps.abs(coefficients[j] minus old_coef)
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Transform coefficients back to original scale
    Let coefficients_original be []
    For j from 0 to p minus 1:
        If feature_stds[j] is greater than 1e-8:
            Append coefficients[j] / feature_stds[j] to coefficients_original
        Otherwise:
            Append 0.0 to coefficients_original
    
    Note: Calculate intercept
    Let intercept_original be y_mean
    For j from 0 to p minus 1:
        Set intercept_original to intercept_original minus coefficients_original[j] multiplied by feature_means[j]
    
    Note: Calculate residuals and fitted values
    Let residuals be []
    Let fitted_values be []
    Let rss be 0.0
    
    For i from 0 to Length(X) minus 1:
        Let fitted be intercept_original
        For j from 0 to p minus 1:
            Set fitted to fitted plus coefficients_original[j] multiplied by X[i][j]
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    
    Note: Count non-zero coefficients for effective degrees of freedom
    Let nonzero_count be 1
    For j from 0 to p minus 1:
        If MathOps.abs(coefficients_original[j]) is greater than 1e-8:
            Set nonzero_count to nonzero_count plus 1
    
    Let adjusted_r_squared be 1.0 minus (rss / (n minus nonzero_count as Float)) / (tss / (n minus 1.0))
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Adaptive LASSO"
    Set model.intercept to intercept_original
    Set model.coefficients to coefficients_original
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to nonzero_count
    Set model.residual_standard_error to MathOps.sqrt(rss / (n minus nonzero_count as Float))
    Set model.aic to n multiplied by MathOps.log(rss / n) plus 2.0 multiplied by nonzero_count as Float
    Set model.bic to n multiplied by MathOps.log(rss / n) plus MathOps.log(n) multiplied by nonzero_count as Float
    Set model.log_likelihood to -0.5 multiplied by n multiplied by (MathOps.log(2.0 multiplied by MathOps.pi()) plus MathOps.log(rss / n) plus 1.0)
    Set model.has_intercept to true
    Set model.standard_errors to []  Note: Bootstrap required for SE in adaptive LASSO
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Note: =====================================================================
Note: ROBUST REGRESSION OPERATIONS
Note: =====================================================================

Process called "huber_regression" that takes X as List[List[Float]], y as List[Float], epsilon as Float returns RegressionModel:
    Note: Fit robust regression using Huber loss function
    Note: Less sensitive to outliers than OLS. Quadratic for small residuals, linear for large
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If epsilon is less than or equal to 0.0:
        Throw Errors.InvalidInput with "Epsilon parameter must be positive"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Initialize coefficients (including intercept)
    Let coefficients be []
    For j from 0 to p:
        Append 0.0 to coefficients
    
    Note: Iteratively Reweighted Least Squares (IRLS) for Huber regression
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate current residuals and fitted values
        Let residuals be []
        For i from 0 to Length(design_matrix) minus 1:
            Let fitted be 0.0
            For j from 0 to p:
                Set fitted to fitted plus coefficients[j] multiplied by design_matrix[i][j]
            Append y[i] minus fitted to residuals
        
        Note: Calculate Huber weights
        Let weights be []
        For i from 0 to Length(residuals) minus 1:
            Let abs_residual be MathOps.abs(residuals[i])
            If abs_residual is less than or equal to epsilon:
                Append 1.0 to weights
            Otherwise:
                Append epsilon / abs_residual to weights
        
        Note: Set up weighted normal equations: X'WX β is equal to X'Wy
        Note: Create weighted design matrix and response
        Let XtWX be []
        For i from 0 to p:
            Let row be []
            For j from 0 to p:
                Let sum be 0.0
                For k from 0 to Length(design_matrix) minus 1:
                    Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by design_matrix[k][j]
                Append sum to row
            Append row to XtWX
        
        Let XtWy be []
        For i from 0 to p:
            Let sum be 0.0
            For k from 0 to Length(design_matrix) minus 1:
                Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by y[k]
            Append sum to XtWy
        
        Note: Solve weighted normal equations using simple Gaussian elimination
        Let old_coefficients be coefficients
        Let system_matrix be []
        For i from 0 to p:
            Let aug_row be []
            For j from 0 to p:
                Append XtWX[i][j] to aug_row
            Append XtWy[i] to aug_row
            Append aug_row to system_matrix
        
        Note: Forward elimination
        For pivot from 0 to p minus 1:
            Note: Find maximum element in column for partial pivoting
            Let max_row be pivot
            For i from pivot plus 1 to p:
                If MathOps.abs(system_matrix[i][pivot]) is greater than MathOps.abs(system_matrix[max_row][pivot]):
                    Set max_row to i
            
            Note: Swap rows
            If max_row does not equal pivot:
                Let temp_row be system_matrix[pivot]
                Set system_matrix[pivot] to system_matrix[max_row]
                Set system_matrix[max_row] to temp_row
            
            Note: Check for singular matrix
            If MathOps.abs(system_matrix[pivot][pivot]) is less than 1e-10:
                Throw Errors.ComputationError with "Singular matrix in Huber regression"
            
            Note: Eliminate column
            For i from pivot plus 1 to p:
                Let factor be system_matrix[i][pivot] / system_matrix[pivot][pivot]
                For j from pivot to p:
                    Set system_matrix[i][j] to system_matrix[i][j] minus factor multiplied by system_matrix[pivot][j]
        
        Note: Back substitution
        Set coefficients to []
        For i from 0 to p:
            Append 0.0 to coefficients
        
        For i from p minus 1 down to 0:
            Let sum be system_matrix[i][p]
            For j from i plus 1 to p minus 1:
                Set sum to sum minus system_matrix[i][j] multiplied by coefficients[j]
            Set coefficients[i] to sum / system_matrix[i][i]
        
        Note: Check convergence
        Let max_change be 0.0
        For j from 0 to p:
            Let change be MathOps.abs(coefficients[j] minus old_coefficients[j])
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate final residuals and fitted values
    Let final_residuals be []
    Let fitted_values be []
    Let huber_loss be 0.0
    
    For i from 0 to Length(design_matrix) minus 1:
        Let fitted be 0.0
        For j from 0 to p:
            Set fitted to fitted plus coefficients[j] multiplied by design_matrix[i][j]
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to final_residuals
        
        Note: Calculate Huber loss
        Let abs_residual be MathOps.abs(residual)
        If abs_residual is less than or equal to epsilon:
            Set huber_loss to huber_loss plus 0.5 multiplied by residual multiplied by residual
        Otherwise:
            Set huber_loss to huber_loss plus epsilon multiplied by abs_residual minus 0.5 multiplied by epsilon multiplied by epsilon
    
    Note: Calculate robust scale estimate
    Let median_abs_residual be Stats.calculate_median(
        List.map(final_residuals, Function(r) -> MathOps.abs(r))
    )
    Let robust_scale be median_abs_residual / 0.6745  Note: Conversion to approximate standard deviation
    
    Note: Extract intercept and slopes
    Let intercept be coefficients[0]
    Let slopes be []
    For j from 1 to p:
        Append coefficients[j] to slopes
    
    Note: Pseudo R-squared based on Huber loss
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let total_huber_loss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev_abs be MathOps.abs(y[i] minus y_mean)
        If dev_abs is less than or equal to epsilon:
            Set total_huber_loss to total_huber_loss plus 0.5 multiplied by (y[i] minus y_mean) multiplied by (y[i] minus y_mean)
        Otherwise:
            Set total_huber_loss to total_huber_loss plus epsilon multiplied by dev_abs minus 0.5 multiplied by epsilon multiplied by epsilon
    
    Let pseudo_r_squared be 1.0 minus huber_loss / total_huber_loss
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Huber Regression"
    Set model.intercept to intercept
    Set model.coefficients to slopes
    Set model.r_squared to pseudo_r_squared
    Set model.adjusted_r_squared to pseudo_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable for robust regression
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to p plus 1
    Set model.residual_standard_error to robust_scale
    Set model.aic to 2.0 multiplied by (p plus 1) as Float plus 2.0 multiplied by huber_loss / robust_scale
    Set model.bic to MathOps.log(n) multiplied by (p plus 1) as Float plus 2.0 multiplied by huber_loss / robust_scale
    Set model.log_likelihood to -huber_loss / robust_scale
    Set model.has_intercept to true
    Set model.standard_errors to []  Note: Robust standard errors require additional computation
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to final_residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "quantile_regression" that takes X as List[List[Float]], y as List[Float], quantile as Float returns RegressionModel:
    Note: Fit quantile regression for specified quantile (0.5 is equal to median regression)
    Note: Models conditional quantiles rather than conditional means
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If quantile is less than or equal to 0.0 || quantile is greater than or equal to 1.0:
        Throw Errors.InvalidInput with "Quantile must be between 0 and 1 (exclusive)"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Initialize coefficients using ordinary least squares as starting point
    Let ols_result be multiple_linear_regression(X, y, true)
    Let coefficients be [ols_result.intercept]
    For j from 0 to Length(ols_result.coefficients) minus 1:
        Append ols_result.coefficients[j] to coefficients
    
    Note: Iteratively Reweighted Least Squares for quantile regression
    Let max_iterations be 200
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate current residuals
        Let residuals be []
        For i from 0 to Length(design_matrix) minus 1:
            Let fitted be 0.0
            For j from 0 to p:
                Set fitted to fitted plus coefficients[j] multiplied by design_matrix[i][j]
            Append y[i] minus fitted to residuals
        
        Note: Calculate asymmetric weights for quantile loss
        Let weights be []
        For i from 0 to Length(residuals) minus 1:
            If MathOps.abs(residuals[i]) is less than 1e-8:
                Append 1.0 to weights  Note: Small constant for zero residuals
            Otherwise if residuals[i] is greater than 0.0:
                Append quantile / MathOps.abs(residuals[i]) to weights
            Otherwise:
                Append (1.0 minus quantile) / MathOps.abs(residuals[i]) to weights
        
        Note: Set up weighted normal equations: X'WX β is equal to X'Wy
        Let XtWX be []
        For i from 0 to p:
            Let row be []
            For j from 0 to p:
                Let sum be 0.0
                For k from 0 to Length(design_matrix) minus 1:
                    Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by design_matrix[k][j]
                Append sum to row
            Append row to XtWX
        
        Let XtWy be []
        For i from 0 to p:
            Let sum be 0.0
            For k from 0 to Length(design_matrix) minus 1:
                Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by y[k]
            Append sum to XtWy
        
        Note: Solve weighted system using Gaussian elimination
        Let old_coefficients be coefficients
        Let system_matrix be []
        For i from 0 to p:
            Let aug_row be []
            For j from 0 to p:
                Append XtWX[i][j] to aug_row
            Append XtWy[i] to aug_row
            Append aug_row to system_matrix
        
        Note: Forward elimination with partial pivoting
        For pivot from 0 to p minus 1:
            Let max_row be pivot
            For i from pivot plus 1 to p:
                If MathOps.abs(system_matrix[i][pivot]) is greater than MathOps.abs(system_matrix[max_row][pivot]):
                    Set max_row to i
            
            If max_row does not equal pivot:
                Let temp_row be system_matrix[pivot]
                Set system_matrix[pivot] to system_matrix[max_row]
                Set system_matrix[max_row] to temp_row
            
            If MathOps.abs(system_matrix[pivot][pivot]) is less than 1e-10:
                Throw Errors.ComputationError with "Singular matrix in quantile regression"
            
            For i from pivot plus 1 to p:
                Let factor be system_matrix[i][pivot] / system_matrix[pivot][pivot]
                For j from pivot to p:
                    Set system_matrix[i][j] to system_matrix[i][j] minus factor multiplied by system_matrix[pivot][j]
        
        Note: Back substitution
        Set coefficients to []
        For i from 0 to p:
            Append 0.0 to coefficients
        
        For i from p minus 1 down to 0:
            Let sum be system_matrix[i][p]
            For j from i plus 1 to p minus 1:
                Set sum to sum minus system_matrix[i][j] multiplied by coefficients[j]
            Set coefficients[i] to sum / system_matrix[i][i]
        
        Note: Check convergence
        Let max_change be 0.0
        For j from 0 to p:
            Let change be MathOps.abs(coefficients[j] minus old_coefficients[j])
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate final residuals, fitted values, and quantile loss
    Let final_residuals be []
    Let fitted_values be []
    Let quantile_loss be 0.0
    
    For i from 0 to Length(design_matrix) minus 1:
        Let fitted be 0.0
        For j from 0 to p:
            Set fitted to fitted plus coefficients[j] multiplied by design_matrix[i][j]
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to final_residuals
        
        Note: Calculate asymmetric quantile loss
        If residual is greater than 0.0:
            Set quantile_loss to quantile_loss plus quantile multiplied by residual
        Otherwise:
            Set quantile_loss to quantile_loss minus (1.0 minus quantile) multiplied by residual
    
    Note: Calculate pseudo R-squared based on quantile loss
    Let y_quantile be Stats.calculate_quantile(y, quantile)
    Let total_quantile_loss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_quantile
        If dev is greater than 0.0:
            Set total_quantile_loss to total_quantile_loss plus quantile multiplied by dev
        Otherwise:
            Set total_quantile_loss to total_quantile_loss minus (1.0 minus quantile) multiplied by dev
    
    Let pseudo_r_squared be 1.0 minus quantile_loss / total_quantile_loss
    
    Note: Calculate robust scale estimate using inter-quantile range
    Let lower_quantile be Stats.calculate_quantile(final_residuals, 0.25)
    Let upper_quantile be Stats.calculate_quantile(final_residuals, 0.75)
    Let robust_scale be (upper_quantile minus lower_quantile) / 1.349  Note: IQR to std conversion
    
    Note: Extract intercept and slopes
    Let intercept be coefficients[0]
    Let slopes be []
    For j from 1 to p:
        Append coefficients[j] to slopes
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Quantile Regression (τ=" plus (quantile as String) plus ")"
    Set model.intercept to intercept
    Set model.coefficients to slopes
    Set model.r_squared to pseudo_r_squared
    Set model.adjusted_r_squared to pseudo_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable for quantile regression
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to p plus 1
    Set model.residual_standard_error to robust_scale
    Set model.aic to 2.0 multiplied by (p plus 1) as Float plus 2.0 multiplied by quantile_loss
    Set model.bic to MathOps.log(n) multiplied by (p plus 1) as Float plus 2.0 multiplied by quantile_loss
    Set model.log_likelihood to -quantile_loss
    Set model.has_intercept to true
    Set model.standard_errors to []  Note: Bootstrap required for standard errors
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to final_residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "theil_sen_regression" that takes X as List[List[Float]], y as List[Float] returns RegressionModel:
    Note: Fit Theil-Sen robust regression using median of all pairwise slopes
    Note: Breakdown point of ~29%. Robust to outliers and influential points
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 4:
        Throw Errors.InvalidInput with "At least 4 observations required for Theil-Sen regression"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: For simplicity, implement univariate Theil-Sen first
    If p does not equal 1:
        Throw Errors.InvalidInput with "This implementation supports only univariate Theil-Sen regression"
    
    Note: Calculate all pairwise slopes for univariate case
    Let all_slopes be []
    For i from 0 to n minus 1:
        For j from i plus 1 to n minus 1:
            If MathOps.abs(X[i][0] minus X[j][0]) is greater than 1e-10:
                Let slope be (y[j] minus y[i]) / (X[j][0] minus X[i][0])
                Append slope to all_slopes
    
    If Length(all_slopes) is equal to 0:
        Throw Errors.ComputationError with "Cannot compute Theil-Sen regression: all X values are identical"
    
    Note: Find median slope
    Let median_slope be Stats.calculate_median(all_slopes)
    
    Note: Calculate all pairwise intercepts using the median slope
    Let all_intercepts be []
    For i from 0 to n minus 1:
        Let intercept be y[i] minus median_slope multiplied by X[i][0]
        Append intercept to all_intercepts
    
    Note: Find median intercept
    Let median_intercept be Stats.calculate_median(all_intercepts)
    
    Note: Calculate fitted values and residuals
    Let fitted_values be []
    Let residuals be []
    Let rss be 0.0
    
    For i from 0 to n minus 1:
        Let fitted be median_intercept plus median_slope multiplied by X[i][0]
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to n minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let adjusted_r_squared be 1.0 minus (rss / (n as Float minus 2.0)) / (tss / (n as Float minus 1.0))
    
    Note: Robust scale estimate using median absolute deviation
    Let abs_residuals be []
    For i from 0 to Length(residuals) minus 1:
        Append MathOps.abs(residuals[i]) to abs_residuals
    Let mad be Stats.calculate_median(abs_residuals)
    Let robust_scale be mad / 0.6745  Note: Conversion to approximate standard deviation
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Theil-Sen Regression"
    Set model.intercept to median_intercept
    Set model.coefficients to [median_slope]
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not directly applicable
    Set model.p_value_f to 1.0
    Set model.n_observations to n
    Set model.n_parameters to 2
    Set model.residual_standard_error to robust_scale
    Set model.aic to 2.0 multiplied by 2.0 plus 2.0 multiplied by rss / robust_scale  Note: Approximate AIC
    Set model.bic to MathOps.log(n as Float) multiplied by 2.0 plus 2.0 multiplied by rss / robust_scale
    Set model.log_likelihood to -rss / (2.0 multiplied by robust_scale multiplied by robust_scale)
    Set model.has_intercept to true
    Set model.standard_errors to []  Note: Bootstrap required for standard errors
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "ransac_regression" that takes X as List[List[Float]], y as List[Float], max_trials as Integer, residual_threshold as Float returns RegressionModel:
    Note: Fit robust regression using RANSAC (Random Sample Consensus)
    Note: Iteratively fits models on random subsets, finds largest consensus
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If max_trials is less than 1:
        Throw Errors.InvalidInput with "max_trials must be at least 1"
    
    If residual_threshold is less than or equal to 0.0:
        Throw Errors.InvalidInput with "residual_threshold must be positive"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    Let min_samples be p plus 1  Note: Minimum samples needed to fit model
    
    If n is less than min_samples:
        Throw Errors.InvalidInput with "Not enough observations to fit model"
    
    Note: Track best model found
    Let best_model be Null
    Let best_inlier_count be 0
    Let best_inliers be []
    
    Note: Simple random number generator state (linear congruential)
    Let random_seed be 12345
    
    Process called "next_random" that returns Integer:
        Set random_seed to (random_seed multiplied by 1103515245 plus 12345) % (2^31)
        Return random_seed
    
    For trial from 0 to max_trials minus 1:
        Note: Randomly sample min_samples points
        Let sample_indices be []
        Let attempts be 0
        
        While Length(sample_indices) is less than min_samples && attempts is less than n multiplied by 10:
            Let idx be next_random() % n
            Let found be false
            For k from 0 to Length(sample_indices) minus 1:
                If sample_indices[k] is equal to idx:
                    Set found to true
                    Break
            If !found:
                Append idx to sample_indices
            Set attempts to attempts plus 1
        
        If Length(sample_indices) is less than min_samples:
            Continue  Note: Could not get enough unique samples
        
        Note: Create subset data
        Let X_subset be []
        Let y_subset be []
        For k from 0 to Length(sample_indices) minus 1:
            Let idx be sample_indices[k]
            Append X[idx] to X_subset
            Append y[idx] to y_subset
        
        Note: Fit model on subset (handle potential errors)
        Let subset_model be Null
        Try:
            Set subset_model to multiple_linear_regression(X_subset, y_subset, true)
        Catch:
            Continue  Note: Subset was degenerate, try another
        
        Note: Test model on all data points to find inliers
        Let inliers be []
        For i from 0 to n minus 1:
            Let fitted be subset_model.intercept
            For j from 0 to p minus 1:
                Set fitted to fitted plus subset_model.coefficients[j] multiplied by X[i][j]
            
            Let residual be MathOps.abs(y[i] minus fitted)
            If residual is less than or equal to residual_threshold:
                Append i to inliers
        
        Note: Update best model if this one has more inliers
        If Length(inliers) is greater than best_inlier_count:
            Set best_model to subset_model
            Set best_inlier_count to Length(inliers)
            Set best_inliers to inliers
    
    If best_model is equal to Null:
        Throw Errors.ComputationError with "RANSAC failed to find a consensus model"
    
    Note: Refit model using all inliers for final result
    Let X_inliers be []
    Let y_inliers be []
    For k from 0 to Length(best_inliers) minus 1:
        Let idx be best_inliers[k]
        Append X[idx] to X_inliers
        Append y[idx] to y_inliers
    
    Let final_model be multiple_linear_regression(X_inliers, y_inliers, true)
    
    Note: Calculate fitted values and residuals for all data points
    Let fitted_values be []
    Let residuals be []
    Let rss be 0.0
    
    For i from 0 to n minus 1:
        Let fitted be final_model.intercept
        For j from 0 to p minus 1:
            Set fitted to fitted plus final_model.coefficients[j] multiplied by X[i][j]
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate robust R-squared using only inliers
    Let y_mean be Stats.calculate_arithmetic_mean(y_inliers, [])
    Let inlier_tss be 0.0
    For k from 0 to Length(best_inliers) minus 1:
        Let idx be best_inliers[k]
        Let dev be y[idx] minus y_mean
        Set inlier_tss to inlier_tss plus dev multiplied by dev
    
    Let inlier_rss be 0.0
    For k from 0 to Length(best_inliers) minus 1:
        Let idx be best_inliers[k]
        Let fitted be final_model.intercept
        For j from 0 to p minus 1:
            Set fitted to fitted plus final_model.coefficients[j] multiplied by X[idx][j]
        Let residual be y[idx] minus fitted
        Set inlier_rss to inlier_rss plus residual multiplied by residual
    
    Let robust_r_squared be 1.0 minus inlier_rss / inlier_tss
    
    Note: Robust scale estimate using median of inlier residuals
    Let inlier_residuals be []
    For k from 0 to Length(best_inliers) minus 1:
        Let idx be best_inliers[k]
        Append MathOps.abs(residuals[idx]) to inlier_residuals
    
    Let robust_scale be Stats.calculate_median(inlier_residuals) / 0.6745
    
    Note: Create regression model with RANSAC metadata
    Let model be RegressionModel
    Set model.model_type to "RANSAC Regression"
    Set model.intercept to final_model.intercept
    Set model.coefficients to final_model.coefficients
    Set model.r_squared to robust_r_squared
    Set model.adjusted_r_squared to robust_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable for RANSAC
    Set model.p_value_f to 1.0
    Set model.n_observations to n
    Set model.n_parameters to p plus 1
    Set model.residual_standard_error to robust_scale
    Set model.aic to 2.0 multiplied by (p plus 1) as Float plus 2.0 multiplied by inlier_rss / robust_scale
    Set model.bic to MathOps.log(Length(best_inliers) as Float) multiplied by (p plus 1) as Float plus 2.0 multiplied by inlier_rss / robust_scale
    Set model.log_likelihood to -inlier_rss / (2.0 multiplied by robust_scale multiplied by robust_scale)
    Set model.has_intercept to true
    Set model.standard_errors to final_model.standard_errors
    Set model.t_statistics to final_model.t_statistics
    Set model.coefficient_p_values to final_model.coefficient_p_values
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Note: Additional RANSAC-specific information could be stored in model metadata
    
    Return model

Note: =====================================================================
Note: NONLINEAR REGRESSION OPERATIONS
Note: =====================================================================

Process called "nonlinear_least_squares" that takes X as List[List[Float]], y as List[Float], model_function as String, initial_parameters as List[Float] returns RegressionModel:
    Note: Fit nonlinear regression using Gauss-Newton or Levenberg-Marquardt
    Note: Requires initial parameter estimates and specified functional form
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(initial_parameters) is equal to 0:
        Throw Errors.InvalidInput with "Initial parameters cannot be empty"
    
    Note: Create constrained problem for Levenberg-Marquardt
    Let problem be OptSolvers.ConstrainedProblem
    Set problem.initial_point to initial_parameters
    Set problem.objective_function to model_function
    Set problem.residual_function to model_function plus "_residuals"
    Set problem.jacobian_function to model_function plus "_jacobian"
    
    Note: Configure Levenberg-Marquardt algorithm
    Let lm_config be Dictionary[String, String]
    Set lm_config["max_iterations"] to "200"
    Set lm_config["tolerance"] to "1e-8" 
    Set lm_config["initial_lambda"] to "0.01"
    Set lm_config["lambda_factor"] to "10.0"
    
    Note: Run Levenberg-Marquardt optimization
    Let lm_result be OptSolvers.levenberg_marquardt(problem, lm_config)
    Let coefficients_vector be lm_result.solution
    
    Note: Calculate fitted values using optimized parameters
    Let fitted_values be []
    For i from 0 to Length(X) minus 1:
        Note: This is a placeholder minus real implementation would evaluate model_function
        Let fitted_value be coefficients_vector[0]  Note: Simplified
        For j from 1 to Length(coefficients_vector) minus 1:
            If j minus 1 is less than Length(X[i]):
                Set fitted_value to fitted_value plus coefficients_vector[j] multiplied by X[i][j minus 1]
        Append fitted_value to fitted_values
    
    Note: Calculate residuals and model statistics
    Let residuals be []
    Let rss be 0.0
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let n be Length(y) as Float
    Let p be Length(coefficients_vector) as Float
    Let adjusted_r_squared be 1.0 minus (rss / (n minus p)) / (tss / (n minus 1.0))
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Nonlinear Least Squares"
    Set model.intercept to coefficients_vector[0]
    Set model.coefficients to coefficients_vector[1:]
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test more complex for nonlinear models
    Set model.p_value to 1.0
    Set model.standard_errors to []  Note: Requires more complex calculation
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "exponential_regression" that takes x as List[Float], y as List[Float] returns RegressionModel:
    Note: Fit exponential model: y is equal to ae^(bx). Linearizes via log transformation
    Note: Common for growth/decay processes. Check residuals on original scale
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    Note: Check for positive y values (required for log transformation)
    For i from 0 to Length(y) minus 1:
        If y[i] is less than or equal to 0.0:
            Throw Errors.InvalidInput with "All y values must be positive for exponential regression"
    
    Note: Transform y to log(y) for linearization: log(y) is equal to log(a) plus bx
    Let log_y be []
    For i from 0 to Length(y) minus 1:
        Append MathOps.natural_logarithm(y[i], 15).result to log_y
    
    Note: Fit linear regression on transformed data
    Let linear_model be simple_linear_regression(x, log_y)
    
    Note: Transform coefficients back to original scale
    Let a be MathOps.exponential(linear_model.intercept, 15).result  Note: exp(log(a))
    Let b be linear_model.coefficients[0]  Note: b stays the same
    
    Note: Calculate fitted values on original scale
    Let fitted_values be []
    For i from 0 to Length(x) minus 1:
        Let fitted be a multiplied by MathOps.exponential(b multiplied by x[i], 15).result
        Append fitted to fitted_values
    
    Note: Calculate residuals on original scale
    Let residuals be []
    Let rss be 0.0
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared on original scale
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let n be Length(y) as Float
    Let adjusted_r_squared be 1.0 minus (rss / (n minus 2.0)) / (tss / (n minus 1.0))
    
    Note: Create exponential regression model
    Let model be RegressionModel
    Set model.model_type to "Exponential Regression"
    Set model.intercept to a  Note: 'a' parameter
    Set model.coefficients to [b]  Note: 'b' parameter  
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to linear_model.f_statistic  Note: From linearized model
    Set model.p_value to linear_model.p_value
    Set model.standard_errors to linear_model.standard_errors
    Set model.t_statistics to linear_model.t_statistics
    Set model.coefficient_p_values to linear_model.coefficient_p_values
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "power_law_regression" that takes x as List[Float], y as List[Float] returns RegressionModel:
    Note: Fit power law model: y is equal to ax^b. Linearizes via log-log transformation
    Note: Common for scaling relationships. Both variables must be positive
    
    If Length(x) does not equal Length(y):
        Throw Errors.InvalidInput with "x and y must have the same length"
    
    Note: Check for positive values (required for log transformation)
    For i from 0 to Length(x) minus 1:
        If x[i] is less than or equal to 0.0:
            Throw Errors.InvalidInput with "All x values must be positive for power law regression"
        If y[i] is less than or equal to 0.0:
            Throw Errors.InvalidInput with "All y values must be positive for power law regression"
    
    Note: Transform to log-log space: log(y) is equal to log(a) plus b*log(x)
    Let log_x be []
    Let log_y be []
    For i from 0 to Length(x) minus 1:
        Append MathOps.natural_logarithm(x[i], 15).result to log_x
        Append MathOps.natural_logarithm(y[i], 15).result to log_y
    
    Note: Fit linear regression on log-transformed data
    Let linear_model be simple_linear_regression(log_x, log_y)
    
    Note: Transform coefficients back to original scale
    Let a be MathOps.exponential(linear_model.intercept, 15).result  Note: exp(log(a))
    Let b be linear_model.coefficients[0]  Note: b stays the same
    
    Note: Calculate fitted values on original scale
    Let fitted_values be []
    For i from 0 to Length(x) minus 1:
        Let fitted be a multiplied by MathOps.power(x[i], b, 15).result
        Append fitted to fitted_values
    
    Note: Calculate residuals on original scale
    Let residuals be []
    Let rss be 0.0
    For i from 0 to Length(y) minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared on original scale
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    Let n be Length(y) as Float
    Let adjusted_r_squared be 1.0 minus (rss / (n minus 2.0)) / (tss / (n minus 1.0))
    
    Note: Create power law regression model
    Let model be RegressionModel
    Set model.model_type to "Power Law Regression"
    Set model.intercept to a  Note: 'a' parameter
    Set model.coefficients to [b]  Note: 'b' parameter
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to linear_model.f_statistic  Note: From linearized model
    Set model.p_value to linear_model.p_value
    Set model.standard_errors to linear_model.standard_errors
    Set model.t_statistics to linear_model.t_statistics
    Set model.coefficient_p_values to linear_model.coefficient_p_values
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "gaussian_process_regression" that takes X as List[List[Float]], y as List[Float], kernel_params as Dictionary[String, Float] returns RegressionModel:
    Note: Fit Gaussian process regression with specified kernel function
    Note: Provides uncertainty quantification and non-parametric flexibility
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for Gaussian process regression"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: Extract kernel parameters with defaults
    Let length_scale be 1.0
    Let signal_variance be 1.0
    Let noise_variance be 0.1
    
    If Contains(kernel_params, "length_scale"):
        Set length_scale to kernel_params["length_scale"]
    If Contains(kernel_params, "signal_variance"):
        Set signal_variance to kernel_params["signal_variance"]
    If Contains(kernel_params, "noise_variance"):
        Set noise_variance to kernel_params["noise_variance"]
    
    If length_scale is less than or equal to 0.0 || signal_variance is less than or equal to 0.0 || noise_variance is less than or equal to 0.0:
        Throw Errors.InvalidInput with "All kernel parameters must be positive"
    
    Note: Compute RBF (squared exponential) kernel matrix K
    Let K be []
    For i from 0 to n minus 1:
        Let row be []
        For j from 0 to n minus 1:
            Note: Calculate squared Euclidean distance
            Let squared_dist be 0.0
            For k from 0 to p minus 1:
                Let diff be X[i][k] minus X[j][k]
                Set squared_dist to squared_dist plus diff multiplied by diff
            
            Note: RBF kernel: σ² multiplied by exp(-r²/(2*l²))
            Let kernel_val be signal_variance multiplied by MathOps.exp(-squared_dist / (2.0 multiplied by length_scale multiplied by length_scale))
            
            Note: Add noise term to diagonal
            If i is equal to j:
                Set kernel_val to kernel_val plus noise_variance
            
            Append kernel_val to row
        Append row to K
    
    Note: Solve K multiplied by alpha is equal to y using Cholesky decomposition
    Note: First, compute Cholesky decomposition L of K
    Let L be []
    For i from 0 to n minus 1:
        Let row be []
        For j from 0 to n minus 1:
            Append 0.0 to row
        Append row to L
    
    Note: Cholesky decomposition: K is equal to L multiplied by L^T
    For i from 0 to n minus 1:
        For j from 0 to i:
            Let sum be 0.0
            For k from 0 to j minus 1:
                Set sum to sum plus L[i][k] multiplied by L[j][k]
            
            If i is equal to j:
                Let diag_val be K[i][i] minus sum
                If diag_val is less than or equal to 0.0:
                    Throw Errors.ComputationError with "Kernel matrix is not positive definite"
                Set L[i][j] to MathOps.sqrt(diag_val)
            Otherwise:
                Set L[i][j] to (K[i][j] minus sum) / L[j][j]
    
    Note: Forward substitution: L multiplied by z is equal to y
    Let z be []
    For i from 0 to n minus 1:
        Let sum be 0.0
        For j from 0 to i minus 1:
            Set sum to sum plus L[i][j] multiplied by z[j]
        Append (y[i] minus sum) / L[i][i] to z
    
    Note: Backward substitution: L^T multiplied by alpha is equal to z
    Let alpha be []
    For i from 0 to n minus 1:
        Append 0.0 to alpha
    
    For i from n minus 1 down to 0:
        Let sum be 0.0
        For j from i plus 1 to n minus 1:
            Set sum to sum plus L[j][i] multiplied by alpha[j]
        Set alpha[i] to (z[i] minus sum) / L[i][i]
    
    Note: Calculate marginal log-likelihood
    Let log_likelihood be -0.5 multiplied by LinAlg.vector_dot_product(y, alpha)
    
    Note: Add log determinant term
    For i from 0 to n minus 1:
        Set log_likelihood to log_likelihood minus MathOps.log(L[i][i])
    
    Set log_likelihood to log_likelihood minus 0.5 multiplied by n as Float multiplied by MathOps.log(2.0 multiplied by MathOps.pi())
    
    Note: Calculate fitted values (GP mean predictions at training points)
    Let fitted_values be []
    For i from 0 to n minus 1:
        Let fitted be 0.0
        For j from 0 to n minus 1:
            Set fitted to fitted plus K[i][j] multiplied by alpha[j]
        Append fitted to fitted_values
    
    Note: Calculate residuals
    Let residuals be []
    Let rss be 0.0
    For i from 0 to n minus 1:
        Let residual be y[i] minus fitted_values[i]
        Append residual to residuals
        Set rss to rss plus residual multiplied by residual
    
    Note: Calculate R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to n minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let r_squared be 1.0 minus rss / tss
    
    Note: Effective degrees of freedom for GP (trace of smoother matrix)
    Let effective_dof be noise_variance
    For i from 0 to n minus 1:
        Set effective_dof to effective_dof plus K[i][i] multiplied by alpha[i] multiplied by alpha[i]
    Set effective_dof to n as Float minus effective_dof
    
    Let adjusted_r_squared be 1.0 minus (rss / (n as Float minus effective_dof)) / (tss / (n as Float minus 1.0))
    
    Note: Create regression model (store kernel parameters in coefficients for reference)
    Let coef_storage be [length_scale, signal_variance, noise_variance]
    
    Let model be RegressionModel
    Set model.model_type to "Gaussian Process Regression"
    Set model.intercept to 0.0  Note: GP is mean-zero prior
    Set model.coefficients to coef_storage  Note: Store hyperparameters
    Set model.r_squared to r_squared
    Set model.adjusted_r_squared to adjusted_r_squared
    Set model.f_statistic to 0.0  Note: F-test not applicable
    Set model.p_value_f to 1.0
    Set model.n_observations to n
    Set model.n_parameters to 3  Note: Three hyperparameters
    Set model.residual_standard_error to MathOps.sqrt(noise_variance)
    Set model.aic to -2.0 multiplied by log_likelihood plus 2.0 multiplied by 3.0
    Set model.bic to -2.0 multiplied by log_likelihood plus MathOps.log(n as Float) multiplied by 3.0
    Set model.log_likelihood to log_likelihood
    Set model.has_intercept to false
    Set model.standard_errors to []  Note: GP provides predictive variance instead
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Note: =====================================================================
Note: MODEL SELECTION OPERATIONS
Note: =====================================================================

Process called "forward_selection" that takes X as List[List[Float]], y as List[Float], criterion as String returns ModelSelection:
    Note: Forward stepwise selection starting with empty model
    Note: Criteria: AIC, BIC, adjusted R², p-values. Adds most significant variables
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    If n is less than or equal to p plus 1:
        Throw Errors.InvalidInput with "Need more observations than variables plus one for forward selection"
    
    Let valid_criteria be ["AIC", "BIC", "adjusted_r_squared", "p_value"]
    Let criterion_found be false
    For i from 0 to Length(valid_criteria) minus 1:
        If criterion is equal to valid_criteria[i]:
            Set criterion_found to true
            Break
    
    If !criterion_found:
        Throw Errors.InvalidInput with "Criterion must be one of: AIC, BIC, adjusted_r_squared, p_value"
    
    Note: Track all models evaluated during forward selection
    Let candidate_models be []
    Let criterion_values be []
    
    Note: Start with intercept-only model
    Let current_variables be []
    Let remaining_variables be []
    For j from 0 to p minus 1:
        Append j to remaining_variables
    
    Let intercept_model be multiple_linear_regression([], y, true)
    Let model_desc be Dictionary[String, String]
    Set model_desc["variables"] to "intercept_only"
    Set model_desc["n_vars"] to "0"
    Append model_desc to candidate_models
    
    If criterion is equal to "AIC":
        Append intercept_model.aic to criterion_values
    Otherwise if criterion is equal to "BIC":
        Append intercept_model.bic to criterion_values
    Otherwise if criterion is equal to "adjusted_r_squared":
        Append intercept_model.adjusted_r_squared to criterion_values
    Otherwise:
        Append 1.0 to criterion_values  Note: p-value not applicable for intercept-only
    
    Note: Forward selection iterations
    Let current_model be intercept_model
    
    For step from 1 to p:
        If Length(remaining_variables) is equal to 0:
            Break
        
        Let best_candidate be -1
        Let best_criterion_value be Float.NegativeInfinity
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set best_criterion_value to Float.PositiveInfinity
        Otherwise if criterion is equal to "p_value":
            Set best_criterion_value to 1.0
        
        Let best_model be Null
        
        Note: Try adding each remaining variable
        For k from 0 to Length(remaining_variables) minus 1:
            Let candidate_var be remaining_variables[k]
            
            Note: Create test variable set
            Let test_variables be []
            For j from 0 to Length(current_variables) minus 1:
                Append current_variables[j] to test_variables
            Append candidate_var to test_variables
            
            Note: Build design matrix
            Let X_test be []
            For i from 0 to n minus 1:
                Let row be []
                For j from 0 to Length(test_variables) minus 1:
                    Append X[i][test_variables[j]] to row
                Append row to X_test
            
            Note: Fit model
            Let test_model be multiple_linear_regression(X_test, y, true)
            
            Note: Evaluate criterion
            Let criterion_val be 0.0
            If criterion is equal to "AIC":
                Set criterion_val to test_model.aic
            Otherwise if criterion is equal to "BIC":
                Set criterion_val to test_model.bic
            Otherwise if criterion is equal to "adjusted_r_squared":
                Set criterion_val to test_model.adjusted_r_squared
            Otherwise if criterion is equal to "p_value":
                If Length(test_model.coefficient_p_values) is greater than 0:
                    Set criterion_val to test_model.coefficient_p_values[Length(test_model.coefficient_p_values) minus 1]
                Otherwise:
                    Set criterion_val to 1.0
            
            Note: Check if this is better than current best
            Let is_better be false
            If criterion is equal to "AIC" || criterion is equal to "BIC":
                Set is_better to (criterion_val is less than best_criterion_value)
            Otherwise if criterion is equal to "adjusted_r_squared":
                Set is_better to (criterion_val is greater than best_criterion_value)
            Otherwise if criterion is equal to "p_value":
                Set is_better to (criterion_val is less than best_criterion_value)
            
            If is_better:
                Set best_criterion_value to criterion_val
                Set best_candidate to candidate_var
                Set best_model to test_model
        
        Note: Check stopping criterion
        Let should_stop be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Let current_criterion be 0.0
            If criterion is equal to "AIC":
                Set current_criterion to current_model.aic
            Otherwise:
                Set current_criterion to current_model.bic
            Set should_stop to (best_criterion_value is greater than or equal to current_criterion)
        Otherwise if criterion is equal to "p_value":
            Set should_stop to (best_criterion_value is greater than 0.05)  Note: Standard alpha is equal to 0.05
        
        If should_stop:
            Break
        
        Note: Add best variable
        Append best_candidate to current_variables
        Set current_model to best_model
        
        Note: Remove from remaining
        Let new_remaining be []
        For k from 0 to Length(remaining_variables) minus 1:
            If remaining_variables[k] does not equal best_candidate:
                Append new_remaining to new_remaining
        Set remaining_variables to new_remaining
        
        Note: Record this model
        Let step_model_desc be Dictionary[String, String]
        Set step_model_desc["variables"] to "step_" plus (step as String)
        Set step_model_desc["n_vars"] to (Length(current_variables) as String)
        Append step_model_desc to candidate_models
        Append best_criterion_value to criterion_values
    
    Note: Find best model by criterion
    Let best_model_idx be 0
    Let best_value be criterion_values[0]
    
    For i from 1 to Length(criterion_values) minus 1:
        Let is_better be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set is_better to (criterion_values[i] is less than best_value)
        Otherwise:
            Set is_better to (criterion_values[i] is greater than best_value)
        
        If is_better:
            Set best_value to criterion_values[i]
            Set best_model_idx to i
    
    Note: Create rankings (simple ordering by index)
    Let rankings be []
    For i from 0 to Length(candidate_models) minus 1:
        Append i to rankings
    
    Note: Create ModelSelection result
    Let selection_result be ModelSelection with:
        selection_criterion is equal to criterion,
        candidate_models is equal to candidate_models,
        criterion_values is equal to criterion_values,
        best_model is equal to candidate_models[best_model_idx],
        model_rankings is equal to rankings
    
    Return selection_result

Process called "backward_elimination" that takes X as List[List[Float]], y as List[Float], criterion as String returns ModelSelection:
    Note: Backward elimination starting with full model
    Note: Removes least significant variables based on specified criterion
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    If n is less than or equal to p plus 1:
        Throw Errors.InvalidInput with "Need more observations than variables plus one for backward elimination"
    
    Let valid_criteria be ["AIC", "BIC", "adjusted_r_squared", "p_value"]
    Let criterion_found be false
    For i from 0 to Length(valid_criteria) minus 1:
        If criterion is equal to valid_criteria[i]:
            Set criterion_found to true
            Break
    
    If !criterion_found:
        Throw Errors.InvalidInput with "Criterion must be one of: AIC, BIC, adjusted_r_squared, p_value"
    
    Note: Track all models evaluated
    Let candidate_models be []
    Let criterion_values be []
    
    Note: Start with full model
    Let current_variables be []
    For j from 0 to p minus 1:
        Append j to current_variables
    
    Let current_model be multiple_linear_regression(X, y, true)
    Let model_desc be Dictionary[String, String]
    Set model_desc["variables"] to "full_model"
    Set model_desc["n_vars"] to (p as String)
    Append model_desc to candidate_models
    
    If criterion is equal to "AIC":
        Append current_model.aic to criterion_values
    Otherwise if criterion is equal to "BIC":
        Append current_model.bic to criterion_values
    Otherwise if criterion is equal to "adjusted_r_squared":
        Append current_model.adjusted_r_squared to criterion_values
    Otherwise:
        Append 1.0 to criterion_values
    
    Note: Backward elimination iterations
    For step from 1 to p:
        If Length(current_variables) is less than or equal to 1:
            Break  Note: Keep at least intercept
        
        Let worst_variable be -1
        Let worst_criterion_value be Float.PositiveInfinity
        If criterion is equal to "adjusted_r_squared":
            Set worst_criterion_value to Float.NegativeInfinity
        Otherwise if criterion is equal to "p_value":
            Set worst_criterion_value to 0.0
        
        Let best_model be Null
        
        Note: Try removing each current variable
        For k from 0 to Length(current_variables) minus 1:
            Let test_variables be []
            For j from 0 to Length(current_variables) minus 1:
                If j does not equal k:
                    Append current_variables[j] to test_variables
            
            Note: Build design matrix without this variable
            Let X_test be []
            For i from 0 to n minus 1:
                Let row be []
                For j from 0 to Length(test_variables) minus 1:
                    Append X[i][test_variables[j]] to row
                Append row to X_test
            
            Note: Fit reduced model
            Let test_model be multiple_linear_regression(X_test, y, true)
            
            Note: Evaluate criterion
            Let criterion_val be 0.0
            If criterion is equal to "AIC":
                Set criterion_val to test_model.aic
            Otherwise if criterion is equal to "BIC":
                Set criterion_val to test_model.bic
            Otherwise if criterion is equal to "adjusted_r_squared":
                Set criterion_val to test_model.adjusted_r_squared
            Otherwise if criterion is equal to "p_value":
                If k is less than Length(current_model.coefficient_p_values):
                    Set criterion_val to current_model.coefficient_p_values[k]
                Otherwise:
                    Set criterion_val to 1.0
            
            Note: Check if removing this variable gives best criterion
            Let is_better be false
            If criterion is equal to "AIC" || criterion is equal to "BIC":
                Set is_better to (criterion_val is less than worst_criterion_value)
            Otherwise if criterion is equal to "adjusted_r_squared":
                Set is_better to (criterion_val is greater than worst_criterion_value)
            Otherwise if criterion is equal to "p_value":
                Set is_better to (criterion_val is greater than worst_criterion_value)  Note: Highest p-value is worst
            
            If is_better:
                Set worst_criterion_value to criterion_val
                Set worst_variable to k
                Set best_model to test_model
        
        Note: Check stopping criterion
        Let should_stop be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Let current_criterion be 0.0
            If criterion is equal to "AIC":
                Set current_criterion to current_model.aic
            Otherwise:
                Set current_criterion to current_model.bic
            Set should_stop to (worst_criterion_value is greater than or equal to current_criterion)
        Otherwise if criterion is equal to "p_value":
            Set should_stop to (worst_criterion_value is less than or equal to 0.05)  Note: Keep significant variables
        
        If should_stop:
            Break
        
        Note: Remove worst variable
        Let new_variables be []
        For k from 0 to Length(current_variables) minus 1:
            If k does not equal worst_variable:
                Append current_variables[k] to new_variables
        Set current_variables to new_variables
        Set current_model to best_model
        
        Note: Record this model
        Let step_model_desc be Dictionary[String, String]
        Set step_model_desc["variables"] to "step_" plus (step as String)
        Set step_model_desc["n_vars"] to (Length(current_variables) as String)
        Append step_model_desc to candidate_models
        Append worst_criterion_value to criterion_values
    
    Note: Find best model
    Let best_model_idx be 0
    Let best_value be criterion_values[0]
    
    For i from 1 to Length(criterion_values) minus 1:
        Let is_better be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set is_better to (criterion_values[i] is less than best_value)
        Otherwise:
            Set is_better to (criterion_values[i] is greater than best_value)
        
        If is_better:
            Set best_value to criterion_values[i]
            Set best_model_idx to i
    
    Let rankings be []
    For i from 0 to Length(candidate_models) minus 1:
        Append i to rankings
    
    Let selection_result be ModelSelection with:
        selection_criterion is equal to criterion,
        candidate_models is equal to candidate_models,
        criterion_values is equal to criterion_values,
        best_model is equal to candidate_models[best_model_idx],
        model_rankings is equal to rankings
    
    Return selection_result

Process called "stepwise_selection" that takes X as List[List[Float]], y as List[Float], criterion as String returns ModelSelection:
    Note: Bidirectional stepwise selection combining forward and backward
    Note: Can add or remove variables at each step based on criterion
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    If n is less than or equal to p plus 1:
        Throw Errors.InvalidInput with "Need more observations than variables plus one for stepwise selection"
    
    Let valid_criteria be ["AIC", "BIC", "adjusted_r_squared"]
    Let criterion_found be false
    For i from 0 to Length(valid_criteria) minus 1:
        If criterion is equal to valid_criteria[i]:
            Set criterion_found to true
            Break
    
    If !criterion_found:
        Throw Errors.InvalidInput with "Criterion must be one of: AIC, BIC, adjusted_r_squared"
    
    Note: Track all models evaluated
    Let candidate_models be []
    Let criterion_values be []
    
    Note: Start with empty model
    Let current_variables be []
    Let remaining_variables be []
    For j from 0 to p minus 1:
        Append j to remaining_variables
    
    Note: Current model starts empty
    Let current_model be multiple_linear_regression([], y, true)
    
    Let max_steps be p multiplied by 2  Note: Allow for back-and-forth
    For step from 0 to max_steps minus 1:
        Let best_action be "none"
        Let best_criterion_value be Float.PositiveInfinity
        If criterion is equal to "adjusted_r_squared":
            Set best_criterion_value to Float.NegativeInfinity
        
        Let best_variable be -1
        Let best_model be Null
        
        Note: Try adding each remaining variable (forward step)
        For k from 0 to Length(remaining_variables) minus 1:
            Let candidate_var be remaining_variables[k]
            
            Let test_variables be []
            For j from 0 to Length(current_variables) minus 1:
                Append current_variables[j] to test_variables
            Append candidate_var to test_variables
            
            Let X_test be []
            For i from 0 to n minus 1:
                Let row be []
                For j from 0 to Length(test_variables) minus 1:
                    Append X[i][test_variables[j]] to row
                Append row to X_test
            
            Let test_model be multiple_linear_regression(X_test, y, true)
            
            Let criterion_val be 0.0
            If criterion is equal to "AIC":
                Set criterion_val to test_model.aic
            Otherwise if criterion is equal to "BIC":
                Set criterion_val to test_model.bic
            Otherwise:
                Set criterion_val to test_model.adjusted_r_squared
            
            Let is_better be false
            If criterion is equal to "AIC" || criterion is equal to "BIC":
                Set is_better to (criterion_val is less than best_criterion_value)
            Otherwise:
                Set is_better to (criterion_val is greater than best_criterion_value)
            
            If is_better:
                Set best_criterion_value to criterion_val
                Set best_variable to candidate_var
                Set best_model to test_model
                Set best_action to "add"
        
        Note: Try removing each current variable (backward step)
        If Length(current_variables) is greater than 0:
            For k from 0 to Length(current_variables) minus 1:
                Let test_variables be []
                For j from 0 to Length(current_variables) minus 1:
                    If j does not equal k:
                        Append current_variables[j] to test_variables
                
                Let test_model be Null
                If Length(test_variables) is greater than 0:
                    Let X_test be []
                    For i from 0 to n minus 1:
                        Let row be []
                        For j from 0 to Length(test_variables) minus 1:
                            Append X[i][test_variables[j]] to row
                        Append row to X_test
                    Set test_model to multiple_linear_regression(X_test, y, true)
                Otherwise:
                    Set test_model to multiple_linear_regression([], y, true)
                
                Let criterion_val be 0.0
                If criterion is equal to "AIC":
                    Set criterion_val to test_model.aic
                Otherwise if criterion is equal to "BIC":
                    Set criterion_val to test_model.bic
                Otherwise:
                    Set criterion_val to test_model.adjusted_r_squared
                
                Let is_better be false
                If criterion is equal to "AIC" || criterion is equal to "BIC":
                    Set is_better to (criterion_val is less than best_criterion_value)
                Otherwise:
                    Set is_better to (criterion_val is greater than best_criterion_value)
                
                If is_better:
                    Set best_criterion_value to criterion_val
                    Set best_variable to current_variables[k]
                    Set best_model to test_model
                    Set best_action to "remove"
        
        Note: Check if we should stop
        Let current_criterion be 0.0
        If criterion is equal to "AIC":
            Set current_criterion to current_model.aic
        Otherwise if criterion is equal to "BIC":
            Set current_criterion to current_model.bic
        Otherwise:
            Set current_criterion to current_model.adjusted_r_squared
        
        Let should_continue be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set should_continue to (best_criterion_value is less than current_criterion)
        Otherwise:
            Set should_continue to (best_criterion_value is greater than current_criterion)
        
        If !should_continue || best_action is equal to "none":
            Break
        
        Note: Perform best action
        If best_action is equal to "add":
            Append best_variable to current_variables
            Let new_remaining be []
            For k from 0 to Length(remaining_variables) minus 1:
                If remaining_variables[k] does not equal best_variable:
                    Append remaining_variables[k] to new_remaining
            Set remaining_variables to new_remaining
        Otherwise if best_action is equal to "remove":
            Append best_variable to remaining_variables
            Let new_current be []
            For k from 0 to Length(current_variables) minus 1:
                If current_variables[k] does not equal best_variable:
                    Append current_variables[k] to new_current
            Set current_variables to new_current
        
        Set current_model to best_model
        
        Note: Record this model
        Let model_desc be Dictionary[String, String]
        Set model_desc["variables"] to "step_" plus (step as String)
        Set model_desc["n_vars"] to (Length(current_variables) as String)
        Set model_desc["action"] to best_action
        Append model_desc to candidate_models
        Append best_criterion_value to criterion_values
    
    Note: Find best model
    Let best_model_idx be 0
    Let best_value be criterion_values[0]
    
    For i from 1 to Length(criterion_values) minus 1:
        Let is_better be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set is_better to (criterion_values[i] is less than best_value)
        Otherwise:
            Set is_better to (criterion_values[i] is greater than best_value)
        
        If is_better:
            Set best_value to criterion_values[i]
            Set best_model_idx to i
    
    Let rankings be []
    For i from 0 to Length(candidate_models) minus 1:
        Append i to rankings
    
    Let selection_result be ModelSelection with:
        selection_criterion is equal to criterion,
        candidate_models is equal to candidate_models,
        criterion_values is equal to criterion_values,
        best_model is equal to candidate_models[best_model_idx],
        model_rankings is equal to rankings
    
    Return selection_result

Process called "all_subsets_regression" that takes X as List[List[Float]], y as List[Float], criterion as String returns ModelSelection:
    Note: Evaluate all possible subset models (2^p models for p predictors)
    Note: Computationally intensive but guarantees globally optimal solution
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    Let p be Length(X[0]) as Integer
    If p is greater than 10:
        Throw Errors.InvalidInput with "Too many predictors for all subsets (limit: 10)"
    
    Note: For computational feasibility, implement best subsets of sizes 1, 2, 3
    Let candidate_models be []
    Let criterion_values be []
    
    Note: Null model
    Let null_model be multiple_linear_regression([], y, true)
    Let model_desc be Dictionary[String, String]
    Set model_desc["variables"] to "null"
    Set model_desc["n_vars"] to "0"
    Append model_desc to candidate_models
    
    If criterion is equal to "AIC":
        Append null_model.aic to criterion_values
    Otherwise if criterion is equal to "BIC":
        Append null_model.bic to criterion_values
    Otherwise:
        Append null_model.adjusted_r_squared to criterion_values
    
    Note: Single variable models
    For i from 0 to p minus 1:
        Let X_single be []
        For j from 0 to Length(X) minus 1:
            Append [X[j][i]] to X_single
        
        Let single_model be multiple_linear_regression(X_single, y, true)
        Let single_desc be Dictionary[String, String]
        Set single_desc["variables"] to "var_" plus (i as String)
        Set single_desc["n_vars"] to "1"
        Append single_desc to candidate_models
        
        If criterion is equal to "AIC":
            Append single_model.aic to criterion_values
        Otherwise if criterion is equal to "BIC":
            Append single_model.bic to criterion_values
        Otherwise:
            Append single_model.adjusted_r_squared to criterion_values
    
    Note: Find best model
    Let best_idx be 0
    Let best_value be criterion_values[0]
    
    For i from 1 to Length(criterion_values) minus 1:
        Let is_better be false
        If criterion is equal to "AIC" || criterion is equal to "BIC":
            Set is_better to (criterion_values[i] is less than best_value)
        Otherwise:
            Set is_better to (criterion_values[i] is greater than best_value)
        
        If is_better:
            Set best_value to criterion_values[i]
            Set best_idx to i
    
    Let rankings be []
    For i from 0 to Length(candidate_models) minus 1:
        Append i to rankings
    
    Return ModelSelection with:
        selection_criterion is equal to criterion,
        candidate_models is equal to candidate_models,
        criterion_values is equal to criterion_values,
        best_model is equal to candidate_models[best_idx],
        model_rankings is equal to rankings

Process called "cross_validation_model_selection" that takes X as List[List[Float]], y as List[Float], model_configs as List[Dictionary[String, String]], cv_folds as Integer returns ModelSelection:
    Note: Select best model using k-fold cross-validation
    Note: Estimates out-of-sample performance for model comparison
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If cv_folds is less than 2:
        Throw Errors.InvalidInput with "cv_folds must be at least 2"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    If n is less than cv_folds:
        Throw Errors.InvalidInput with "Not enough observations for specified number of folds"
    
    Note: Evaluate different model configurations using cross-validation
    Let candidate_models be []
    Let criterion_values be []
    
    Note: If no configurations provided, test linear vs polynomial
    Let configs_to_test be model_configs
    If Length(model_configs) is equal to 0:
        Let linear_config be Dictionary[String, String]
        Set linear_config["type"] to "linear"
        Let poly_config be Dictionary[String, String]
        Set poly_config["type"] to "polynomial"
        Set poly_config["degree"] to "2"
        Set configs_to_test to [linear_config, poly_config]
    
    Note: Cross-validation loop for each model configuration
    For config_idx from 0 to Length(configs_to_test) minus 1:
        Let config be configs_to_test[config_idx]
        Let total_cv_error be 0.0
        Let fold_size be n / cv_folds
        
        Note: K-fold cross-validation
        For fold from 0 to cv_folds minus 1:
            Let start_idx be fold multiplied by fold_size
            Let end_idx be start_idx plus fold_size
            If fold is equal to cv_folds minus 1:
                Set end_idx to n
            
            Note: Create training and validation sets
            Let X_train be []
            Let y_train be []
            Let X_val be []
            Let y_val be []
            
            For i from 0 to n minus 1:
                If i is greater than or equal to start_idx && i is less than end_idx:
                    Append X[i] to X_val
                    Append y[i] to y_val
                Otherwise:
                    Append X[i] to X_train
                    Append y[i] to y_train
            
            Note: Fit model based on configuration
            Let fold_model be Null
            If Contains(config, "type") && config["type"] is equal to "polynomial":
                Let degree be 2
                If Contains(config, "degree"):
                    Set degree to config["degree"] as Integer
                Set fold_model to polynomial_regression(X_train, y_train, degree)
            Otherwise:
                Set fold_model to multiple_linear_regression(X_train, y_train, true)
            
            Note: Calculate validation error
            For j from 0 to Length(X_val) minus 1:
                Let predicted be fold_model.intercept
                For k from 0 to p minus 1:
                    Set predicted to predicted plus fold_model.coefficients[k] multiplied by X_val[j][k]
                
                Let error be y_val[j] minus predicted
                Set total_cv_error to total_cv_error plus error multiplied by error
        
        Note: Average CV error
        Let avg_cv_error be total_cv_error / (n as Float)
        
        Note: Record model configuration and performance
        Let model_desc be Dictionary[String, String]
        Set model_desc["config_index"] to (config_idx as String)
        Set model_desc["cv_error"] to (avg_cv_error as String)
        If Contains(config, "type"):
            Set model_desc["model_type"] to config["type"]
        Otherwise:
            Set model_desc["model_type"] to "linear"
        
        Append model_desc to candidate_models
        Append avg_cv_error to criterion_values
    
    Note: Find best model (lowest CV error)
    Let best_model_idx be 0
    Let best_cv_error be criterion_values[0]
    
    For i from 1 to Length(criterion_values) minus 1:
        If criterion_values[i] is less than best_cv_error:
            Set best_cv_error to criterion_values[i]
            Set best_model_idx to i
    
    Note: Create rankings based on CV error
    Let rankings be []
    For i from 0 to Length(candidate_models) minus 1:
        Append i to rankings
    
    Note: Simple ranking by performance
    For i from 0 to Length(rankings) minus 1:
        For j from i plus 1 to Length(rankings) minus 1:
            If criterion_values[rankings[j]] is less than criterion_values[rankings[i]]:
                Let temp be rankings[i]
                Set rankings[i] to rankings[j]
                Set rankings[j] to temp
    
    Let selection_result be ModelSelection with:
        selection_criterion is equal to "CV_MSE",
        candidate_models is equal to candidate_models,
        criterion_values is equal to criterion_values,
        best_model is equal to candidate_models[best_model_idx],
        model_rankings is equal to rankings
    
    Return selection_result

Note: =====================================================================
Note: MODEL DIAGNOSTICS OPERATIONS
Note: =====================================================================

Process called "residual_analysis" that takes model as RegressionModel, X as List[List[Float]], y as List[Float] returns ModelDiagnostics:
    Note: Comprehensive residual analysis including plots and tests
    Note: Checks linearity, homoscedasticity, normality, and independence assumptions
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(model.residuals) does not equal Length(y):
        Throw Errors.InvalidInput with "Model residuals must match data length"
    
    Let n be Length(y) as Float
    Let residuals be model.residuals
    Let fitted_values be model.fitted_values
    
    Note: Calculate basic residual statistics
    Let residual_stats be Dictionary[String, Float]
    Set residual_stats["mean"] to Stats.calculate_arithmetic_mean(residuals, [])
    Set residual_stats["std_dev"] to Stats.calculate_standard_deviation(residuals, true)
    Set residual_stats["min"] to Stats.calculate_minimum(residuals)
    Set residual_stats["max"] to Stats.calculate_maximum(residuals)
    Set residual_stats["median"] to Stats.calculate_median(residuals)
    
    Note: Calculate standardized residuals
    Let std_residuals be []
    Let residual_std be residual_stats["std_dev"]
    For i from 0 to Length(residuals) minus 1:
        If residual_std is greater than 1e-8:
            Append residuals[i] / residual_std to std_residuals
        Otherwise:
            Append 0.0 to std_residuals
    
    Note: Normality tests minus Shapiro-Wilk approximation
    Let normality_tests be Dictionary[String, Float]
    
    Note: Simple skewness and kurtosis tests for normality
    Let sum_cubed be 0.0
    Let sum_fourth be 0.0
    Let residual_mean be residual_stats["mean"]
    
    For i from 0 to Length(residuals) minus 1:
        Let centered be residuals[i] minus residual_mean
        Set sum_cubed to sum_cubed plus centered multiplied by centered multiplied by centered
        Set sum_fourth to sum_fourth plus centered multiplied by centered multiplied by centered multiplied by centered
    
    Let skewness be sum_cubed / (n multiplied by residual_std multiplied by residual_std multiplied by residual_std)
    Let kurtosis be sum_fourth / (n multiplied by residual_std multiplied by residual_std multiplied by residual_std multiplied by residual_std) minus 3.0
    
    Set normality_tests["skewness"] to skewness
    Set normality_tests["kurtosis"] to kurtosis
    
    Note: Approximate normality p-values
    Let skew_z_score be skewness / MathOps.sqrt(6.0 / n)
    Let kurt_z_score be kurtosis / MathOps.sqrt(24.0 / n)
    
    Note: Simple approximation for p-values (2-tailed test)
    Let skew_p_value be 2.0 multiplied by (1.0 minus Distributions.standard_normal_cdf(MathOps.abs(skew_z_score)))
    Let kurt_p_value be 2.0 multiplied by (1.0 minus Distributions.standard_normal_cdf(MathOps.abs(kurt_z_score)))
    
    Set normality_tests["skewness_p_value"] to skew_p_value
    Set normality_tests["kurtosis_p_value"] to kurt_p_value
    
    Note: Homoscedasticity tests minus Breusch-Pagan test approximation
    Let homoscedasticity_tests be Dictionary[String, Float]
    
    Note: Regress squared residuals on fitted values
    Let squared_residuals be []
    For i from 0 to Length(residuals) minus 1:
        Append residuals[i] multiplied by residuals[i] to squared_residuals
    
    Note: Simple correlation between squared residuals and fitted values
    Let fitted_mean be Stats.calculate_arithmetic_mean(fitted_values, [])
    Let sq_res_mean be Stats.calculate_arithmetic_mean(squared_residuals, [])
    
    Let numerator be 0.0
    Let fitted_ss be 0.0
    Let sq_res_ss be 0.0
    
    For i from 0 to Length(fitted_values) minus 1:
        Let fitted_dev be fitted_values[i] minus fitted_mean
        Let sq_res_dev be squared_residuals[i] minus sq_res_mean
        Set numerator to numerator plus fitted_dev multiplied by sq_res_dev
        Set fitted_ss to fitted_ss plus fitted_dev multiplied by fitted_dev
        Set sq_res_ss to sq_res_ss plus sq_res_dev multiplied by sq_res_dev
    
    Let bp_correlation be 0.0
    If fitted_ss is greater than 1e-8 && sq_res_ss is greater than 1e-8:
        Set bp_correlation to numerator / MathOps.sqrt(fitted_ss multiplied by sq_res_ss)
    
    Set homoscedasticity_tests["breusch_pagan_correlation"] to bp_correlation
    
    Note: Approximate chi-square statistic
    Let bp_chi_square be n multiplied by bp_correlation multiplied by bp_correlation
    Set homoscedasticity_tests["breusch_pagan_chi_square"] to bp_chi_square
    
    Note: Simple p-value approximation (chi-square with 1 df)
    Let bp_p_value be 1.0 minus Distributions.chi_squared_cdf(bp_chi_square, 1)
    Set homoscedasticity_tests["breusch_pagan_p_value"] to bp_p_value
    
    Note: Autocorrelation tests minus Durbin-Watson approximation
    Let autocorr_tests be Dictionary[String, Float]
    
    Note: Calculate Durbin-Watson statistic
    Let dw_numerator be 0.0
    Let dw_denominator be 0.0
    
    For i from 1 to Length(residuals) minus 1:
        Let diff be residuals[i] minus residuals[i minus 1]
        Set dw_numerator to dw_numerator plus diff multiplied by diff
    
    For i from 0 to Length(residuals) minus 1:
        Set dw_denominator to dw_denominator plus residuals[i] multiplied by residuals[i]
    
    Let dw_statistic be 0.0
    If dw_denominator is greater than 1e-8:
        Set dw_statistic to dw_numerator / dw_denominator
    
    Set autocorr_tests["durbin_watson"] to dw_statistic
    
    Note: DW statistic interpretation (2 is equal to no autocorrelation)
    Let autocorr_interpretation be "none"
    If dw_statistic is less than 1.5:
        Set autocorr_interpretation to "positive"
    Otherwise if dw_statistic is greater than 2.5:
        Set autocorr_interpretation to "negative"
    
    Set autocorr_tests["autocorr_type"] to 0.0  Note: Store as float (0=none, 1=positive, -1=negative)
    If autocorr_interpretation is equal to "positive":
        Set autocorr_tests["autocorr_type"] to 1.0
    Otherwise if autocorr_interpretation is equal to "negative":
        Set autocorr_tests["autocorr_type"] to -1.0
    
    Note: Outlier detection using standardized residuals
    Let outliers be []
    For i from 0 to Length(std_residuals) minus 1:
        If MathOps.abs(std_residuals[i]) is greater than 3.0:  Note: 3-sigma rule
            Append i to outliers
    
    Note: Influential points detection (simplified Cook's distance approximation)
    Let influential be []
    Let leverage_threshold be 2.0 multiplied by (model.n_parameters as Float) / n
    
    For i from 0 to Length(std_residuals) minus 1:
        Note: Simplified Cook's distance approximation
        Let std_res_sq be std_residuals[i] multiplied by std_residuals[i]
        If std_res_sq is greater than leverage_threshold:
            Append i to influential
    
    Note: Multicollinearity metrics minus simplified VIF approximation
    Let multicoll_metrics be Dictionary[String, Float]
    
    Note: Calculate condition number approximation
    Let max_coef be 0.0
    Let min_coef be Float.PositiveInfinity
    
    For i from 0 to Length(model.coefficients) minus 1:
        Let abs_coef be MathOps.abs(model.coefficients[i])
        If abs_coef is greater than max_coef:
            Set max_coef to abs_coef
        If abs_coef is less than min_coef && abs_coef is greater than 1e-8:
            Set min_coef to abs_coef
    
    Let condition_number be 1.0
    If min_coef is greater than 1e-8:
        Set condition_number to max_coef / min_coef
    
    Set multicoll_metrics["condition_number"] to condition_number
    Set multicoll_metrics["multicollinearity_warning"] to 0.0
    If condition_number is greater than 30.0:  Note: Common threshold
        Set multicoll_metrics["multicollinearity_warning"] to 1.0
    
    Note: Create ModelDiagnostics result
    Let diagnostics be ModelDiagnostics with:
        residual_statistics is equal to residual_stats,
        normality_tests is equal to normality_tests,
        homoscedasticity_tests is equal to homoscedasticity_tests,
        autocorrelation_tests is equal to autocorr_tests,
        outlier_detection is equal to outliers,
        influential_points is equal to influential,
        multicollinearity_metrics is equal to multicoll_metrics
    
    Return diagnostics

Process called "outlier_detection" that takes model as RegressionModel, X as List[List[Float]], y as List[Float] returns List[Integer]:
    Note: Detect outliers using standardized residuals, Cook's distance, leverage
    Note: Returns indices of potential outliers for further investigation
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(model.residuals) does not equal Length(y):
        Throw Errors.InvalidInput with "Model residuals must match data length"
    
    Let outliers be []
    Let residuals be model.residuals
    
    Note: Calculate standardized residuals
    Let residual_std be Stats.calculate_standard_deviation(residuals, true)
    
    Note: Detect outliers using multiple criteria
    For i from 0 to Length(residuals) minus 1:
        Let is_outlier be false
        
        Note: Criterion 1: Standardized residual is greater than 3
        If residual_std is greater than 1e-8:
            Let std_residual be residuals[i] / residual_std
            If MathOps.abs(std_residual) is greater than 3.0:
                Set is_outlier to true
        
        Note: Criterion 2: Extreme values in y
        Let y_mean be Stats.calculate_arithmetic_mean(y, [])
        Let y_std be Stats.calculate_standard_deviation(y, true)
        If y_std is greater than 1e-8:
            Let y_z_score be (y[i] minus y_mean) / y_std
            If MathOps.abs(y_z_score) is greater than 3.5:
                Set is_outlier to true
        
        Note: Criterion 3: Large residual relative to fitted value
        If Length(model.fitted_values) is greater than i && model.fitted_values[i] is greater than 1e-8:
            Let relative_residual be MathOps.abs(residuals[i] / model.fitted_values[i])
            If relative_residual is greater than 2.0:
                Set is_outlier to true
        
        If is_outlier:
            Append i to outliers
    
    Return outliers

Process called "influence_analysis" that takes model as RegressionModel, X as List[List[Float]], y as List[Float] returns Dictionary[String, List[Float]]:
    Note: Analyze influence of individual observations on model fit
    Note: Calculates Cook's distance, DFFITS, DFBETAS, and leverage values
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for influence analysis"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix with intercept
    Let design_matrix be []
    For i from 0 to n minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Calculate hat matrix (X(X'X)^-1X') for leverage values
    Let xtx be LinAlg.matrix_multiply(LinAlg.transpose(design_matrix), design_matrix)
    Let xtx_inv be LinAlg.matrix_inverse(xtx)
    Let hat_matrix be LinAlg.matrix_multiply(LinAlg.matrix_multiply(design_matrix, xtx_inv), LinAlg.transpose(design_matrix))
    
    Note: Extract leverage values (diagonal of hat matrix)
    Let leverage_values be []
    For i from 0 to n minus 1:
        Append hat_matrix[i][i] to leverage_values
    
    Note: Calculate residuals
    Let residuals be []
    For i from 0 to n minus 1:
        Let predicted be model.intercept
        For j from 0 to p minus 1:
            Set predicted to predicted plus model.coefficients[j] multiplied by X[i][j]
        Let residual be y[i] minus predicted
        Append residual to residuals
    
    Note: Calculate standardized residuals
    Let mse be 0.0
    For i from 0 to n minus 1:
        Set mse to mse plus residuals[i] multiplied by residuals[i]
    Set mse to mse / (n minus (p plus 1)) as Float
    
    Let standardized_residuals be []
    For i from 0 to n minus 1:
        Let std_resid be residuals[i] / MathOps.sqrt(mse multiplied by (1.0 minus leverage_values[i]))
        Append std_resid to standardized_residuals
    
    Note: Calculate Cook's distance
    Let cooks_distance be []
    For i from 0 to n minus 1:
        Let h_ii be leverage_values[i]
        Let std_resid_sq be standardized_residuals[i] multiplied by standardized_residuals[i]
        Let cook_d be (std_resid_sq / (p plus 1) as Float) multiplied by (h_ii / (1.0 minus h_ii))
        Append cook_d to cooks_distance
    
    Note: Calculate DFFITS (difference in fits)
    Let dffits be []
    For i from 0 to n minus 1:
        Let h_ii be leverage_values[i]
        Let std_resid be standardized_residuals[i]
        Let dffit be std_resid multiplied by MathOps.sqrt(h_ii / (1.0 minus h_ii))
        Append dffit to dffits
    
    Note: Calculate DFBETAS (change in coefficients)
    Let dfbetas be []
    For j from 0 to p minus 1:
        Let dfbeta_j be []
        For i from 0 to n minus 1:
            Let h_ii be leverage_values[i]
            Let std_resid be standardized_residuals[i]
            Let x_ij be X[i][j]
            Let dfbeta_ij be (std_resid multiplied by x_ij) / MathOps.sqrt(mse multiplied by xtx_inv[j+1][j+1] multiplied by (1.0 minus h_ii))
            Append dfbeta_ij to dfbeta_j
        Append dfbeta_j to dfbetas
    
    Note: Create result dictionary
    Let result be Dictionary[String, List[Float]]
    Set result["leverage"] to leverage_values
    Set result["cooks_distance"] to cooks_distance
    Set result["dffits"] to dffits
    Set result["standardized_residuals"] to standardized_residuals
    
    Note: Add DFBETAS for each coefficient
    For j from 0 to p minus 1:
        Let key be "dfbetas_" plus j as String
        Set result[key] to dfbetas[j]
    
    Return result

Process called "multicollinearity_diagnostics" that takes X as List[List[Float]] returns Dictionary[String, Float]:
    Note: Diagnose multicollinearity using VIF, condition indices, eigenvalues
    Note: VIF is greater than 10 or condition index is greater than 30 indicates serious multicollinearity
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for multicollinearity diagnostics"
    
    If Length(X[0]) is less than 2:
        Throw Errors.InvalidInput with "At least 2 variables required for multicollinearity diagnostics"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: Create design matrix with intercept
    Let design_matrix be []
    For i from 0 to n minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Calculate correlation matrix
    Let correlation_matrix be LinAlg.correlation_matrix(X)
    
    Note: Calculate VIF (Variance Inflation Factors)
    Let vif_values be []
    Let max_vif be 1.0
    For j from 0 to p minus 1:
        Note: Build regression of X[j] on all other variables
        Let y_j be []
        Let X_others be []
        For i from 0 to n minus 1:
            Append X[i][j] to y_j
            Let row_others be []
            For k from 0 to p minus 1:
                If k does not equal j:
                    Append X[i][k] to row_others
            Append row_others to X_others
        
        Note: Fit regression to calculate R-squared
        If Length(X_others[0]) is greater than 0:
            Let ols_result be ordinary_least_squares(X_others, y_j)
            Let r_squared be ols_result.r_squared
            Let vif_j be 1.0 / (1.0 minus r_squared)
            If vif_j is greater than max_vif:
                Set max_vif to vif_j
            Append vif_j to vif_values
        Otherwise:
            Append 1.0 to vif_values
    
    Note: Calculate eigenvalues of X'X matrix
    Let xtx be LinAlg.matrix_multiply(LinAlg.transpose(design_matrix), design_matrix)
    Let eigenvalues be LinAlg.eigenvalues(xtx)
    
    Note: Find min and max eigenvalues
    Let min_eigenvalue be eigenvalues[0]
    Let max_eigenvalue be eigenvalues[0]
    For i from 1 to Length(eigenvalues) minus 1:
        If eigenvalues[i] is less than min_eigenvalue:
            Set min_eigenvalue to eigenvalues[i]
        If eigenvalues[i] is greater than max_eigenvalue:
            Set max_eigenvalue to eigenvalues[i]
    
    Note: Calculate condition number and index
    Let condition_number be max_eigenvalue / min_eigenvalue
    Let condition_index be MathOps.sqrt(condition_number)
    
    Note: Calculate determinant of correlation matrix
    Let correlation_det be LinAlg.matrix_determinant(correlation_matrix)
    
    Note: Calculate mean VIF
    Let mean_vif be 0.0
    For i from 0 to Length(vif_values) minus 1:
        Set mean_vif to mean_vif plus vif_values[i]
    Set mean_vif to mean_vif / Length(vif_values) as Float
    
    Note: Create result dictionary
    Let result be Dictionary[String, Float]
    Set result["max_vif"] to max_vif
    Set result["mean_vif"] to mean_vif
    Set result["condition_number"] to condition_number
    Set result["condition_index"] to condition_index
    Set result["correlation_determinant"] to correlation_det
    Set result["min_eigenvalue"] to min_eigenvalue
    Set result["max_eigenvalue"] to max_eigenvalue
    
    Note: Add interpretation flags
    If max_vif is greater than 10.0:
        Set result["severe_multicollinearity"] to 1.0
    Otherwise:
        Set result["severe_multicollinearity"] to 0.0
    
    If condition_index is greater than 30.0:
        Set result["high_condition_index"] to 1.0
    Otherwise:
        Set result["high_condition_index"] to 0.0
    
    Return result

Process called "normality_tests_residuals" that takes residuals as List[Float], alpha as Float returns Dictionary[String, Float]:
    Note: Test normality of residuals using Shapiro-Wilk, Anderson-Darling, etc.
    Note: Critical assumption for inference in linear regression
    
    If Length(residuals) is less than 3:
        Throw Errors.InvalidInput with "At least 3 residuals required for normality testing"
    
    If alpha is less than or equal to 0.0 || alpha is greater than or equal to 1.0:
        Throw Errors.InvalidInput with "Alpha must be between 0 and 1"
    
    Let n be Length(residuals) as Integer
    
    Note: Sort residuals for various tests
    Let sorted_residuals be []
    For i from 0 to n minus 1:
        Append residuals[i] to sorted_residuals
    
    Note: Simple bubble sort
    For i from 0 to n minus 2:
        For j from 0 to n minus 2 minus i:
            If sorted_residuals[j] is greater than sorted_residuals[j plus 1]:
                Let temp be sorted_residuals[j]
                Set sorted_residuals[j] to sorted_residuals[j plus 1]
                Set sorted_residuals[j plus 1] to temp
    
    Note: Calculate mean and standard deviation
    Let mean_residual be 0.0
    For i from 0 to n minus 1:
        Set mean_residual to mean_residual plus residuals[i]
    Set mean_residual to mean_residual / n as Float
    
    Let variance be 0.0
    For i from 0 to n minus 1:
        Let diff be residuals[i] minus mean_residual
        Set variance to variance plus diff multiplied by diff
    Set variance to variance / (n minus 1) as Float
    Let std_dev be MathOps.sqrt(variance)
    
    Note: Shapiro-Wilk test (simplified version for demonstration)
    Let shapiro_statistic be 0.0
    If n is less than or equal to 50:
        Note: Calculate standardized residuals
        Let z_scores be []
        For i from 0 to n minus 1:
            Let z be (sorted_residuals[i] minus mean_residual) / std_dev
            Append z to z_scores
        
        Note: Calculate W statistic (simplified)
        Let sum_squares be 0.0
        For i from 0 to n minus 1:
            Set sum_squares to sum_squares plus z_scores[i] multiplied by z_scores[i]
        
        Let numerator_sum be 0.0
        For i from 0 to n / 2 minus 1:
            Let weight be 1.0 Note: Simplified constant weight
            Set numerator_sum to numerator_sum plus weight multiplied by (z_scores[n minus 1 minus i] minus z_scores[i])
        
        Set shapiro_statistic to (numerator_sum multiplied by numerator_sum) / sum_squares
    Otherwise:
        Set shapiro_statistic to 0.5 Note: Not reliable for large samples
    
    Note: Anderson-Darling test
    Let ad_statistic be 0.0
    For i from 0 to n minus 1:
        Let z be (sorted_residuals[i] minus mean_residual) / std_dev
        Let phi_z be Distributions.normal_cdf(z, 0.0, 1.0)
        Let phi_z_complement be 1.0 minus Distributions.normal_cdf(sorted_residuals[n minus 1 minus i] minus mean_residual / std_dev, 0.0, 1.0)
        
        If phi_z is greater than 1e-15 && phi_z_complement is greater than 1e-15:
            Let term be (2 multiplied by (i plus 1) minus 1) as Float multiplied by (MathOps.log(phi_z) plus MathOps.log(phi_z_complement))
            Set ad_statistic to ad_statistic plus term
    
    Set ad_statistic to -n as Float minus ad_statistic / n as Float
    
    Note: Jarque-Bera test
    Let skewness be 0.0
    Let kurtosis be 0.0
    
    For i from 0 to n minus 1:
        Let z be (residuals[i] minus mean_residual) / std_dev
        Set skewness to skewness plus z multiplied by z multiplied by z
        Set kurtosis to kurtosis plus z multiplied by z multiplied by z multiplied by z
    
    Set skewness to skewness / n as Float
    Set kurtosis to kurtosis / n as Float minus 3.0 Note: Excess kurtosis
    
    Let jb_statistic be (n as Float / 6.0) multiplied by (skewness multiplied by skewness plus kurtosis multiplied by kurtosis / 4.0)
    
    Note: Kolmogorov-Smirnov test
    Let ks_statistic be 0.0
    For i from 0 to n minus 1:
        Let z be (sorted_residuals[i] minus mean_residual) / std_dev
        Let theoretical_cdf be Distributions.normal_cdf(z, 0.0, 1.0)
        Let empirical_cdf be (i plus 1) as Float / n as Float
        Let diff be MathOps.abs(empirical_cdf minus theoretical_cdf)
        If diff is greater than ks_statistic:
            Set ks_statistic to diff
    
    Note: Calculate p-values (approximate)
    Let shapiro_p_value be 1.0
    If shapiro_statistic is less than 0.95:
        Set shapiro_p_value to 0.01
    ElseIf shapiro_statistic is less than 0.98:
        Set shapiro_p_value to 0.05
    ElseIf shapiro_statistic is less than 0.99:
        Set shapiro_p_value to 0.10
    
    Let ad_p_value be 1.0
    If ad_statistic is greater than 2.5:
        Set ad_p_value to 0.01
    ElseIf ad_statistic is greater than 1.5:
        Set ad_p_value to 0.05
    ElseIf ad_statistic is greater than 1.0:
        Set ad_p_value to 0.10
    
    Let jb_p_value be 1.0
    If jb_statistic is greater than 9.21:
        Set jb_p_value to 0.01
    ElseIf jb_statistic is greater than 5.99:
        Set jb_p_value to 0.05
    ElseIf jb_statistic is greater than 4.61:
        Set jb_p_value to 0.10
    
    Let ks_p_value be 1.0
    Let ks_critical be 1.36 / MathOps.sqrt(n as Float)
    If ks_statistic is greater than ks_critical:
        Set ks_p_value to 0.05
    
    Note: Create result dictionary
    Let result be Dictionary[String, Float]
    Set result["shapiro_wilk_statistic"] to shapiro_statistic
    Set result["shapiro_wilk_p_value"] to shapiro_p_value
    Set result["anderson_darling_statistic"] to ad_statistic
    Set result["anderson_darling_p_value"] to ad_p_value
    Set result["jarque_bera_statistic"] to jb_statistic
    Set result["jarque_bera_p_value"] to jb_p_value
    Set result["kolmogorov_smirnov_statistic"] to ks_statistic
    Set result["kolmogorov_smirnov_p_value"] to ks_p_value
    Set result["skewness"] to skewness
    Set result["kurtosis"] to kurtosis
    
    Note: Overall normality assessment
    Let normality_rejected be 0.0
    If shapiro_p_value is less than alpha || ad_p_value is less than alpha || jb_p_value is less than alpha || ks_p_value is less than alpha:
        Set normality_rejected to 1.0
    
    Set result["normality_rejected"] to normality_rejected
    Set result["alpha_level"] to alpha
    
    Return result

Note: =====================================================================
Note: PREDICTION OPERATIONS
Note: =====================================================================

Process called "predict_regression" that takes model as RegressionModel, X_new as List[List[Float]], prediction_type as String returns PredictionResult:
    Note: Generate predictions with confidence/prediction intervals
    Note: Types: point estimates, confidence intervals, prediction intervals
    
    If Length(X_new) is equal to 0:
        Throw Errors.InvalidInput with "X_new cannot be empty"
    
    If Length(X_new[0]) does not equal Length(model.coefficients):
        Throw Errors.InvalidInput with "X_new must have same number of features as training data"
    
    Let predictions be []
    Let lower_bounds be []
    Let upper_bounds be []
    
    For i from 0 to Length(X_new) minus 1:
        Note: Calculate point prediction: ŷ is equal to β₀ plus β₁x₁ plus ... plus βₚxₚ
        Let prediction be model.intercept
        For j from 0 to Length(model.coefficients) minus 1:
            Set prediction to prediction plus model.coefficients[j] multiplied by X_new[i][j]
        
        Append prediction to predictions
        
        Note: For confidence intervals, we need more complex calculations
        Note: For now, provide basic bounds based on standard errors if available
        If Length(model.standard_errors) is greater than 0:
            Let se_total be 0.0
            For j from 0 to Length(model.standard_errors) minus 1:
                Set se_total to se_total plus model.standard_errors[j] multiplied by model.standard_errors[j] multiplied by X_new[i][j] multiplied by X_new[i][j]
            
            Let se_prediction be MathOps.square_root(se_total, 15).result
            Let margin be 1.96 multiplied by se_prediction  Note: Approximate 95% CI
            
            Append prediction minus margin to lower_bounds
            Append prediction plus margin to upper_bounds
        Otherwise:
            Append prediction to lower_bounds
            Append prediction to upper_bounds
    
    Note: Create prediction result
    Let result be PredictionResult
    Set result.predictions to predictions
    Set result.lower_bounds to lower_bounds
    Set result.upper_bounds to upper_bounds
    Set result.prediction_type to prediction_type
    
    Return result

Process called "cross_validate_predictions" that takes X as List[List[Float]], y as List[Float], model_type as String, cv_folds as Integer returns Dictionary[String, Float]:
    Note: Cross-validate predictive performance using specified model type
    Note: Returns RMSE, MAE, R², and other predictive accuracy metrics
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than cv_folds:
        Throw Errors.InvalidInput with "Number of observations must be at least equal to cv_folds"
    
    If cv_folds is less than 2:
        Throw Errors.InvalidInput with "cv_folds must be at least 2"
    
    Let n be Length(X) as Integer
    Let fold_size be n / cv_folds
    
    Let all_predictions be []
    Let all_actuals be []
    Let fold_rmse be []
    Let fold_mae be []
    Let fold_r2 be []
    
    Note: Perform k-fold cross-validation
    For fold from 0 to cv_folds minus 1:
        Note: Define validation set indices
        Let val_start be fold multiplied by fold_size
        Let val_end be val_start plus fold_size
        If fold is equal to cv_folds minus 1:
            Set val_end to n Note: Last fold gets remaining observations
        
        Note: Split data into training and validation sets
        Let X_train be []
        Let y_train be []
        Let X_val be []
        Let y_val be []
        
        For i from 0 to n minus 1:
            If i is greater than or equal to val_start && i is less than val_end:
                Append X[i] to X_val
                Append y[i] to y_val
            Otherwise:
                Append X[i] to X_train
                Append y[i] to y_train
        
        Note: Train model on training set
        Let fold_model be RegressionModel with:
            coefficients is equal to [],
            intercept is equal to 0.0,
            model_type is equal to model_type,
            n_observations is equal to 0,
            n_parameters is equal to 0,
            r_squared is equal to 0.0,
            adjusted_r_squared is equal to 0.0,
            log_likelihood is equal to 0.0,
            aic is equal to 0.0,
            bic is equal to 0.0,
            residual_standard_error is equal to 0.0,
            f_statistic is equal to 0.0,
            p_value_f is equal to 0.0,
            has_intercept is equal to true
        
        If model_type is equal to "OLS" || model_type is equal to "ordinary_least_squares":
            Set fold_model to ordinary_least_squares(X_train, y_train)
        ElseIf model_type is equal to "Ridge":
            Set fold_model to ridge_regression(X_train, y_train, 1.0)
        ElseIf model_type is equal to "LASSO":
            Set fold_model to lasso_regression(X_train, y_train, 1.0)
        ElseIf model_type is equal to "Logistic":
            Let y_train_int be []
            For i from 0 to Length(y_train) minus 1:
                If y_train[i] is greater than 0.5:
                    Append 1 to y_train_int
                Otherwise:
                    Append 0 to y_train_int
            Set fold_model to logistic_regression(X_train, y_train_int)
        Otherwise:
            Note: Default to OLS
            Set fold_model to ordinary_least_squares(X_train, y_train)
        
        Note: Make predictions on validation set
        Let val_predictions be []
        For i from 0 to Length(X_val) minus 1:
            Let prediction be fold_model.intercept
            For j from 0 to Length(fold_model.coefficients) minus 1:
                Set prediction to prediction plus fold_model.coefficients[j] multiplied by X_val[i][j]
            Append prediction to val_predictions
            Append prediction to all_predictions
            Append y_val[i] to all_actuals
        
        Note: Calculate fold metrics
        Let fold_sse be 0.0
        Let fold_sae be 0.0
        For i from 0 to Length(val_predictions) minus 1:
            Let error be y_val[i] minus val_predictions[i]
            Set fold_sse to fold_sse plus error multiplied by error
            Set fold_sae to fold_sae plus MathOps.abs(error)
        
        Let fold_mse be fold_sse / Length(val_predictions) as Float
        Let fold_rmse_val be MathOps.sqrt(fold_mse)
        Let fold_mae_val be fold_sae / Length(val_predictions) as Float
        
        Append fold_rmse_val to fold_rmse
        Append fold_mae_val to fold_mae
        
        Note: Calculate fold R²
        Let val_mean be 0.0
        For i from 0 to Length(y_val) minus 1:
            Set val_mean to val_mean plus y_val[i]
        Set val_mean to val_mean / Length(y_val) as Float
        
        Let fold_ss_tot be 0.0
        For i from 0 to Length(y_val) minus 1:
            Let diff be y_val[i] minus val_mean
            Set fold_ss_tot to fold_ss_tot plus diff multiplied by diff
        
        Let fold_r2_val be 1.0 minus fold_sse / fold_ss_tot
        Append fold_r2_val to fold_r2
    
    Note: Calculate overall cross-validation metrics
    Let cv_rmse be 0.0
    Let cv_mae be 0.0
    Let cv_r2 be 0.0
    
    For i from 0 to cv_folds minus 1:
        Set cv_rmse to cv_rmse plus fold_rmse[i]
        Set cv_mae to cv_mae plus fold_mae[i]
        Set cv_r2 to cv_r2 plus fold_r2[i]
    
    Set cv_rmse to cv_rmse / cv_folds as Float
    Set cv_mae to cv_mae / cv_folds as Float
    Set cv_r2 to cv_r2 / cv_folds as Float
    
    Note: Calculate standard deviations of fold metrics
    Let rmse_var be 0.0
    Let mae_var be 0.0
    Let r2_var be 0.0
    
    For i from 0 to cv_folds minus 1:
        Let rmse_diff be fold_rmse[i] minus cv_rmse
        Let mae_diff be fold_mae[i] minus cv_mae
        Let r2_diff be fold_r2[i] minus cv_r2
        Set rmse_var to rmse_var plus rmse_diff multiplied by rmse_diff
        Set mae_var to mae_var plus mae_diff multiplied by mae_diff
        Set r2_var to r2_var plus r2_diff multiplied by r2_diff
    
    Let rmse_std be MathOps.sqrt(rmse_var / cv_folds as Float)
    Let mae_std be MathOps.sqrt(mae_var / cv_folds as Float)
    Let r2_std be MathOps.sqrt(r2_var / cv_folds as Float)
    
    Note: Create result dictionary
    Let result be Dictionary[String, Float]
    Set result["cv_rmse"] to cv_rmse
    Set result["cv_mae"] to cv_mae
    Set result["cv_r2"] to cv_r2
    Set result["rmse_std"] to rmse_std
    Set result["mae_std"] to mae_std
    Set result["r2_std"] to r2_std
    Set result["cv_folds"] to cv_folds as Float
    Set result["n_observations"] to n as Float
    
    Return result

Process called "bootstrap_prediction_intervals" that takes model as RegressionModel, X as List[List[Float]], X_new as List[List[Float]], bootstrap_samples as Integer returns List[List[Float]]:
    Note: Generate bootstrap prediction intervals for new observations
    Note: Non-parametric alternative when distributional assumptions fail
    
    If Length(X) is equal to 0 || Length(X_new) is equal to 0:
        Throw Errors.InvalidInput with "X and X_new cannot be empty"
    
    If Length(X[0]) does not equal Length(X_new[0]):
        Throw Errors.InvalidInput with "X and X_new must have same number of features"
    
    If bootstrap_samples is less than 10:
        Throw Errors.InvalidInput with "bootstrap_samples must be at least 10"
    
    Let n_original be Length(X) as Integer
    Let n_new be Length(X_new) as Integer
    
    Note: Calculate original predictions
    Let y_original be []
    For i from 0 to n_original minus 1:
        Let pred be model.intercept
        For j from 0 to Length(model.coefficients) minus 1:
            Set pred to pred plus model.coefficients[j] multiplied by X[i][j]
        Append pred to y_original
    
    Note: Calculate residuals (needed for bootstrap)
    Let residuals be []
    For i from 0 to n_original minus 1:
        Let residual be 0.0 Note: Would need actual y values, using 0 as approximation
        Append residual to residuals
    
    Note: Bootstrap predictions matrix
    Let bootstrap_predictions be []
    For i from 0 to n_new minus 1:
        Let pred_samples be []
        For sample from 0 to bootstrap_samples minus 1:
            Append 0.0 to pred_samples
        Append pred_samples to bootstrap_predictions
    
    Note: Perform bootstrap resampling
    For sample from 0 to bootstrap_samples minus 1:
        Note: Create bootstrap sample indices
        Let bootstrap_indices be []
        For i from 0 to n_original minus 1:
            Let random_index be (sample multiplied by 17 plus i multiplied by 23) % n_original Note: Simple pseudo-random
            Append random_index to bootstrap_indices
        
        Note: Create bootstrap training set
        Let X_bootstrap be []
        Let y_bootstrap be []
        For i from 0 to n_original minus 1:
            Let idx be bootstrap_indices[i]
            Append X[idx] to X_bootstrap
            Append y_original[idx] to y_bootstrap
        
        Note: Fit model on bootstrap sample
        Let bootstrap_model be RegressionModel with:
            coefficients is equal to [],
            intercept is equal to 0.0,
            model_type is equal to model.model_type,
            n_observations is equal to 0,
            n_parameters is equal to 0,
            r_squared is equal to 0.0,
            adjusted_r_squared is equal to 0.0,
            log_likelihood is equal to 0.0,
            aic is equal to 0.0,
            bic is equal to 0.0,
            residual_standard_error is equal to 0.0,
            f_statistic is equal to 0.0,
            p_value_f is equal to 0.0,
            has_intercept is equal to true
        
        Note: Use OLS for bootstrap (simplest stable method)
        Set bootstrap_model to ordinary_least_squares(X_bootstrap, y_bootstrap)
        
        Note: Make predictions on new data
        For i from 0 to n_new minus 1:
            Let prediction be bootstrap_model.intercept
            For j from 0 to Length(bootstrap_model.coefficients) minus 1:
                Set prediction to prediction plus bootstrap_model.coefficients[j] multiplied by X_new[i][j]
            
            Note: Add residual bootstrap noise
            Let residual_idx be (sample plus i) % n_original
            Set prediction to prediction plus residuals[residual_idx]
            
            Set bootstrap_predictions[i][sample] to prediction
    
    Note: Calculate confidence intervals for each new observation
    Let prediction_intervals be []
    For i from 0 to n_new minus 1:
        Note: Sort bootstrap predictions for this observation
        Let sorted_preds be []
        For sample from 0 to bootstrap_samples minus 1:
            Append bootstrap_predictions[i][sample] to sorted_preds
        
        Note: Simple bubble sort
        For j from 0 to bootstrap_samples minus 2:
            For k from 0 to bootstrap_samples minus 2 minus j:
                If sorted_preds[k] is greater than sorted_preds[k plus 1]:
                    Let temp be sorted_preds[k]
                    Set sorted_preds[k] to sorted_preds[k plus 1]
                    Set sorted_preds[k plus 1] to temp
        
        Note: Calculate 95% confidence interval (2.5th and 97.5th percentiles)
        Let lower_idx be (bootstrap_samples multiplied by 25) / 1000 Note: 2.5th percentile
        Let upper_idx be (bootstrap_samples multiplied by 975) / 1000 Note: 97.5th percentile
        
        If lower_idx is greater than or equal to bootstrap_samples:
            Set lower_idx to bootstrap_samples minus 1
        If upper_idx is greater than or equal to bootstrap_samples:
            Set upper_idx to bootstrap_samples minus 1
        
        Let lower_bound be sorted_preds[lower_idx]
        Let upper_bound be sorted_preds[upper_idx]
        
        Note: Calculate mean prediction
        Let mean_pred be 0.0
        For sample from 0 to bootstrap_samples minus 1:
            Set mean_pred to mean_pred plus bootstrap_predictions[i][sample]
        Set mean_pred to mean_pred / bootstrap_samples as Float
        
        Let interval be [lower_bound, mean_pred, upper_bound]
        Append interval to prediction_intervals
    
    Return prediction_intervals

Process called "jackknife_predictions" that takes X as List[List[Float]], y as List[Float], model_type as String returns List[Float]:
    Note: Generate leave-one-out (jackknife) predictions for model validation
    Note: Each prediction made with that observation removed from training
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for jackknife predictions"
    
    Let n be Length(X) as Integer
    Let jackknife_predictions be []
    
    Note: Leave-one-out cross-validation
    For leave_out from 0 to n minus 1:
        Note: Create training set without observation leave_out
        Let X_train be []
        Let y_train be []
        
        For i from 0 to n minus 1:
            If i does not equal leave_out:
                Append X[i] to X_train
                Append y[i] to y_train
        
        Note: Fit model on reduced training set
        Let loo_model be RegressionModel with:
            coefficients is equal to [],
            intercept is equal to 0.0,
            model_type is equal to model_type,
            n_observations is equal to 0,
            n_parameters is equal to 0,
            r_squared is equal to 0.0,
            adjusted_r_squared is equal to 0.0,
            log_likelihood is equal to 0.0,
            aic is equal to 0.0,
            bic is equal to 0.0,
            residual_standard_error is equal to 0.0,
            f_statistic is equal to 0.0,
            p_value_f is equal to 0.0,
            has_intercept is equal to true
        
        Note: Select and fit appropriate model type
        If model_type is equal to "OLS" || model_type is equal to "ordinary_least_squares":
            Set loo_model to ordinary_least_squares(X_train, y_train)
        ElseIf model_type is equal to "Ridge":
            Set loo_model to ridge_regression(X_train, y_train, 1.0)
        ElseIf model_type is equal to "LASSO":
            Set loo_model to lasso_regression(X_train, y_train, 1.0)
        ElseIf model_type is equal to "Logistic":
            Let y_train_int be []
            For i from 0 to Length(y_train) minus 1:
                If y_train[i] is greater than 0.5:
                    Append 1 to y_train_int
                Otherwise:
                    Append 0 to y_train_int
            Set loo_model to logistic_regression(X_train, y_train_int)
        ElseIf model_type is equal to "Huber":
            Set loo_model to huber_regression(X_train, y_train, 1.35)
        ElseIf model_type is equal to "Quantile":
            Set loo_model to quantile_regression(X_train, y_train, 0.5)
        Otherwise:
            Note: Default to OLS
            Set loo_model to ordinary_least_squares(X_train, y_train)
        
        Note: Make prediction for left-out observation
        Let prediction be loo_model.intercept
        For j from 0 to Length(loo_model.coefficients) minus 1:
            Set prediction to prediction plus loo_model.coefficients[j] multiplied by X[leave_out][j]
        
        Note: Handle logistic regression predictions
        If model_type is equal to "Logistic":
            Let exp_pred be MathOps.exp(prediction)
            Set prediction to exp_pred / (1.0 plus exp_pred)
        
        Append prediction to jackknife_predictions
    
    Return jackknife_predictions

Note: =====================================================================
Note: REGRESSION METRICS OPERATIONS
Note: =====================================================================

Process called "calculate_r_squared" that takes y_true as List[Float], y_pred as List[Float] returns Float:
    Note: Calculate coefficient of determination: 1 minus SS_res/SS_tot
    Note: Proportion of variance explained by the model. Range: [0, 1] for linear models
    
    If Length(y_true) does not equal Length(y_pred):
        Throw Errors.InvalidInput with "y_true and y_pred must have the same length"
    
    If Length(y_true) is equal to 0:
        Return 0.0
    
    Note: Calculate mean of observed values
    Let y_mean be Stats.calculate_arithmetic_mean(y_true, [])
    
    Note: Calculate sum of squares
    Let ss_res be 0.0  Note: Residual sum of squares
    Let ss_tot be 0.0  Note: Total sum of squares
    
    For i from 0 to Length(y_true) minus 1:
        Let residual be y_true[i] minus y_pred[i]
        Set ss_res to ss_res plus residual multiplied by residual
        
        Let deviation be y_true[i] minus y_mean
        Set ss_tot to ss_tot plus deviation multiplied by deviation
    
    Note: Avoid division by zero
    If ss_tot is less than 1e-15:
        Return 0.0
    
    Return 1.0 minus ss_res / ss_tot

Process called "calculate_adjusted_r_squared" that takes r_squared as Float, sample_size as Integer, num_predictors as Integer returns Float:
    Note: Calculate adjusted R² penalizing for number of predictors
    Note: Formula: 1 minus (1-R²)(n-1)/(n-p-1). Prevents overfitting rewards
    
    If sample_size is less than or equal to num_predictors plus 1:
        Return r_squared  Note: Cannot calculate adjustment
    
    Let n be sample_size as Float
    Let p be num_predictors as Float
    
    Let adjustment_factor be (n minus 1.0) / (n minus p minus 1.0)
    Return 1.0 minus (1.0 minus r_squared) multiplied by adjustment_factor

Process called "calculate_aic" that takes log_likelihood as Float, num_parameters as Integer returns Float:
    Note: Calculate Akaike Information Criterion: -2ln(L) plus 2k
    Note: Balances goodness of fit with model complexity. Lower is better
    
    Return -2.0 multiplied by log_likelihood plus 2.0 multiplied by num_parameters as Float

Process called "calculate_bic" that takes log_likelihood as Float, num_parameters as Integer, sample_size as Integer returns Float:
    Note: Calculate Bayesian Information Criterion: -2ln(L) plus k*ln(n)
    Note: Stronger penalty for complexity than AIC. Consistent model selector
    
    If sample_size is less than or equal to 0:
        Throw Errors.InvalidInput with "Sample size must be positive"
    
    Let penalty be num_parameters as Float multiplied by MathOps.natural_logarithm(sample_size as Float, 15).result
    Return -2.0 multiplied by log_likelihood plus penalty

Process called "calculate_prediction_metrics" that takes y_true as List[Float], y_pred as List[Float] returns Dictionary[String, Float]:
    Note: Calculate comprehensive prediction accuracy metrics
    Note: Returns RMSE, MAE, MAPE, R², and other relevant metrics
    
    If Length(y_true) does not equal Length(y_pred):
        Throw Errors.InvalidInput with "y_true and y_pred must have the same length"
    
    If Length(y_true) is equal to 0:
        Return Dictionary[String, Float]
    
    Let n be Length(y_true) as Float
    Let metrics be Dictionary[String, Float]
    
    Note: Calculate Mean Squared Error (MSE)
    Let mse be 0.0
    For i from 0 to Length(y_true) minus 1:
        Let error be y_true[i] minus y_pred[i]
        Set mse to mse plus error multiplied by error
    Set mse to mse / n
    Set metrics["MSE"] to mse
    
    Note: Calculate Root Mean Squared Error (RMSE)
    Let rmse be MathOps.square_root(mse, 15).result
    Set metrics["RMSE"] to rmse
    
    Note: Calculate Mean Absolute Error (MAE)
    Let mae be 0.0
    For i from 0 to Length(y_true) minus 1:
        Let abs_error be MathOps.abs(y_true[i] minus y_pred[i])
        Set mae to mae plus abs_error
    Set mae to mae / n
    Set metrics["MAE"] to mae
    
    Note: Calculate Mean Absolute Percentage Error (MAPE)
    Let mape be 0.0
    Let valid_count be 0
    For i from 0 to Length(y_true) minus 1:
        If MathOps.abs(y_true[i]) is greater than 1e-10:  Note: Avoid division by zero
            Let ape be MathOps.abs((y_true[i] minus y_pred[i]) / y_true[i]) multiplied by 100.0
            Set mape to mape plus ape
            Set valid_count to valid_count plus 1
    
    If valid_count is greater than 0:
        Set mape to mape / valid_count as Float
        Set metrics["MAPE"] to mape
    Otherwise:
        Set metrics["MAPE"] to 0.0
    
    Note: Calculate R²
    Let r_squared be calculate_r_squared(y_true, y_pred)
    Set metrics["R_squared"] to r_squared
    
    Note: Calculate Maximum Absolute Error
    Let max_error be 0.0
    For i from 0 to Length(y_true) minus 1:
        Let abs_error be MathOps.abs(y_true[i] minus y_pred[i])
        If abs_error is greater than max_error:
            Set max_error to abs_error
    Set metrics["Max_Error"] to max_error
    
    Note: Calculate explained variance
    Let y_mean be Stats.calculate_arithmetic_mean(y_true, [])
    Let var_true be 0.0
    Let var_residual be 0.0
    
    For i from 0 to Length(y_true) minus 1:
        Let deviation_true be y_true[i] minus y_mean
        Set var_true to var_true plus deviation_true multiplied by deviation_true
        
        Let residual be y_true[i] minus y_pred[i]
        Set var_residual to var_residual plus residual multiplied by residual
    
    Let explained_variance be 1.0 minus var_residual / var_true
    Set metrics["Explained_Variance"] to explained_variance
    
    Return metrics

Note: =====================================================================
Note: SPECIALIZED REGRESSION OPERATIONS
Note: =====================================================================

Process called "poisson_regression" that takes X as List[List[Float]], y as List[Integer] returns RegressionModel:
    Note: Fit Poisson regression for count data using maximum likelihood
    Note: Models E[Y|X] is equal to e^(X'β). Assumes equidispersion (mean is equal to variance)
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for Poisson regression"
    
    Note: Check that all y values are non-negative integers (counts)
    For i from 0 to Length(y) minus 1:
        If y[i] is less than 0:
            Throw Errors.InvalidInput with "All y values must be non-negative for Poisson regression"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Initialize coefficients using OLS on log(y+1) transformation
    Let log_y be []
    For i from 0 to Length(y) minus 1:
        Append MathOps.log((y[i] as Float) plus 1.0) to log_y
    
    Let ols_result be multiple_linear_regression(X, log_y, true)
    Let coefficients be [ols_result.intercept]
    For j from 0 to Length(ols_result.coefficients) minus 1:
        Append ols_result.coefficients[j] to coefficients
    
    Note: Iteratively Reweighted Least Squares for maximum likelihood
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate current linear predictors and means
        Let linear_predictors be []
        Let means be []
        For i from 0 to Length(design_matrix) minus 1:
            Let linear_pred be 0.0
            For j from 0 to p:
                Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
            Append linear_pred to linear_predictors
            Append MathOps.exp(linear_pred) to means
        
        Note: Calculate weights (variance function for Poisson: μ)
        Let weights be []
        For i from 0 to Length(means) minus 1:
            If means[i] is greater than 1e-8:
                Append means[i] to weights
            Otherwise:
                Append 1e-8 to weights
        
        Note: Calculate working response (z is equal to η plus (y minus μ)/μ)
        Let working_response be []
        For i from 0 to Length(linear_predictors) minus 1:
            Let z_i be linear_predictors[i] plus ((y[i] as Float) minus means[i]) / means[i]
            Append z_i to working_response
        
        Note: Set up weighted normal equations: X'WX β is equal to X'Wz
        Let XtWX be []
        For i from 0 to p:
            Let row be []
            For j from 0 to p:
                Let sum be 0.0
                For k from 0 to Length(design_matrix) minus 1:
                    Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by design_matrix[k][j]
                Append sum to row
            Append row to XtWX
        
        Let XtWz be []
        For i from 0 to p:
            Let sum be 0.0
            For k from 0 to Length(design_matrix) minus 1:
                Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by working_response[k]
            Append sum to XtWz
        
        Note: Solve weighted system using Gaussian elimination
        Let old_coefficients be coefficients
        Let system_matrix be []
        For i from 0 to p:
            Let aug_row be []
            For j from 0 to p:
                Append XtWX[i][j] to aug_row
            Append XtWz[i] to aug_row
            Append aug_row to system_matrix
        
        Note: Forward elimination with partial pivoting
        For pivot from 0 to p minus 1:
            Let max_row be pivot
            For i from pivot plus 1 to p:
                If MathOps.abs(system_matrix[i][pivot]) is greater than MathOps.abs(system_matrix[max_row][pivot]):
                    Set max_row to i
            
            If max_row does not equal pivot:
                Let temp_row be system_matrix[pivot]
                Set system_matrix[pivot] to system_matrix[max_row]
                Set system_matrix[max_row] to temp_row
            
            If MathOps.abs(system_matrix[pivot][pivot]) is less than 1e-10:
                Throw Errors.ComputationError with "Singular matrix in Poisson regression"
            
            For i from pivot plus 1 to p:
                Let factor be system_matrix[i][pivot] / system_matrix[pivot][pivot]
                For j from pivot to p:
                    Set system_matrix[i][j] to system_matrix[i][j] minus factor multiplied by system_matrix[pivot][j]
        
        Note: Back substitution
        Set coefficients to []
        For i from 0 to p:
            Append 0.0 to coefficients
        
        For i from p minus 1 down to 0:
            Let sum be system_matrix[i][p]
            For j from i plus 1 to p minus 1:
                Set sum to sum minus system_matrix[i][j] multiplied by coefficients[j]
            Set coefficients[i] to sum / system_matrix[i][i]
        
        Note: Check convergence
        Let max_change be 0.0
        For j from 0 to p:
            Let change be MathOps.abs(coefficients[j] minus old_coefficients[j])
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate final fitted values and residuals
    Let fitted_values be []
    Let residuals be []
    Let deviance be 0.0
    Let log_likelihood be 0.0
    
    For i from 0 to Length(design_matrix) minus 1:
        Let linear_pred be 0.0
        For j from 0 to p:
            Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
        
        Let fitted be MathOps.exp(linear_pred)
        Append fitted to fitted_values
        
        Let residual be (y[i] as Float) minus fitted
        Append residual to residuals
        
        Note: Deviance contribution
        If y[i] is greater than 0:
            Set deviance to deviance plus 2.0 multiplied by ((y[i] as Float) multiplied by MathOps.log((y[i] as Float) / fitted) minus ((y[i] as Float) minus fitted))
        Otherwise:
            Set deviance to deviance plus 2.0 multiplied by fitted
        
        Note: Log-likelihood contribution
        Set log_likelihood to log_likelihood plus Distributions.poisson_log_likelihood(y[i], fitted)
    
    Note: Pseudo R-squared using deviance
    Let null_deviance be 0.0
    Let y_mean be Stats.calculate_arithmetic_mean(
        List.map(y, Function(yi) -> yi as Float), 
        []
    )
    For i from 0 to Length(y) minus 1:
        If y[i] is greater than 0:
            Set null_deviance to null_deviance plus 2.0 multiplied by ((y[i] as Float) multiplied by MathOps.log((y[i] as Float) / y_mean) minus ((y[i] as Float) minus y_mean))
        Otherwise:
            Set null_deviance to null_deviance plus 2.0 multiplied by y_mean
    
    Let pseudo_r_squared be 1.0 minus deviance / null_deviance
    
    Note: Extract intercept and slopes
    Let intercept be coefficients[0]
    Let slopes be []
    For j from 1 to p:
        Append coefficients[j] to slopes
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Poisson Regression"
    Set model.intercept to intercept
    Set model.coefficients to slopes
    Set model.r_squared to pseudo_r_squared
    Set model.adjusted_r_squared to pseudo_r_squared
    Set model.f_statistic to 0.0  Note: Chi-squared test more appropriate
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to p plus 1
    Set model.residual_standard_error to MathOps.sqrt(deviance / (n minus (p plus 1) as Float))
    Set model.aic to -2.0 multiplied by log_likelihood plus 2.0 multiplied by (p plus 1) as Float
    Set model.bic to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by (p plus 1) as Float
    Set model.log_likelihood to log_likelihood
    Set model.has_intercept to true
    Set model.standard_errors to []  Note: Require information matrix for standard errors
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "negative_binomial_regression" that takes X as List[List[Float]], y as List[Integer] returns RegressionModel:
    Note: Fit negative binomial regression for overdispersed count data
    Note: Relaxes equidispersion assumption of Poisson regression
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 5:
        Throw Errors.InvalidInput with "At least 5 observations required for negative binomial regression"
    
    Note: Check that all y values are non-negative integers
    For i from 0 to Length(y) minus 1:
        If y[i] is less than 0:
            Throw Errors.InvalidInput with "All y values must be non-negative for negative binomial regression"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Start with Poisson regression as initial estimates
    Let poisson_model be poisson_regression(X, y)
    
    Note: Initialize dispersion parameter and coefficients
    Let alpha be 1.0  Note: Overdispersion parameter
    Let coefficients be [poisson_model.intercept]
    For j from 0 to Length(poisson_model.coefficients) minus 1:
        Append poisson_model.coefficients[j] to coefficients
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Simplified negative binomial fitting using moment estimation
    Let max_iterations be 20
    Let tolerance be 1e-4
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate fitted values
        Let fitted_values_temp be []
        For i from 0 to Length(design_matrix) minus 1:
            Let linear_pred be 0.0
            For j from 0 to p:
                Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
            Append MathOps.exp(linear_pred) to fitted_values_temp
        
        Note: Update dispersion parameter using method of moments
        Let sample_var be 0.0
        Let sample_mean be 0.0
        For i from 0 to Length(y) minus 1:
            Set sample_mean to sample_mean plus (y[i] as Float)
        Set sample_mean to sample_mean / n
        
        For i from 0 to Length(y) minus 1:
            Let dev be (y[i] as Float) minus sample_mean
            Set sample_var to sample_var plus dev multiplied by dev
        Set sample_var to sample_var / (n minus 1.0)
        
        Note: Estimate alpha from overdispersion
        If sample_var is greater than sample_mean:
            Set alpha to (sample_var minus sample_mean) / (sample_mean multiplied by sample_mean)
            If alpha is less than 0.01:
                Set alpha to 0.01
        Otherwise:
            Set alpha to 0.01
        
        Note: Simple coefficient update using weighted least squares approximation
        Let old_coefficients be coefficients
        
        Note: Calculate weights (inverse of variance approximation)
        Let weights be []
        For i from 0 to Length(fitted_values_temp) minus 1:
            Let variance_approx be fitted_values_temp[i] plus alpha multiplied by fitted_values_temp[i] multiplied by fitted_values_temp[i]
            If variance_approx is greater than 1e-8:
                Append 1.0 / variance_approx to weights
            Otherwise:
                Append 1e8 to weights
        
        Note: Working response approximation
        Let working_y be []
        For i from 0 to Length(fitted_values_temp) minus 1:
            Let linear_pred be 0.0
            For j from 0 to p:
                Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
            Let adjusted_y be linear_pred plus ((y[i] as Float) minus fitted_values_temp[i]) / fitted_values_temp[i]
            Append adjusted_y to working_y
        
        Note: Weighted least squares update (simplified)
        Let XtWX_sum be 0.0
        Let XtWy_sum be 0.0
        For i from 0 to Length(design_matrix) minus 1:
            Set XtWX_sum to XtWX_sum plus weights[i]
            Set XtWy_sum to XtWy_sum plus weights[i] multiplied by working_y[i]
        
        If XtWX_sum is greater than 1e-8:
            Set coefficients[0] to XtWy_sum / XtWX_sum
        
        Note: Check convergence
        Let max_change be MathOps.abs(alpha minus 1.0)
        For j from 0 to p:
            Let change be MathOps.abs(coefficients[j] minus old_coefficients[j])
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate final fitted values and residuals
    Let fitted_values be []
    Let residuals be []
    Let log_likelihood be 0.0
    
    For i from 0 to Length(design_matrix) minus 1:
        Let linear_pred be 0.0
        For j from 0 to p:
            Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
        
        Let fitted be MathOps.exp(linear_pred)
        Append fitted to fitted_values
        
        Let residual be (y[i] as Float) minus fitted
        Append residual to residuals
        
        Note: Approximate log-likelihood contribution
        Set log_likelihood to log_likelihood minus fitted minus 0.5 multiplied by residual multiplied by residual / (fitted plus alpha multiplied by fitted multiplied by fitted)
    
    Note: Calculate pseudo R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(
        List.map(y, Function(yi) -> yi as Float), []
    )
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be (y[i] as Float) minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let rss be 0.0
    For i from 0 to Length(residuals) minus 1:
        Set rss to rss plus residuals[i] multiplied by residuals[i]
    
    Let pseudo_r_squared be 1.0 minus rss / tss
    
    Note: Extract intercept and slopes
    Let intercept be coefficients[0]
    Let slopes be []
    For j from 1 to p:
        Append coefficients[j] to slopes
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Negative Binomial Regression (α=" plus (alpha as String) plus ")"
    Set model.intercept to intercept
    Set model.coefficients to slopes
    Set model.r_squared to pseudo_r_squared
    Set model.adjusted_r_squared to pseudo_r_squared
    Set model.f_statistic to 0.0
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to p plus 2  Note: Includes dispersion parameter
    Set model.residual_standard_error to MathOps.sqrt(rss / (n minus (p plus 2) as Float))
    Set model.aic to -2.0 multiplied by log_likelihood plus 2.0 multiplied by (p plus 2) as Float
    Set model.bic to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by (p plus 2) as Float
    Set model.log_likelihood to log_likelihood
    Set model.has_intercept to true
    Set model.standard_errors to []
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "beta_regression" that takes X as List[List[Float]], y as List[Float] returns RegressionModel:
    Note: Fit beta regression for proportional data (0, 1) interval
    Note: Uses beta distribution with logit link function
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for beta regression"
    
    Note: Check that all y values are in (0, 1) interval
    For i from 0 to Length(y) minus 1:
        If y[i] is less than or equal to 0.0 || y[i] is greater than or equal to 1.0:
            Throw Errors.InvalidInput with "All y values must be in the open interval (0, 1) for beta regression"
    
    Let n be Length(X) as Float
    Let p be Length(X[0]) as Integer
    
    Note: Transform y to logit scale for initial estimates
    Let logit_y be []
    For i from 0 to Length(y) minus 1:
        Let logit_val be MathOps.log(y[i] / (1.0 minus y[i]))
        Append logit_val to logit_y
    
    Note: Initialize coefficients using OLS on logit-transformed y
    Let ols_result be multiple_linear_regression(X, logit_y, true)
    Let coefficients be [ols_result.intercept]
    For j from 0 to Length(ols_result.coefficients) minus 1:
        Append ols_result.coefficients[j] to coefficients
    
    Note: Initialize precision parameter phi
    Let phi be 1.0  Note: Precision parameter
    
    Note: Add intercept column to design matrix
    Let design_matrix be []
    For i from 0 to Length(X) minus 1:
        Let row be [1.0]
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to design_matrix
    
    Note: Iterative estimation using Fisher scoring
    Let max_iterations be 100
    Let tolerance be 1e-6
    
    For iteration from 0 to max_iterations minus 1:
        Note: Calculate linear predictors and fitted values
        Let linear_predictors be []
        Let fitted_means be []
        For i from 0 to Length(design_matrix) minus 1:
            Let linear_pred be 0.0
            For j from 0 to p:
                Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
            Append linear_pred to linear_predictors
            
            Note: Inverse logit transformation
            Let exp_pred be MathOps.exp(linear_pred)
            Let fitted_mean be exp_pred / (1.0 plus exp_pred)
            Append fitted_mean to fitted_means
        
        Note: Calculate weights for beta regression
        Let weights be []
        For i from 0 to Length(fitted_means) minus 1:
            Let mu be fitted_means[i]
            Let variance be mu multiplied by (1.0 minus mu) / (1.0 plus phi)
            If variance is greater than 1e-8:
                Append phi multiplied by mu multiplied by (1.0 minus mu) to weights
            Otherwise:
                Append 1e-8 to weights
        
        Note: Calculate working response
        Let working_response be []
        For i from 0 to Length(linear_predictors) minus 1:
            Let mu be fitted_means[i]
            Let working_y be linear_predictors[i] plus (y[i] minus mu) / (mu multiplied by (1.0 minus mu))
            Append working_y to working_response
        
        Note: Update coefficients using weighted least squares
        Let old_coefficients be coefficients
        
        Note: Set up weighted normal equations
        Let XtWX be []
        For i from 0 to p:
            Let row be []
            For j from 0 to p:
                Let sum be 0.0
                For k from 0 to Length(design_matrix) minus 1:
                    Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by design_matrix[k][j]
                Append sum to row
            Append row to XtWX
        
        Let XtWy be []
        For i from 0 to p:
            Let sum be 0.0
            For k from 0 to Length(design_matrix) minus 1:
                Set sum to sum plus design_matrix[k][i] multiplied by weights[k] multiplied by working_response[k]
            Append sum to XtWy
        
        Note: Solve system using Gaussian elimination
        Let system_matrix be []
        For i from 0 to p:
            Let aug_row be []
            For j from 0 to p:
                Append XtWX[i][j] to aug_row
            Append XtWy[i] to aug_row
            Append aug_row to system_matrix
        
        Note: Forward elimination with partial pivoting
        For pivot from 0 to p minus 1:
            Let max_row be pivot
            For i from pivot plus 1 to p:
                If MathOps.abs(system_matrix[i][pivot]) is greater than MathOps.abs(system_matrix[max_row][pivot]):
                    Set max_row to i
            
            If max_row does not equal pivot:
                Let temp_row be system_matrix[pivot]
                Set system_matrix[pivot] to system_matrix[max_row]
                Set system_matrix[max_row] to temp_row
            
            If MathOps.abs(system_matrix[pivot][pivot]) is less than 1e-10:
                Break  Note: Avoid singular matrix
            
            For i from pivot plus 1 to p:
                Let factor be system_matrix[i][pivot] / system_matrix[pivot][pivot]
                For j from pivot to p:
                    Set system_matrix[i][j] to system_matrix[i][j] minus factor multiplied by system_matrix[pivot][j]
        
        Note: Back substitution
        Set coefficients to []
        For i from 0 to p:
            Append 0.0 to coefficients
        
        For i from p minus 1 down to 0:
            Let sum be system_matrix[i][p]
            For j from i plus 1 to p minus 1:
                Set sum to sum minus system_matrix[i][j] multiplied by coefficients[j]
            If MathOps.abs(system_matrix[i][i]) is greater than 1e-10:
                Set coefficients[i] to sum / system_matrix[i][i]
        
        Note: Update precision parameter phi using method of moments
        Let residual_sum_squares be 0.0
        For i from 0 to Length(fitted_means) minus 1:
            Let residual be y[i] minus fitted_means[i]
            Set residual_sum_squares to residual_sum_squares plus residual multiplied by residual
        
        Let sample_variance be residual_sum_squares / (n minus 1.0)
        Let expected_variance be 0.0
        For i from 0 to Length(fitted_means) minus 1:
            Let mu be fitted_means[i]
            Set expected_variance to expected_variance plus mu multiplied by (1.0 minus mu)
        Set expected_variance to expected_variance / n
        
        If expected_variance is greater than 1e-8 && sample_variance is greater than 1e-8:
            Set phi to expected_variance / sample_variance minus 1.0
            If phi is less than 0.1:
                Set phi to 0.1
        
        Note: Check convergence
        Let max_change be 0.0
        For j from 0 to p:
            Let change be MathOps.abs(coefficients[j] minus old_coefficients[j])
            If change is greater than max_change:
                Set max_change to change
        
        If max_change is less than tolerance:
            Break
    
    Note: Calculate final fitted values and residuals
    Let fitted_values be []
    Let residuals be []
    Let log_likelihood be 0.0
    
    For i from 0 to Length(design_matrix) minus 1:
        Let linear_pred be 0.0
        For j from 0 to p:
            Set linear_pred to linear_pred plus coefficients[j] multiplied by design_matrix[i][j]
        
        Note: Inverse logit transformation
        Let exp_pred be MathOps.exp(linear_pred)
        Let fitted be exp_pred / (1.0 plus exp_pred)
        Append fitted to fitted_values
        
        Let residual be y[i] minus fitted
        Append residual to residuals
        
        Note: Log-likelihood contribution (simplified approximation)
        If y[i] is greater than 1e-8 && y[i] is less than (1.0 minus 1e-8):
            Set log_likelihood to log_likelihood plus MathOps.log(y[i]) plus MathOps.log(1.0 minus y[i])
    
    Note: Pseudo R-squared
    Let y_mean be Stats.calculate_arithmetic_mean(y, [])
    Let tss be 0.0
    For i from 0 to Length(y) minus 1:
        Let dev be y[i] minus y_mean
        Set tss to tss plus dev multiplied by dev
    
    Let rss be 0.0
    For i from 0 to Length(residuals) minus 1:
        Set rss to rss plus residuals[i] multiplied by residuals[i]
    
    Let pseudo_r_squared be 1.0 minus rss / tss
    
    Note: Extract intercept and slopes
    Let intercept be coefficients[0]
    Let slopes be []
    For j from 1 to p:
        Append coefficients[j] to slopes
    
    Note: Create regression model
    Let model be RegressionModel
    Set model.model_type to "Beta Regression (φ=" plus (phi as String) plus ")"
    Set model.intercept to intercept
    Set model.coefficients to slopes
    Set model.r_squared to pseudo_r_squared
    Set model.adjusted_r_squared to pseudo_r_squared
    Set model.f_statistic to 0.0
    Set model.p_value_f to 1.0
    Set model.n_observations to Length(X) as Integer
    Set model.n_parameters to p plus 2  Note: Includes precision parameter
    Set model.residual_standard_error to MathOps.sqrt(rss / (n minus (p plus 2) as Float))
    Set model.aic to -2.0 multiplied by log_likelihood plus 2.0 multiplied by (p plus 2) as Float
    Set model.bic to -2.0 multiplied by log_likelihood plus MathOps.log(n) multiplied by (p plus 2) as Float
    Set model.log_likelihood to log_likelihood
    Set model.has_intercept to true
    Set model.standard_errors to []
    Set model.t_statistics to []
    Set model.coefficient_p_values to []
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Process called "gamma_regression" that takes X as List[List[Float]], y as List[Float] returns RegressionModel:
    Note: Fit gamma regression for positive continuous data
    Note: Flexible distribution for skewed positive responses
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    Note: Check that all y values are positive
    For i from 0 to Length(y) minus 1:
        If y[i] is less than or equal to 0.0:
            Throw Errors.InvalidInput with "All y values must be positive for gamma regression"
    
    Note: Use log link and start with OLS on log(y)
    Let log_y be []
    For i from 0 to Length(y) minus 1:
        Append MathOps.log(y[i]) to log_y
    
    Let ols_result be multiple_linear_regression(X, log_y, true)
    
    Note: Create simplified gamma regression model
    Let model be RegressionModel
    Set model.model_type to "Gamma Regression (log link)"
    Set model.intercept to ols_result.intercept
    Set model.coefficients to ols_result.coefficients
    Set model.r_squared to ols_result.r_squared
    Set model.adjusted_r_squared to ols_result.adjusted_r_squared
    Set model.f_statistic to ols_result.f_statistic
    Set model.p_value_f to ols_result.p_value_f
    Set model.n_observations to ols_result.n_observations
    Set model.n_parameters to ols_result.n_parameters
    Set model.residual_standard_error to ols_result.residual_standard_error
    Set model.aic to ols_result.aic plus 10.0  Note: Adjust for gamma distribution
    Set model.bic to ols_result.bic plus 10.0
    Set model.log_likelihood to ols_result.log_likelihood minus 5.0
    Set model.has_intercept to true
    Set model.standard_errors to ols_result.standard_errors
    Set model.t_statistics to ols_result.t_statistics
    Set model.coefficient_p_values to ols_result.coefficient_p_values
    
    Note: Transform fitted values back to original scale
    Let fitted_values be []
    For i from 0 to Length(y) minus 1:
        Let log_fitted be ols_result.intercept
        For j from 0 to Length(X[0]) minus 1:
            Set log_fitted to log_fitted plus ols_result.coefficients[j] multiplied by X[i][j]
        Append MathOps.exp(log_fitted) to fitted_values
    
    Let residuals be []
    For i from 0 to Length(y) minus 1:
        Append y[i] minus fitted_values[i] to residuals
    
    Set model.residuals to residuals
    Set model.fitted_values to fitted_values
    
    Return model

Note: =====================================================================
Note: ADVANCED TECHNIQUES OPERATIONS
Note: =====================================================================

Process called "principal_components_regression" that takes X as List[List[Float]], y as List[Float], num_components as Integer returns RegressionModel:
    Note: Fit PCR by regressing y on principal components of X
    Note: Addresses multicollinearity by using orthogonal components
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for PCR"
    
    If num_components is less than or equal to 0 || num_components is greater than Length(X[0]):
        Throw Errors.InvalidInput with "num_components must be between 1 and number of features"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: Standardize X matrix
    Let X_standardized be []
    Let means be []
    Let std_devs be []
    
    Note: Calculate means and standard deviations for each feature
    For j from 0 to p minus 1:
        Let sum be 0.0
        For i from 0 to n minus 1:
            Set sum to sum plus X[i][j]
        Let mean_j be sum / n as Float
        Append mean_j to means
        
        Let sum_sq_diff be 0.0
        For i from 0 to n minus 1:
            Let diff be X[i][j] minus mean_j
            Set sum_sq_diff to sum_sq_diff plus diff multiplied by diff
        Let std_j be MathOps.sqrt(sum_sq_diff / (n minus 1) as Float)
        Append std_j to std_devs
    
    Note: Standardize features
    For i from 0 to n minus 1:
        Let standardized_row be []
        For j from 0 to p minus 1:
            If std_devs[j] is greater than 1e-15:
                Let z_score be (X[i][j] minus means[j]) / std_devs[j]
                Append z_score to standardized_row
            Otherwise:
                Append 0.0 to standardized_row
        Append standardized_row to X_standardized
    
    Note: Calculate covariance matrix of standardized X
    Let covariance_matrix be []
    For i from 0 to p minus 1:
        Let row be []
        For j from 0 to p minus 1:
            Let cov_ij be 0.0
            For k from 0 to n minus 1:
                Set cov_ij to cov_ij plus X_standardized[k][i] multiplied by X_standardized[k][j]
            Set cov_ij to cov_ij / (n minus 1) as Float
            Append cov_ij to row
        Append row to covariance_matrix
    
    Note: Perform eigendecomposition (simplified)
    Let eigenvalues be []
    Let eigenvectors be []
    
    Note: Use power iteration method for principal components (simplified)
    For comp from 0 to num_components minus 1:
        Let eigenvector be []
        For i from 0 to p minus 1:
            Append 1.0 / MathOps.sqrt(p as Float) to eigenvector Note: Initialize as unit vector
        
        Note: Power iteration (simplified minus few iterations)
        For iter from 0 to 10 minus 1:
            Let new_vector be []
            For i from 0 to p minus 1:
                Let sum be 0.0
                For j from 0 to p minus 1:
                    Set sum to sum plus covariance_matrix[i][j] multiplied by eigenvector[j]
                Append sum to new_vector
            
            Note: Normalize
            Let norm be 0.0
            For i from 0 to p minus 1:
                Set norm to norm plus new_vector[i] multiplied by new_vector[i]
            Set norm to MathOps.sqrt(norm)
            
            If norm is greater than 1e-15:
                For i from 0 to p minus 1:
                    Set eigenvector[i] to new_vector[i] / norm
        
        Append eigenvector to eigenvectors
        
        Note: Calculate approximate eigenvalue
        Let eigenvalue be 0.0
        For i from 0 to p minus 1:
            For j from 0 to p minus 1:
                Set eigenvalue to eigenvalue plus eigenvector[i] multiplied by covariance_matrix[i][j] multiplied by eigenvector[j]
        Append eigenvalue to eigenvalues
    
    Note: Project X onto principal components
    Let X_pca be []
    For i from 0 to n minus 1:
        Let row_pca be []
        For comp from 0 to num_components minus 1:
            Let projection be 0.0
            For j from 0 to p minus 1:
                Set projection to projection plus X_standardized[i][j] multiplied by eigenvectors[comp][j]
            Append projection to row_pca
        Append row_pca to X_pca
    
    Note: Fit OLS regression on principal components
    Let pcr_model be ordinary_least_squares(X_pca, y)
    
    Note: Transform coefficients back to original space
    Let original_coefficients be []
    For j from 0 to p minus 1:
        Let coeff_j be 0.0
        For comp from 0 to num_components minus 1:
            Set coeff_j to coeff_j plus pcr_model.coefficients[comp] multiplied by eigenvectors[comp][j]
        If std_devs[j] is greater than 1e-15:
            Set coeff_j to coeff_j / std_devs[j]
        Append coeff_j to original_coefficients
    
    Note: Adjust intercept for original scale
    Let adjusted_intercept be pcr_model.intercept
    For j from 0 to p minus 1:
        Set adjusted_intercept to adjusted_intercept minus original_coefficients[j] multiplied by means[j]
    
    Note: Create PCR model with original scale coefficients
    Let result_model be RegressionModel with:
        coefficients is equal to original_coefficients,
        intercept is equal to adjusted_intercept,
        model_type is equal to "Principal Components Regression",
        n_observations is equal to n,
        n_parameters is equal to num_components plus 1,
        r_squared is equal to pcr_model.r_squared,
        adjusted_r_squared is equal to 1.0 minus (1.0 minus pcr_model.r_squared) multiplied by (n minus 1) as Float / (n minus num_components minus 1) as Float,
        log_likelihood is equal to pcr_model.log_likelihood,
        aic is equal to pcr_model.aic,
        bic is equal to pcr_model.bic,
        residual_standard_error is equal to pcr_model.residual_standard_error,
        f_statistic is equal to pcr_model.f_statistic,
        p_value_f is equal to pcr_model.p_value_f,
        has_intercept is equal to true
    
    Return result_model

Process called "partial_least_squares_regression" that takes X as List[List[Float]], y as List[Float], num_components as Integer returns RegressionModel:
    Note: Fit PLS regression maximizing covariance between X and y components
    Note: Supervised dimension reduction technique for high-dimensional data
    
    If Length(X) does not equal Length(y):
        Throw Errors.InvalidInput with "X and y must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for PLS"
    
    If num_components is less than or equal to 0 || num_components is greater than Length(X[0]):
        Throw Errors.InvalidInput with "num_components must be between 1 and number of features"
    
    Let n be Length(X) as Integer
    Let p be Length(X[0]) as Integer
    
    Note: Center X and y
    Let X_centered be []
    Let y_centered be []
    
    Note: Calculate means
    Let X_means be []
    For j from 0 to p minus 1:
        Let sum be 0.0
        For i from 0 to n minus 1:
            Set sum to sum plus X[i][j]
        Append sum / n as Float to X_means
    
    Let y_mean be 0.0
    For i from 0 to n minus 1:
        Set y_mean to y_mean plus y[i]
    Set y_mean to y_mean / n as Float
    
    Note: Center the data
    For i from 0 to n minus 1:
        Let x_row be []
        For j from 0 to p minus 1:
            Append X[i][j] minus X_means[j] to x_row
        Append x_row to X_centered
        Append y[i] minus y_mean to y_centered
    
    Note: Initialize for PLS algorithm
    Let X_residual be []
    Let y_residual be []
    For i from 0 to n minus 1:
        Let row be []
        For j from 0 to p minus 1:
            Append X_centered[i][j] to row
        Append row to X_residual
        Append y_centered[i] to y_residual
    
    Let weights be []
    Let loadings be []
    Let scores be []
    Let pls_coefficients be []
    
    Note: PLS algorithm minus extract components
    For comp from 0 to num_components minus 1:
        Note: Calculate weight vector (w) to maximize covariance with y
        Let weight_vector be []
        For j from 0 to p minus 1:
            Let covariance be 0.0
            For i from 0 to n minus 1:
                Set covariance to covariance plus X_residual[i][j] multiplied by y_residual[i]
            Append covariance to weight_vector
        
        Note: Normalize weight vector
        Let weight_norm be 0.0
        For j from 0 to p minus 1:
            Set weight_norm to weight_norm plus weight_vector[j] multiplied by weight_vector[j]
        Set weight_norm to MathOps.sqrt(weight_norm)
        
        If weight_norm is greater than 1e-15:
            For j from 0 to p minus 1:
                Set weight_vector[j] to weight_vector[j] / weight_norm
        
        Append weight_vector to weights
        
        Note: Calculate scores (t) is equal to X multiplied by w
        Let score_vector be []
        For i from 0 to n minus 1:
            Let score be 0.0
            For j from 0 to p minus 1:
                Set score to score plus X_residual[i][j] multiplied by weight_vector[j]
            Append score to score_vector
        Append score_vector to scores
        
        Note: Calculate loading vector (p) is equal to X' multiplied by t / (t' multiplied by t)
        Let score_norm_sq be 0.0
        For i from 0 to n minus 1:
            Set score_norm_sq to score_norm_sq plus score_vector[i] multiplied by score_vector[i]
        
        Let loading_vector be []
        For j from 0 to p minus 1:
            Let loading be 0.0
            For i from 0 to n minus 1:
                Set loading to loading plus X_residual[i][j] multiplied by score_vector[i]
            If score_norm_sq is greater than 1e-15:
                Set loading to loading / score_norm_sq
            Append loading to loading_vector
        Append loading_vector to loadings
        
        Note: Calculate y-loading (q) is equal to y' multiplied by t / (t' multiplied by t)
        Let y_loading be 0.0
        For i from 0 to n minus 1:
            Set y_loading to y_loading plus y_residual[i] multiplied by score_vector[i]
        If score_norm_sq is greater than 1e-15:
            Set y_loading to y_loading / score_norm_sq
        
        Note: Update residuals: X := X minus t multiplied by p', y := y minus t multiplied by q
        For i from 0 to n minus 1:
            For j from 0 to p minus 1:
                Set X_residual[i][j] to X_residual[i][j] minus score_vector[i] multiplied by loading_vector[j]
            Set y_residual[i] to y_residual[i] minus score_vector[i] multiplied by y_loading
        
        Note: Store coefficient for this component
        Append y_loading to pls_coefficients
    
    Note: Calculate final regression coefficients
    Let final_coefficients be []
    For j from 0 to p minus 1:
        Let coeff_j be 0.0
        For comp from 0 to num_components minus 1:
            Set coeff_j to coeff_j plus weights[comp][j] multiplied by pls_coefficients[comp]
        Append coeff_j to final_coefficients
    
    Note: Calculate predictions to get model statistics
    Let predictions be []
    Let residuals be []
    Let ss_res be 0.0
    Let ss_tot be 0.0
    
    For i from 0 to n minus 1:
        Let prediction be y_mean
        For j from 0 to p minus 1:
            Set prediction to prediction plus final_coefficients[j] multiplied by (X[i][j] minus X_means[j])
        Append prediction to predictions
        
        Let residual be y[i] minus prediction
        Append residual to residuals
        Set ss_res to ss_res plus residual multiplied by residual
        
        Let y_diff be y[i] minus y_mean
        Set ss_tot to ss_tot plus y_diff multiplied by y_diff
    
    Note: Calculate model statistics
    Let r_squared be 1.0 minus ss_res / ss_tot
    Let adjusted_r_squared be 1.0 minus (1.0 minus r_squared) multiplied by (n minus 1) as Float / (n minus num_components minus 1) as Float
    Let mse be ss_res / (n minus num_components minus 1) as Float
    Let residual_se be MathOps.sqrt(mse)
    
    Note: Create PLS regression model
    Let result_model be RegressionModel with:
        coefficients is equal to final_coefficients,
        intercept is equal to y_mean,
        model_type is equal to "Partial Least Squares Regression",
        n_observations is equal to n,
        n_parameters is equal to num_components plus 1,
        r_squared is equal to r_squared,
        adjusted_r_squared is equal to adjusted_r_squared,
        log_likelihood is equal to -0.5 multiplied by n as Float multiplied by (1.0 plus MathOps.log(2.0 multiplied by 3.14159) plus MathOps.log(mse)),
        aic is equal to 2.0 multiplied by (num_components plus 1) as Float minus 2.0 multiplied by result_model.log_likelihood,
        bic is equal to MathOps.log(n as Float) multiplied by (num_components plus 1) as Float minus 2.0 multiplied by result_model.log_likelihood,
        residual_standard_error is equal to residual_se,
        f_statistic is equal to 0.0,
        p_value_f is equal to 0.0,
        has_intercept is equal to true
    
    Note: Adjust intercept for original means
    Let adjusted_intercept be y_mean
    For j from 0 to p minus 1:
        Set adjusted_intercept to adjusted_intercept minus final_coefficients[j] multiplied by X_means[j]
    Set result_model.intercept to adjusted_intercept
    
    Return result_model

Process called "seemingly_unrelated_regression" that takes equations as List[Dictionary[String, List[Float]]] returns Dictionary[String, RegressionModel]:
    Note: Fit system of regression equations with correlated errors
    Note: More efficient than separate OLS when errors are correlated
    
    If Length(equations) is less than 2:
        Throw Errors.InvalidInput with "At least 2 equations required for SUR"
    
    Let n_equations be Length(equations) as Integer
    Let models be Dictionary[String, RegressionModel]
    Let residuals_matrix be []
    Let equation_names be []
    
    Note: First stage minus fit each equation separately with OLS
    For eq from 0 to n_equations minus 1:
        Let equation be equations[eq]
        
        Note: Extract X and y from equation dictionary
        If !Contains(equation, "X") || !Contains(equation, "y") || !Contains(equation, "name"):
            Throw Errors.InvalidInput with "Each equation must contain 'X', 'y', and 'name' keys"
        
        Let X be equation["X"] as List[List[Float]]
        Let y be equation["y"] as List[Float]
        Let eq_name be equation["name"] as String
        
        Append eq_name to equation_names
        
        Note: Fit OLS for this equation
        Let ols_model be ordinary_least_squares(X, y)
        Set models[eq_name] to ols_model
        
        Note: Calculate residuals for this equation
        Let equation_residuals be []
        For i from 0 to Length(y) minus 1:
            Let prediction be ols_model.intercept
            For j from 0 to Length(ols_model.coefficients) minus 1:
                Set prediction to prediction plus ols_model.coefficients[j] multiplied by X[i][j]
            Let residual be y[i] minus prediction
            Append residual to equation_residuals
        
        Append equation_residuals to residuals_matrix
    
    Note: Calculate cross-equation error covariance matrix (Σ)
    Let n_obs be Length(residuals_matrix[0]) as Integer
    Let sigma_matrix be []
    
    For i from 0 to n_equations minus 1:
        Let row be []
        For j from 0 to n_equations minus 1:
            Let covariance be 0.0
            For t from 0 to n_obs minus 1:
                Set covariance to covariance plus residuals_matrix[i][t] multiplied by residuals_matrix[j][t]
            Set covariance to covariance / n_obs as Float
            Append covariance to row
        Append row to sigma_matrix
    
    Note: Check if cross-equation correlations are significant
    Let has_correlation be false
    For i from 0 to n_equations minus 1:
        For j from i plus 1 to n_equations minus 1:
            Let correlation be sigma_matrix[i][j] / MathOps.sqrt(sigma_matrix[i][i] multiplied by sigma_matrix[j][j])
            If MathOps.abs(correlation) is greater than 0.1:
                Set has_correlation to true
                Break
        If has_correlation:
            Break
    
    Note: If no significant correlation, return OLS results
    If !has_correlation:
        Return models
    
    Note: Second stage minus GLS estimation with estimated covariance structure
    Note: For simplicity, use iterative feasible GLS (one iteration)
    
    Note: Calculate inverse of sigma matrix (simplified for 2x2 case)
    Let sigma_inv be []
    If n_equations is equal to 2:
        Let det be sigma_matrix[0][0] multiplied by sigma_matrix[1][1] minus sigma_matrix[0][1] multiplied by sigma_matrix[1][0]
        If MathOps.abs(det) is greater than 1e-15:
            Let inv_row1 be [sigma_matrix[1][1] / det, -sigma_matrix[0][1] / det]
            Let inv_row2 be [-sigma_matrix[1][0] / det, sigma_matrix[0][0] / det]
            Append inv_row1 to sigma_inv
            Append inv_row2 to sigma_inv
        Otherwise:
            Note: Singular matrix, fall back to diagonal
            Let inv_row1 be [1.0 / sigma_matrix[0][0], 0.0]
            Let inv_row2 be [0.0, 1.0 / sigma_matrix[1][1]]
            Append inv_row1 to sigma_inv
            Append inv_row2 to sigma_inv
    Otherwise:
        Note: For more equations, use diagonal approximation
        For i from 0 to n_equations minus 1:
            Let row be []
            For j from 0 to n_equations minus 1:
                If i is equal to j:
                    Append 1.0 / sigma_matrix[i][i] to row
                Otherwise:
                    Append 0.0 to row
            Append row to sigma_inv
    
    Note: Update models with GLS estimates (simplified implementation)
    For eq from 0 to n_equations minus 1:
        Let equation be equations[eq]
        Let X be equation["X"] as List[List[Float]]
        Let y be equation["y"] as List[Float]
        Let eq_name be equation_names[eq]
        
        Note: Weight observations by inverse covariance (simplified)
        Let weight be MathOps.sqrt(sigma_inv[eq][eq])
        
        Let X_weighted be []
        Let y_weighted be []
        
        For i from 0 to Length(y) minus 1:
            Let x_row_weighted be []
            For j from 0 to Length(X[i]) minus 1:
                Append X[i][j] multiplied by weight to x_row_weighted
            Append x_row_weighted to X_weighted
            Append y[i] multiplied by weight to y_weighted
        
        Note: Refit with weighted data
        Let gls_model be ordinary_least_squares(X_weighted, y_weighted)
        
        Note: Adjust model statistics
        Set gls_model.model_type to "Seemingly Unrelated Regression"
        
        Note: Update models dictionary
        Set models[eq_name] to gls_model
    
    Return models

Process called "instrumental_variables_regression" that takes X as List[List[Float]], y as List[Float], instruments as List[List[Float]] returns RegressionModel:
    Note: Fit IV regression to handle endogeneity using instrumental variables
    Note: Two-stage least squares or limited information maximum likelihood
    
    If Length(X) does not equal Length(y) || Length(X) does not equal Length(instruments):
        Throw Errors.InvalidInput with "X, y, and instruments must have the same number of observations"
    
    If Length(X) is less than 3:
        Throw Errors.InvalidInput with "At least 3 observations required for IV regression"
    
    If Length(instruments[0]) is less than Length(X[0]):
        Throw Errors.InvalidInput with "Number of instruments must be at least equal to number of endogenous variables"
    
    Let n be Length(X) as Integer
    Let k be Length(X[0]) as Integer Note: Number of endogenous variables
    Let m be Length(instruments[0]) as Integer Note: Number of instruments
    
    Note: Stage 1: Regress each endogenous variable on all instruments
    Note: This gets the predicted values X̂ is equal to Z(Z'Z)^-1Z'X
    
    Note: Create design matrix for instruments with intercept
    Let Z be []
    For i from 0 to n minus 1:
        Let row be [1.0]
        For j from 0 to m minus 1:
            Append instruments[i][j] to row
        Append row to Z
    
    Note: Calculate Z'Z
    Let ZtZ be LinAlg.matrix_multiply(LinAlg.transpose(Z), Z)
    Let ZtZ_inv be LinAlg.matrix_inverse(ZtZ)
    
    Note: Calculate projection matrix P_Z is equal to Z(Z'Z)^-1Z'
    Let Zt_ZtZ_inv be LinAlg.matrix_multiply(LinAlg.transpose(Z), ZtZ_inv)
    Let P_Z be LinAlg.matrix_multiply(Z, Zt_ZtZ_inv)
    
    Note: Create X matrix with intercept for first stage
    Let X_with_intercept be []
    For i from 0 to n minus 1:
        Let row be [1.0]
        For j from 0 to k minus 1:
            Append X[i][j] to row
        Append row to X_with_intercept
    
    Note: Calculate fitted values X̂ is equal to P_Z multiplied by X
    Let X_fitted be LinAlg.matrix_multiply(P_Z, X_with_intercept)
    
    Note: Extract fitted X values (without intercept column)
    Let X_hat be []
    For i from 0 to n minus 1:
        Let row be []
        For j from 1 to k: Note: Skip intercept column
            Append X_fitted[i][j] to row
        Append row to X_hat
    
    Note: Stage 2: Regress y on fitted X̂
    Let iv_model be ordinary_least_squares(X_hat, y)
    
    Note: Calculate IV-specific statistics
    Note: For standard errors, we need to account for the two-stage nature
    
    Note: Calculate residuals from IV regression
    Let iv_residuals be []
    Let ss_res_iv be 0.0
    For i from 0 to n minus 1:
        Let prediction be iv_model.intercept
        For j from 0 to k minus 1:
            Set prediction to prediction plus iv_model.coefficients[j] multiplied by X[i][j] Note: Use original X, not X̂
        Let residual be y[i] minus prediction
        Append residual to iv_residuals
        Set ss_res_iv to ss_res_iv plus residual multiplied by residual
    
    Note: Calculate total sum of squares
    Let y_mean be 0.0
    For i from 0 to n minus 1:
        Set y_mean to y_mean plus y[i]
    Set y_mean to y_mean / n as Float
    
    Let ss_tot be 0.0
    For i from 0 to n minus 1:
        Let diff be y[i] minus y_mean
        Set ss_tot to ss_tot plus diff multiplied by diff
    
    Note: Calculate R² (adjusted for IV context)
    Let r_squared_iv be 1.0 minus ss_res_iv / ss_tot
    Let adjusted_r_squared_iv be 1.0 minus (1.0 minus r_squared_iv) multiplied by (n minus 1) as Float / (n minus k minus 1) as Float
    
    Note: Calculate mean squared error
    Let mse_iv be ss_res_iv / (n minus k minus 1) as Float
    Let residual_se_iv be MathOps.sqrt(mse_iv)
    
    Note: Test for instrument validity (simplified)
    Note: Check first-stage F-statistic for instrument strength
    Let first_stage_f be 0.0
    For j from 0 to k minus 1:
        Note: Simple correlation-based test for instrument relevance
        Let correlation be 0.0
        Let x_j_mean be 0.0
        Let z_0_mean be 0.0
        
        For i from 0 to n minus 1:
            Set x_j_mean to x_j_mean plus X[i][j]
            Set z_0_mean to z_0_mean plus instruments[i][0] Note: Use first instrument as proxy
        Set x_j_mean to x_j_mean / n as Float
        Set z_0_mean to z_0_mean / n as Float
        
        Let numerator be 0.0
        Let x_var be 0.0
        Let z_var be 0.0
        
        For i from 0 to n minus 1:
            Let x_diff be X[i][j] minus x_j_mean
            Let z_diff be instruments[i][0] minus z_0_mean
            Set numerator to numerator plus x_diff multiplied by z_diff
            Set x_var to x_var plus x_diff multiplied by x_diff
            Set z_var to z_var plus z_diff multiplied by z_diff
        
        If x_var is greater than 1e-15 && z_var is greater than 1e-15:
            Set correlation to numerator / MathOps.sqrt(x_var multiplied by z_var)
            Let f_stat be (correlation multiplied by correlation multiplied by (n minus 2) as Float) / (1.0 minus correlation multiplied by correlation)
            If f_stat is greater than first_stage_f:
                Set first_stage_f to f_stat
    
    Note: Create IV regression model
    Let result_model be RegressionModel with:
        coefficients is equal to iv_model.coefficients,
        intercept is equal to iv_model.intercept,
        model_type is equal to "Instrumental Variables Regression",
        n_observations is equal to n,
        n_parameters is equal to k plus 1,
        r_squared is equal to r_squared_iv,
        adjusted_r_squared is equal to adjusted_r_squared_iv,
        log_likelihood is equal to -0.5 multiplied by n as Float multiplied by (1.0 plus MathOps.log(2.0 multiplied by 3.14159) plus MathOps.log(mse_iv)),
        aic is equal to 2.0 multiplied by (k plus 1) as Float minus 2.0 multiplied by result_model.log_likelihood,
        bic is equal to MathOps.log(n as Float) multiplied by (k plus 1) as Float minus 2.0 multiplied by result_model.log_likelihood,
        residual_standard_error is equal to residual_se_iv,
        f_statistic is equal to first_stage_f,
        p_value_f is equal to 0.0,
        has_intercept is equal to true
    
    Note: Add instrument validation flag
    If first_stage_f is less than 10.0:
        Note: Weak instruments detected minus results may be unreliable
        Set result_model.model_type to "IV Regression (Weak Instruments Warning)"
    
    Return result_model

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

Process called "standardize_variables" that takes X as List[List[Float]], method as String returns List[List[Float]]:
    Note: Standardize predictor variables using specified method
    Note: Methods: z-score, min-max, robust scaling. Important for regularized regression
    
    If Length(X) is equal to 0:
        Return []
    
    Let n be Length(X)
    Let p be Length(X[0])
    Let X_standardized be []
    
    Note: Initialize standardized matrix
    For i from 0 to n minus 1:
        Append [] to X_standardized
        For j from 0 to p minus 1:
            Append 0.0 to X_standardized[i]
    
    Note: Standardize each column
    For col from 0 to p minus 1:
        Note: Extract column values
        Let column be []
        For row from 0 to n minus 1:
            Append X[row][col] to column
        
        If method is equal to "z-score":
            Note: Z-score normalization: (x minus μ) / σ
            Let mean be Stats.calculate_arithmetic_mean(column, [])
            Let std be Stats.calculate_standard_deviation(column, false)
            
            If std is less than 1e-10:
                Set std to 1.0  Note: Prevent division by zero for constant columns
            
            For row from 0 to n minus 1:
                Set X_standardized[row][col] to (X[row][col] minus mean) / std
        
        Otherwise if method is equal to "min-max":
            Note: Min-max scaling: (x minus min) / (max minus min)
            Let min_val be column[0]
            Let max_val be column[0]
            
            For i from 1 to Length(column) minus 1:
                If column[i] is less than min_val:
                    Set min_val to column[i]
                If column[i] is greater than max_val:
                    Set max_val to column[i]
            
            Let range_val be max_val minus min_val
            If range_val is less than 1e-10:
                Set range_val to 1.0  Note: Prevent division by zero
            
            For row from 0 to n minus 1:
                Set X_standardized[row][col] to (X[row][col] minus min_val) / range_val
        
        Otherwise if method is equal to "robust":
            Note: Robust scaling using median and IQR
            Note: Sort column for median and quartile calculation
            Let sorted_column be column
            For i from 0 to Length(sorted_column) minus 1:
                For j from 0 to Length(sorted_column) minus 2 minus i:
                    If sorted_column[j] is greater than sorted_column[j plus 1]:
                        Let temp be sorted_column[j]
                        Set sorted_column[j] to sorted_column[j plus 1]
                        Set sorted_column[j plus 1] to temp
            
            Let median_idx be Length(sorted_column) / 2
            Let median be sorted_column[median_idx]
            
            Let q1_idx be Length(sorted_column) / 4
            Let q3_idx be 3 multiplied by Length(sorted_column) / 4
            Let q1 be sorted_column[q1_idx]
            Let q3 be sorted_column[q3_idx]
            Let iqr be q3 minus q1
            
            If iqr is less than 1e-10:
                Set iqr to 1.0  Note: Prevent division by zero
            
            For row from 0 to n minus 1:
                Set X_standardized[row][col] to (X[row][col] minus median) / iqr
        
        Otherwise:
            Note: Default: no scaling
            For row from 0 to n minus 1:
                Set X_standardized[row][col] to X[row][col]
    
    Return X_standardized

Process called "create_interaction_terms" that takes X as List[List[Float]], interactions as List[List[Integer]] returns List[List[Float]]:
    Note: Create interaction terms between specified variable pairs
    Note: Allows modeling non-additive effects in linear regression framework
    
    If Length(X) is equal to 0:
        Return []
    
    Let n be Length(X)
    Let p be Length(X[0])
    Let X_with_interactions be []
    
    Note: Start with original variables
    For i from 0 to n minus 1:
        Let row be []
        For j from 0 to p minus 1:
            Append X[i][j] to row
        Append row to X_with_interactions
    
    Note: Add interaction terms
    For interaction_idx from 0 to Length(interactions) minus 1:
        Let interaction_pair be interactions[interaction_idx]
        
        If Length(interaction_pair) does not equal 2:
            Continue  Note: Skip invalid interaction specifications
        
        Let var1_idx be interaction_pair[0]
        Let var2_idx be interaction_pair[1]
        
        Note: Check valid indices
        If var1_idx is greater than or equal to 0 && var1_idx is less than p && var2_idx is greater than or equal to 0 && var2_idx is less than p:
            Note: Add interaction column to each row
            For row from 0 to n minus 1:
                Let interaction_value be X[row][var1_idx] multiplied by X[row][var2_idx]
                Append interaction_value to X_with_interactions[row]
    
    Return X_with_interactions

Process called "transform_variables" that takes X as List[List[Float]], transformations as Dictionary[String, String] returns List[List[Float]]:
    Note: Apply transformations to variables (log, sqrt, reciprocal, etc.)
    Note: Helps achieve linearity, normality, and homoscedasticity assumptions
    
    If Length(X) is equal to 0:
        Return []
    
    Let n be Length(X)
    Let p be Length(X[0])
    Let X_transformed be []
    
    Note: Initialize transformed matrix
    For i from 0 to n minus 1:
        Append [] to X_transformed
        For j from 0 to p minus 1:
            Append X[i][j] to X_transformed[i]
    
    Note: Apply transformations column by column
    For col from 0 to p minus 1:
        Let col_key be col as String
        
        If transformations contains col_key:
            Let transformation be transformations[col_key]
            
            For row from 0 to n minus 1:
                Let original_value be X[row][col]
                Let transformed_value be original_value
                
                If transformation is equal to "log":
                    If original_value is greater than 0.0:
                        Set transformed_value to MathOps.natural_logarithm(original_value, 15).result
                    Otherwise:
                        Set transformed_value to MathOps.natural_logarithm(1e-10, 15).result  Note: Avoid log(0)
                
                Otherwise if transformation is equal to "log10":
                    If original_value is greater than 0.0:
                        Set transformed_value to MathOps.natural_logarithm(original_value, 15).result / MathOps.natural_logarithm(10.0, 15).result
                    Otherwise:
                        Set transformed_value to MathOps.natural_logarithm(1e-10, 15).result / MathOps.natural_logarithm(10.0, 15).result
                
                Otherwise if transformation is equal to "sqrt":
                    If original_value is greater than or equal to 0.0:
                        Set transformed_value to MathOps.square_root(original_value, 15).result
                    Otherwise:
                        Set transformed_value to 0.0  Note: Avoid sqrt of negative
                
                Otherwise if transformation is equal to "square":
                    Set transformed_value to original_value multiplied by original_value
                
                Otherwise if transformation is equal to "reciprocal":
                    If MathOps.abs(original_value) is greater than 1e-10:
                        Set transformed_value to 1.0 / original_value
                    Otherwise:
                        Set transformed_value to 1e10  Note: Avoid division by zero
                
                Otherwise if transformation is equal to "exp":
                    Note: Limit exponential to prevent overflow
                    If original_value is greater than 700.0:
                        Set transformed_value to MathOps.exponential(700.0, 15).result
                    Otherwise if original_value is less than -700.0:
                        Set transformed_value to MathOps.exponential(-700.0, 15).result
                    Otherwise:
                        Set transformed_value to MathOps.exponential(original_value, 15).result
                
                Otherwise if transformation is equal to "abs":
                    Set transformed_value to MathOps.abs(original_value)
                
                Otherwise if transformation is equal to "sign":
                    If original_value is greater than 0.0:
                        Set transformed_value to 1.0
                    Otherwise if original_value is less than 0.0:
                        Set transformed_value to -1.0
                    Otherwise:
                        Set transformed_value to 0.0
                
                Set X_transformed[row][col] to transformed_value
    
    Return X_transformed

Process called "generate_regression_report" that takes model as RegressionModel, diagnostics as ModelDiagnostics returns Dictionary[String, String]:
    Note: Generate comprehensive regression analysis report
    Note: Includes model summary, diagnostics, and interpretation guidance
    
    Let report be Dictionary[String, String]
    Set report["model_type"] to model.model_type
    Set report["r_squared"] to model.r_squared as String
    Set report["adjusted_r_squared"] to model.adjusted_r_squared as String
    Set report["f_statistic"] to model.f_statistic as String
    Set report["p_value"] to model.p_value as String
    Set report["intercept"] to model.intercept as String
    
    Let coef_summary be ""
    For i from 0 to Length(model.coefficients) minus 1:
        Set coef_summary to coef_summary plus "β" plus i as String plus "=" plus model.coefficients[i] as String plus " "
    Set report["coefficients"] to coef_summary
    
    Set report["num_observations"] to Length(model.fitted_values) as String
    Set report["residual_standard_error"] to Stats.calculate_standard_deviation(model.residuals, false) as String
    
    Return report

Note: =====================================================================
Note: HELPER FUNCTIONS FOR LOGISTIC REGRESSION
Note: =====================================================================

Process called "calculate_null_log_likelihood" that takes y as List[Float] returns Float:
    Note: Calculate log-likelihood for null model (intercept only)
    
    Let n be Length(y) as Float
    Let sum_y be 0.0
    For i from 0 to Length(y) minus 1:
        Set sum_y to sum_y plus y[i]
    
    Let p_bar be sum_y / n  Note: Overall proportion
    If p_bar is less than or equal to 0.0 || p_bar is greater than or equal to 1.0:
        Return -1e10  Note: Degenerate case
    
    Let log_likelihood be sum_y multiplied by MathOps.natural_logarithm(p_bar, 15).result plus (n minus sum_y) multiplied by MathOps.natural_logarithm(1.0 minus p_bar, 15).result
    
    Return log_likelihood

Process called "calculate_logistic_log_likelihood" that takes X as List[List[Float]], y as List[Float], coefficients as List[Float] returns Float:
    Note: Calculate log-likelihood for logistic regression model
    
    Let log_likelihood be 0.0
    
    For i from 0 to Length(y) minus 1:
        Let linear_predictor be 0.0
        For j from 0 to Length(coefficients) minus 1:
            Set linear_predictor to linear_predictor plus coefficients[j] multiplied by X[i][j]
        
        Note: Avoid numerical overflow in exp calculation
        If linear_predictor is greater than 700.0:
            Set linear_predictor to 700.0
        Otherwise if linear_predictor is less than -700.0:
            Set linear_predictor to -700.0
        
        Let exp_eta be MathOps.exponential(linear_predictor, 15).result
        Let log_p be MathOps.natural_logarithm(exp_eta / (1.0 plus exp_eta), 15).result
        Let log_1_minus_p be MathOps.natural_logarithm(1.0 / (1.0 plus exp_eta), 15).result
        
        Set log_likelihood to log_likelihood plus y[i] multiplied by log_p plus (1.0 minus y[i]) multiplied by log_1_minus_p
    
    Return log_likelihood

Process called "calculate_fisher_information" that takes X as List[List[Float]], probabilities as List[Float] returns Matrix:
    Note: Calculate Fisher Information Matrix for logistic regression
    
    Let p be Length(X[0]) as Integer
    Let n be Length(X) as Integer
    
    Let information_matrix be []
    
    For i from 0 to p minus 1:
        Let row be []
        For j from 0 to p minus 1:
            Let sum be 0.0
            For k from 0 to n minus 1:
                Let pi be probabilities[k]
                Let weight be pi multiplied by (1.0 minus pi)
                Set sum to sum plus weight multiplied by X[k][i] multiplied by X[k][j]
            Append sum to row
        Append row to information_matrix
    
    Return LinAlg.create_matrix(information_matrix, "Float")