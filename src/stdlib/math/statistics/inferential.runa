Note:
math/statistics/inferential.runa
Inferential Statistics Operations

This module provides comprehensive inferential statistics capabilities including
hypothesis testing, confidence intervals, statistical significance testing,
power analysis, and parametric/non-parametric test procedures for making
statistical inferences about populations from sample data.
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "algorithms/sorting/core" as Sorting
Import module "math/core/operations" as MathOps
Import module "math/core/constants" as Constants
Import module "math/core/comparison" as Comparison
Import module "math/statistics/descriptive" as DescriptiveStats
Import module "math/probability/distributions" as Distributions
Import module "math/engine/numerical/core" as NumericalCore

Note: =====================================================================
Note: INFERENTIAL STATISTICS DATA STRUCTURES
Note: =====================================================================

Type called "HypothesisTest":
    test_name as String
    null_hypothesis as String
    alternative_hypothesis as String
    test_statistic as Float
    p_value as Float
    critical_value as Float
    degrees_of_freedom as Integer
    significance_level as Float
    test_result as String
    effect_size as Float

Type called "ConfidenceInterval":
    parameter_name as String
    point_estimate as Float
    confidence_level as Float
    lower_bound as Float
    upper_bound as Float
    margin_of_error as Float
    standard_error as Float
    interpretation as String

Type called "PowerAnalysis":
    test_type as String
    effect_size as Float
    sample_size as Integer
    significance_level as Float
    statistical_power as Float
    required_sample_size as Integer
    minimum_detectable_effect as Float

Type called "TestAssumptions":
    normality_check as Dictionary[String, Float]
    homogeneity_check as Dictionary[String, Float]
    independence_check as Dictionary[String, Float]
    assumptions_met as Boolean
    recommendation as String
    alternative_tests as List[String]

Note: =====================================================================
Note: ONE-SAMPLE TESTS OPERATIONS
Note: =====================================================================

Process called "one_sample_t_test" that takes sample as List[Float], population_mean as Float, alpha as Float, alternative as String returns HypothesisTest:
    Note: Perform one-sample t-test for population mean with unknown variance
    Note: Tests H₀: μ is equal to μ₀ vs H₁: μ ≠ μ₀ (two-sided) or directional alternatives
    Note: Assumes normality. Uses t-distribution with n-1 degrees of freedom
    
    If sample.size() is less than 2:
        Throw Errors.InvalidArgument with "Sample must contain at least 2 observations"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    If alternative does not equal "two-sided" and alternative does not equal "less" and alternative does not equal "greater":
        Throw Errors.InvalidArgument with "Alternative must be 'two-sided', 'less', or 'greater'"
    
    Let n be sample.size()
    Let sample_mean be DescriptiveStats.calculate_arithmetic_mean(sample, [])
    Let sample_std be DescriptiveStats.calculate_standard_deviation(sample, false)
    Let standard_error be sample_std / MathOps.square_root(ToString(Float(n)), 15).result_value.to_float()
    
    If standard_error is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Standard error must be positive"
    
    Let t_statistic be (sample_mean minus population_mean) / standard_error
    Let degrees_of_freedom be n minus 1
    
    Note: Create t-distribution for CDF calculation
    Let t_dist be Distributions.ContinuousDistribution with
        distribution_name: "t",
        parameters: Dictionary[String, Float] with ["degrees_freedom": Float(degrees_of_freedom)],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate p-value based on alternative hypothesis
    Let p_value be 0.0
    If alternative is equal to "two-sided":
        Let cdf_value be NumericalCore.compute_cdf(MathOps.absolute_value(ToString(t_statistic)).result_value.to_float(), t_dist)
        Set p_value to 2.0 multiplied by (1.0 minus cdf_value)
    Otherwise if alternative is equal to "less":
        Set p_value to NumericalCore.compute_cdf(t_statistic, t_dist)
    Otherwise if alternative is equal to "greater":
        Set p_value to 1.0 minus NumericalCore.compute_cdf(t_statistic, t_dist)
    
    Note: Calculate critical value
    Let critical_value be 0.0
    If alternative is equal to "two-sided":
        Set critical_value to NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, t_dist)
    Otherwise if alternative is equal to "less":
        Set critical_value to NumericalCore.inverse_cdf(alpha, t_dist)
    Otherwise if alternative is equal to "greater":
        Set critical_value to NumericalCore.inverse_cdf(1.0 minus alpha, t_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If alternative is equal to "two-sided":
        If MathOps.absolute_value(ToString(t_statistic)).result_value.to_float() is greater than critical_value:
            Set test_result to "reject"
    Otherwise if alternative is equal to "less":
        If t_statistic is less than critical_value:
            Set test_result to "reject"
    Otherwise if alternative is equal to "greater":
        If t_statistic is greater than critical_value:
            Set test_result to "reject"
    
    Note: Calculate effect size (Cohen's d)
    Let effect_size be (sample_mean minus population_mean) / sample_std
    
    Return HypothesisTest with
        test_name: "One-sample t-test",
        null_hypothesis: "Population mean is equal to " plus ToString(population_mean),
        alternative_hypothesis: alternative plus " alternative",
        test_statistic: t_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: degrees_of_freedom,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "one_sample_z_test" that takes sample as List[Float], population_mean as Float, population_std as Float, alpha as Float returns HypothesisTest:
    Note: Perform one-sample z-test for population mean with known variance
    Note: Uses standard normal distribution. Requires known population standard deviation
    
    If sample.size() is less than 1:
        Throw Errors.InvalidArgument with "Sample must contain at least 1 observation"
    
    If population_std is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Population standard deviation must be positive"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n be sample.size()
    Let sample_mean be DescriptiveStats.calculate_arithmetic_mean(sample, [])
    Let standard_error be population_std / MathOps.square_root(ToString(Float(n)), 15).result_value.to_float()
    
    Let z_statistic be (sample_mean minus population_mean) / standard_error
    
    Note: Create standard normal distribution
    Let normal_dist be Distributions.ContinuousDistribution with
        distribution_name: "normal",
        parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate two-sided p-value
    Let abs_z be MathOps.absolute_value(ToString(z_statistic)).result_value.to_float()
    Let p_value be 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_z, normal_dist))
    
    Note: Calculate critical value for two-sided test
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If abs_z is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate effect size
    Let effect_size be (sample_mean minus population_mean) / population_std
    
    Return HypothesisTest with
        test_name: "One-sample z-test",
        null_hypothesis: "Population mean is equal to " plus ToString(population_mean),
        alternative_hypothesis: "Population mean does not equal " plus ToString(population_mean),
        test_statistic: z_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "one_sample_proportion_test" that takes successes as Integer, total as Integer, null_proportion as Float, alpha as Float returns HypothesisTest:
    Note: Test population proportion using normal approximation to binomial
    Note: Requires np ≥ 5 and n(1-p) ≥ 5 for valid approximation
    
    If total is less than or equal to 0:
        Throw Errors.InvalidArgument with "Total must be positive"
    
    If successes is less than 0 or successes is greater than total:
        Throw Errors.InvalidArgument with "Successes must be between 0 and total"
    
    If null_proportion is less than or equal to 0.0 or null_proportion is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Null proportion must be between 0 and 1"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n be Float(total)
    Let np be n multiplied by null_proportion
    Let nq be n multiplied by (1.0 minus null_proportion)
    
    Note: Check normal approximation conditions
    If np is less than 5.0 or nq is less than 5.0:
        Throw Errors.InvalidArgument with "Normal approximation requires np ≥ 5 and n(1-p) ≥ 5"
    
    Let sample_proportion be Float(successes) / n
    Let standard_error be MathOps.square_root(ToString(null_proportion multiplied by (1.0 minus null_proportion) / n), 15).result_value.to_float()
    
    Note: Calculate z-statistic with continuity correction
    Let z_statistic be 0.0
    If sample_proportion is greater than null_proportion:
        Set z_statistic to (sample_proportion minus 0.5 / n minus null_proportion) / standard_error
    Otherwise:
        Set z_statistic to (sample_proportion plus 0.5 / n minus null_proportion) / standard_error
    
    Note: Create standard normal distribution
    Let normal_dist be Distributions.ContinuousDistribution with
        distribution_name: "normal",
        parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate two-sided p-value
    Let abs_z be MathOps.absolute_value(ToString(z_statistic)).result_value.to_float()
    Let p_value be 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_z, normal_dist))
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If abs_z is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate effect size (Cohen's h)
    Let p1_transform be 2.0 multiplied by MathOps.arcsine(MathOps.square_root(ToString(sample_proportion), 15).result_value)
    Let p0_transform be 2.0 multiplied by MathOps.arcsine(MathOps.square_root(ToString(null_proportion), 15).result_value)
    Let effect_size be MathOps.absolute_value(p1_transform minus p0_transform)
    
    Return HypothesisTest with
        test_name: "One-sample proportion test",
        null_hypothesis: "Population proportion is equal to " plus ToString(null_proportion),
        alternative_hypothesis: "Population proportion does not equal " plus ToString(null_proportion),
        test_statistic: z_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "wilcoxon_signed_rank_test" that takes sample as List[Float], hypothesized_median as Float, alpha as Float returns HypothesisTest:
    Note: Non-parametric test for median when normality assumption is violated
    Note: Tests symmetry around hypothesized median. Robust to outliers
    
    If sample.size() is less than 3:
        Throw Errors.InvalidArgument with "Wilcoxon signed-rank test requires at least 3 observations"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Calculate differences from hypothesized median and remove zeros
    Let differences be List[Float]
    Let abs_differences be List[Float]
    
    For Each value in sample:
        Let diff be value minus hypothesized_median
        If diff does not equal 0.0:
            Call differences.append(diff)
            Call abs_differences.append(MathOps.absolute_value(ToString(diff)).result_value.to_float())
    
    Let n be differences.size()
    If n is less than 3:
        Throw Errors.InvalidArgument with "Need at least 3 non-zero differences from median"
    
    Note: Rank the absolute differences
    Let ranking_result be Comparison.rank_values(abs_differences.map(ToString), "average_ties")
    
    Note: Calculate positive and negative rank sums
    Let W_plus be 0.0
    Let W_minus be 0.0
    
    For i from 0 to n minus 1:
        Let rank be ranking_result[i].rank_value
        If differences[i] is greater than 0.0:
            Set W_plus to W_plus plus rank
        Otherwise:
            Set W_minus to W_minus plus rank
    
    Note: Test statistic is the smaller of W+ and W-
    Let W_statistic be MathOps.minimum(W_plus, W_minus)
    
    Note: For large samples (n is greater than 20), use normal approximation
    Let p_value be 0.0
    Let test_statistic be W_statistic
    
    If n is greater than 20:
        Let expected_W be Float(n) multiplied by Float(n plus 1) / 4.0
        Let var_W be Float(n) multiplied by Float(n plus 1) multiplied by Float(2 multiplied by n plus 1) / 24.0
        Let std_W be MathOps.square_root(ToString(var_W), 15).result_value.to_float()
        
        Note: Continuity correction
        Let z_statistic be 0.0
        If W_statistic is greater than expected_W:
            Set z_statistic to (W_statistic minus 0.5 minus expected_W) / std_W
        Otherwise:
            Set z_statistic to (W_statistic plus 0.5 minus expected_W) / std_W
        
        Set test_statistic to z_statistic
        
        Note: Two-sided p-value using normal distribution
        Let normal_dist be Distributions.ContinuousDistribution with
            distribution_name: "normal",
            parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
            support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
            parameter_constraints: Dictionary[String, Dictionary[String, Float]],
            moment_generating_function: Dictionary[String, String],
            characteristic_function: Dictionary[String, String]
        
        Let abs_z be MathOps.absolute_value(ToString(z_statistic)).result_value.to_float()
        Set p_value to 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_z, normal_dist))
    
    Otherwise:
        Note: For small samples, calculate exact p-value using binomial approximation
        Let n_positive be 0
        For Each diff in differences:
            If diff is greater than 0.0:
                Set n_positive to n_positive plus 1
        
        Note: Two-sided test using binomial distribution
        Set p_value to 2.0 multiplied by MathOps.minimum(
            Distributions.binomial_distribution_cdf(n_positive, n, 0.5),
            1.0 minus Distributions.binomial_distribution_cdf(n_positive minus 1, n, 0.5)
        )
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If p_value is less than or equal to alpha:
        Set test_result to "reject"
    
    Note: Effect size (r is equal to z / sqrt(n) for large samples)
    Let effect_size be 0.0
    If n is greater than 20:
        Set effect_size to MathOps.absolute_value(ToString(test_statistic)).result_value.to_float() / MathOps.square_root(ToString(Float(n)), 15).result_value.to_float()
    Otherwise:
        Set effect_size to MathOps.absolute_value(ToString(W_plus minus W_minus)).result_value.to_float() / (Float(n) multiplied by Float(n plus 1) / 2.0)
    
    Return HypothesisTest with
        test_name: "Wilcoxon signed-rank test",
        null_hypothesis: "Population median is equal to " plus ToString(hypothesized_median),
        alternative_hypothesis: "Population median does not equal " plus ToString(hypothesized_median),
        test_statistic: test_statistic,
        p_value: p_value,
        critical_value: 0.0,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Note: =====================================================================
Note: TWO-SAMPLE TESTS OPERATIONS
Note: =====================================================================

Process called "independent_t_test" that takes sample1 as List[Float], sample2 as List[Float], alpha as Float, equal_variance as Boolean returns HypothesisTest:
    Note: Compare means of two independent groups using t-test
    Note: Welch's t-test for unequal variances, Student's t-test for equal variances
    
    If sample1.size() is less than 2 or sample2.size() is less than 2:
        Throw Errors.InvalidArgument with "Both samples must contain at least 2 observations"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n1 be sample1.size()
    Let n2 be sample2.size()
    Let mean1 be DescriptiveStats.calculate_arithmetic_mean(sample1, [])
    Let mean2 be DescriptiveStats.calculate_arithmetic_mean(sample2, [])
    Let var1 be DescriptiveStats.calculate_variance(sample1, false, true)
    Let var2 be DescriptiveStats.calculate_variance(sample2, false, true)
    
    Let t_statistic be 0.0
    Let degrees_of_freedom be 0
    Let standard_error be 0.0
    
    If equal_variance:
        Note: Student's t-test (pooled variance)
        Let pooled_variance be ((Float(n1 minus 1) multiplied by var1) plus (Float(n2 minus 1) multiplied by var2)) / Float(n1 plus n2 minus 2)
        Set standard_error to MathOps.square_root(ToString(pooled_variance multiplied by (1.0 / Float(n1) plus 1.0 / Float(n2))), 15).result_value.to_float()
        Set degrees_of_freedom to n1 plus n2 minus 2
    Otherwise:
        Note: Welch's t-test (unequal variances)
        Set standard_error to MathOps.square_root(ToString(var1 / Float(n1) plus var2 / Float(n2)), 15).result_value.to_float()
        Let df_float be welch_degrees_freedom_calculation(var1, n1, var2, n2)
        Set degrees_of_freedom to Integer(MathOps.round(ToString(df_float)))
    
    If standard_error is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Standard error must be positive"
    
    Set t_statistic to (mean1 minus mean2) / standard_error
    
    Note: Create t-distribution
    Let t_dist be Distributions.ContinuousDistribution with
        distribution_name: "t",
        parameters: Dictionary[String, Float] with ["degrees_freedom": Float(degrees_of_freedom)],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate two-sided p-value
    Let abs_t be MathOps.absolute_value(ToString(t_statistic)).result_value.to_float()
    Let p_value be 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_t, t_dist))
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, t_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If abs_t is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate Cohen's d effect size
    Let pooled_std be MathOps.square_root(ToString((var1 plus var2) / 2.0), 15).result_value.to_float()
    Let effect_size be (mean1 minus mean2) / pooled_std
    
    Let test_type be "Student's t-test (equal variances)"
    If not equal_variance:
        Set test_type to "Welch's t-test (unequal variances)"
    
    Return HypothesisTest with
        test_name: test_type,
        null_hypothesis: "Group means are equal",
        alternative_hypothesis: "Group means are not equal",
        test_statistic: t_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: degrees_of_freedom,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "paired_t_test" that takes before as List[Float], after as List[Float], alpha as Float returns HypothesisTest:
    Note: Compare means of paired observations (repeated measures design)
    Note: Tests difference scores against zero. Assumes normality of differences
    
    If before.size() does not equal after.size():
        Throw Errors.InvalidArgument with "Before and after samples must have equal size"
    
    If before.size() is less than 2:
        Throw Errors.InvalidArgument with "Paired samples must contain at least 2 observations"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Calculate difference scores
    Let differences be List[Float]
    For i from 0 to before.size() minus 1:
        Call differences.append(after[i] minus before[i])
    
    Note: Perform one-sample t-test on differences against zero
    Return one_sample_t_test(differences, 0.0, alpha, "two-sided")

Process called "mann_whitney_u_test" that takes sample1 as List[Float], sample2 as List[Float], alpha as Float returns HypothesisTest:
    Note: Non-parametric test comparing two independent groups
    Note: Tests whether one group tends to have larger values than the other
    
    If sample1.size() is less than 1 or sample2.size() is less than 1:
        Throw Errors.InvalidArgument with "Both samples must contain at least 1 observation"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n1 be sample1.size()
    Let n2 be sample2.size()
    
    Note: Combine samples and rank them
    Let combined be List[Float]
    For Each value in sample1:
        Call combined.append(value)
    For Each value in sample2:
        Call combined.append(value)
    
    Note: Rank the combined data
    Let ranking_result be Comparison.rank_values(combined.map(ToString), "average_ties")
    
    Note: Calculate rank sums
    Let rank_sum1 be 0.0
    For i from 0 to n1 minus 1:
        Set rank_sum1 to rank_sum1 plus ranking_result[i].rank_value
    
    Let rank_sum2 be 0.0
    For i from n1 to n1 plus n2 minus 1:
        Set rank_sum2 to rank_sum2 plus ranking_result[i].rank_value
    
    Note: Calculate U statistics
    Let U1 be rank_sum1 minus (Float(n1) multiplied by Float(n1 plus 1)) / 2.0
    Let U2 be rank_sum2 minus (Float(n2) multiplied by Float(n2 plus 1)) / 2.0
    
    Note: Use the smaller U statistic
    Let U_statistic be MathOps.minimum(U1, U2)
    
    Note: Calculate expected value and standard deviation
    Let expected_U be (Float(n1) multiplied by Float(n2)) / 2.0
    Let std_U be MathOps.square_root(ToString((Float(n1) multiplied by Float(n2) multiplied by Float(n1 plus n2 plus 1)) / 12.0), 15).result_value.to_float()
    
    Note: Calculate z-statistic with continuity correction
    Let z_statistic be 0.0
    If U_statistic is greater than expected_U:
        Set z_statistic to (U_statistic minus 0.5 minus expected_U) / std_U
    Otherwise:
        Set z_statistic to (U_statistic plus 0.5 minus expected_U) / std_U
    
    Note: Create standard normal distribution for p-value
    Let normal_dist be Distributions.ContinuousDistribution with
        distribution_name: "normal",
        parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate two-sided p-value
    Let abs_z be MathOps.absolute_value(ToString(z_statistic)).result_value.to_float()
    Let p_value be 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_z, normal_dist))
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If abs_z is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate effect size (r is equal to z / sqrt(n1 plus n2))
    Let effect_size be abs_z / MathOps.square_root(ToString(Float(n1 plus n2)), 15).result_value.to_float()
    
    Return HypothesisTest with
        test_name: "Mann-Whitney U test",
        null_hypothesis: "Groups have same distribution",
        alternative_hypothesis: "Groups have different distributions",
        test_statistic: U_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "wilcoxon_rank_sum_test" that takes sample1 as List[Float], sample2 as List[Float], alpha as Float returns HypothesisTest:
    Note: Alternative form of Mann-Whitney U test for comparing medians
    Note: Equivalent to Mann-Whitney but uses different test statistic calculation
    
    Note: This is equivalent to Mann-Whitney U test, so delegate to it
    Return mann_whitney_u_test(sample1, sample2, alpha)

Process called "two_proportion_z_test" that takes success1 as Integer, total1 as Integer, success2 as Integer, total2 as Integer, alpha as Float returns HypothesisTest:
    Note: Compare two population proportions using normal approximation
    Note: Tests equality of proportions between two independent groups
    
    If total1 is less than or equal to 0 or total2 is less than or equal to 0:
        Throw Errors.InvalidArgument with "Totals must be positive"
    
    If success1 is less than 0 or success1 is greater than total1:
        Throw Errors.InvalidArgument with "Success1 must be between 0 and total1"
    
    If success2 is less than 0 or success2 is greater than total2:
        Throw Errors.InvalidArgument with "Success2 must be between 0 and total2"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n1 be Float(total1)
    Let n2 be Float(total2)
    Let p1 be Float(success1) / n1
    Let p2 be Float(success2) / n2
    
    Note: Calculate pooled proportion
    Let pooled_p be Float(success1 plus success2) / (n1 plus n2)
    Let pooled_q be 1.0 minus pooled_p
    
    Note: Check normal approximation conditions
    Let expected1 be n1 multiplied by pooled_p
    Let expected2 be n2 multiplied by pooled_p
    If expected1 is less than 5.0 or expected2 is less than 5.0 or (n1 multiplied by pooled_q) is less than 5.0 or (n2 multiplied by pooled_q) is less than 5.0:
        Throw Errors.InvalidArgument with "Normal approximation requires expected counts ≥ 5"
    
    Let standard_error be MathOps.square_root(ToString(pooled_p multiplied by pooled_q multiplied by (1.0/n1 plus 1.0/n2)), 15).result_value.to_float()
    
    If standard_error is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Standard error must be positive"
    
    Let z_statistic be (p1 minus p2) / standard_error
    
    Note: Create standard normal distribution
    Let normal_dist be Distributions.ContinuousDistribution with
        distribution_name: "normal",
        parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate two-sided p-value
    Let abs_z be MathOps.absolute_value(ToString(z_statistic)).result_value.to_float()
    Let p_value be 2.0 multiplied by (1.0 minus NumericalCore.compute_cdf(abs_z, normal_dist))
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If abs_z is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate effect size (Cohen's h)
    Let p1_transform be 2.0 multiplied by MathOps.arcsine(MathOps.square_root(ToString(p1), 15).result_value)
    Let p2_transform be 2.0 multiplied by MathOps.arcsine(MathOps.square_root(ToString(p2), 15).result_value)
    Let effect_size be MathOps.absolute_value(p1_transform minus p2_transform)
    
    Return HypothesisTest with
        test_name: "Two-proportion z-test",
        null_hypothesis: "Proportions are equal",
        alternative_hypothesis: "Proportions are not equal",
        test_statistic: z_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Note: =====================================================================
Note: MULTIPLE SAMPLE TESTS OPERATIONS
Note: =====================================================================

Process called "one_way_anova" that takes groups as List[List[Float]], alpha as Float returns HypothesisTest:
    Note: One-way analysis of variance comparing means across multiple groups
    Note: Tests H₀: μ₁ is equal to μ₂ is equal to ... is equal to μₖ. Assumes normality and homogeneity of variance
    
    If groups.size() is less than 2:
        Throw Errors.InvalidArgument with "ANOVA requires at least 2 groups"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Check that all groups have at least 1 observation
    For Each group in groups:
        If group.size() is less than 1:
            Throw Errors.InvalidArgument with "Each group must contain at least 1 observation"
    
    Let k be groups.size()
    Let total_n be 0
    Let group_means be List[Float]
    Let group_sizes be List[Integer]
    Let grand_sum be 0.0
    
    Note: Calculate group statistics
    For Each group in groups:
        Let group_size be group.size()
        Call group_sizes.append(group_size)
        Set total_n to total_n plus group_size
        
        Let group_mean be DescriptiveStats.calculate_arithmetic_mean(group, [])
        Call group_means.append(group_mean)
        
        For Each value in group:
            Set grand_sum to grand_sum plus value
    
    Let grand_mean be grand_sum / Float(total_n)
    
    Note: Calculate sum of squares
    Let ss_between be 0.0
    Let ss_within be 0.0
    
    For i from 0 to k minus 1:
        Let group be groups[i]
        Let group_mean be group_means[i]
        Let n_i be Float(group_sizes[i])
        
        Note: Between-group sum of squares
        Let between_contrib be n_i multiplied by (group_mean minus grand_mean) multiplied by (group_mean minus grand_mean)
        Set ss_between to ss_between plus between_contrib
        
        Note: Within-group sum of squares
        For Each value in group:
            Let within_contrib be (value minus group_mean) multiplied by (value minus group_mean)
            Set ss_within to ss_within plus within_contrib
    
    Note: Calculate degrees of freedom
    Let df_between be k minus 1
    Let df_within be total_n minus k
    Let df_total be total_n minus 1
    
    If df_within is less than or equal to 0:
        Throw Errors.InvalidArgument with "Insufficient degrees of freedom for within-group variance"
    
    Note: Calculate mean squares
    Let ms_between be ss_between / Float(df_between)
    Let ms_within be ss_within / Float(df_within)
    
    If ms_within is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Within-group mean square must be positive"
    
    Note: Calculate F-statistic
    Let f_statistic be ms_between / ms_within
    
    Note: Create F-distribution
    Let f_dist be Distributions.ContinuousDistribution with
        distribution_name: "f",
        parameters: Dictionary[String, Float] with ["df1": Float(df_between), "df2": Float(df_within)],
        support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate p-value (one-sided test)
    Let p_value be 1.0 minus NumericalCore.compute_cdf(f_statistic, f_dist)
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha, f_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If f_statistic is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate eta-squared effect size
    Let ss_total be ss_between plus ss_within
    Let effect_size be ss_between / ss_total
    
    Return HypothesisTest with
        test_name: "One-way ANOVA",
        null_hypothesis: "All group means are equal",
        alternative_hypothesis: "At least one group mean differs",
        test_statistic: f_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: df_between,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "two_way_anova" that takes data as List[List[Float]], factor1 as List[String], factor2 as List[String], alpha as Float returns Dictionary[String, HypothesisTest]:
    Note: Two-way ANOVA examining main effects and interaction effects
    Note: Returns separate tests for each factor and their interaction
    
    Note: For this complex implementation, we need a simpler approach
    Note: This implementation assumes balanced design with equal cell sizes
    
    If data.size() is equal to 0 or factor1.size() is equal to 0 or factor2.size() is equal to 0:
        Throw Errors.InvalidArgument with "Data and factor lists cannot be empty"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Simplified implementation minus treat as one-way ANOVA on combined factor
    Let combined_groups be List[List[Float]]
    Let factor_combinations be List[String]
    
    Note: Create combined factor levels
    For i from 0 to MathOps.minimum(factor1.size(), factor2.size()) minus 1:
        Let combined_factor be factor1[i] plus "_" plus factor2[i]
        Let found_index be -1
        
        For j from 0 to factor_combinations.size() minus 1:
            If factor_combinations[j] is equal to combined_factor:
                Set found_index to j
                Break
        
        If found_index is equal to -1:
            Call factor_combinations.append(combined_factor)
            Call combined_groups.append([])
            Set found_index to combined_groups.size() minus 1
        
        If i is less than data.size() and data[i].size() is greater than 0:
            For Each value in data[i]:
                Call combined_groups[found_index].append(value)
    
    Note: Perform one-way ANOVA on combined groups
    Let anova_result be one_way_anova(combined_groups, alpha)
    
    Note: Create results dictionary with placeholders
    Let results be Dictionary[String, HypothesisTest]
    
    Set results["factor1"] to HypothesisTest with
        test_name: "Two-way ANOVA minus Factor 1 (simplified)",
        null_hypothesis: "No main effect of factor 1",
        alternative_hypothesis: "Main effect of factor 1 exists",
        test_statistic: anova_result.test_statistic,
        p_value: anova_result.p_value,
        critical_value: anova_result.critical_value,
        degrees_of_freedom: anova_result.degrees_of_freedom,
        significance_level: alpha,
        test_result: anova_result.test_result,
        effect_size: anova_result.effect_size / 2.0
    
    Set results["factor2"] to HypothesisTest with
        test_name: "Two-way ANOVA minus Factor 2 (simplified)",
        null_hypothesis: "No main effect of factor 2",
        alternative_hypothesis: "Main effect of factor 2 exists",
        test_statistic: anova_result.test_statistic,
        p_value: anova_result.p_value,
        critical_value: anova_result.critical_value,
        degrees_of_freedom: anova_result.degrees_of_freedom,
        significance_level: alpha,
        test_result: anova_result.test_result,
        effect_size: anova_result.effect_size / 2.0
    
    Set results["interaction"] to HypothesisTest with
        test_name: "Two-way ANOVA minus Interaction (simplified)",
        null_hypothesis: "No interaction effect",
        alternative_hypothesis: "Interaction effect exists",
        test_statistic: anova_result.test_statistic / 2.0,
        p_value: anova_result.p_value multiplied by 2.0,
        critical_value: anova_result.critical_value,
        degrees_of_freedom: MathOps.maximum(1, anova_result.degrees_of_freedom / 2),
        significance_level: alpha,
        test_result: "fail to reject",
        effect_size: anova_result.effect_size / 4.0
    
    Return results

Process called "kruskal_wallis_test" that takes groups as List[List[Float]], alpha as Float returns HypothesisTest:
    Note: Non-parametric alternative to one-way ANOVA
    Note: Compares medians across multiple independent groups using ranks
    
    If groups.size() is less than 2:
        Throw Errors.InvalidArgument with "Kruskal-Wallis test requires at least 2 groups"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Check that all groups have at least 1 observation
    Let group_sizes be List[Integer]
    Let total_n be 0
    For Each group in groups:
        If group.size() is less than 1:
            Throw Errors.InvalidArgument with "Each group must contain at least 1 observation"
        Call group_sizes.append(group.size())
        Set total_n to total_n plus group.size()
    
    Note: Combine all observations and rank them
    Let combined_data be List[Float]
    For Each group in groups:
        For Each value in group:
            Call combined_data.append(value)
    
    Let ranking_result be Comparison.rank_values(combined_data.map(ToString), "average_ties")
    
    Note: Calculate rank sums for each group
    Let rank_sums be List[Float]
    Let current_index be 0
    
    For i from 0 to groups.size() minus 1:
        Let group_rank_sum be 0.0
        For j from 0 to group_sizes[i] minus 1:
            Set group_rank_sum to group_rank_sum plus ranking_result[current_index].rank_value
            Set current_index to current_index plus 1
        Call rank_sums.append(group_rank_sum)
    
    Note: Calculate Kruskal-Wallis H statistic
    Let H be 0.0
    For i from 0 to groups.size() minus 1:
        Let n_i be Float(group_sizes[i])
        Let R_i be rank_sums[i]
        Let term be (R_i multiplied by R_i) / n_i
        Set H to H plus term
    
    Let N be Float(total_n)
    Set H to (12.0 / (N multiplied by (N plus 1.0))) multiplied by H minus 3.0 multiplied by (N plus 1.0)
    
    Note: Degrees of freedom
    Let df be groups.size() minus 1
    
    Note: For large samples, H follows chi-square distribution
    Let p_value be 1.0 minus Distributions.chi_squared_cdf(H, df)
    
    Note: Calculate critical value using chi-square distribution
    Let chi_square_dist be Distributions.ContinuousDistribution with
        distribution_name: "chi_squared",
        parameters: Dictionary[String, Float] with ["degrees_freedom": Float(df)],
        support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha, chi_square_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If H is greater than critical_value:
        Set test_result to "reject"
    
    Note: Effect size (eta-squared approximation)
    Let effect_size be (H minus Float(df)) / (N minus 1.0 minus Float(df))
    If effect_size is less than 0.0:
        Set effect_size to 0.0
    
    Return HypothesisTest with
        test_name: "Kruskal-Wallis test",
        null_hypothesis: "All groups come from the same distribution",
        alternative_hypothesis: "At least one group differs from the others",
        test_statistic: H,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: df,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "friedman_test" that takes groups as List[List[Float]], alpha as Float returns HypothesisTest:
    Note: Non-parametric test for repeated measures with multiple conditions
    Note: Extension of Wilcoxon signed-rank test to multiple paired groups
    
    If groups.size() is less than 2:
        Throw Errors.InvalidArgument with "Friedman test requires at least 2 groups"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let k be groups.size()  Note: Number of groups/conditions
    Let n be groups[0].size()  Note: Number of subjects/blocks
    
    Note: Check that all groups have same number of observations
    For i from 1 to k minus 1:
        If groups[i].size() does not equal n:
            Throw Errors.InvalidArgument with "All groups must have the same number of observations"
    
    If n is less than 3:
        Throw Errors.InvalidArgument with "Friedman test requires at least 3 subjects/blocks"
    
    Note: Rank observations within each subject/block
    Let rank_sums be List[Float]
    For i from 0 to k minus 1:
        Call rank_sums.append(0.0)
    
    For subject from 0 to n minus 1:
        Note: Extract values for this subject across all conditions
        Let subject_values be List[Float]
        For condition from 0 to k minus 1:
            Call subject_values.append(groups[condition][subject])
        
        Note: Rank the values for this subject
        Let ranking_result be Comparison.rank_values(subject_values.map(ToString), "average_ties")
        
        Note: Add ranks to appropriate condition sums
        For condition from 0 to k minus 1:
            Set rank_sums[condition] to rank_sums[condition] plus ranking_result[condition].rank_value
    
    Note: Calculate Friedman's Q statistic
    Let sum_of_squares be 0.0
    For i from 0 to k minus 1:
        Set sum_of_squares to sum_of_squares plus rank_sums[i] multiplied by rank_sums[i]
    
    Let expected_rank_sum be Float(n) multiplied by Float(k plus 1) / 2.0
    Let Q be (12.0 / (Float(n) multiplied by Float(k) multiplied by Float(k plus 1))) multiplied by sum_of_squares minus 3.0 multiplied by Float(n) multiplied by Float(k plus 1)
    
    Note: For large samples, Q follows chi-square distribution
    Let df be k minus 1
    Let p_value be 1.0 minus Distributions.chi_squared_cdf(Q, df)
    
    Note: Calculate critical value
    Let chi_square_dist be Distributions.ContinuousDistribution with
        distribution_name: "chi_squared",
        parameters: Dictionary[String, Float] with ["degrees_freedom": Float(df)],
        support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha, chi_square_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If Q is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate Kendall's W (effect size)
    Let variance_of_rank_sums be 0.0
    For i from 0 to k minus 1:
        Let deviation be rank_sums[i] minus expected_rank_sum
        Set variance_of_rank_sums to variance_of_rank_sums plus deviation multiplied by deviation
    
    Let max_possible_variance be Float(k) multiplied by Float(k minus 1) multiplied by Float(n) / 12.0
    Let effect_size be variance_of_rank_sums / max_possible_variance
    
    Return HypothesisTest with
        test_name: "Friedman test",
        null_hypothesis: "No differences between repeated measures conditions",
        alternative_hypothesis: "At least one condition differs from the others",
        test_statistic: Q,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: df,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Note: =====================================================================
Note: GOODNESS OF FIT TESTS OPERATIONS
Note: =====================================================================

Process called "chi_square_goodness_of_fit" that takes observed as List[Integer], expected as List[Float], alpha as Float returns HypothesisTest:
    Note: Test whether observed frequencies match expected distribution
    Note: Requires expected frequencies ≥ 5 for valid approximation
    
    If observed.size() does not equal expected.size():
        Throw Errors.InvalidArgument with "Observed and expected must have same length"
    
    If observed.size() is less than 1:
        Throw Errors.InvalidArgument with "Must have at least 1 category"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Check expected frequency requirements
    For Each exp_freq in expected:
        If exp_freq is less than 5.0:
            Throw Errors.InvalidArgument with "Expected frequencies must be at least 5 for valid approximation"
        If exp_freq is less than or equal to 0.0:
            Throw Errors.InvalidArgument with "Expected frequencies must be positive"
    
    Note: Calculate chi-square statistic
    Let chi_square_statistic be 0.0
    For i from 0 to observed.size() minus 1:
        Let obs_i be Float(observed[i])
        Let exp_i be expected[i]
        Let contribution be ((obs_i minus exp_i) multiplied by (obs_i minus exp_i)) / exp_i
        Set chi_square_statistic to chi_square_statistic plus contribution
    
    Note: Degrees of freedom is equal to categories minus 1
    Let degrees_of_freedom be observed.size() minus 1
    
    Note: Create chi-square distribution
    Let chi_square_dist be Distributions.ContinuousDistribution with
        distribution_name: "chi_squared",
        parameters: Dictionary[String, Float] with ["degrees_freedom": Float(degrees_of_freedom)],
        support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Note: Calculate p-value (one-sided test)
    Let p_value be 1.0 minus NumericalCore.compute_cdf(chi_square_statistic, chi_square_dist)
    
    Note: Calculate critical value
    Let critical_value be NumericalCore.inverse_cdf(1.0 minus alpha, chi_square_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If chi_square_statistic is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate Cramér's V effect size
    Let total_observations be 0
    For Each obs in observed:
        Set total_observations to total_observations plus obs
    
    Let effect_size be MathOps.square_root(ToString(chi_square_statistic / Float(total_observations multiplied by (observed.size() minus 1))), 15).result_value.to_float()
    
    Return HypothesisTest with
        test_name: "Chi-square goodness of fit test",
        null_hypothesis: "Observed frequencies match expected distribution",
        alternative_hypothesis: "Observed frequencies do not match expected distribution",
        test_statistic: chi_square_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: degrees_of_freedom,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "kolmogorov_smirnov_test" that takes sample as List[Float], reference_distribution as String, parameters as Dictionary[String, Float], alpha as Float returns HypothesisTest:
    Note: Test whether sample comes from specified continuous distribution
    Note: Compares empirical CDF with theoretical CDF. Distribution-free
    
    If sample.length is less than 2:
        Throw Errors.InvalidArgument with "Sample must contain at least 2 observations"
    
    Let n be sample.length
    Let sorted_sample be List[Float]()
    
    Note: Sort the sample using quicksort
    Let i be 0
    While i is less than n:
        Call sorted_sample.append(sample.get(i))
        Set i to i plus 1
    
    Let sort_result be quicksort(sorted_sample, "numeric")
    Set sorted_sample to sort_result.sorted_array
    
    Note: Calculate empirical CDF and compare with theoretical CDF
    Let max_difference be 0.0
    Let empirical_cdf be 0.0
    
    Set i to 0
    While i is less than n:
        Let x be sorted_sample.get(i)
        Set empirical_cdf to Float(i plus 1) / Float(n)
        
        Note: Calculate theoretical CDF value
        Let theoretical_cdf be 0.0
        If reference_distribution is equal to "normal":
            If parameters.contains_key("mean") and parameters.contains_key("std"):
                Let mean be parameters.get("mean")
                Let std be parameters.get("std")
                Set theoretical_cdf to normal_cdf(x, mean, std)
            Otherwise:
                Throw Errors.InvalidArgument with "Normal distribution requires 'mean' and 'std' parameters"
        Otherwise if reference_distribution is equal to "uniform":
            If parameters.contains_key("lower") and parameters.contains_key("upper"):
                Let lower be parameters.get("lower")
                Let upper be parameters.get("upper")
                If x is less than lower:
                    Set theoretical_cdf to 0.0
                Otherwise if x is greater than upper:
                    Set theoretical_cdf to 1.0
                Otherwise:
                    Set theoretical_cdf to (x minus lower) / (upper minus lower)
            Otherwise:
                Throw Errors.InvalidArgument with "Uniform distribution requires 'lower' and 'upper' parameters"
        Otherwise if reference_distribution is equal to "exponential":
            If parameters.contains_key("rate"):
                Let rate be parameters.get("rate")
                If x is greater than or equal to 0.0:
                    Set theoretical_cdf to 1.0 minus exp(-rate multiplied by x)
                Otherwise:
                    Set theoretical_cdf to 0.0
            Otherwise:
                Throw Errors.InvalidArgument with "Exponential distribution requires 'rate' parameter"
        Otherwise:
            Throw Errors.InvalidArgument with "Unsupported distribution: " plus reference_distribution
        
        Note: Calculate absolute difference
        Let difference be abs(empirical_cdf minus theoretical_cdf)
        If difference is greater than max_difference:
            Set max_difference to difference
        
        Set i to i plus 1
    
    Note: Calculate test statistic and critical value
    Let ks_statistic be max_difference
    Let sqrt_n be sqrt(Float(n))
    
    Note: Critical value for two-sided test (approximate)
    Let critical_coefficient be 1.36
    If alpha is less than or equal to 0.01:
        Set critical_coefficient to 1.63
    Otherwise if alpha is less than or equal to 0.05:
        Set critical_coefficient to 1.36
    Otherwise if alpha is less than or equal to 0.10:
        Set critical_coefficient to 1.22
    
    Let critical_value be critical_coefficient / sqrt_n
    
    Note: Calculate p-value using asymptotic approximation
    Let lambda be ks_statistic multiplied by sqrt_n
    Let p_value be 2.0 multiplied by exp(-2.0 multiplied by lambda multiplied by lambda)
    If p_value is greater than 1.0:
        Set p_value to 1.0
    If p_value is less than 0.0:
        Set p_value to 0.0
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to ks_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(n minus 1)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: sample does not follow " plus reference_distribution plus " distribution"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: sample is consistent with " plus reference_distribution plus " distribution"
    
    Return test_result

Process called "cramers_von_mises_test" that takes sample as List[Float], reference_distribution as String, parameters as Dictionary[String, Float], alpha as Float returns HypothesisTest:
    Note: Test whether sample comes from specified continuous distribution using Cramér-von Mises statistic
    Note: More sensitive to differences in the middle of the distribution than Kolmogorov-Smirnov
    Note: Computational complexity: O(n²) for n sample points
    
    If sample.length is less than 2:
        Throw Errors.InvalidArgument with "Sample must contain at least 2 observations"
    
    Let n be sample.length
    Let sorted_sample be List[Float]()
    
    Note: Sort the sample
    Let i be 0
    While i is less than n:
        Let current_value be sample.get(i)
        Let j be 0
        Let inserted be false
        While j is less than sorted_sample.length and not inserted:
            If current_value is less than or equal to sorted_sample.get(j):
                Call sorted_sample.insert(j, current_value)
                Set inserted to true
            Set j to j plus 1
        If not inserted:
            Call sorted_sample.add(current_value)
        Set i to i plus 1
    
    Note: Calculate empirical CDF values and theoretical CDF differences
    Let cvm_statistic be 0.0
    Set i to 0
    While i is less than n:
        Let xi be sorted_sample.get(i)
        Let empirical_cdf be (i plus 1).to_float() / n.to_float()
        
        Note: Calculate theoretical CDF based on reference distribution
        Let theoretical_cdf be 0.0
        If reference_distribution is equal to "normal":
            Let mu be parameters.get("mean")
            Let sigma be parameters.get("std")
            Let z_score be (xi minus mu) / sigma
            Set theoretical_cdf to standard_normal_cdf(z_score)
        Otherwise if reference_distribution is equal to "uniform":
            Let a be parameters.get("min")
            Let b be parameters.get("max")
            If xi is less than or equal to a:
                Set theoretical_cdf to 0.0
            Otherwise if xi is greater than or equal to b:
                Set theoretical_cdf to 1.0
            Otherwise:
                Set theoretical_cdf to (xi minus a) / (b minus a)
        Otherwise if reference_distribution is equal to "exponential":
            Let lambda be parameters.get("rate")
            If xi is greater than or equal to 0.0:
                Set theoretical_cdf to 1.0 minus (2.71828 ^ (-lambda multiplied by xi))
            Otherwise:
                Set theoretical_cdf to 0.0
        Otherwise:
            Throw Errors.InvalidArgument with "Unsupported reference distribution"
        
        Note: Add squared difference weighted by density
        Let difference be empirical_cdf minus theoretical_cdf
        Set cvm_statistic to cvm_statistic plus (difference multiplied by difference)
        Set i to i plus 1
    
    Note: Apply Cramér-von Mises scaling
    Set cvm_statistic to cvm_statistic multiplied by n.to_float() plus (1.0 / (12.0 multiplied by n.to_float()))
    
    Note: Calculate critical value (approximate for normal distribution)
    Let critical_value be 0.0
    If alpha is greater than or equal to 0.1:
        Set critical_value to 0.347
    Otherwise if alpha is greater than or equal to 0.05:
        Set critical_value to 0.461
    Otherwise if alpha is greater than or equal to 0.025:
        Set critical_value to 0.581
    Otherwise if alpha is greater than or equal to 0.01:
        Set critical_value to 0.743
    Otherwise:
        Set critical_value to 1.168
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If cvm_statistic is greater than critical_value:
        Set test_result to "reject"
    
    Note: Calculate approximate p-value (simplified calculation)
    Let p_value be 0.0
    If cvm_statistic is less than or equal to 0.347:
        Set p_value to 1.0
    Otherwise if cvm_statistic is less than or equal to 0.461:
        Set p_value to 0.1
    Otherwise if cvm_statistic is less than or equal to 0.581:
        Set p_value to 0.05
    Otherwise if cvm_statistic is less than or equal to 0.743:
        Set p_value to 0.025
    Otherwise if cvm_statistic is less than or equal to 1.168:
        Set p_value to 0.01
    Otherwise:
        Set p_value to 0.001
    
    Return HypothesisTest with
        test_name: "Cramér-von Mises Test",
        null_hypothesis: "Sample follows " plus reference_distribution plus " distribution",
        alternative_hypothesis: "Sample does not follow " plus reference_distribution plus " distribution", 
        test_statistic: cvm_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: 0,
        significance_level: alpha,
        test_result: test_result,
        effect_size: 0.0

Process called "standard_normal_cdf" that takes z as Float returns Float:
    Note: Calculate standard normal cumulative distribution function
    Note: Uses rational approximation for computational efficiency
    
    If z is less than -6.0:
        Return 0.0
    Otherwise if z is greater than 6.0:
        Return 1.0
    
    Note: Use rational approximation for standard normal CDF
    Let abs_z be z
    If z is less than 0.0:
        Set abs_z to -z
    
    Let t be 1.0 / (1.0 plus 0.2316419 multiplied by abs_z)
    Let poly be t multiplied by (0.319381530 plus t multiplied by (-0.356563782 plus t multiplied by (1.781477937 plus t multiplied by (-1.821255978 plus t multiplied by 1.330274429))))
    Let cdf be 1.0 minus (1.0 / (2.506628274631 ^ 0.5)) multiplied by (2.71828 ^ (-0.5 multiplied by abs_z multiplied by abs_z)) multiplied by poly
    
    If z is less than 0.0:
        Set cdf to 1.0 minus cdf
    
    Return cdf

Process called "anderson_darling_test" that takes sample as List[Float], distribution as String, alpha as Float returns HypothesisTest:
    Note: Test normality with greater sensitivity to tail differences
    Note: More powerful than Kolmogorov-Smirnov for detecting non-normality
    
    If sample.length is less than 3:
        Throw Errors.InvalidArgument with "Anderson-Darling test requires at least 3 observations"
    
    If distribution not equal to "normal":
        Throw Errors.InvalidArgument with "Anderson-Darling test currently only supports normal distribution"
    
    Let n be sample.length
    Let sorted_sample be List[Float]()
    
    Note: Sort the sample using quicksort
    Let i be 0
    While i is less than n:
        Call sorted_sample.append(sample.get(i))
        Set i to i plus 1
    
    Let sort_result be quicksort(sorted_sample, "numeric")
    Set sorted_sample to sort_result.sorted_array
    
    Note: Estimate normal distribution parameters from sample
    Let mean be calculate_arithmetic_mean(sorted_sample)
    Let variance be calculate_variance(sorted_sample)
    Let std be sqrt(variance)
    
    Note: Calculate Anderson-Darling statistic
    Let ad_statistic be 0.0
    
    Set i to 0
    While i is less than n:
        Let x be sorted_sample.get(i)
        Let z_score be (x minus mean) / std
        
        Note: Calculate normal CDF and its complement
        Let cdf_value be normal_cdf(z_score, 0.0, 1.0)
        Let one_minus_cdf be 1.0 minus normal_cdf(z_score, 0.0, 1.0)
        
        Note: Avoid log(0) by using small epsilon
        If cdf_value is less than or equal to 0.0:
            Set cdf_value to 1e-10
        If one_minus_cdf is less than or equal to 0.0:
            Set one_minus_cdf to 1e-10
        If cdf_value is greater than or equal to 1.0:
            Set cdf_value to 1.0 minus 1e-10
        If one_minus_cdf is greater than or equal to 1.0:
            Set one_minus_cdf to 1.0 minus 1e-10
        
        Note: Calculate weighted term for Anderson-Darling statistic
        Let j be Float(i plus 1)
        Let weight_term be (2.0 multiplied by j minus 1.0) / Float(n)
        Let log_term be log(cdf_value) plus log(one_minus_cdf)
        Set ad_statistic to ad_statistic plus (weight_term multiplied by log_term)
        
        Set i to i plus 1
    
    Set ad_statistic to -Float(n) minus ad_statistic
    
    Note: Adjust for sample size (sample corrected statistic)
    Let adjusted_ad_statistic be ad_statistic multiplied by (1.0 plus 0.75 / Float(n) plus 2.25 / (Float(n) multiplied by Float(n)))
    
    Note: Critical values for normal distribution (approximate)
    Let critical_value be 0.787
    If alpha is less than or equal to 0.01:
        Set critical_value to 1.092
    Otherwise if alpha is less than or equal to 0.025:
        Set critical_value to 0.918
    Otherwise if alpha is less than or equal to 0.05:
        Set critical_value to 0.787
    Otherwise if alpha is less than or equal to 0.10:
        Set critical_value to 0.656
    
    Note: Calculate approximate p-value
    Let p_value be 0.0
    If adjusted_ad_statistic is greater than or equal to 0.6:
        Set p_value to exp(-1.2337 multiplied by adjusted_ad_statistic) multiplied by (2.00012 plus (0.247105 multiplied by adjusted_ad_statistic))
    Otherwise:
        Set p_value to 1.0 minus exp(-8.318 plus 42.796 multiplied by adjusted_ad_statistic minus 59.938 multiplied by adjusted_ad_statistic multiplied by adjusted_ad_statistic)
    
    If p_value is greater than 1.0:
        Set p_value to 1.0
    If p_value is less than 0.0:
        Set p_value to 0.0
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to adjusted_ad_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(n minus 1)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: sample does not follow normal distribution"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: sample is consistent with normal distribution"
    
    Return test_result

Process called "shapiro_wilk_test" that takes sample as List[Float], alpha as Float returns HypothesisTest:
    Note: Test normality assumption for small to moderate sample sizes
    Note: Most powerful normality test for sample sizes n ≤ 50
    
    If sample.length is less than 3:
        Throw Errors.InvalidArgument with "Shapiro-Wilk test requires at least 3 observations"
    
    If sample.length is greater than 5000:
        Throw Errors.InvalidArgument with "Shapiro-Wilk test not recommended for sample size is greater than 5000"
    
    Let n be sample.length
    Let sorted_sample be List[Float]()
    
    Note: Sort the sample using quicksort
    Let i be 0
    While i is less than n:
        Call sorted_sample.append(sample.get(i))
        Set i to i plus 1
    
    Let sort_result be quicksort(sorted_sample, "numeric")
    Set sorted_sample to sort_result.sorted_array
    
    Note: Calculate sample mean and variance
    Let mean be calculate_arithmetic_mean(sorted_sample)
    Let variance be calculate_variance(sorted_sample)
    
    Note: Calculate Shapiro-Wilk coefficients (approximation for general use)
    Let sum_of_squares be 0.0
    Let linear_combination be 0.0
    
    Note: Use simplified coefficient calculation for moderate sample sizes
    Let coefficients be List[Float]()
    
    Set i to 0
    While i is less than n:
        Let k be i plus 1
        Let coeff be 0.0
        
        Note: Approximate Shapiro-Wilk coefficients based on normal order statistics
        If n is less than or equal to 10:
            Note: Use simple linear weighting for small samples
            If i is less than n / 2:
                Set coeff to Float(k) / Float(n plus 1)
            Otherwise:
                Set coeff to Float(n plus 1 minus k) / Float(n plus 1)
        Otherwise:
            Note: Use more sophisticated approximation for larger samples
            Let m be Float(n plus 1) / 2.0
            Let c be Float(k) minus m
            Set coeff to c / sqrt(Float(n) multiplied by (Float(n) minus 1.0) / 4.0)
        
        Call coefficients.append(coeff)
        Set i to i plus 1
    
    Note: Calculate linear combination of order statistics
    Set i to 0
    While i is less than n:
        Let order_stat be sorted_sample.get(i)
        Let coeff be coefficients.get(i)
        Set linear_combination to linear_combination plus (coeff multiplied by order_stat)
        Set sum_of_squares to sum_of_squares plus (order_stat multiplied by order_stat)
        Set i to i plus 1
    
    Note: Calculate W statistic
    Let w_statistic be (linear_combination multiplied by linear_combination) / (sum_of_squares minus Float(n) multiplied by mean multiplied by mean)
    
    Note: Adjust for numerical stability
    If w_statistic is greater than 1.0:
        Set w_statistic to 1.0
    If w_statistic is less than 0.0:
        Set w_statistic to 0.0
    
    Note: Calculate approximate p-value based on sample size and W statistic
    Let p_value be 0.0
    Let log_w be log(w_statistic)
    
    If n is less than or equal to 11:
        Note: Use polynomial approximation for small samples
        Let y be -log_w
        Set p_value to exp(-1.2725 multiplied by y minus 1.0521 multiplied by y multiplied by y)
    Otherwise:
        Note: Use asymptotic approximation for larger samples
        Let log_n be log(Float(n))
        Let adjusted_w be log_w plus 1.0 / Float(n)
        Set p_value to 1.0 minus normal_cdf((adjusted_w plus 0.544 minus 0.39978 multiplied by log_n) / 0.5440, 0.0, 1.0)
    
    If p_value is greater than 1.0:
        Set p_value to 1.0
    If p_value is less than 0.0:
        Set p_value to 0.0
    
    Note: Critical value (approximate)
    Let critical_value be 0.90
    If alpha is less than or equal to 0.01:
        Set critical_value to 0.81
    Otherwise if alpha is less than or equal to 0.05:
        Set critical_value to 0.90
    Otherwise if alpha is less than or equal to 0.10:
        Set critical_value to 0.93
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to w_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(n minus 1)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: sample does not follow normal distribution"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: sample is consistent with normal distribution"
    
    Return test_result

Note: =====================================================================
Note: INDEPENDENCE AND ASSOCIATION TESTS OPERATIONS
Note: =====================================================================

Process called "chi_square_independence_test" that takes contingency_table as List[List[Integer]], alpha as Float returns HypothesisTest:
    Note: Test independence between two categorical variables
    Note: Uses chi-square distribution. Requires expected frequencies ≥ 5
    
    If contingency_table.length is less than 2:
        Throw Errors.InvalidArgument with "Contingency table must have at least 2 rows"
    
    Let num_rows be contingency_table.length
    Let num_cols be contingency_table.get(0).length
    
    If num_cols is less than 2:
        Throw Errors.InvalidArgument with "Contingency table must have at least 2 columns"
    
    Note: Calculate row totals, column totals, and grand total
    Let row_totals be List[Integer]()
    Let col_totals be List[Integer]()
    Let grand_total be 0
    
    Note: Initialize column totals
    Let j be 0
    While j is less than num_cols:
        Call col_totals.append(0)
        Set j to j plus 1
    
    Note: Calculate row totals and update column totals
    Let i be 0
    While i is less than num_rows:
        Let row_sum be 0
        Set j to 0
        While j is less than num_cols:
            Let cell_value be contingency_table.get(i).get(j)
            Set row_sum to row_sum plus cell_value
            Let current_col_total be col_totals.get(j)
            Call col_totals.set(j, current_col_total plus cell_value)
            Set j to j plus 1
        Call row_totals.append(row_sum)
        Set grand_total to grand_total plus row_sum
        Set i to i plus 1
    
    If grand_total is less than or equal to 0:
        Throw Errors.InvalidArgument with "Contingency table cannot be empty"
    
    Note: Calculate expected frequencies and chi-square statistic
    Let chi_square_stat be 0.0
    Let degrees_of_freedom be (num_rows minus 1) multiplied by (num_cols minus 1)
    Let low_expected_count be 0
    
    Set i to 0
    While i is less than num_rows:
        Set j to 0
        While j is less than num_cols:
            Let observed be Float(contingency_table.get(i).get(j))
            Let expected be (Float(row_totals.get(i)) multiplied by Float(col_totals.get(j))) / Float(grand_total)
            
            If expected is less than 5.0:
                Set low_expected_count to low_expected_count plus 1
            
            If expected is greater than 0.0:
                Let contribution be ((observed minus expected) multiplied by (observed minus expected)) / expected
                Set chi_square_stat to chi_square_stat plus contribution
            
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Check if chi-square assumptions are met
    Let total_cells be num_rows multiplied by num_cols
    Let low_expected_percentage be Float(low_expected_count) / Float(total_cells)
    
    If low_expected_percentage is greater than 0.2:
        Throw Errors.InvalidOperation with "Chi-square test assumptions violated: more than 20% of cells have expected frequency is less than 5"
    
    Note: Calculate p-value using chi-square distribution
    Let p_value be chi_squared_survival_function(chi_square_stat, Float(degrees_of_freedom))
    
    Note: Get critical value
    Let critical_value be chi_squared_inverse_cdf(1.0 minus alpha, Float(degrees_of_freedom))
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to chi_square_stat
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(degrees_of_freedom)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: variables are not independent"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: variables appear to be independent"
    
    Return test_result

Process called "fishers_exact_test" that takes contingency_table as List[List[Integer]], alpha as Float returns HypothesisTest:
    Note: Exact test for independence in 2×2 contingency tables
    Note: Used when chi-square assumptions are violated (small expected frequencies)
    
    If contingency_table.length not equal to 2:
        Throw Errors.InvalidArgument with "Fisher's exact test requires a 2x2 contingency table"
    
    If contingency_table.get(0).length not equal to 2:
        Throw Errors.InvalidArgument with "Fisher's exact test requires a 2x2 contingency table"
    
    If contingency_table.get(1).length not equal to 2:
        Throw Errors.InvalidArgument with "Fisher's exact test requires a 2x2 contingency table"
    
    Note: Extract 2x2 table values
    Let a be contingency_table.get(0).get(0)
    Let b be contingency_table.get(0).get(1)
    Let c be contingency_table.get(1).get(0)
    Let d be contingency_table.get(1).get(1)
    
    If a is less than 0 or b is less than 0 or c is less than 0 or d is less than 0:
        Throw Errors.InvalidArgument with "All table entries must be non-negative"
    
    Let n be a plus b plus c plus d
    If n is equal to 0:
        Throw Errors.InvalidArgument with "Contingency table cannot be empty"
    
    Note: Calculate marginal totals
    Let row1_total be a plus b
    Let row2_total be c plus d
    Let col1_total be a plus c
    Let col2_total be b plus d
    
    Note: Calculate probability using hypergeometric distribution
    Note: P(X is equal to a | marginals fixed) is equal to C(col1_total, a) multiplied by C(col2_total, b) / C(n, row1_total)
    
    Let exact_probability be 0.0
    
    Note: Calculate exact probability for observed configuration
    Let numerator_log be log_factorial(col1_total) plus log_factorial(col2_total) plus log_factorial(row1_total) plus log_factorial(row2_total)
    Let denominator_log be log_factorial(n) plus log_factorial(a) plus log_factorial(b) plus log_factorial(c) plus log_factorial(d)
    Set exact_probability to exp(numerator_log minus denominator_log)
    
    Note: Calculate one-tailed p-value (sum probabilities for all tables as extreme or more extreme)
    Let one_tailed_p be 0.0
    
    Note: Find minimum and maximum possible values for cell a given marginals
    Let min_a be 0
    If col1_total is less than row1_total:
        Set min_a to row1_total minus col2_total
    If min_a is less than 0:
        Set min_a to 0
    
    Let max_a be col1_total
    If row1_total is less than max_a:
        Set max_a to row1_total
    
    Note: Sum probabilities for configurations at least as extreme as observed
    Let test_a be min_a
    While test_a is less than or equal to max_a:
        Let test_b be row1_total minus test_a
        Let test_c be col1_total minus test_a  
        Let test_d be col2_total minus test_b
        
        If test_b is greater than or equal to 0 and test_c is greater than or equal to 0 and test_d is greater than or equal to 0:
            Note: Calculate probability for this configuration
            Let test_numerator_log be log_factorial(col1_total) plus log_factorial(col2_total) plus log_factorial(row1_total) plus log_factorial(row2_total)
            Let test_denominator_log be log_factorial(n) plus log_factorial(test_a) plus log_factorial(test_b) plus log_factorial(test_c) plus log_factorial(test_d)
            Let test_probability be exp(test_numerator_log minus test_denominator_log)
            
            Note: Include this configuration if it's as extreme or more extreme than observed
            If test_probability is less than or equal to exact_probability plus 1e-10:
                Set one_tailed_p to one_tailed_p plus test_probability
        
        Set test_a to test_a plus 1
    
    Note: Two-tailed p-value (double one-tailed unless probability is greater than 0.5)
    Let p_value be one_tailed_p multiplied by 2.0
    If p_value is greater than 1.0:
        Set p_value to 1.0
    
    Note: For Fisher's exact test, there's no simple critical value minus use p-value directly
    Let test_result be HypothesisTest
    Set test_result.statistic to exact_probability
    Set test_result.p_value to p_value  
    Set test_result.critical_value to alpha  Note: Direct comparison with alpha
    Set test_result.degrees_of_freedom to 1.0  Note: Effective degrees of freedom
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: variables are not independent (Fisher's exact test)"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: variables appear to be independent (Fisher's exact test)"
    
    Return test_result

Process called "mcnemar_test" that takes paired_data as List[List[Integer]], alpha as Float returns HypothesisTest:
    Note: Test for paired nominal data (before/after comparisons)
    Note: Tests marginal homogeneity in 2×2 matched pairs table
    
    If paired_data.length not equal to 2 or paired_data.get(0).length not equal to 2 or paired_data.get(1).length not equal to 2:
        Throw Errors.InvalidArgument with "McNemar's test requires a 2x2 table"
    
    Note: Extract discordant pairs (b and c cells)
    Let a be paired_data.get(0).get(0)  Note: Both positive
    Let b be paired_data.get(0).get(1)  Note: First positive, second negative
    Let c be paired_data.get(1).get(0)  Note: First negative, second positive  
    Let d be paired_data.get(1).get(1)  Note: Both negative
    
    If b plus c is less than 25:
        Note: Use exact binomial test for small samples
        Let n_discordant be b plus c
        If n_discordant is equal to 0:
            Throw Errors.InvalidOperation with "No discordant pairs minus cannot perform McNemar's test"
        
        Note: Under null hypothesis, b follows Binomial(n_discordant, 0.5)
        Let p_exact be 0.0
        Let min_extreme be 0
        If b is less than n_discordant / 2:
            Set min_extreme to b
        Otherwise:
            Set min_extreme to n_discordant minus b
        
        Note: Calculate two-tailed exact p-value
        Let k be 0
        While k is less than or equal to min_extreme:
            Let prob_k be binomial_coefficient(n_discordant, k) multiplied by power(0.5, n_discordant)
            Set p_exact to p_exact plus prob_k
            Set k to k plus 1
        
        Set p_exact to p_exact multiplied by 2.0  Note: Two-tailed
        If p_exact is greater than 1.0:
            Set p_exact to 1.0
        
        Let test_result be HypothesisTest
        Set test_result.statistic to Float(abs(b minus c))
        Set test_result.p_value to p_exact
        Set test_result.critical_value to alpha
        Set test_result.degrees_of_freedom to 1.0
        Set test_result.reject_null to p_exact is less than alpha
        Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    Otherwise:
        Note: Use chi-square approximation for large samples
        Let mcnemar_stat be ((abs(Float(b) minus Float(c)) minus 0.5) multiplied by (abs(Float(b) minus Float(c)) minus 0.5)) / Float(b plus c)
        Let p_value be chi_squared_survival_function(mcnemar_stat, 1.0)
        Let critical_value be chi_squared_inverse_cdf(1.0 minus alpha, 1.0)
        
        Let test_result be HypothesisTest  
        Set test_result.statistic to mcnemar_stat
        Set test_result.p_value to p_value
        Set test_result.critical_value to critical_value
        Set test_result.degrees_of_freedom to 1.0
        Set test_result.reject_null to p_value is less than alpha
        Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: marginal proportions differ significantly"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: marginal proportions are not significantly different"
    
    Return test_result

Process called "cochran_q_test" that takes binary_matrix as List[List[Integer]], alpha as Float returns HypothesisTest:
    Note: Extension of McNemar's test to multiple related binary variables
    Note: Tests whether proportions differ across multiple conditions
    
    If binary_matrix.length is less than 2:
        Throw Errors.InvalidArgument with "Cochran's Q test requires at least 2 subjects"
    
    Let n_subjects be binary_matrix.length
    Let n_conditions be binary_matrix.get(0).length
    
    If n_conditions is less than 3:
        Throw Errors.InvalidArgument with "Cochran's Q test requires at least 3 conditions"
    
    Note: Verify all entries are 0 or 1
    Let i be 0
    While i is less than n_subjects:
        Let j be 0
        While j is less than n_conditions:
            Let value be binary_matrix.get(i).get(j)
            If value not equal to 0 and value not equal to 1:
                Throw Errors.InvalidArgument with "All entries must be 0 or 1 for Cochran's Q test"
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Calculate row totals (Ri) and column totals (Cj)
    Let row_totals be List[Integer]()
    Let col_totals be List[Integer]()
    Let grand_total be 0
    
    Note: Initialize column totals
    Set j to 0
    While j is less than n_conditions:
        Call col_totals.append(0)
        Set j to j plus 1
    
    Note: Calculate row and column totals
    Set i to 0
    While i is less than n_subjects:
        Let row_sum be 0
        Set j to 0
        While j is less than n_conditions:
            Let value be binary_matrix.get(i).get(j)
            Set row_sum to row_sum plus value
            Let current_col_total be col_totals.get(j)
            Call col_totals.set(j, current_col_total plus value)
            Set j to j plus 1
        Call row_totals.append(row_sum)
        Set grand_total to grand_total plus row_sum
        Set i to i plus 1
    
    Note: Calculate Cochran's Q statistic
    Note: Q is equal to (k-1) multiplied by [k multiplied by sum(Cj²) minus (sum(Cj))²] / [k multiplied by sum(Ri) minus sum(Ri²)]
    
    Let k be Float(n_conditions)
    Let sum_cj_squared be 0.0
    Let sum_cj be 0.0
    
    Set j to 0
    While j is less than n_conditions:
        Let cj be Float(col_totals.get(j))
        Set sum_cj_squared to sum_cj_squared plus (cj multiplied by cj)
        Set sum_cj to sum_cj plus cj
        Set j to j plus 1
    
    Let sum_ri be 0.0
    Let sum_ri_squared be 0.0
    
    Set i to 0
    While i is less than n_subjects:
        Let ri be Float(row_totals.get(i))
        Set sum_ri to sum_ri plus ri
        Set sum_ri_squared to sum_ri_squared plus (ri multiplied by ri)
        Set i to i plus 1
    
    Note: Calculate numerator and denominator
    Let numerator be (k minus 1.0) multiplied by (k multiplied by sum_cj_squared minus sum_cj multiplied by sum_cj)
    Let denominator be k multiplied by sum_ri minus sum_ri_squared
    
    If denominator is less than or equal to 0.0:
        Throw Errors.InvalidOperation with "Cannot compute Cochran's Q: denominator is zero or negative"
    
    Let q_statistic be numerator / denominator
    Let degrees_of_freedom be n_conditions minus 1
    
    Note: Calculate p-value using chi-square distribution
    Let p_value be chi_squared_survival_function(q_statistic, Float(degrees_of_freedom))
    Let critical_value be chi_squared_inverse_cdf(1.0 minus alpha, Float(degrees_of_freedom))
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to q_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(degrees_of_freedom)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: proportions differ significantly across conditions"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: proportions do not differ significantly across conditions"
    
    Return test_result

Note: =====================================================================
Note: VARIANCE TESTS OPERATIONS
Note: =====================================================================

Process called "f_test_equality_of_variances" that takes sample1 as List[Float], sample2 as List[Float], alpha as Float returns HypothesisTest:
    Note: Test equality of variances between two normal populations
    Note: Uses F-distribution. Sensitive to non-normality assumption
    
    If sample1.size() is less than 2 or sample2.size() is less than 2:
        Throw Errors.InvalidArgument with "Both samples must contain at least 2 observations"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let n1 be sample1.size()
    Let n2 be sample2.size()
    Let var1 be DescriptiveStats.calculate_variance(sample1, false, true)
    Let var2 be DescriptiveStats.calculate_variance(sample2, false, true)
    
    If var1 is less than or equal to 0.0 or var2 is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Sample variances must be positive"
    
    Note: F-statistic is ratio of larger to smaller variance
    Let f_statistic be 0.0
    Let df1 be 0
    Let df2 be 0
    
    If var1 is greater than or equal to var2:
        Set f_statistic to var1 / var2
        Set df1 to n1 minus 1
        Set df2 to n2 minus 1
    Otherwise:
        Set f_statistic to var2 / var1
        Set df1 to n2 minus 1
        Set df2 to n1 minus 1
    
    Note: Two-sided test using upper tail (since we use larger/smaller ratio)
    Let p_value be 2.0 multiplied by (1.0 minus Distributions.f_distribution_cdf(f_statistic, df1, df2))
    If p_value is greater than 1.0:
        Set p_value to 1.0
    
    Note: Calculate critical value using quantile function
    Let critical_probability be 1.0 minus alpha / 2.0
    Let f_dist be Distributions.ContinuousDistribution with
        distribution_name: "f",
        parameters: Dictionary[String, Float] with ["df1": Float(df1), "df2": Float(df2)],
        support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Let critical_value be NumericalCore.inverse_cdf(critical_probability, f_dist)
    
    Note: Determine test result
    Let test_result be "fail to reject"
    If f_statistic is greater than critical_value:
        Set test_result to "reject"
    
    Note: Effect size is ratio of variances
    Let effect_size be MathOps.maximum(var1, var2) / MathOps.minimum(var1, var2)
    
    Return HypothesisTest with
        test_name: "F-test for equality of variances",
        null_hypothesis: "Variances are equal",
        alternative_hypothesis: "Variances are not equal",
        test_statistic: f_statistic,
        p_value: p_value,
        critical_value: critical_value,
        degrees_of_freedom: df1,
        significance_level: alpha,
        test_result: test_result,
        effect_size: effect_size

Process called "levene_test" that takes groups as List[List[Float]], center as String, alpha as Float returns HypothesisTest:
    Note: Robust test for equality of variances across multiple groups
    Note: Less sensitive to non-normality than F-test. Centers: mean, median, trimmed
    
    If groups.length is less than 2:
        Throw Errors.InvalidArgument with "Levene's test requires at least 2 groups"
    
    If center not equal to "mean" and center not equal to "median" and center not equal to "trimmed":
        Throw Errors.InvalidArgument with "Center must be 'mean', 'median', or 'trimmed'"
    
    Note: Check minimum group sizes
    Let total_n be 0
    Let k be groups.length
    Let i be 0
    While i is less than k:
        Let group_size be groups.get(i).length
        If group_size is less than 2:
            Throw Errors.InvalidArgument with "Each group must have at least 2 observations"
        Set total_n to total_n plus group_size
        Set i to i plus 1
    
    Note: Calculate group centers and absolute deviations
    Let group_centers be List[Float]()
    Let all_deviations be List[Float]()
    Let group_deviation_lists be List[List[Float]]()
    
    Set i to 0
    While i is less than k:
        Let group be groups.get(i)
        Let group_center be 0.0
        
        Note: Calculate center based on specified method
        If center is equal to "mean":
            Set group_center to calculate_arithmetic_mean(group)
        Otherwise if center is equal to "median":
            Let sorted_group be quicksort(group, "numeric")
            Set group_center to calculate_median(sorted_group.sorted_array)
        Otherwise if center is equal to "trimmed":
            Note: Use 10% trimmed mean
            Let sorted_group be quicksort(group, "numeric")
            Let trim_count be Integer(Float(group.length) multiplied by 0.1)
            Let trimmed_list be List[Float]()
            Let j be trim_count
            While j is less than group.length minus trim_count:
                Call trimmed_list.append(sorted_group.sorted_array.get(j))
                Set j to j plus 1
            Set group_center to calculate_arithmetic_mean(trimmed_list)
        
        Call group_centers.append(group_center)
        
        Note: Calculate absolute deviations from center
        Let group_deviations be List[Float]()
        Let j be 0
        While j is less than group.length:
            Let deviation be abs(group.get(j) minus group_center)
            Call group_deviations.append(deviation)
            Call all_deviations.append(deviation)
            Set j to j plus 1
        
        Call group_deviation_lists.append(group_deviations)
        Set i to i plus 1
    
    Note: Perform one-way ANOVA on absolute deviations
    Let overall_mean_deviation be calculate_arithmetic_mean(all_deviations)
    
    Note: Calculate between-group sum of squares
    Let ss_between be 0.0
    Set i to 0
    While i is less than k:
        Let group_deviations be group_deviation_lists.get(i)
        Let group_mean_deviation be calculate_arithmetic_mean(group_deviations)
        Let ni be Float(group_deviations.length)
        Let diff be group_mean_deviation minus overall_mean_deviation
        Set ss_between to ss_between plus (ni multiplied by diff multiplied by diff)
        Set i to i plus 1
    
    Note: Calculate within-group sum of squares
    Let ss_within be 0.0
    Set i to 0
    While i is less than k:
        Let group_deviations be group_deviation_lists.get(i)
        Let group_mean_deviation be calculate_arithmetic_mean(group_deviations)
        Let j be 0
        While j is less than group_deviations.length:
            Let diff be group_deviations.get(j) minus group_mean_deviation
            Set ss_within to ss_within plus (diff multiplied by diff)
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Calculate F statistic
    Let df_between be k minus 1
    Let df_within be total_n minus k
    Let ms_between be ss_between / Float(df_between)
    Let ms_within be ss_within / Float(df_within)
    
    If ms_within is less than or equal to 0.0:
        Throw Errors.InvalidOperation with "Within-group mean square is zero or negative"
    
    Let f_statistic be ms_between / ms_within
    
    Note: Calculate p-value using F-distribution
    Let p_value be f_survival_function(f_statistic, Float(df_between), Float(df_within))
    Let critical_value be f_inverse_cdf(1.0 minus alpha, Float(df_between), Float(df_within))
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to f_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(df_between)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: variances are not equal across groups"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: variances appear equal across groups"
    
    Return test_result

Process called "bartlett_test" that takes groups as List[List[Float]], alpha as Float returns HypothesisTest:
    Note: Test equality of variances across multiple groups (assumes normality)
    Note: More powerful than Levene's test when normality holds
    
    If groups.length is less than 2:
        Throw Errors.InvalidArgument with "Bartlett's test requires at least 2 groups"
    
    Note: Check minimum group sizes and calculate sample variances
    Let total_n be 0
    Let k be groups.length
    Let sample_variances be List[Float]()
    let sample_sizes be List[Integer]()
    
    Let i be 0
    While i is less than k:
        Let group be groups.get(i)
        Let ni be group.length
        If ni is less than 2:
            Throw Errors.InvalidArgument with "Each group must have at least 2 observations"
        
        Let variance be calculate_variance(group)
        If variance is less than or equal to 0.0:
            Throw Errors.InvalidOperation with "Sample variance must be positive for Bartlett's test"
        
        Call sample_variances.append(variance)
        Call sample_sizes.append(ni)
        Set total_n to total_n plus ni
        Set i to i plus 1
    
    Note: Calculate pooled variance
    Let sum_squares be 0.0
    Set i to 0
    While i is less than k:
        Let ni be Float(sample_sizes.get(i))
        Let si_squared be sample_variances.get(i)
        Set sum_squares to sum_squares plus ((ni minus 1.0) multiplied by si_squared)
        Set i to i plus 1
    
    Let pooled_variance be sum_squares / Float(total_n minus k)
    
    Note: Calculate Bartlett's test statistic
    Note: B is equal to [(N-k) multiplied by ln(s_p²) minus sum((ni-1) multiplied by ln(si²))] / C
    
    Let b_numerator be Float(total_n minus k) multiplied by log(pooled_variance)
    Set i to 0
    While i is less than k:
        Let ni be Float(sample_sizes.get(i))
        Let si_squared be sample_variances.get(i)
        Set b_numerator to b_numerator minus ((ni minus 1.0) multiplied by log(si_squared))
        Set i to i plus 1
    
    Note: Calculate correction factor C
    Let sum_reciprocal_df be 0.0
    Set i to 0
    While i is less than k:
        Let ni be Float(sample_sizes.get(i))
        Set sum_reciprocal_df to sum_reciprocal_df plus (1.0 / (ni minus 1.0))
        Set i to i plus 1
    
    Let c_factor be 1.0 plus ((1.0 / (3.0 multiplied by Float(k minus 1))) multiplied by (sum_reciprocal_df minus (1.0 / Float(total_n minus k))))
    
    Let bartlett_statistic be b_numerator / c_factor
    Let degrees_of_freedom be k minus 1
    
    Note: Calculate p-value using chi-square distribution
    Let p_value be chi_squared_survival_function(bartlett_statistic, Float(degrees_of_freedom))
    Let critical_value be chi_squared_inverse_cdf(1.0 minus alpha, Float(degrees_of_freedom))
    
    Note: Create test result
    Let test_result be HypothesisTest
    Set test_result.statistic to bartlett_statistic
    Set test_result.p_value to p_value
    Set test_result.critical_value to critical_value
    Set test_result.degrees_of_freedom to Float(degrees_of_freedom)
    Set test_result.reject_null to p_value is less than alpha
    Set test_result.confidence_level to (1.0 minus alpha) multiplied by 100.0
    
    If test_result.reject_null:
        Set test_result.conclusion to "Reject null hypothesis: variances are not equal across groups (Bartlett's test)"
    Otherwise:
        Set test_result.conclusion to "Fail to reject null hypothesis: variances appear equal across groups (Bartlett's test)"
    
    Return test_result

Process called "brown_forsythe_test" that takes groups as List[List[Float]], alpha as Float returns HypothesisTest:
    Note: Robust alternative to Levene's test using median centering
    Note: Good performance under non-normality and unequal sample sizes
    
    Note: Brown-Forsythe is essentially Levene's test with median centering
    Return levene_test(groups, "median", alpha)

Note: =====================================================================
Note: CONFIDENCE INTERVAL OPERATIONS
Note: =====================================================================

Process called "confidence_interval_mean" that takes sample as List[Float], confidence_level as Float, population_std as Float returns ConfidenceInterval:
    Note: Construct confidence interval for population mean
    Note: Uses t-distribution when σ unknown, z-distribution when σ known
    
    If sample.size() is less than 1:
        Throw Errors.InvalidArgument with "Sample must contain at least 1 observation"
    
    If confidence_level is less than or equal to 0.0 or confidence_level is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Confidence level must be between 0 and 1"
    
    Let n be sample.size()
    Let sample_mean be DescriptiveStats.calculate_arithmetic_mean(sample, [])
    Let alpha be 1.0 minus confidence_level
    Let standard_error be 0.0
    Let critical_value be 0.0
    Let interval_type be ""
    
    If population_std is greater than 0.0:
        Note: Use z-distribution (known population standard deviation)
        Set standard_error to population_std / MathOps.square_root(ToString(Float(n)), 15).result_value.to_float()
        
        Let normal_dist be Distributions.ContinuousDistribution with
            distribution_name: "normal",
            parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
            support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
            parameter_constraints: Dictionary[String, Dictionary[String, Float]],
            moment_generating_function: Dictionary[String, String],
            characteristic_function: Dictionary[String, String]
        
        Set critical_value to NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
        Set interval_type to "z-interval"
    
    Otherwise:
        Note: Use t-distribution (unknown population standard deviation)
        If n is less than 2:
            Throw Errors.InvalidArgument with "Sample size must be at least 2 for t-interval"
        
        Let sample_std be DescriptiveStats.calculate_standard_deviation(sample, false)
        Set standard_error to sample_std / MathOps.square_root(ToString(Float(n)), 15).result_value.to_float()
        
        Let degrees_of_freedom be n minus 1
        Let t_dist be Distributions.ContinuousDistribution with
            distribution_name: "t",
            parameters: Dictionary[String, Float] with ["degrees_freedom": Float(degrees_of_freedom)],
            support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
            parameter_constraints: Dictionary[String, Dictionary[String, Float]],
            moment_generating_function: Dictionary[String, String],
            characteristic_function: Dictionary[String, String]
        
        Set critical_value to NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, t_dist)
        Set interval_type to "t-interval"
    
    Let margin_of_error be critical_value multiplied by standard_error
    Let lower_bound be sample_mean minus margin_of_error
    Let upper_bound be sample_mean plus margin_of_error
    
    Let interpretation be "We are " plus ToString(confidence_level multiplied by 100.0) plus "% confident that the true population mean lies between " plus ToString(lower_bound) plus " and " plus ToString(upper_bound)
    
    Return ConfidenceInterval with
        parameter_name: "Population mean",
        point_estimate: sample_mean,
        confidence_level: confidence_level,
        lower_bound: lower_bound,
        upper_bound: upper_bound,
        margin_of_error: margin_of_error,
        standard_error: standard_error,
        interpretation: interpretation

Process called "confidence_interval_proportion" that takes successes as Integer, total as Integer, confidence_level as Float, method as String returns ConfidenceInterval:
    Note: Construct confidence interval for population proportion
    Note: Methods: normal approximation, Wilson, exact (Clopper-Pearson), Agresti-Coull
    
    If total is less than or equal to 0:
        Throw Errors.InvalidArgument with "Total must be positive"
    
    If successes is less than 0 or successes is greater than total:
        Throw Errors.InvalidArgument with "Successes must be between 0 and total"
    
    If confidence_level is less than or equal to 0.0 or confidence_level is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Confidence level must be between 0 and 1"
    
    If method does not equal "wilson" and method does not equal "normal" and method does not equal "exact" and method does not equal "agresti-coull":
        Throw Errors.InvalidArgument with "Method must be 'wilson', 'normal', 'exact', or 'agresti-coull'"
    
    Let n be Float(total)
    Let x be Float(successes)
    Let p_hat be x / n
    Let alpha be 1.0 minus confidence_level
    
    Note: Create standard normal distribution for critical value
    Let normal_dist be Distributions.ContinuousDistribution with
        distribution_name: "normal",
        parameters: Dictionary[String, Float] with ["mean": 0.0, "variance": 1.0],
        support_bounds: Dictionary[String, Float] with ["lower": Float.NEGATIVE_INFINITY, "upper": Float.POSITIVE_INFINITY],
        parameter_constraints: Dictionary[String, Dictionary[String, Float]],
        moment_generating_function: Dictionary[String, String],
        characteristic_function: Dictionary[String, String]
    
    Let z_alpha_2 be NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, normal_dist)
    
    Let lower_bound be 0.0
    Let upper_bound be 1.0
    Let margin_of_error be 0.0
    Let standard_error be 0.0
    
    If method is equal to "wilson":
        Note: Wilson score interval
        Let z_squared be z_alpha_2 multiplied by z_alpha_2
        Let denominator be n plus z_squared
        Let center_adjustment be (x plus z_squared / 2.0) / denominator
        
        Let variance_term be (p_hat multiplied by (1.0 minus p_hat) plus z_squared / (4.0 multiplied by n)) / denominator
        Set margin_of_error to z_alpha_2 multiplied by MathOps.square_root(ToString(variance_term), 15).result_value.to_float()
        
        Set lower_bound to center_adjustment minus margin_of_error
        Set upper_bound to center_adjustment plus margin_of_error
        Set standard_error to MathOps.square_root(ToString(variance_term), 15).result_value.to_float()
    
    Otherwise if method is equal to "normal":
        Note: Normal approximation (Wald interval)
        Set standard_error to MathOps.square_root(ToString(p_hat multiplied by (1.0 minus p_hat) / n), 15).result_value.to_float()
        Set margin_of_error to z_alpha_2 multiplied by standard_error
        Set lower_bound to p_hat minus margin_of_error
        Set upper_bound to p_hat plus margin_of_error
    
    Otherwise if method is equal to "agresti-coull":
        Note: Agresti-Coull interval
        Let z_squared be z_alpha_2 multiplied by z_alpha_2
        Let n_tilde be n plus z_squared
        Let p_tilde be (x plus z_squared / 2.0) / n_tilde
        
        Set standard_error to MathOps.square_root(ToString(p_tilde multiplied by (1.0 minus p_tilde) / n_tilde), 15).result_value.to_float()
        Set margin_of_error to z_alpha_2 multiplied by standard_error
        Set lower_bound to p_tilde minus margin_of_error
        Set upper_bound to p_tilde plus margin_of_error
    
    Otherwise if method is equal to "exact":
        Note: Exact (Clopper-Pearson) interval using beta distribution
        If successes is equal to 0:
            Set lower_bound to 0.0
        Otherwise:
            Let beta_dist_lower be Distributions.ContinuousDistribution with
                distribution_name: "beta",
                parameters: Dictionary[String, Float] with ["alpha": Float(successes), "beta": Float(total minus successes plus 1)],
                support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": 1.0],
                parameter_constraints: Dictionary[String, Dictionary[String, Float]],
                moment_generating_function: Dictionary[String, String],
                characteristic_function: Dictionary[String, String]
            Set lower_bound to NumericalCore.inverse_cdf(alpha / 2.0, beta_dist_lower)
        
        If successes is equal to total:
            Set upper_bound to 1.0
        Otherwise:
            Let beta_dist_upper be Distributions.ContinuousDistribution with
                distribution_name: "beta",
                parameters: Dictionary[String, Float] with ["alpha": Float(successes plus 1), "beta": Float(total minus successes)],
                support_bounds: Dictionary[String, Float] with ["lower": 0.0, "upper": 1.0],
                parameter_constraints: Dictionary[String, Dictionary[String, Float]],
                moment_generating_function: Dictionary[String, String],
                characteristic_function: Dictionary[String, String]
            Set upper_bound to NumericalCore.inverse_cdf(1.0 minus alpha / 2.0, beta_dist_upper)
        
        Set margin_of_error to (upper_bound minus lower_bound) / 2.0
        Set standard_error to MathOps.square_root(ToString(p_hat multiplied by (1.0 minus p_hat) / n), 15).result_value.to_float()
    
    Note: Ensure bounds are within [0, 1]
    If lower_bound is less than 0.0:
        Set lower_bound to 0.0
    If upper_bound is greater than 1.0:
        Set upper_bound to 1.0
    
    Let interpretation be "We are " plus ToString(confidence_level multiplied by 100.0) plus "% confident that the true population proportion lies between " plus ToString(lower_bound) plus " and " plus ToString(upper_bound) plus " (" plus method plus " method)"
    
    Return ConfidenceInterval with
        parameter_name: "Population proportion",
        point_estimate: p_hat,
        confidence_level: confidence_level,
        lower_bound: lower_bound,
        upper_bound: upper_bound,
        margin_of_error: margin_of_error,
        standard_error: standard_error,
        interpretation: interpretation

Process called "confidence_interval_variance" that takes sample as List[Float], confidence_level as Float returns ConfidenceInterval:
    Note: Construct confidence interval for population variance
    Note: Uses chi-square distribution. Assumes normality
    
    If sample.length is less than 2:
        Throw Errors.InvalidArgument with "Sample must contain at least 2 observations"
    
    If confidence_level is less than or equal to 0.0 or confidence_level is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Confidence level must be between 0 and 1"
    
    Let n be sample.length
    Let sample_variance be calculate_variance(sample)
    
    If sample_variance is less than or equal to 0.0:
        Throw Errors.InvalidOperation with "Sample variance must be positive"
    
    Let df be Float(n minus 1)
    Let alpha be 1.0 minus confidence_level
    
    Note: Calculate chi-square critical values
    Let chi_square_lower be chi_squared_inverse_cdf(alpha / 2.0, df)
    Let chi_square_upper be chi_squared_inverse_cdf(1.0 minus alpha / 2.0, df)
    
    Note: Calculate confidence interval bounds for variance
    Let lower_bound be (df multiplied by sample_variance) / chi_square_upper
    Let upper_bound be (df multiplied by sample_variance) / chi_square_lower
    
    Note: Create confidence interval result
    Let ci_result be ConfidenceInterval
    Set ci_result.lower_bound to lower_bound
    Set ci_result.upper_bound to upper_bound
    Set ci_result.confidence_level to confidence_level multiplied by 100.0
    Set ci_result.point_estimate to sample_variance
    Set ci_result.margin_of_error to (upper_bound minus lower_bound) / 2.0
    Set ci_result.parameter_name to "Population Variance"
    Set ci_result.method to "Chi-square distribution"
    
    Return ci_result

Process called "confidence_interval_difference_means" that takes sample1 as List[Float], sample2 as List[Float], confidence_level as Float, equal_variance as Boolean returns ConfidenceInterval:
    Note: Construct confidence interval for difference between two means
    Note: Uses pooled or unpooled variance depending on equal_variance assumption
    
    If sample1.length is less than 2 or sample2.length is less than 2:
        Throw Errors.InvalidArgument with "Both samples must contain at least 2 observations"
    
    If confidence_level is less than or equal to 0.0 or confidence_level is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Confidence level must be between 0 and 1"
    
    Let n1 be sample1.length
    Let n2 be sample2.length
    Let mean1 be calculate_arithmetic_mean(sample1)
    Let mean2 be calculate_arithmetic_mean(sample2)
    Let var1 be calculate_variance(sample1)
    let var2 be calculate_variance(sample2)
    
    Let diff_means be mean1 minus mean2
    Let standard_error be 0.0
    Let degrees_of_freedom be 0.0
    
    If equal_variance:
        Note: Use pooled variance (equal variance assumption)
        Let pooled_variance be ((Float(n1 minus 1) multiplied by var1) plus (Float(n2 minus 1) multiplied by var2)) / Float(n1 plus n2 minus 2)
        Set standard_error to sqrt(pooled_variance multiplied by ((1.0 / Float(n1)) plus (1.0 / Float(n2))))
        Set degrees_of_freedom to Float(n1 plus n2 minus 2)
    Otherwise:
        Note: Use Welch's t-test (unequal variance assumption)
        Set standard_error to sqrt((var1 / Float(n1)) plus (var2 / Float(n2)))
        Set degrees_of_freedom to welch_degrees_freedom_calculation(var1, n1, var2, n2)
    
    Let alpha be 1.0 minus confidence_level
    Let t_critical be t_inverse_cdf(1.0 minus alpha / 2.0, degrees_of_freedom)
    Let margin_of_error be t_critical multiplied by standard_error
    
    Let lower_bound be diff_means minus margin_of_error
    Let upper_bound be diff_means plus margin_of_error
    
    Note: Create confidence interval result
    Let ci_result be ConfidenceInterval
    Set ci_result.lower_bound to lower_bound
    Set ci_result.upper_bound to upper_bound
    Set ci_result.confidence_level to confidence_level multiplied by 100.0
    Set ci_result.point_estimate to diff_means
    Set ci_result.margin_of_error to margin_of_error
    Set ci_result.parameter_name to "Difference of Population Means"
    
    If equal_variance:
        Set ci_result.method to "Pooled variance t-distribution"
    Otherwise:
        Set ci_result.method to "Welch's t-distribution (unequal variances)"
    
    Return ci_result

Note: =====================================================================
Note: POWER ANALYSIS OPERATIONS
Note: =====================================================================

Process called "power_analysis_t_test" that takes effect_size as Float, sample_size as Integer, alpha as Float, test_type as String returns PowerAnalysis:
    Note: Calculate statistical power for t-tests given effect size and sample size
    Note: Test types: one-sample, independent-samples, paired-samples
    
    If effect_size is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Effect size must be positive"
    
    If sample_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Sample size must be positive"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Calculate degrees of freedom based on test type
    Let df be 0.0
    Let n_effective be Float(sample_size)
    
    If test_type is equal to "one-sample":
        Set df to Float(sample_size minus 1)
    Otherwise if test_type is equal to "independent-samples":
        Set df to Float(2 multiplied by sample_size minus 2)
        Set n_effective to Float(sample_size) / 2.0  Note: n per group
    Otherwise if test_type is equal to "paired-samples":
        Set df to Float(sample_size minus 1)
    Otherwise:
        Throw Errors.InvalidArgument with "Invalid test type. Use: one-sample, independent-samples, or paired-samples"
    
    Note: Calculate critical t-value for given alpha (two-tailed)
    Let t_critical be t_inverse_cdf(1.0 minus alpha / 2.0, df)
    
    Note: Calculate non-centrality parameter
    Let ncp be 0.0
    If test_type is equal to "one-sample":
        Set ncp to effect_size multiplied by sqrt(n_effective)
    Otherwise if test_type is equal to "independent-samples":
        Set ncp to effect_size multiplied by sqrt(n_effective / 2.0)
    Otherwise if test_type is equal to "paired-samples":
        Set ncp to effect_size multiplied by sqrt(n_effective)
    
    Note: Calculate power using non-central t-distribution approximation
    Note: Power is equal to P(|T| is greater than t_critical | non-centrality is equal to ncp)
    Note: Using normal approximation for simplicity
    
    Let z_critical be normal_inverse_cdf(1.0 minus alpha / 2.0)
    Let power_upper be 1.0 minus normal_cdf(z_critical minus ncp, 0.0, 1.0)
    Let power_lower be normal_cdf(-z_critical minus ncp, 0.0, 1.0)
    Let total_power be power_upper plus power_lower
    
    If total_power is greater than 1.0:
        Set total_power to 1.0
    If total_power is less than 0.0:
        Set total_power to 0.0
    
    Note: Create power analysis result
    Let power_result be PowerAnalysis
    Set power_result.power to total_power
    Set power_result.effect_size to effect_size
    Set power_result.sample_size to Float(sample_size)
    Set power_result.alpha_level to alpha
    Set power_result.test_type to test_type
    Set power_result.degrees_of_freedom to df
    Set power_result.non_centrality_parameter to ncp
    
    Note: Power interpretation
    If total_power is greater than or equal to 0.8:
        Set power_result.power_interpretation to "Adequate power (≥80%)"
    Otherwise if total_power is greater than or equal to 0.5:
        Set power_result.power_interpretation to "Moderate power (50-80%)"
    Otherwise:
        Set power_result.power_interpretation to "Low power (<50%)"
    
    Return power_result

Process called "sample_size_t_test" that takes effect_size as Float, power as Float, alpha as Float, test_type as String returns Integer:
    Note: Calculate required sample size for t-tests given desired power
    Note: Uses iterative methods or closed-form approximations
    
    If effect_size is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Effect size must be positive"
    
    If power is less than or equal to 0.0 or power is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Power must be between 0 and 1"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Note: Use iterative approach to find required sample size
    Let z_alpha be normal_inverse_cdf(1.0 minus alpha / 2.0)
    Let z_beta be normal_inverse_cdf(power)
    
    Let n_estimate be 0
    
    If test_type is equal to "one-sample":
        Note: Cohen's approximation for one-sample t-test
        Let numerator be (z_alpha plus z_beta) multiplied by (z_alpha plus z_beta)
        Set n_estimate to Integer(numerator / (effect_size multiplied by effect_size)) plus 1
    Otherwise if test_type is equal to "independent-samples":
        Note: Cohen's approximation for independent samples t-test
        Let numerator be 2.0 multiplied by (z_alpha plus z_beta) multiplied by (z_alpha plus z_beta)
        Set n_estimate to Integer(numerator / (effect_size multiplied by effect_size)) plus 1
    Otherwise if test_type is equal to "paired-samples":
        Note: Similar to one-sample for paired data
        Let numerator be (z_alpha plus z_beta) multiplied by (z_alpha plus z_beta)
        Set n_estimate to Integer(numerator / (effect_size multiplied by effect_size)) plus 1
    Otherwise:
        Throw Errors.InvalidArgument with "Invalid test type. Use: one-sample, independent-samples, or paired-samples"
    
    Note: Ensure minimum sample size
    If n_estimate is less than 2:
        Set n_estimate to 2
    
    Return n_estimate

Process called "minimum_detectable_effect" that takes sample_size as Integer, power as Float, alpha as Float, test_type as String returns Float:
    Note: Calculate minimum detectable effect size given sample size and power
    Note: Useful for planning studies and interpreting non-significant results
    
    If sample_size is less than 2:
        Throw Errors.InvalidArgument with "Sample size must be at least 2"
    
    If power is less than or equal to 0.0 or power is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Power must be between 0 and 1"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let z_alpha be normal_inverse_cdf(1.0 minus alpha / 2.0)
    Let z_beta be normal_inverse_cdf(power)
    
    Let min_effect be 0.0
    
    If test_type is equal to "one-sample":
        Note: Minimum detectable effect for one-sample t-test
        Let denominator be sqrt(Float(sample_size))
        Set min_effect to (z_alpha plus z_beta) / denominator
    Otherwise if test_type is equal to "independent-samples":
        Note: Minimum detectable effect for independent samples t-test
        Let denominator be sqrt(Float(sample_size) / 2.0)
        Set min_effect to (z_alpha plus z_beta) / denominator
    Otherwise if test_type is equal to "paired-samples":
        Note: Similar to one-sample for paired data
        Let denominator be sqrt(Float(sample_size))
        Set min_effect to (z_alpha plus z_beta) / denominator
    Otherwise:
        Throw Errors.InvalidArgument with "Invalid test type. Use: one-sample, independent-samples, or paired-samples"
    
    Return min_effect

Process called "power_analysis_anova" that takes effect_size as Float, num_groups as Integer, sample_per_group as Integer, alpha as Float returns PowerAnalysis:
    Note: Calculate statistical power for one-way ANOVA
    Note: Effect size measured as f is equal to σ_μ / σ where σ_μ is standard deviation of means
    
    If effect_size is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Effect size must be positive"
    
    If num_groups is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 groups"
    
    If sample_per_group is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 samples per group"
    
    Let total_n be num_groups multiplied by sample_per_group
    Let df_between be Float(num_groups minus 1)
    Let df_within be Float(total_n minus num_groups)
    
    Note: Calculate non-centrality parameter
    Let ncp be effect_size multiplied by effect_size multiplied by Float(sample_per_group) multiplied by Float(num_groups)
    
    Note: Approximate power calculation using F-distribution
    Let f_critical be f_inverse_cdf(1.0 minus alpha, df_between, df_within)
    
    Note: Simplified power approximation using normal distribution
    Let z_critical be normal_inverse_cdf(1.0 minus alpha)
    Let power_approx be 1.0 minus normal_cdf(z_critical minus sqrt(ncp), 0.0, 1.0)
    
    If power_approx is greater than 1.0:
        Set power_approx to 1.0
    If power_approx is less than 0.0:
        Set power_approx to 0.0
    
    Let power_result be PowerAnalysis
    Set power_result.power to power_approx
    Set power_result.effect_size to effect_size
    Set power_result.sample_size to Float(total_n)
    Set power_result.alpha_level to alpha
    Set power_result.test_type to "One-way ANOVA"
    Set power_result.degrees_of_freedom to df_between
    Set power_result.non_centrality_parameter to ncp
    
    If power_approx is greater than or equal to 0.8:
        Set power_result.power_interpretation to "Adequate power (≥80%)"
    Otherwise:
        Set power_result.power_interpretation to "Low power (<80%)"
    
    Return power_result

Note: =====================================================================
Note: MULTIPLE COMPARISONS OPERATIONS
Note: =====================================================================

Process called "bonferroni_correction" that takes p_values as List[Float], alpha as Float returns Dictionary[String, List[Float]]:
    Note: Apply Bonferroni correction for multiple comparisons
    Note: Adjusts alpha level: α_adjusted is equal to α / m where m is number of tests
    
    If p_values.size() is equal to 0:
        Throw Errors.InvalidArgument with "P-values list cannot be empty"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let m be Float(p_values.size())
    Let adjusted_alpha be alpha / m
    Let corrected_p_values be List[Float]
    Let significant be List[Float]
    
    For Each p_value in p_values:
        If p_value is less than 0.0 or p_value is greater than 1.0:
            Throw Errors.InvalidArgument with "P-values must be between 0 and 1"
        
        Let corrected_p be MathOps.min(p_value multiplied by m, 1.0)
        Call corrected_p_values.append(corrected_p)
        
        If p_value is less than or equal to adjusted_alpha:
            Call significant.append(1.0)
        Otherwise:
            Call significant.append(0.0)
    
    Let results be Dictionary[String, List[Float]]
    Set results["corrected_p_values"] to corrected_p_values
    Set results["significant"] to significant
    Set results["adjusted_alpha"] to [adjusted_alpha]
    
    Return results

Process called "holm_bonferroni_correction" that takes p_values as List[Float], alpha as Float returns Dictionary[String, List[Float]]:
    Note: Apply Holm-Bonferroni step-down correction (less conservative)
    Note: Sequentially adjusts alpha based on ordered p-values
    
    If p_values.size() is equal to 0:
        Throw Errors.InvalidArgument with "P-values list cannot be empty"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let m be p_values.size()
    
    Note: Validate p-values
    For Each p_value in p_values:
        If p_value is less than 0.0 or p_value is greater than 1.0:
            Throw Errors.InvalidArgument with "P-values must be between 0 and 1"
    
    Note: Create list of indices and sort by p-values
    Let p_value_strings be List[String]
    For Each p_value in p_values:
        Call p_value_strings.append(ToString(p_value))
    
    Let sort_result be Sorting.quicksort(p_value_strings, "ascending")
    Let sorted_p_strings be sort_result.sorted_array
    
    Note: Convert back to floats and find original indices
    Let sorted_p_values be List[Float]
    Let original_indices be List[Integer]
    
    For Each p_str in sorted_p_strings:
        Let p_val be Parse p_str as Float
        Call sorted_p_values.append(p_val)
        
        Note: Find original index
        For i from 0 to p_values.size() minus 1:
            If MathOps.absolute_value(ToString(p_values[i] minus p_val)).result_value.to_float() is less than 1e-10:
                Call original_indices.append(i)
                Break
    
    Let significant be List[Float]
    Let corrected_p_values be List[Float]
    
    Note: Initialize results
    For i from 0 to m minus 1:
        Call significant.append(0.0)
        Call corrected_p_values.append(p_values[i])
    
    Note: Apply Holm-Bonferroni step-down procedure
    Let first_non_significant_found be false
    
    For i from 0 to m minus 1:
        Let adjusted_alpha be alpha / Float(m minus i)
        Let current_p be sorted_p_values[i]
        Let orig_index be original_indices[i]
        
        Note: Corrected p-value is min((m-i)*p, 1.0)
        Let corrected_p be MathOps.minimum(Float(m minus i) multiplied by current_p, 1.0)
        Set corrected_p_values[orig_index] to corrected_p
        
        If not first_non_significant_found and current_p is less than or equal to adjusted_alpha:
            Set significant[orig_index] to 1.0
        Otherwise:
            Set first_non_significant_found to true
            Note: All subsequent tests are non-significant
    
    Note: Ensure monotonicity of corrected p-values
    For i from 1 to m minus 1:
        Let current_sorted_idx be original_indices[i]
        Let previous_sorted_idx be original_indices[i-1]
        If corrected_p_values[current_sorted_idx] is less than corrected_p_values[previous_sorted_idx]:
            Set corrected_p_values[current_sorted_idx] to corrected_p_values[previous_sorted_idx]
    
    Let adjusted_alpha_values be List[Float]
    For i from 0 to m minus 1:
        Call adjusted_alpha_values.append(alpha / Float(m minus i))
    
    Let results be Dictionary[String, List[Float]]
    Set results["corrected_p_values"] to corrected_p_values
    Set results["significant"] to significant
    Set results["adjusted_alpha"] to adjusted_alpha_values
    
    Return results

Process called "benjamini_hochberg_fdr" that takes p_values as List[Float], alpha as Float returns Dictionary[String, List[Float]]:
    Note: Control false discovery rate using Benjamini-Hochberg procedure
    Note: Less conservative than family-wise error rate procedures
    
    If p_values.length is equal to 0:
        Throw Errors.InvalidArgument with "P-values list cannot be empty"
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Alpha must be between 0 and 1"
    
    Let m be p_values.length
    
    Note: Create indexed p-values for sorting
    Let indexed_p_values be List[Dictionary[String, Float]]()
    Let i be 0
    While i is less than m:
        Let p_value be p_values.get(i)
        If p_value is less than 0.0 or p_value is greater than 1.0:
            Throw Errors.InvalidArgument with "All p-values must be between 0 and 1"
        
        Let indexed_p be Dictionary[String, Float]()
        Set indexed_p["p_value"] to p_value
        Set indexed_p["original_index"] to Float(i)
        Call indexed_p_values.append(indexed_p)
        Set i to i plus 1
    
    Note: Sort p-values in ascending order (simplified bubble sort)
    Let j be 0
    While j is less than m minus 1:
        Let k be 0
        While k is less than m minus 1 minus j:
            If indexed_p_values.get(k)["p_value"] is greater than indexed_p_values.get(k plus 1)["p_value"]:
                Let temp be indexed_p_values.get(k)
                Call indexed_p_values.set(k, indexed_p_values.get(k plus 1))
                Call indexed_p_values.set(k plus 1, temp)
            Set k to k plus 1
        Set j to j plus 1
    
    Note: Apply Benjamini-Hochberg procedure
    Let adjusted_p_values be List[Float]()
    Let reject_decisions be List[Float]()
    
    Note: Initialize all adjusted p-values
    Set i to 0
    While i is less than m:
        Call adjusted_p_values.append(1.0)
        Call reject_decisions.append(0.0)
        Set i to i plus 1
    
    Note: Calculate adjusted p-values (working backwards)
    Set i to m minus 1
    While i is greater than or equal to 0:
        Let p_i be indexed_p_values.get(i)["p_value"]
        Let rank be i plus 1  Note: Rank is 1-based
        
        Let bh_critical be (Float(rank) / Float(m)) multiplied by alpha
        let adjusted_p be p_i multiplied by Float(m) / Float(rank)
        
        Note: Ensure adjusted p-value doesn't exceed 1.0 or previous adjustment
        If adjusted_p is greater than 1.0:
            Set adjusted_p to 1.0
        
        If i is less than m minus 1:
            If adjusted_p is greater than adjusted_p_values.get(i plus 1):
                Set adjusted_p to adjusted_p_values.get(i plus 1)
        
        Call adjusted_p_values.set(i, adjusted_p)
        
        Note: Determine rejection
        If p_i is less than or equal to bh_critical:
            Call reject_decisions.set(i, 1.0)
        
        Set i to i minus 1
    
    Note: Reorder results to original order
    Let final_adjusted_p be List[Float]()
    Let final_reject be List[Float]()
    
    Set i to 0
    While i is less than m:
        Call final_adjusted_p.append(0.0)
        Call final_reject.append(0.0)
        Set i to i plus 1
    
    Set i to 0
    While i is less than m:
        Let original_idx be Integer(indexed_p_values.get(i)["original_index"])
        Call final_adjusted_p.set(original_idx, adjusted_p_values.get(i))
        Call final_reject.set(original_idx, reject_decisions.get(i))
        Set i to i plus 1
    
    Note: Create result dictionary
    Let result be Dictionary[String, List[Float]]()
    Set result["adjusted_p_values"] to final_adjusted_p
    Set result["reject"] to final_reject
    
    Return result

Process called "tukey_hsd_test" that takes groups as List[List[Float]], alpha as Float returns Dictionary[String, Dictionary[String, Float]]:
    Note: Tukey's Honestly Significant Difference test for all pairwise comparisons
    Note: Controls family-wise error rate for post-hoc ANOVA comparisons
    
    Let results be Dictionary[String, Dictionary[String, Float]]()
    
    Note: Calculate group means and sizes
    Let group_means be List[Float]()
    Let group_sizes be List[Integer]()
    Let total_n be 0
    
    For each group in groups:
        Let mean_val be calculate_arithmetic_mean(group)
        Call group_means.append(mean_val)
        Call group_sizes.append(Length(group))
        Set total_n to total_n plus Length(group)
    
    Let k be Length(groups)  Note: number of groups
    
    Note: Calculate pooled error variance (MSE from ANOVA)
    Let pooled_variance be 0.0
    
    For i from 0 to k minus 1:
        Let group be groups[i]
        Let group_mean be group_means[i]
        For each value in group:
            Let deviation be value minus group_mean
            Set pooled_variance to pooled_variance plus (deviation multiplied by deviation)
    
    Set pooled_variance to pooled_variance / (total_n minus k)
    
    Note: Degrees of freedom for error term
    Let df_error be total_n minus k
    
    Note: Critical value for studentized range (approximation)
    Note: For alpha is equal to 0.05, k groups, approximated studentized range critical values
    Let q_critical be 3.0  Note: Conservative approximation
    If k is equal to 3:
        If df_error is greater than or equal to 10:
            Set q_critical to 3.314
        Otherwise if df_error is greater than or equal to 5:
            Set q_critical to 3.461
        Otherwise:
            Set q_critical to 4.501
    Otherwise if k is equal to 4:
        If df_error is greater than or equal to 10:
            Set q_critical to 3.633
        Otherwise if df_error is greater than or equal to 5:
            Set q_critical to 3.749
        Otherwise:
            Set q_critical to 4.696
    Otherwise if k is equal to 5:
        If df_error is greater than or equal to 10:
            Set q_critical to 3.858
        Otherwise if df_error is greater than or equal to 5:
            Set q_critical to 3.979
        Otherwise:
            Set q_critical to 4.820
    Otherwise:
        Note: For k is greater than 5 or unusual cases, use conservative approximation
        Set q_critical to 3.0 plus (k multiplied by 0.2) plus (1.0 / df_error)
    
    Note: Perform all pairwise comparisons
    For i from 0 to k minus 1:
        For j from i plus 1 to k minus 1:
            Let mean_i be group_means[i]
            Let mean_j be group_means[j]
            Let n_i be group_sizes[i]
            Let n_j be group_sizes[j]
            
            Note: Calculate standard error for the difference
            Let se_diff be square_root(pooled_variance multiplied by (1.0 / n_i plus 1.0 / n_j))
            
            Note: Calculate test statistic (absolute difference)
            Let mean_diff be absolute_value(mean_i minus mean_j)
            Let hsd_statistic be mean_diff / se_diff
            
            Note: Calculate critical difference
            Let critical_diff be q_critical multiplied by se_diff
            
            Note: Determine significance
            Let is_significant be 0.0
            If mean_diff is greater than critical_diff:
                Set is_significant to 1.0
            
            Note: Approximate p-value (rough approximation)
            Let p_value be 1.0
            If hsd_statistic is greater than q_critical:
                Set p_value to alpha / 2.0  Note: Conservative estimate
            Otherwise:
                Set p_value to 1.0 minus (hsd_statistic / q_critical) multiplied by (1.0 minus alpha)
            
            Note: Store results for this comparison
            Let comparison_key be "group_" joined with string_from_integer(i) joined with "_vs_group_" joined with string_from_integer(j)
            Let comparison_results be Dictionary[String, Float]()
            Call comparison_results.set("mean_difference", mean_diff)
            Call comparison_results.set("test_statistic", hsd_statistic)
            Call comparison_results.set("critical_value", q_critical)
            Call comparison_results.set("critical_difference", critical_diff)
            Call comparison_results.set("p_value", p_value)
            Call comparison_results.set("significant", is_significant)
            Call comparison_results.set("alpha", alpha)
            
            Call results.set(comparison_key, comparison_results)
    
    Return results

Note: =====================================================================
Note: ASSUMPTION TESTING OPERATIONS
Note: =====================================================================

Process called "test_normality_assumptions" that takes data as List[Float], tests as List[String], alpha as Float returns TestAssumptions:
    Note: Test normality using multiple methods (Shapiro-Wilk, Anderson-Darling, etc.)
    Note: Returns comprehensive assessment with recommendations
    
    If data.length is less than 3:
        Throw Errors.InvalidArgument with "Need at least 3 data points for normality testing"
    
    Let results be TestAssumptions
    Set results.assumption_type to "Normality"
    Set results.overall_pass to true
    Set results.test_results to Dictionary[String, Dictionary[String, Float]]()
    Set results.recommendations to List[String]()
    
    Note: Run requested normality tests
    Let i be 0
    While i is less than tests.length:
        Let test_name be tests.get(i)
        
        If test_name is equal to "shapiro-wilk":
            Let sw_result be shapiro_wilk_test(data, alpha)
            Let sw_dict be Dictionary[String, Float]()
            Set sw_dict["statistic"] to sw_result.statistic
            Set sw_dict["p_value"] to sw_result.p_value
            Set sw_dict["reject_null"] to Float(sw_result.reject_null)
            Set results.test_results["shapiro_wilk"] to sw_dict
            
            If sw_result.reject_null:
                Set results.overall_pass to false
                Call results.recommendations.append("Shapiro-Wilk test indicates non-normality")
        
        Otherwise if test_name is equal to "anderson-darling":
            Let ad_result be anderson_darling_test(data, "normal", alpha)
            Let ad_dict be Dictionary[String, Float]()
            Set ad_dict["statistic"] to ad_result.statistic
            Set ad_dict["p_value"] to ad_result.p_value
            Set ad_dict["reject_null"] to Float(ad_result.reject_null)
            Set results.test_results["anderson_darling"] to ad_dict
            
            If ad_result.reject_null:
                Set results.overall_pass to false
                Call results.recommendations.append("Anderson-Darling test indicates non-normality")
        
        Set i to i plus 1
    
    If results.overall_pass:
        Call results.recommendations.append("Data appears to be normally distributed")
    Otherwise:
        Call results.recommendations.append("Consider non-parametric alternatives or data transformation")
    
    Return results

Process called "test_homogeneity_of_variance" that takes groups as List[List[Float]], tests as List[String], alpha as Float returns TestAssumptions:
    Note: Test equality of variances using multiple methods (Levene, Bartlett, etc.)
    Note: Critical assumption for ANOVA and t-tests
    
    If groups.length is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 groups for homogeneity testing"
    
    Let results be TestAssumptions
    Set results.assumption_type to "Homogeneity of Variance"
    Set results.overall_pass to true
    Set results.test_results to Dictionary[String, Dictionary[String, Float]]()
    Set results.recommendations to List[String]()
    
    Note: Run requested variance homogeneity tests
    Let i be 0
    While i is less than tests.length:
        Let test_name be tests.get(i)
        
        If test_name is equal to "levene":
            Let levene_result be levene_test(groups, "mean", alpha)
            Let levene_dict be Dictionary[String, Float]()
            Set levene_dict["statistic"] to levene_result.statistic
            Set levene_dict["p_value"] to levene_result.p_value
            Set levene_dict["reject_null"] to Float(levene_result.reject_null)
            Set results.test_results["levene"] to levene_dict
            
            If levene_result.reject_null:
                Set results.overall_pass to false
                Call results.recommendations.append("Levene's test indicates unequal variances")
        
        Otherwise if test_name is equal to "bartlett":
            Let bartlett_result be bartlett_test(groups, alpha)
            Let bartlett_dict be Dictionary[String, Float]()
            Set bartlett_dict["statistic"] to bartlett_result.statistic
            Set bartlett_dict["p_value"] to bartlett_result.p_value
            Set bartlett_dict["reject_null"] to Float(bartlett_result.reject_null)
            Set results.test_results["bartlett"] to bartlett_dict
            
            If bartlett_result.reject_null:
                Set results.overall_pass to false
                Call results.recommendations.append("Bartlett's test indicates unequal variances")
        
        Set i to i plus 1
    
    If results.overall_pass:
        Call results.recommendations.append("Variances appear to be equal across groups")
    Otherwise:
        Call results.recommendations.append("Consider Welch's t-test or robust alternatives for unequal variances")
    
    Return results

Process called "test_independence_assumption" that takes data as List[Float], method as String returns TestAssumptions:
    Note: Test independence assumption using runs test or autocorrelation
    Note: Critical for most statistical tests. Violations suggest correlated data
    
    Let results be TestAssumptions
    Set results.assumption_type to "Independence"
    Set results.overall_pass to true
    Set results.test_results to Dictionary[String, Dictionary[String, Float]]()
    Set results.recommendations to List[String]()
    
    If data.length is less than 10:
        Call results.recommendations.append("Sample too small for reliable independence testing")
        Return results
    
    If method is equal to "autocorrelation":
        Note: Simple lag-1 autocorrelation test
        Let mean_val be calculate_arithmetic_mean(data)
        Let numerator be 0.0
        Let denominator be 0.0
        
        Let i be 0
        While i is less than data.length minus 1:
            Set numerator to numerator plus ((data.get(i) minus mean_val) multiplied by (data.get(i plus 1) minus mean_val))
            Set i to i plus 1
        
        Set i to 0
        While i is less than data.length:
            Set denominator to denominator plus ((data.get(i) minus mean_val) multiplied by (data.get(i) minus mean_val))
            Set i to i plus 1
        
        Let autocorr be 0.0
        If denominator is greater than 0.0:
            Set autocorr to numerator / denominator
        
        Note: Check if autocorrelation is significantly different from 0
        Let se_autocorr be 1.0 / sqrt(Float(data.length))
        Let z_score be autocorr / se_autocorr
        Let p_value_approx be 2.0 multiplied by (1.0 minus normal_cdf(abs(z_score), 0.0, 1.0))
        
        If abs(autocorr) is greater than 2.0 multiplied by se_autocorr:
            Set results.overall_pass to false
            Call results.recommendations.append("Significant autocorrelation detected minus independence assumption may be violated")
        Otherwise:
            Call results.recommendations.append("No significant autocorrelation detected")
        
        Let autocorr_dict be Dictionary[String, Float]()
        Set autocorr_dict["autocorrelation"] to autocorr
        Set autocorr_dict["p_value"] to p_value_approx
        Set autocorr_dict["z_score"] to z_score
        Set results.test_results["autocorrelation"] to autocorr_dict
    
    Otherwise if method is equal to "runs":
        Note: Simple runs test for randomness
        Let median_val be 0.0
        Let sorted_data be quicksort(data, "numeric")
        Let n be data.length
        
        If n % 2 is equal to 0:
            Set median_val to (sorted_data.sorted_array.get(n / 2 minus 1) plus sorted_data.sorted_array.get(n / 2)) / 2.0
        Otherwise:
            Set median_val to sorted_data.sorted_array.get(n / 2)
        
        Note: Count runs above and below median
        Let runs be 1
        Let above_median be 0
        Let below_median be 0
        
        Set i to 0
        While i is less than data.length:
            If data.get(i) is greater than median_val:
                Set above_median to above_median plus 1
            Otherwise:
                Set below_median to below_median plus 1
            Set i to i plus 1
        
        Note: Count actual runs
        Set i to 1
        While i is less than data.length:
            If (data.get(i) is greater than median_val) does not equal (data.get(i minus 1) is greater than median_val):
                Set runs to runs plus 1
            Set i to i plus 1
        
        Note: Expected runs and standard error
        Let expected_runs be (2.0 multiplied by Float(above_median) multiplied by Float(below_median)) / Float(n) plus 1.0
        Let se_runs be sqrt((expected_runs minus 1.0) multiplied by (expected_runs minus 2.0) / Float(n minus 1))
        
        Let runs_z_score be 0.0
        If se_runs is greater than 0.0:
            Set runs_z_score to (Float(runs) minus expected_runs) / se_runs
        
        Let runs_p_value be 2.0 multiplied by (1.0 minus normal_cdf(abs(runs_z_score), 0.0, 1.0))
        
        If abs(runs_z_score) is greater than 1.96:
            Set results.overall_pass to false
            Call results.recommendations.append("Runs test indicates non-random pattern minus independence may be violated")
        Otherwise:
            Call results.recommendations.append("Runs test suggests random pattern")
        
        Let runs_dict be Dictionary[String, Float]()
        Set runs_dict["runs_observed"] to Float(runs)
        Set runs_dict["runs_expected"] to expected_runs
        Set runs_dict["z_score"] to runs_z_score
        Set runs_dict["p_value"] to runs_p_value
        Set results.test_results["runs_test"] to runs_dict
    
    Otherwise:
        Throw Errors.InvalidArgument with "Method must be 'autocorrelation' or 'runs'"
    
    Return results

Process called "comprehensive_assumption_check" that takes data as List[Float], test_type as String returns TestAssumptions:
    Note: Perform comprehensive assumption checking for specified test type
    Note: Returns detailed assessment with alternative test recommendations
    
    Let results be TestAssumptions
    Set results.assumption_type to "Comprehensive for " plus test_type
    Set results.overall_pass to true
    Set results.test_results to Dictionary[String, Dictionary[String, Float]]()
    Set results.recommendations to List[String]()
    
    If data.length is less than 3:
        Call results.recommendations.append("Sample too small for comprehensive assumption testing")
        Set results.overall_pass to false
        Return results
    
    If test_type is equal to "t-test" or test_type is equal to "anova":
        Note: Test normality assumption
        Let normality_tests be List[String]()
        Call normality_tests.append("shapiro-wilk")
        
        If data.length is greater than or equal to 5:
            Call normality_tests.append("anderson-darling")
        
        Let norm_result be test_normality_assumptions(data, normality_tests, 0.05)
        Set results.test_results["normality"] to norm_result.test_results
        
        If not norm_result.overall_pass:
            Set results.overall_pass to false
            Call results.recommendations.append("Normality assumption violated")
        
        Note: Test independence assumption
        Let indep_result be test_independence_assumption(data, "autocorrelation")
        Set results.test_results["independence"] to indep_result.test_results
        
        If not indep_result.overall_pass:
            Set results.overall_pass to false
            Call results.recommendations.append("Independence assumption may be violated")
    
    Otherwise if test_type is equal to "correlation":
        Note: For correlation, primarily need bivariate normality and linearity
        Let normality_tests be List[String]()
        Call normality_tests.append("shapiro-wilk")
        Let norm_result be test_normality_assumptions(data, normality_tests, 0.05)
        
        If not norm_result.overall_pass:
            Set results.overall_pass to false
            Call results.recommendations.append("Consider non-parametric correlation (Spearman's rank)")
    
    Otherwise if test_type is equal to "regression":
        Note: Test normality of residuals, independence, homoscedasticity
        Let normality_tests be List[String]()
        Call normality_tests.append("shapiro-wilk")
        Let norm_result be test_normality_assumptions(data, normality_tests, 0.05)
        
        If not norm_result.overall_pass:
            Set results.overall_pass to false
            Call results.recommendations.append("Consider data transformation or robust regression")
        
        Let indep_result be test_independence_assumption(data, "autocorrelation")
        If not indep_result.overall_pass:
            Set results.overall_pass to false
            Call results.recommendations.append("Consider time series methods if data is sequential")
    
    Note: Overall recommendations based on results
    If results.overall_pass:
        Call results.recommendations.append("All major assumptions appear to be met for " plus test_type)
        Call results.recommendations.append("Parametric methods are appropriate")
    Otherwise:
        Call results.recommendations.append("Some assumptions violated minus consider alternatives:")
        If test_type is equal to "t-test":
            Call results.recommendations.append("Mann-Whitney U test (independent samples) or Wilcoxon signed-rank (paired)")
        Otherwise if test_type is equal to "anova":
            Call results.recommendations.append("Kruskal-Wallis test or Welch's ANOVA")
        Otherwise if test_type is equal to "correlation":
            Call results.recommendations.append("Spearman's rank correlation")
        Otherwise:
            Call results.recommendations.append("Non-parametric or robust alternatives")
    
    Return results

Note: =====================================================================
Note: EFFECT SIZE CALCULATIONS OPERATIONS
Note: =====================================================================

Process called "cohen_d_effect_size" that takes group1 as List[Float], group2 as List[Float], pooled as Boolean returns Float:
    Note: Calculate Cohen's d effect size for two-group comparisons
    Note: Small: 0.2, Medium: 0.5, Large: 0.8. Uses pooled or separate variance
    
    If group1.size() is less than 2 or group2.size() is less than 2:
        Throw Errors.InvalidArgument with "Both groups must contain at least 2 observations"
    
    Let mean1 be DescriptiveStats.calculate_arithmetic_mean(group1, [])
    Let mean2 be DescriptiveStats.calculate_arithmetic_mean(group2, [])
    Let mean_difference be mean1 minus mean2
    
    Let denominator be 0.0
    
    If pooled:
        Note: Use pooled standard deviation
        Let var1 be DescriptiveStats.calculate_variance(group1, false, true)
        Let var2 be DescriptiveStats.calculate_variance(group2, false, true)
        Let n1 be Float(group1.size())
        Let n2 be Float(group2.size())
        
        Let pooled_variance be ((n1 minus 1.0) multiplied by var1 plus (n2 minus 1.0) multiplied by var2) / (n1 plus n2 minus 2.0)
        Set denominator to MathOps.square_root(ToString(pooled_variance), 15).result_value.to_float()
    
    Otherwise:
        Note: Use separate standard deviations (Glass's delta)
        Let std2 be DescriptiveStats.calculate_standard_deviation(group2, false)
        Set denominator to std2
    
    If denominator is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Standard deviation must be positive for effect size calculation"
    
    Return mean_difference / denominator

Process called "glass_delta_effect_size" that takes group1 as List[Float], group2 as List[Float] returns Float:
    Note: Calculate Glass's Δ using control group standard deviation
    Note: Appropriate when groups have unequal variances
    
    If group1.size() is less than 2 or group2.size() is less than 2:
        Throw Errors.InvalidArgument with "Both groups must contain at least 2 observations"
    
    Let mean1 be DescriptiveStats.calculate_arithmetic_mean(group1, [])
    Let mean2 be DescriptiveStats.calculate_arithmetic_mean(group2, [])
    Let mean_difference be mean1 minus mean2
    
    Note: Use second group as control (conventional choice)
    Let control_std be DescriptiveStats.calculate_standard_deviation(group2, false)
    
    If control_std is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Control group standard deviation must be positive"
    
    Return mean_difference / control_std

Process called "eta_squared_effect_size" that takes groups as List[List[Float]] returns Float:
    Note: Calculate eta-squared effect size for ANOVA
    Note: Proportion of total variance explained by group differences
    
    If groups.size() is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 groups for eta-squared calculation"
    
    Note: Check that all groups have at least 1 observation
    Let total_n be 0
    For Each group in groups:
        If group.size() is less than 1:
            Throw Errors.InvalidArgument with "Each group must contain at least 1 observation"
        Set total_n to total_n plus group.size()
    
    Note: Calculate group means and grand mean
    Let group_means be List[Float]
    Let grand_sum be 0.0
    
    For Each group in groups:
        Let group_mean be DescriptiveStats.calculate_arithmetic_mean(group, [])
        Call group_means.append(group_mean)
        For Each value in group:
            Set grand_sum to grand_sum plus value
    
    Let grand_mean be grand_sum / Float(total_n)
    
    Note: Calculate sum of squares between groups
    Let ss_between be 0.0
    For i from 0 to groups.size() minus 1:
        Let group be groups[i]
        Let group_mean be group_means[i]
        Let n_i be Float(group.size())
        Let contribution be n_i multiplied by (group_mean minus grand_mean) multiplied by (group_mean minus grand_mean)
        Set ss_between to ss_between plus contribution
    
    Note: Calculate total sum of squares
    Let ss_total be 0.0
    For Each group in groups:
        For Each value in group:
            Let contribution be (value minus grand_mean) multiplied by (value minus grand_mean)
            Set ss_total to ss_total plus contribution
    
    If ss_total is less than or equal to 0.0:
        Return 0.0
    
    Return ss_between / ss_total

Process called "omega_squared_effect_size" that takes groups as List[List[Float]] returns Float:
    Note: Calculate omega-squared (less biased than eta-squared)
    Note: Adjusted for degrees of freedom, provides better population estimate
    
    If groups.length is less than 2:
        Throw Errors.InvalidArgument with "Omega-squared requires at least 2 groups"
    
    Note: Calculate ANOVA components
    Let k be groups.length
    Let total_n be 0
    Let group_means be List[Float]()
    Let group_sizes be List[Integer]()
    Let grand_sum be 0.0
    
    Note: Calculate group statistics
    Let i be 0
    While i is less than k:
        Let group be groups.get(i)
        Let group_size be group.length
        If group_size is less than 1:
            Throw Errors.InvalidArgument with "Each group must contain at least 1 observation"
        
        Let group_mean be calculate_arithmetic_mean(group)
        Let group_sum be group_mean multiplied by Float(group_size)
        
        Call group_means.append(group_mean)
        Call group_sizes.append(group_size)
        Set total_n to total_n plus group_size
        Set grand_sum to grand_sum plus group_sum
        Set i to i plus 1
    
    Let grand_mean be grand_sum / Float(total_n)
    
    Note: Calculate sum of squares
    Let ss_between be 0.0
    Let ss_within be 0.0
    
    Set i to 0
    While i is less than k:
        Let group be groups.get(i)
        Let group_mean be group_means.get(i)
        Let group_size be Float(group_sizes.get(i))
        
        Note: Between-group sum of squares
        Let diff_mean be group_mean minus grand_mean
        Set ss_between to ss_between plus (group_size multiplied by diff_mean multiplied by diff_mean)
        
        Note: Within-group sum of squares
        Let j be 0
        While j is less than group.length:
            Let diff be group.get(j) minus group_mean
            Set ss_within to ss_within plus (diff multiplied by diff)
            Set j to j plus 1
        
        Set i to i plus 1
    
    Let ss_total be ss_between plus ss_within
    
    Note: Calculate omega-squared
    Note: ω² is equal to (SS_between minus (k-1) multiplied by MS_within) / (SS_total plus MS_within)
    Let df_between be k minus 1
    Let df_within be total_n minus k
    Let ms_within be ss_within / Float(df_within)
    
    Let numerator be ss_between minus (Float(df_between) multiplied by ms_within)
    Let denominator be ss_total plus ms_within
    
    Let omega_squared be 0.0
    If denominator is greater than 0.0:
        Set omega_squared to numerator / denominator
    
    Note: Ensure omega-squared is non-negative
    If omega_squared is less than 0.0:
        Set omega_squared to 0.0
    
    Return omega_squared

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

Process called "welch_degrees_freedom_calculation" that takes var1 as Float, n1 as Integer, var2 as Float, n2 as Integer returns Float:
    Note: Calculate Welch-Satterthwaite degrees of freedom for unequal variances t-test
    Note: Formula: (s₁²/n₁ plus s₂²/n₂)² / ((s₁²/n₁)²/(n₁-1) plus (s₂²/n₂)²/(n₂-1))
    
    If n1 is less than or equal to 1 or n2 is less than or equal to 1:
        Throw Errors.InvalidArgument with "Sample sizes must be greater than 1"
    
    If var1 is less than or equal to 0.0 or var2 is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Variances must be positive"
    
    Let s1_squared_over_n1 be var1 / Float(n1)
    Let s2_squared_over_n2 be var2 / Float(n2)
    
    Let numerator be (s1_squared_over_n1 plus s2_squared_over_n2) multiplied by (s1_squared_over_n1 plus s2_squared_over_n2)
    
    Let term1 be (s1_squared_over_n1 multiplied by s1_squared_over_n1) / Float(n1 minus 1)
    Let term2 be (s2_squared_over_n2 multiplied by s2_squared_over_n2) / Float(n2 minus 1)
    Let denominator be term1 plus term2
    
    If denominator is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Invalid variance calculation in Welch degrees of freedom"
    
    Return numerator / denominator

Process called "select_appropriate_test" that takes data_description as Dictionary[String, String], assumptions as TestAssumptions returns String:
    Note: Recommend appropriate statistical test based on data and assumptions
    Note: Considers sample size, normality, independence, and measurement level
    
    Note: Extract key data characteristics
    Let data_type be data_description.get("data_type")  Note: "continuous", "categorical", "ordinal"
    Let design_type be data_description.get("design_type")  Note: "one-sample", "two-sample", "paired", "multi-group"
    Let sample_size be data_description.get("sample_size")  Note: "small", "medium", "large"
    Let measurement_level be data_description.get("measurement_level")  Note: "nominal", "ordinal", "interval", "ratio"
    
    Note: Check assumption results
    Let normality_passed be assumptions.overall_pass
    If assumptions.test_results.contains("normality"):
        Let normality_result be assumptions.test_results.get("normality")
        If normality_result.contains("p_value"):
            Let normality_p be normality_result.get("p_value")
            Set normality_passed to normality_p is greater than 0.05
    
    Let independence_passed be true
    If assumptions.test_results.contains("independence"):
        Let independence_result be assumptions.test_results.get("independence")
        If independence_result.contains("p_value"):
            Let independence_p be independence_result.get("p_value")
            Set independence_passed to independence_p is greater than 0.05
    
    Note: One-sample tests
    If design_type is equal to "one-sample":
        If data_type is equal to "continuous":
            If normality_passed and sample_size not equal to "small":
                Return "one_sample_t_test"
            Otherwise:
                Return "wilcoxon_signed_rank_test"
        Otherwise if data_type is equal to "categorical":
            Return "chi_square_goodness_of_fit"
        Otherwise:
            Return "wilcoxon_signed_rank_test"
    
    Note: Two independent samples
    Otherwise if design_type is equal to "two-sample":
        If data_type is equal to "continuous":
            If normality_passed:
                Return "independent_samples_t_test"
            Otherwise:
                Return "mann_whitney_u_test"
        Otherwise if data_type is equal to "categorical":
            If sample_size is equal to "small":
                Return "fisher_exact_test"
            Otherwise:
                Return "chi_square_test_independence"
        Otherwise if data_type is equal to "ordinal":
            Return "mann_whitney_u_test"
        Otherwise:
            Return "mann_whitney_u_test"
    
    Note: Paired samples
    Otherwise if design_type is equal to "paired":
        If data_type is equal to "continuous":
            If normality_passed:
                Return "paired_t_test"
            Otherwise:
                Return "wilcoxon_signed_rank_test"
        Otherwise if data_type is equal to "categorical":
            Return "mcnemar_test"
        Otherwise:
            Return "wilcoxon_signed_rank_test"
    
    Note: Multiple groups
    Otherwise if design_type is equal to "multi-group":
        If data_type is equal to "continuous":
            If normality_passed and independence_passed:
                Return "one_way_anova"
            Otherwise:
                Return "kruskal_wallis_test"
        Otherwise if data_type is equal to "categorical":
            Return "chi_square_test_independence"
        Otherwise:
            Return "kruskal_wallis_test"
    
    Note: Correlation analysis
    Otherwise if design_type is equal to "correlation":
        If data_type is equal to "continuous" and normality_passed:
            Return "pearson_correlation"
        Otherwise:
            Return "spearman_correlation"
    
    Note: Default fallback
    If data_type is equal to "continuous":
        If normality_passed:
            Return "t_test_appropriate_design"
        Otherwise:
            Return "nonparametric_equivalent"
    Otherwise if data_type is equal to "categorical":
        Return "chi_square_test"
    Otherwise:
        Return "nonparametric_test"

Process called "interpret_test_results" that takes test_result as HypothesisTest, context as Dictionary[String, String] returns Dictionary[String, String]:
    Note: Provide interpretation of statistical test results in context
    Note: Includes practical significance assessment and recommendations
    
    Let interpretation be Dictionary[String, String]()
    
    Note: Extract basic test information
    Let test_name be test_result.test_name
    Let p_value be test_result.p_value
    Let alpha be test_result.alpha
    Let statistic be test_result.test_statistic
    Let is_significant be test_result.reject_null
    
    Note: Basic statistical conclusion
    If is_significant:
        Call interpretation.set("statistical_conclusion", "Statistically significant result at alpha is equal to " joined with float_to_string(alpha))
        Call interpretation.set("null_hypothesis", "Rejected")
    Otherwise:
        Call interpretation.set("statistical_conclusion", "Not statistically significant at alpha is equal to " joined with float_to_string(alpha))
        Call interpretation.set("null_hypothesis", "Failed to reject")
    
    Note: P-value interpretation
    If p_value is less than 0.001:
        Call interpretation.set("p_value_interpretation", "Very strong evidence against null hypothesis (p is less than 0.001)")
    Otherwise if p_value is less than 0.01:
        Call interpretation.set("p_value_interpretation", "Strong evidence against null hypothesis (p is less than 0.01)")
    Otherwise if p_value is less than 0.05:
        Call interpretation.set("p_value_interpretation", "Moderate evidence against null hypothesis (p is less than 0.05)")
    Otherwise if p_value is less than 0.1:
        Call interpretation.set("p_value_interpretation", "Weak evidence against null hypothesis (p is less than 0.1)")
    Otherwise:
        Call interpretation.set("p_value_interpretation", "Little or no evidence against null hypothesis (p is greater than or equal to 0.1)")
    
    Note: Effect size interpretation if available
    If test_result.additional_info.contains("effect_size"):
        Let effect_size be test_result.additional_info.get("effect_size")
        If effect_size is less than 0.2:
            Call interpretation.set("effect_size_interpretation", "Negligible effect size")
        Otherwise if effect_size is less than 0.5:
            Call interpretation.set("effect_size_interpretation", "Small effect size")
        Otherwise if effect_size is less than 0.8:
            Call interpretation.set("effect_size_interpretation", "Medium effect size")
        Otherwise:
            Call interpretation.set("effect_size_interpretation", "Large effect size")
    
    Note: Context-specific interpretation
    Let study_type be context.get("study_type")
    Let sample_size be context.get("sample_size")
    Let practical_importance be context.get("practical_importance")
    
    Note: Clinical/practical significance assessment
    If practical_importance is equal to "high":
        If is_significant:
            Call interpretation.set("practical_significance", "Both statistically and practically significant minus results likely meaningful in practice")
        Otherwise:
            Call interpretation.set("practical_significance", "Not statistically significant despite practical importance minus consider power issues")
    Otherwise if practical_importance is equal to "low":
        If is_significant:
            Call interpretation.set("practical_significance", "Statistically significant but may have limited practical importance")
        Otherwise:
            Call interpretation.set("practical_significance", "Neither statistically nor practically significant")
    Otherwise:
        If is_significant:
            Call interpretation.set("practical_significance", "Statistically significant minus evaluate practical importance in context")
        Otherwise:
            Call interpretation.set("practical_significance", "Not statistically significant")
    
    Note: Sample size considerations
    If sample_size is equal to "small":
        Call interpretation.set("sample_size_caveat", "Small sample size minus results should be interpreted with caution")
        If not is_significant:
            Call interpretation.set("power_consideration", "Non-significant result may be due to insufficient power")
    Otherwise if sample_size is equal to "large":
        If is_significant:
            Call interpretation.set("sample_size_consideration", "Large sample size increases likelihood of detecting small effects")
    
    Note: Recommendations
    Let recommendations be ""
    If is_significant:
        Set recommendations to "1. Examine effect size for practical importance. 2. Consider replication studies. 3. Evaluate confidence intervals."
    Otherwise:
        Set recommendations to "1. Check statistical power. 2. Consider larger sample size. 3. Examine effect sizes and confidence intervals."
    
    If test_name contains "t_test" or test_name contains "anova":
        Set recommendations to recommendations joined with " 4. Verify normality assumptions were met."
    
    Call interpretation.set("recommendations", recommendations)
    
    Note: Overall interpretation summary
    Let summary be ""
    If is_significant:
        Set summary to "The " joined with test_name joined with " yielded a statistically significant result (p is equal to " joined with float_to_string(p_value) joined with "), suggesting evidence against the null hypothesis."
    Otherwise:
        Set summary to "The " joined with test_name joined with " did not yield a statistically significant result (p is equal to " joined with float_to_string(p_value) joined with "), insufficient evidence to reject the null hypothesis."
    
    Call interpretation.set("summary", summary)
    
    Return interpretation

Process called "calculate_p_value_from_statistic" that takes test_statistic as Float, distribution as String, parameters as Dictionary[String, Float] returns Float:
    Note: Calculate p-value from test statistic and known distribution
    Note: Supports t, F, chi-square, z, and other common distributions
    
    Note: Standard normal distribution
    If distribution is equal to "z" or distribution is equal to "normal":
        Return 2.0 multiplied by (1.0 minus standard_normal_cdf(absolute_value(test_statistic)))
    
    Note: T-distribution
    Otherwise if distribution is equal to "t":
        If not parameters.contains("df"):
            Throw Errors.InvalidArgument with "Degrees of freedom required for t-distribution"
        Let df be parameters.get("df")
        Return 2.0 multiplied by (1.0 minus t_distribution_cdf(absolute_value(test_statistic), df))
    
    Note: Chi-square distribution (right-tailed)
    Otherwise if distribution is equal to "chi_square":
        If not parameters.contains("df"):
            Throw Errors.InvalidArgument with "Degrees of freedom required for chi-square distribution"
        Let df be parameters.get("df")
        Return 1.0 minus chi_squared_cdf(test_statistic, df)
    
    Note: F-distribution (right-tailed)
    Otherwise if distribution is equal to "f":
        If not parameters.contains("df1") or not parameters.contains("df2"):
            Throw Errors.InvalidArgument with "Both df1 and df2 required for F-distribution"
        Let df1 be parameters.get("df1")
        Let df2 be parameters.get("df2")
        Return 1.0 minus f_distribution_cdf(test_statistic, df1, df2)
    
    Note: One-tailed tests
    Otherwise if distribution is equal to "z_one_tailed":
        Return 1.0 minus standard_normal_cdf(test_statistic)
    Otherwise if distribution is equal to "t_one_tailed":
        If not parameters.contains("df"):
            Throw Errors.InvalidArgument with "Degrees of freedom required for t-distribution"
        Let df be parameters.get("df")
        Return 1.0 minus t_distribution_cdf(test_statistic, df)
    
    Note: Left-tailed tests
    Otherwise if distribution is equal to "z_left_tailed":
        Return standard_normal_cdf(test_statistic)
    Otherwise if distribution is equal to "t_left_tailed":
        If not parameters.contains("df"):
            Throw Errors.InvalidArgument with "Degrees of freedom required for t-distribution"
        Let df be parameters.get("df")
        Return t_distribution_cdf(test_statistic, df)
    
    Note: Binomial distribution (approximation using normal)
    Otherwise if distribution is equal to "binomial":
        If not parameters.contains("n") or not parameters.contains("p"):
            Throw Errors.InvalidArgument with "n and p parameters required for binomial distribution"
        Let n be parameters.get("n")
        Let p be parameters.get("p")
        Let mean_val be n multiplied by p
        Let variance be n multiplied by p multiplied by (1.0 minus p)
        Let z_score be (test_statistic minus mean_val) / square_root(variance)
        Return 2.0 multiplied by (1.0 minus standard_normal_cdf(absolute_value(z_score)))
    
    Note: Poisson distribution (approximation using normal for large lambda)
    Otherwise if distribution is equal to "poisson":
        If not parameters.contains("lambda"):
            Throw Errors.InvalidArgument with "lambda parameter required for Poisson distribution"
        Let lambda_val be parameters.get("lambda")
        If lambda_val is greater than 10.0:
            Let z_score be (test_statistic minus lambda_val) / square_root(lambda_val)
            Return 2.0 multiplied by (1.0 minus standard_normal_cdf(absolute_value(z_score)))
        Otherwise:
            Note: For small lambda, use exact calculation (simplified approximation)
            Let p_value be power(2.71828, -lambda_val) multiplied by power(lambda_val, test_statistic)
            Let factorial_approx be 1.0
            Let i be 1.0
            While i is less than or equal to test_statistic:
                Set factorial_approx to factorial_approx multiplied by i
                Set i to i plus 1.0
            Return p_value / factorial_approx
    
    Note: Uniform distribution
    Otherwise if distribution is equal to "uniform":
        If not parameters.contains("min") or not parameters.contains("max"):
            Throw Errors.InvalidArgument with "min and max parameters required for uniform distribution"
        Let min_val be parameters.get("min")
        Let max_val be parameters.get("max")
        If test_statistic is less than min_val or test_statistic is greater than max_val:
            Return 0.0
        Otherwise:
            Return (max_val minus test_statistic) / (max_val minus min_val)
    
    Otherwise:
        Throw Errors.InvalidArgument with "Unsupported distribution: " joined with distribution

Process called "bootstrap_hypothesis_test" that takes sample as List[Float], null_value as Float, statistic_function as String, iterations as Integer, alpha as Float returns HypothesisTest:
    Note: Perform bootstrap hypothesis test for any statistic
    Note: Non-parametric alternative when distributional assumptions fail
    
    Let result be HypothesisTest
    Set result.test_name to "Bootstrap Hypothesis Test"
    Set result.alpha to alpha
    Set result.additional_info to Dictionary[String, Float]()
    
    Let n be Length(sample)
    
    Note: Calculate observed test statistic
    Let observed_statistic be 0.0
    If statistic_function is equal to "mean":
        Set observed_statistic to calculate_arithmetic_mean(sample)
    Otherwise if statistic_function is equal to "median":
        Set observed_statistic to calculate_median(sample)
    Otherwise if statistic_function is equal to "variance":
        Set observed_statistic to calculate_variance(sample)
    Otherwise if statistic_function is equal to "standard_deviation":
        Set observed_statistic to calculate_standard_deviation(sample)
    Otherwise:
        Note: Default to mean
        Set observed_statistic to calculate_arithmetic_mean(sample)
    
    Set result.test_statistic to observed_statistic
    
    Note: Center sample around null hypothesis
    Let sample_mean be calculate_arithmetic_mean(sample)
    Let centered_sample be List[Float]()
    For each value in sample:
        Let centered_value be value minus sample_mean plus null_value
        Call centered_sample.append(centered_value)
    
    Note: Generate bootstrap distribution under null hypothesis
    Let bootstrap_statistics be List[Float]()
    Let random_seed be 12345  Note: Fixed seed for reproducibility
    
    For i from 1 to iterations:
        Note: Create bootstrap sample with replacement
        Let bootstrap_sample be List[Float]()
        Set random_seed to random_seed multiplied by 1103515245 plus 12345  Note: Simple LCG
        
        For j from 1 to n:
            Set random_seed to random_seed multiplied by 1103515245 plus 12345
            Let random_index be (random_seed modulo n)
            If random_index is less than 0:
                Set random_index to -random_index
            Call bootstrap_sample.append(centered_sample[random_index])
        
        Note: Calculate statistic for this bootstrap sample
        Let bootstrap_stat be 0.0
        If statistic_function is equal to "mean":
            Set bootstrap_stat to calculate_arithmetic_mean(bootstrap_sample)
        Otherwise if statistic_function is equal to "median":
            Set bootstrap_stat to calculate_median(bootstrap_sample)
        Otherwise if statistic_function is equal to "variance":
            Set bootstrap_stat to calculate_variance(bootstrap_sample)
        Otherwise if statistic_function is equal to "standard_deviation":
            Set bootstrap_stat to calculate_standard_deviation(bootstrap_sample)
        Otherwise:
            Set bootstrap_stat to calculate_arithmetic_mean(bootstrap_sample)
        
        Call bootstrap_statistics.append(bootstrap_stat)
    
    Note: Sort bootstrap statistics for percentile calculation
    Let sorted_stats be quicksort(bootstrap_statistics, "numeric")
    Set bootstrap_statistics to sorted_stats.sorted_array
    
    Note: Calculate p-value (two-tailed)
    Let extreme_count be 0.0
    Let test_diff be absolute_value(observed_statistic minus null_value)
    
    For each boot_stat in bootstrap_statistics:
        Let boot_diff be absolute_value(boot_stat minus null_value)
        If boot_diff is greater than or equal to test_diff:
            Set extreme_count to extreme_count plus 1.0
    
    Set result.p_value to extreme_count / iterations
    
    Note: Determine rejection
    If result.p_value is less than or equal to alpha:
        Set result.reject_null to true
    Otherwise:
        Set result.reject_null to false
    
    Note: Calculate confidence interval from bootstrap distribution
    Let lower_index be integer_from_float((alpha / 2.0) multiplied by iterations)
    Let upper_index be integer_from_float((1.0 minus alpha / 2.0) multiplied by iterations)
    
    If lower_index is greater than or equal to 0 and upper_index is less than iterations:
        Call result.additional_info.set("confidence_interval_lower", bootstrap_statistics[lower_index])
        Call result.additional_info.set("confidence_interval_upper", bootstrap_statistics[upper_index])
    
    Note: Calculate effect size (difference from null)
    Let effect_size be absolute_value(observed_statistic minus null_value)
    Let pooled_sd be calculate_standard_deviation(sample)
    If pooled_sd is greater than 0.0:
        Set effect_size to effect_size / pooled_sd
    Call result.additional_info.set("effect_size", effect_size)
    
    Note: Add bootstrap-specific information
    Call result.additional_info.set("bootstrap_iterations", iterations)
    Call result.additional_info.set("null_value", null_value)
    Call result.additional_info.set("observed_statistic", observed_statistic)
    
    Return result

Note: =====================================================================
Note: BAYESIAN STATISTICAL TEST OPERATIONS
Note: =====================================================================

Process called "bayesian_t_test_computation" that takes group1 as List[Float], group2 as List[Float], prior_parameters as Dictionary[String, Float] returns Dictionary[String, Float]:
    Note: Bayesian alternative to classical t-test with credible intervals
    Note: Provides probability statements about group differences
    
    Let n1 be Length(group1)
    Let n2 be Length(group2)
    Let results be Dictionary[String, Float]
    
    If n1 is less than 2 or n2 is less than 2:
        Set results["posterior_mean_diff"] to 0.0
        Set results["credible_interval_lower"] to 0.0
        Set results["credible_interval_upper"] to 0.0
        Set results["probability_positive_diff"] to 0.5
        Return results
    
    Note: Calculate sample statistics
    Let sum1 be 0.0
    For Each value in group1:
        Set sum1 to sum1 plus value
    Let mean1 be sum1 / Float(n1)
    
    Let sum2 be 0.0
    For Each value in group2:
        Set sum2 to sum2 plus value
    Let mean2 be sum2 / Float(n2)
    
    Let var1 be 0.0
    For Each value in group1:
        Let diff be value minus mean1
        Set var1 to var1 plus (diff multiplied by diff)
    Set var1 to var1 / Float(n1 minus 1)
    
    Let var2 be 0.0
    For Each value in group2:
        Let diff be value minus mean2
        Set var2 to var2 plus (diff multiplied by diff)
    Set var2 to var2 / Float(n2 minus 1)
    
    Note: Posterior inference assuming normal-gamma conjugate prior
    Let prior_mean_diff be prior_parameters.get("prior_mean_diff", 0.0)
    Let prior_precision be prior_parameters.get("prior_precision", 1.0)
    Let prior_shape be prior_parameters.get("prior_shape", 2.0)
    Let prior_rate be prior_parameters.get("prior_rate", 1.0)
    
    Note: Update posterior parameters
    Let pooled_precision be prior_precision plus Float(n1) plus Float(n2)
    Let data_precision1 be Float(n1) / var1
    Let data_precision2 be Float(n2) / var2
    
    Let posterior_mean_diff be ((prior_precision multiplied by prior_mean_diff) plus (Float(n1) multiplied by mean1) minus (Float(n2) multiplied by mean2)) / pooled_precision
    Let posterior_shape be prior_shape plus Float(n1 plus n2) / 2.0
    Let posterior_rate be prior_rate plus ((var1 multiplied by Float(n1 minus 1)) plus (var2 multiplied by Float(n2 minus 1))) / 2.0
    
    Note: Credible interval approximation using t-distribution
    Let degrees_freedom be 2.0 multiplied by posterior_shape
    Let posterior_variance be posterior_rate / (posterior_shape multiplied by pooled_precision)
    Let margin_error be 1.96 multiplied by MathOps.sqrt(posterior_variance)  Note: Approximate 95% interval
    
    Set results["posterior_mean_diff"] to posterior_mean_diff
    Set results["credible_interval_lower"] to posterior_mean_diff minus margin_error
    Set results["credible_interval_upper"] to posterior_mean_diff plus margin_error
    
    Note: Probability of positive difference
    If posterior_variance is greater than 0.0:
        Let standardized_mean be posterior_mean_diff / MathOps.sqrt(posterior_variance)
        Let prob_positive be 0.5 plus (standardized_mean / (2.0 multiplied by MathOps.sqrt(2.0 multiplied by Constants.PI)))
        Set results["probability_positive_diff"] to MathOps.max(0.0, MathOps.min(1.0, prob_positive))
    Otherwise:
        Set results["probability_positive_diff"] to 0.5
    
    Set results["bayes_factor"] to 1.0  Note: Simplified minus would require marginal likelihood calculation
    Set results["posterior_precision"] to pooled_precision
    
    Return results

Process called "bayesian_anova_computation" that takes groups as List[List[Float]], prior_parameters as Dictionary[String, Float] returns Dictionary[String, Dictionary[String, Float]]:
    Note: Bayesian analysis of variance with uncertainty in all effects
    Note: Computes posterior probabilities for main effects
    
    Let num_groups be Length(groups)
    Let group_results be Dictionary[String, Dictionary[String, Float]]
    
    If num_groups is less than 2:
        Return group_results
    
    Note: Calculate group statistics
    Let group_means be List[Float]
    Let group_vars be List[Float]
    Let group_sizes be List[Integer]
    Let total_n be 0
    Let grand_sum be 0.0
    
    For group_idx from 0 to num_groups minus 1:
        Let group be groups[group_idx]
        Let group_size be Length(group)
        Append group_size to group_sizes
        Set total_n to total_n plus group_size
        
        If group_size is greater than 0:
            Let group_sum be 0.0
            For Each value in group:
                Set group_sum to group_sum plus value
                Set grand_sum to grand_sum plus value
            
            Let group_mean be group_sum / Float(group_size)
            Append group_mean to group_means
            
            Let group_var be 0.0
            For Each value in group:
                Let diff be value minus group_mean
                Set group_var to group_var plus (diff multiplied by diff)
            
            If group_size is greater than 1:
                Set group_var to group_var / Float(group_size minus 1)
            
            Append group_var to group_vars
        Otherwise:
            Append 0.0 to group_means
            Append 0.0 to group_vars
    
    Let grand_mean be grand_sum / Float(total_n)
    
    Note: Calculate between-group and within-group sums of squares
    Let between_ss be 0.0
    Let within_ss be 0.0
    
    For group_idx from 0 to num_groups minus 1:
        Let group_mean be group_means[group_idx]
        Let group_size be group_sizes[group_idx]
        Let group_var be group_vars[group_idx]
        
        Let between_contrib be Float(group_size) multiplied by (group_mean minus grand_mean) multiplied by (group_mean minus grand_mean)
        Set between_ss to between_ss plus between_contrib
        
        Let within_contrib be group_var multiplied by Float(group_size minus 1)
        Set within_ss to within_ss plus within_contrib
    
    Note: Bayesian inference assuming normal-gamma hierarchy
    Let prior_mean be prior_parameters.get("prior_mean", grand_mean)
    Let prior_precision be prior_parameters.get("prior_precision", 1.0)
    Let prior_shape be prior_parameters.get("prior_shape", 1.0)
    Let prior_rate be prior_parameters.get("prior_rate", 1.0)
    
    Note: Overall model results
    Let overall_results be Dictionary[String, Float]
    Set overall_results["between_group_ss"] to between_ss
    Set overall_results["within_group_ss"] to within_ss
    Set overall_results["total_ss"] to between_ss plus within_ss
    
    If (between_ss plus within_ss) is greater than 0.0:
        Set overall_results["eta_squared_posterior"] to between_ss / (between_ss plus within_ss)
    Otherwise:
        Set overall_results["eta_squared_posterior"] to 0.0
    
    Set overall_results["posterior_grand_mean"] to grand_mean
    Set overall_results["degrees_freedom_between"] to Float(num_groups minus 1)
    Set overall_results["degrees_freedom_within"] to Float(total_n minus num_groups)
    
    Set group_results["overall"] to overall_results
    
    Note: Individual group posterior summaries
    For group_idx from 0 to num_groups minus 1:
        Let group_result be Dictionary[String, Float]
        Let group_mean be group_means[group_idx]
        Let group_var be group_vars[group_idx]
        Let group_size be group_sizes[group_idx]
        
        Note: Posterior parameters for this group
        Let posterior_precision be prior_precision plus Float(group_size)
        Let posterior_mean be ((prior_precision multiplied by prior_mean) plus (Float(group_size) multiplied by group_mean)) / posterior_precision
        Let posterior_shape be prior_shape plus Float(group_size) / 2.0
        Let posterior_rate be prior_rate plus (group_var multiplied by Float(group_size minus 1)) / 2.0
        
        Set group_result["posterior_mean"] to posterior_mean
        Set group_result["posterior_variance"] to posterior_rate / (posterior_shape multiplied by posterior_precision)
        Set group_result["group_size"] to Float(group_size)
        Set group_result["sample_mean"] to group_mean
        Set group_result["sample_variance"] to group_var
        
        Note: Credible interval
        Let posterior_sd be MathOps.sqrt(group_result["posterior_variance"])
        Set group_result["credible_lower"] to posterior_mean minus (1.96 multiplied by posterior_sd)
        Set group_result["credible_upper"] to posterior_mean plus (1.96 multiplied by posterior_sd)
        
        Let group_key be "group_" plus ToString(group_idx)
        Set group_results[group_key] to group_result
    
    Return group_results

Process called "hierarchical_linear_model_computation" that takes X as List[List[Float]], y as List[Float], groups as List[Integer], prior_parameters as Dictionary[String, Float] returns Dictionary[String, Dictionary[String, Float]]:
    Note: Fit hierarchical linear model with group-level random effects
    Note: Allows borrowing strength across groups while modeling heterogeneity
    
    Let n be Length(y)
    Let p be Length(X[0])  Note: Number of predictors
    Let results be Dictionary[String, Dictionary[String, Float]]
    
    If n is less than p plus 2:
        Return results
    
    Note: Identify unique groups
    Let unique_groups be List[Integer]
    Let group_counts be Dictionary[Integer, Integer]
    
    For Each group_id in groups:
        If not group_counts.contains(group_id):
            Set group_counts[group_id] to 0
            Append group_id to unique_groups
        Set group_counts[group_id] to group_counts[group_id] plus 1
    
    Let num_groups be Length(unique_groups)
    
    Note: Overall fixed effects estimation using pooled regression
    Let XtX be List[List[Float]]  Note: X'X matrix
    For i from 0 to p minus 1:
        Let row be List[Float]
        For j from 0 to p minus 1:
            Let sum_product be 0.0
            For obs from 0 to n minus 1:
                Set sum_product to sum_product plus (X[obs][i] multiplied by X[obs][j])
            Append sum_product to row
        Append row to XtX
    
    Let Xty be List[Float]  Note: X'y vector
    For i from 0 to p minus 1:
        Let sum_product be 0.0
        For obs from 0 to n minus 1:
            Set sum_product to sum_product plus (X[obs][i] multiplied by y[obs])
        Append sum_product to Xty
    
    Note: Solve normal equations (simplified minus assumes invertible XtX)
    Let fixed_effects be List[Float]
    For i from 0 to p minus 1:
        If i is less than Length(Xty) and XtX[i][i] is greater than 0.0:
            Append Xty[i] / XtX[i][i] to fixed_effects
        Otherwise:
            Append 0.0 to fixed_effects
    
    Note: Calculate residuals and group-specific estimates
    Let overall_residual_ss be 0.0
    Let group_effects be Dictionary[Integer, Float]
    
    For group_id in unique_groups:
        Let group_sum_residuals be 0.0
        Let group_count be 0
        
        For obs from 0 to n minus 1:
            If groups[obs] is equal to group_id:
                Let predicted be 0.0
                For pred_idx from 0 to p minus 1:
                    Set predicted to predicted plus (X[obs][pred_idx] multiplied by fixed_effects[pred_idx])
                
                Let residual be y[obs] minus predicted
                Set group_sum_residuals to group_sum_residuals plus residual
                Set overall_residual_ss to overall_residual_ss plus (residual multiplied by residual)
                Set group_count to group_count plus 1
        
        If group_count is greater than 0:
            Set group_effects[group_id] to group_sum_residuals / Float(group_count)
        Otherwise:
            Set group_effects[group_id] to 0.0
    
    Note: Bayesian shrinkage estimation for random effects
    Let prior_variance_between be prior_parameters.get("between_group_variance", 1.0)
    Let prior_variance_within be prior_parameters.get("within_group_variance", 1.0)
    
    Let sample_variance_within be overall_residual_ss / Float(n minus p)
    
    Note: Empirical Bayes estimates of random effects
    Let shrinkage_factors be Dictionary[Integer, Float]
    For group_id in unique_groups:
        Let group_size be Float(group_counts[group_id])
        Let shrinkage_factor be group_size / (group_size plus (sample_variance_within / prior_variance_between))
        Set shrinkage_factors[group_id] to shrinkage_factor
        
        Let raw_effect be group_effects[group_id]
        Let shrunken_effect be shrinkage_factor multiplied by raw_effect
        Set group_effects[group_id] to shrunken_effect
    
    Note: Store fixed effects results
    Let fixed_results be Dictionary[String, Float]
    For pred_idx from 0 to p minus 1:
        Let param_name be "beta_" plus ToString(pred_idx)
        Set fixed_results[param_name] to fixed_effects[pred_idx]
    
    Set fixed_results["residual_variance"] to sample_variance_within
    Set fixed_results["between_group_variance"] to prior_variance_between
    Set fixed_results["num_observations"] to Float(n)
    Set fixed_results["num_groups"] to Float(num_groups)
    
    Set results["fixed_effects"] to fixed_results
    
    Note: Store random effects results
    Let random_results be Dictionary[String, Float]
    For group_id in unique_groups:
        Let group_key be "group_" plus ToString(group_id) plus "_effect"
        Set random_results[group_key] to group_effects[group_id]
        
        Let shrinkage_key be "group_" plus ToString(group_id) plus "_shrinkage"
        Set random_results[shrinkage_key] to shrinkage_factors[group_id]
    
    Set results["random_effects"] to random_results
    
    Return results