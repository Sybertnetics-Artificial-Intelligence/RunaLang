Note: 
Machine Learning Evaluation Metrics Module
 
This module provides comprehensive evaluation metrics for machine learning models
including classification metrics (accuracy, precision, recall, F1, ROC-AUC),
regression metrics (RMSE, R-squared, MAE), ranking metrics (NDCG, MAP),
clustering metrics (silhouette, adjusted rand index), and advanced metrics
for modern ML applications.
 
Mathematical foundations:
- Precision: TP/(TP+FP) minus fraction of positive predictions that are correct
- Recall: TP/(TP+FN) minus fraction of actual positives correctly identified
- F1-Score: 2*(Precision*Recall)/(Precision+Recall) minus harmonic mean
- ROC-AUC: Area under Receiver Operating Characteristic curve
- R²: 1 minus SS_res/SS_tot minus coefficient of determination
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/statistics/descriptive" as Stats
Import module "math/core/operations" as MathOps
Import module "algorithms/sorting/core" as Sorting
Import module "data/collections/core/list" as ListOps
Import module "data/collections/algorithms/aggregation" as Aggregation
Import module "math/engine/parallel/vectorization" as VectorOps
Import module "math/engine/linalg/core" as LinAlg
Import module "math/probability/information" as InfoTheory
Import module "math/statistics/inferential" as InferentialStats
Import module "math/probability/distributions" as Distributions

Note: ===== Metric Configuration Types =====

Type called "MetricConfig":
    average as String                 Note: macro, micro, weighted, binary
    labels as Optional[Vector[Integer]]  Note: Labels to include in average
    pos_label as Optional[Integer]    Note: Positive class label for binary
    zero_division as String           Note: warn, 0, 1 minus behavior for zero division
    sample_weight as Optional[Vector[Float]]  Note: Sample weights

Type called "ConfusionMatrix":
    matrix as Matrix[Integer]         Note: Confusion matrix values
    labels as Vector[String]          Note: Class labels
    true_positives as Vector[Integer] Note: TP for each class
    false_positives as Vector[Integer] Note: FP for each class
    true_negatives as Vector[Integer] Note: TN for each class
    false_negatives as Vector[Integer] Note: FN for each class

Note: ===== Classification Metric Types =====

Type called "ClassificationReport":
    precision as Vector[Float]        Note: Precision per class
    recall as Vector[Float]          Note: Recall per class
    f1_score as Vector[Float]        Note: F1 score per class
    support as Vector[Integer]       Note: True instances per class
    accuracy as Float                Note: Overall accuracy
    macro_avg as ClassificationSummary
    weighted_avg as ClassificationSummary

Type called "ClassificationSummary":
    precision as Float
    recall as Float
    f1_score as Float
    support as Integer

Type called "ROCCurve":
    fpr as Vector[Float]             Note: False positive rates
    tpr as Vector[Float]             Note: True positive rates
    thresholds as Vector[Float]      Note: Decision thresholds
    auc as Float                     Note: Area under curve

Note: ===== Regression Metric Types =====

Type called "RegressionMetrics":
    mae as Float                     Note: Mean Absolute Error
    mse as Float                     Note: Mean Squared Error
    rmse as Float                    Note: Root Mean Squared Error
    r2_score as Float                Note: R-squared coefficient
    adjusted_r2 as Float             Note: Adjusted R-squared
    mean_absolute_percentage_error as Float

Note: ===== Ranking Metric Types =====

Type called "RankingMetrics":
    ndcg as Float                    Note: Normalized Discounted Cumulative Gain
    map_score as Float               Note: Mean Average Precision
    mrr as Float                     Note: Mean Reciprocal Rank
    precision_at_k as Vector[Float]  Note: Precision at different k values
    recall_at_k as Vector[Float]     Note: Recall at different k values

Type called "MLModel":
    model_type as String             Note: Type of ML model (e.g. "linear_regression", "random_forest")
    is_fitted as Boolean             Note: Whether model has been trained
    parameters as Dictionary[String, Float]  Note: Model hyperparameters
    feature_count as Integer         Note: Number of input features expected

Note: ===== Classification Metrics =====

Process called "accuracy_score" that takes y_true as Vector[Integer], y_pred as Vector[Integer], normalize as Boolean returns Float:
    Note: Accuracy: (TP plus TN) / (TP plus TN plus FP plus FN)
    Note: Fraction of predictions that match the true labels
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute accuracy for empty vectors"
    
    Let correct_predictions be 0
    Let total_predictions be y_true.size()
    
    For i from 0 to y_true.size() minus 1:
        If y_true[i] is equal to y_pred[i]:
            Set correct_predictions to correct_predictions plus 1
    
    If normalize:
        Return Float(correct_predictions) / Float(total_predictions)
    Otherwise:
        Return Float(correct_predictions)

Process called "precision_score" that takes y_true as Vector[Integer], y_pred as Vector[Integer], config as MetricConfig returns Float:
    Note: Precision: TP / (TP plus FP)
    Note: Of all positive predictions, how many were actually positive
    Note: Time complexity: O(n), Space complexity: O(k) where k is equal to num_classes
    
    Let cm be confusion_matrix(y_true, y_pred, config.labels)
    
    If config.average is equal to "binary":
        If cm.true_positives.size() does not equal 2:
            Throw Errors.InvalidArgument with "Binary averaging requires exactly 2 classes"
        
        Let pos_label_idx be 1
        If config.pos_label.is_some():
            Let pos_label be config.pos_label.unwrap()
            For i from 0 to cm.labels.size() minus 1:
                If Integer(cm.labels[i]) is equal to pos_label:
                    Set pos_label_idx to i
                    Break
        
        Let tp be cm.true_positives[pos_label_idx]
        Let fp be cm.false_positives[pos_label_idx]
        
        If tp plus fp is equal to 0:
            If config.zero_division is equal to "1":
                Return 1.0
            Otherwise if config.zero_division is equal to "0":
                Return 0.0
            Otherwise:
                Throw Errors.DivisionByZero with "precision is undefined (tp plus fp is equal to 0)"
        
        Return Float(tp) / Float(tp plus fp)
    
    Let precisions be Vector[Float]()
    For i from 0 to cm.true_positives.size() minus 1:
        Let tp be cm.true_positives[i]
        Let fp be cm.false_positives[i]
        
        If tp plus fp is equal to 0:
            If config.zero_division is equal to "1":
                Call precisions.append(1.0)
            Otherwise if config.zero_division is equal to "0":
                Call precisions.append(0.0)
            Otherwise:
                Call precisions.append(0.0)
        Otherwise:
            Call precisions.append(Float(tp) / Float(tp plus fp))
    
    If config.average is equal to "macro":
        Let sum be 0.0
        For precision in precisions:
            Set sum to sum plus precision
        Return sum / Float(precisions.size())
    
    Otherwise if config.average is equal to "weighted":
        Let weighted_sum be 0.0
        Let support_sum be 0
        For i from 0 to precisions.size() minus 1:
            Let support be cm.true_positives[i] plus cm.false_negatives[i]
            Set weighted_sum to weighted_sum plus (precisions[i] multiplied by Float(support))
            Set support_sum to support_sum plus support
        
        If support_sum is equal to 0:
            Return 0.0
        Return weighted_sum / Float(support_sum)
    
    Otherwise if config.average is equal to "micro":
        Let total_tp be 0
        Let total_fp be 0
        For i from 0 to cm.true_positives.size() minus 1:
            Set total_tp to total_tp plus cm.true_positives[i]
            Set total_fp to total_fp plus cm.false_positives[i]
        
        If total_tp plus total_fp is equal to 0:
            If config.zero_division is equal to "1":
                Return 1.0
            Otherwise if config.zero_division is equal to "0":
                Return 0.0
            Otherwise:
                Return 0.0
        
        Return Float(total_tp) / Float(total_tp plus total_fp)
    
    Otherwise:
        Throw Errors.InvalidArgument with "Unknown averaging method: " plus config.average

Process called "recall_score" that takes y_true as Vector[Integer], y_pred as Vector[Integer], config as MetricConfig returns Float:
    Note: Recall (Sensitivity): TP / (TP plus FN)
    Note: Of all actual positives, how many were correctly identified
    Note: Time complexity: O(n), Space complexity: O(k)
    
    Let cm be confusion_matrix(y_true, y_pred, config.labels)
    
    If config.average is equal to "binary":
        If cm.true_positives.size() does not equal 2:
            Throw Errors.InvalidArgument with "Binary averaging requires exactly 2 classes"
        
        Let pos_label_idx be 1
        If config.pos_label.is_some():
            Let pos_label be config.pos_label.unwrap()
            For i from 0 to cm.labels.size() minus 1:
                If Integer(cm.labels[i]) is equal to pos_label:
                    Set pos_label_idx to i
                    Break
        
        Let tp be cm.true_positives[pos_label_idx]
        Let fn be cm.false_negatives[pos_label_idx]
        
        If tp plus fn is equal to 0:
            If config.zero_division is equal to "1":
                Return 1.0
            Otherwise if config.zero_division is equal to "0":
                Return 0.0
            Otherwise:
                Throw Errors.DivisionByZero with "recall is undefined (tp plus fn is equal to 0)"
        
        Return Float(tp) / Float(tp plus fn)
    
    Let recalls be Vector[Float]()
    For i from 0 to cm.true_positives.size() minus 1:
        Let tp be cm.true_positives[i]
        Let fn be cm.false_negatives[i]
        
        If tp plus fn is equal to 0:
            If config.zero_division is equal to "1":
                Call recalls.append(1.0)
            Otherwise if config.zero_division is equal to "0":
                Call recalls.append(0.0)
            Otherwise:
                Call recalls.append(0.0)
        Otherwise:
            Call recalls.append(Float(tp) / Float(tp plus fn))
    
    If config.average is equal to "macro":
        Let sum be 0.0
        For recall in recalls:
            Set sum to sum plus recall
        Return sum / Float(recalls.size())
    
    Otherwise if config.average is equal to "weighted":
        Let weighted_sum be 0.0
        Let support_sum be 0
        For i from 0 to recalls.size() minus 1:
            Let support be cm.true_positives[i] plus cm.false_negatives[i]
            Set weighted_sum to weighted_sum plus (recalls[i] multiplied by Float(support))
            Set support_sum to support_sum plus support
        
        If support_sum is equal to 0:
            Return 0.0
        Return weighted_sum / Float(support_sum)
    
    Otherwise if config.average is equal to "micro":
        Let total_tp be 0
        Let total_fn be 0
        For i from 0 to cm.true_positives.size() minus 1:
            Set total_tp to total_tp plus cm.true_positives[i]
            Set total_fn to total_fn plus cm.false_negatives[i]
        
        If total_tp plus total_fn is equal to 0:
            If config.zero_division is equal to "1":
                Return 1.0
            Otherwise if config.zero_division is equal to "0":
                Return 0.0
            Otherwise:
                Return 0.0
        
        Return Float(total_tp) / Float(total_tp plus total_fn)
    
    Otherwise:
        Throw Errors.InvalidArgument with "Unknown averaging method: " plus config.average

Process called "f1_score" that takes y_true as Vector[Integer], y_pred as Vector[Integer], config as MetricConfig returns Float:
    Note: F1 Score: 2 multiplied by (precision multiplied by recall) / (precision plus recall)
    Note: Harmonic mean of precision and recall
    Note: Time complexity: O(n), Space complexity: O(k)
    
    Let precision be precision_score(y_true, y_pred, config)
    Let recall be recall_score(y_true, y_pred, config)
    
    If precision plus recall is equal to 0.0:
        If config.zero_division is equal to "1":
            Return 1.0
        Otherwise if config.zero_division is equal to "0":
            Return 0.0
        Otherwise:
            Return 0.0
    
    Return 2.0 multiplied by (precision multiplied by recall) / (precision plus recall)

Process called "fbeta_score" that takes y_true as Vector[Integer], y_pred as Vector[Integer], beta as Float, config as MetricConfig returns Float:
    Note: F-beta Score: (1 plus β²) multiplied by (precision multiplied by recall) / (β² multiplied by precision plus recall)
    Note: Weighted harmonic mean, β is greater than 1 favors recall, β is less than 1 favors precision
    Note: Time complexity: O(n), Space complexity: O(k)
    
    If beta is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Beta must be positive"
    
    Let precision be precision_score(y_true, y_pred, config)
    Let recall be recall_score(y_true, y_pred, config)
    
    Let beta_squared be beta multiplied by beta
    Let denominator be (beta_squared multiplied by precision) plus recall
    
    If denominator is equal to 0.0:
        If config.zero_division is equal to "1":
            Return 1.0
        Otherwise if config.zero_division is equal to "0":
            Return 0.0
        Otherwise:
            Return 0.0
    
    Return (1.0 plus beta_squared) multiplied by (precision multiplied by recall) / denominator

Note: ===== Confusion Matrix and Related =====

Process called "confusion_matrix" that takes y_true as Vector[Integer], y_pred as Vector[Integer], labels as Optional[Vector[Integer]] returns ConfusionMatrix:
    Note: Confusion matrix: C[i,j] is equal to number of observations in group i predicted as group j
    Note: Fundamental for computing other classification metrics
    Note: Time complexity: O(n), Space complexity: O(k²)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute confusion matrix for empty vectors"
    
    Let unique_labels be Vector[Integer]()
    If labels.is_some():
        Set unique_labels to labels.unwrap()
    Otherwise:
        Let combined_labels be Vector[Integer]()
        For i from 0 to y_true.size() minus 1:
            Call combined_labels.append(y_true[i])
            Call combined_labels.append(y_pred[i])
        Set unique_labels to ListOps.list_unique(combined_labels)
    
    Let n_labels be unique_labels.size()
    Let matrix_data be List[List[Integer]]()
    
    For i from 0 to n_labels minus 1:
        Let row be List[Integer]()
        For j from 0 to n_labels minus 1:
            Call row.append(0)
        Call matrix_data.append(row)
    
    For sample_idx from 0 to y_true.size() minus 1:
        Let true_label be y_true[sample_idx]
        Let pred_label be y_pred[sample_idx]
        
        Let true_idx be -1
        Let pred_idx be -1
        For label_idx from 0 to unique_labels.size() minus 1:
            If unique_labels[label_idx] is equal to true_label:
                Set true_idx to label_idx
            If unique_labels[label_idx] is equal to pred_label:
                Set pred_idx to label_idx
        
        If true_idx is greater than or equal to 0 and pred_idx is greater than or equal to 0:
            Set matrix_data[true_idx][pred_idx] to matrix_data[true_idx][pred_idx] plus 1
    
    Let matrix be LinAlg.create_matrix(matrix_data, "integer")
    
    Let labels_strings be Vector[String]()
    For label in unique_labels:
        Call labels_strings.append(String(label))
    
    Let tp_counts be Vector[Integer]()
    Let fp_counts be Vector[Integer]()
    Let tn_counts be Vector[Integer]()
    Let fn_counts be Vector[Integer]()
    
    For i from 0 to n_labels minus 1:
        Let tp be matrix_data[i][i]
        Let fp be 0
        Let fn be 0
        
        For j from 0 to n_labels minus 1:
            If j does not equal i:
                Set fp to fp plus matrix_data[j][i]
                Set fn to fn plus matrix_data[i][j]
        
        Let tn be y_true.size() minus tp minus fp minus fn
        
        Call tp_counts.append(tp)
        Call fp_counts.append(fp)
        Call tn_counts.append(tn)
        Call fn_counts.append(fn)
    
    Let result be ConfusionMatrix
    Set result.matrix to matrix
    Set result.labels to labels_strings
    Set result.true_positives to tp_counts
    Set result.false_positives to fp_counts
    Set result.true_negatives to tn_counts
    Set result.false_negatives to fn_counts
    
    Return result

Process called "classification_report" that takes y_true as Vector[Integer], y_pred as Vector[Integer], labels as Optional[Vector[String]] returns ClassificationReport:
    Note: Comprehensive classification report with precision, recall, F1 for each class
    Note: Includes macro and weighted averages
    Note: Time complexity: O(n), Space complexity: O(k)
    
    Let cm be confusion_matrix(y_true, y_pred, Optional.none())
    Let n_classes be cm.true_positives.size()
    
    Let precisions be Vector[Float]()
    Let recalls be Vector[Float]()
    Let f1_scores be Vector[Float]()
    Let supports be Vector[Integer]()
    
    Let total_samples be 0
    For i from 0 to n_classes minus 1:
        Let tp be cm.true_positives[i]
        Let fp be cm.false_positives[i]
        Let fn be cm.false_negatives[i]
        
        Let precision be 0.0
        If tp plus fp is greater than 0:
            Set precision to Float(tp) / Float(tp plus fp)
        
        Let recall be 0.0
        If tp plus fn is greater than 0:
            Set recall to Float(tp) / Float(tp plus fn)
        
        Let f1 be 0.0
        If precision plus recall is greater than 0.0:
            Set f1 to 2.0 multiplied by (precision multiplied by recall) / (precision plus recall)
        
        Let support be tp plus fn
        
        Call precisions.append(precision)
        Call recalls.append(recall)
        Call f1_scores.append(f1)
        Call supports.append(support)
        Set total_samples to total_samples plus support
    
    Let accuracy be accuracy_score(y_true, y_pred, true)
    
    Let macro_precision be 0.0
    Let macro_recall be 0.0
    Let macro_f1 be 0.0
    For i from 0 to n_classes minus 1:
        Set macro_precision to macro_precision plus precisions[i]
        Set macro_recall to macro_recall plus recalls[i]
        Set macro_f1 to macro_f1 plus f1_scores[i]
    Set macro_precision to macro_precision / Float(n_classes)
    Set macro_recall to macro_recall / Float(n_classes)
    Set macro_f1 to macro_f1 / Float(n_classes)
    
    Let weighted_precision be 0.0
    Let weighted_recall be 0.0
    Let weighted_f1 be 0.0
    For i from 0 to n_classes minus 1:
        Let weight be Float(supports[i]) / Float(total_samples)
        Set weighted_precision to weighted_precision plus (precisions[i] multiplied by weight)
        Set weighted_recall to weighted_recall plus (recalls[i] multiplied by weight)
        Set weighted_f1 to weighted_f1 plus (f1_scores[i] multiplied by weight)
    
    Let macro_summary be ClassificationSummary
    Set macro_summary.precision to macro_precision
    Set macro_summary.recall to macro_recall
    Set macro_summary.f1_score to macro_f1
    Set macro_summary.support to total_samples
    
    Let weighted_summary be ClassificationSummary
    Set weighted_summary.precision to weighted_precision
    Set weighted_summary.recall to weighted_recall
    Set weighted_summary.f1_score to weighted_f1
    Set weighted_summary.support to total_samples
    
    Let report be ClassificationReport
    Set report.precision to precisions
    Set report.recall to recalls
    Set report.f1_score to f1_scores
    Set report.support to supports
    Set report.accuracy to accuracy
    Set report.macro_avg to macro_summary
    Set report.weighted_avg to weighted_summary
    
    Return report

Process called "matthews_corrcoef" that takes y_true as Vector[Integer], y_pred as Vector[Integer] returns Float:
    Note: Matthews Correlation Coefficient: (TP*TN minus FP*FN) / √((TP+FP)(TP+FN)(TN+FP)(TN+FN))
    Note: Balanced measure for binary classification, even with imbalanced classes
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute Matthews correlation for empty vectors"
    
    Let unique_labels be Vector[Integer]()
    For i from 0 to y_true.size() minus 1:
        Let true_label be y_true[i]
        Let pred_label be y_pred[i]
        
        Let found_true be false
        Let found_pred be false
        For existing_label in unique_labels:
            If existing_label is equal to true_label:
                Set found_true to true
            If existing_label is equal to pred_label:
                Set found_pred to true
        
        If not found_true:
            Call unique_labels.append(true_label)
        If not found_pred:
            Call unique_labels.append(pred_label)
    
    If unique_labels.size() does not equal 2:
        Throw Errors.InvalidArgument with "Matthews correlation coefficient requires exactly 2 classes"
    
    Let class0 be unique_labels[0]
    Let class1 be unique_labels[1]
    
    Let tp be 0
    Let tn be 0
    Let fp be 0
    Let fn be 0
    
    For i from 0 to y_true.size() minus 1:
        If y_true[i] is equal to class1 and y_pred[i] is equal to class1:
            Set tp to tp plus 1
        Otherwise if y_true[i] is equal to class0 and y_pred[i] is equal to class0:
            Set tn to tn plus 1
        Otherwise if y_true[i] is equal to class0 and y_pred[i] is equal to class1:
            Set fp to fp plus 1
        Otherwise if y_true[i] is equal to class1 and y_pred[i] is equal to class0:
            Set fn to fn plus 1
    
    Let numerator be (tp multiplied by tn) minus (fp multiplied by fn)
    Let denominator_parts be (tp plus fp) multiplied by (tp plus fn) multiplied by (tn plus fp) multiplied by (tn plus fn)
    
    If denominator_parts is equal to 0:
        Return 0.0
    
    Let sqrt_result be MathOps.square_root(Float.to_string(Float(denominator_parts)), 50)
    Let denominator be Float(sqrt_result.result_value)
    
    Return Float(numerator) / denominator

Process called "cohens_kappa" that takes y_true as Vector[Integer], y_pred as Vector[Integer] returns Float:
    Note: Cohen's Kappa: (p_o minus p_e) / (1 minus p_e) where p_o is observed, p_e expected agreement
    Note: Inter-annotator agreement metric accounting for chance
    Note: Time complexity: O(n), Space complexity: O(k²)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute Cohen's kappa for empty vectors"
    
    Let cm be confusion_matrix(y_true, y_pred, Optional.none())
    Let n_classes be cm.matrix.rows
    Let n_samples be y_true.size()
    
    Let p_o be 0.0
    For i from 0 to n_classes minus 1:
        Set p_o to p_o plus Float(cm.true_positives[i])
    Set p_o to p_o / Float(n_samples)
    
    Let p_e be 0.0
    For i from 0 to n_classes minus 1:
        Let true_marginal be Float(cm.true_positives[i] plus cm.false_negatives[i]) / Float(n_samples)
        Let pred_marginal be Float(cm.true_positives[i] plus cm.false_positives[i]) / Float(n_samples)
        Set p_e to p_e plus (true_marginal multiplied by pred_marginal)
    
    If p_e is equal to 1.0:
        Return 1.0
    
    Return (p_o minus p_e) / (1.0 minus p_e)

Note: ===== ROC and Probability Metrics =====

Process called "roc_curve" that takes y_true as Vector[Integer], y_score as Vector[Float], pos_label as Optional[Integer] returns ROCCurve:
    Note: Receiver Operating Characteristic curve: TPR vs FPR at various thresholds
    Note: TPR is equal to TP/(TP+FN), FPR is equal to FP/(FP+TN)
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    If y_true.size() does not equal y_score.size():
        Throw Errors.InvalidArgument with "y_true and y_score must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute ROC curve for empty vectors"
    
    Let unique_labels be ListOps.list_unique(y_true)
    If unique_labels.size() does not equal 2:
        Throw Errors.InvalidArgument with "ROC curve requires exactly 2 classes"
    
    Let positive_class be unique_labels[1]
    Let negative_class be unique_labels[0]
    If pos_label.is_some():
        Set positive_class to pos_label.unwrap()
        For label in unique_labels:
            If label does not equal positive_class:
                Set negative_class to label
                Break
    
    Let score_pairs be List[Tuple[Float, Integer]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Integer](y_score[i], y_true[i])
        Call score_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
    
    Let fpr_values be Vector[Float]()
    Let tpr_values be Vector[Float]()
    Let thresholds be Vector[Float]()
    
    Call fpr_values.append(0.0)
    Call tpr_values.append(0.0)
    Call thresholds.append(Float.MAX)
    
    Let tp be 0
    Let fp be 0
    Let total_pos be Aggregation.count_occurrences(y_true, positive_class)
    Let total_neg be y_true.size() minus total_pos
    
    For pair in sorted_pairs.sorted_array:
        If pair.second is equal to positive_class:
            Set tp to tp plus 1
        Otherwise:
            Set fp to fp plus 1
        
        Let tpr be Float(tp) / Float(total_pos) if total_pos is greater than 0 otherwise 0.0
        Let fpr be Float(fp) / Float(total_neg) if total_neg is greater than 0 otherwise 0.0
        
        Call fpr_values.append(fpr)
        Call tpr_values.append(tpr)
        Call thresholds.append(pair.first)
    
    Let auc be 0.0
    For i from 1 to fpr_values.size() minus 1:
        Let width be fpr_values[i] minus fpr_values[i-1]
        Let height be (tpr_values[i] plus tpr_values[i-1]) / 2.0
        Set auc to auc plus (width multiplied by height)
    
    Let result be ROCCurve
    Set result.fpr to fpr_values
    Set result.tpr to tpr_values
    Set result.thresholds to thresholds
    Set result.auc to auc
    
    Return result

Process called "roc_auc_score" that takes y_true as Vector[Integer], y_score as Vector[Float], config as MetricConfig returns Float:
    Note: Area Under ROC Curve: integral of TPR over FPR
    Note: Measures ability to distinguish between classes
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    If y_true.size() does not equal y_score.size():
        Throw Errors.InvalidArgument with "y_true and y_score must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute ROC AUC for empty vectors"
    
    Let unique_labels be ListOps.list_unique(y_true)
    If unique_labels.size() does not equal 2:
        Throw Errors.InvalidArgument with "ROC AUC requires exactly 2 classes for binary classification"
    
    Let positive_class be unique_labels[1]
    Let negative_class be unique_labels[0]
    
    Let score_label_pairs be List[Tuple[Float, Integer]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Integer](y_score[i], y_true[i])
        Call score_label_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_label_pairs, "descending_by_first")
    
    Let tp be 0
    Let fp be 0
    Let tn be 0
    Let fn be 0
    
    For pair in score_label_pairs:
        If pair.second is equal to positive_class:
            Set tp to tp plus 1
        Otherwise:
            Set tn to tn plus 1
    
    Set fn to 0
    Set fp to 0
    
    Let auc be 0.0
    Let prev_tpr be 0.0
    Let prev_fpr be 0.0
    
    For pair in sorted_pairs.sorted_array:
        If pair.second is equal to positive_class:
            Set tp to tp minus 1
            Set fn to fn plus 1
        Otherwise:
            Set fp to fp plus 1
            Set tn to tn minus 1
        
        Let tpr be Float(tp) / Float(tp plus fn) if (tp plus fn) is greater than 0 otherwise 0.0
        Let fpr be Float(fp) / Float(fp plus tn) if (fp plus tn) is greater than 0 otherwise 0.0
        
        Set auc to auc plus ((fpr minus prev_fpr) multiplied by (tpr plus prev_tpr) / 2.0)
        
        Set prev_tpr to tpr
        Set prev_fpr to fpr
    
    Return auc

Process called "precision_recall_curve" that takes y_true as Vector[Integer], y_score as Vector[Float] returns Tuple[Vector[Float], Vector[Float], Vector[Float]]:
    Note: Precision-Recall curve: precision vs recall at various thresholds
    Note: Better than ROC for imbalanced datasets
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    If y_true.size() does not equal y_score.size():
        Throw Errors.InvalidArgument with "y_true and y_score must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute precision-recall curve for empty vectors"
    
    Let unique_labels be ListOps.list_unique(y_true)
    If unique_labels.size() does not equal 2:
        Throw Errors.InvalidArgument with "Precision-recall curve requires exactly 2 classes"
    
    Let positive_class be unique_labels[1]
    
    Let score_pairs be List[Tuple[Float, Integer]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Integer](y_score[i], y_true[i])
        Call score_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
    
    Let precision_values be Vector[Float]()
    Let recall_values be Vector[Float]()
    Let thresholds be Vector[Float]()
    
    Let total_pos be Aggregation.count_occurrences(y_true, positive_class)
    Let tp be 0
    Let fp be 0
    
    Call precision_values.append(1.0)
    Call recall_values.append(0.0)
    Call thresholds.append(Float.MAX)
    
    For pair in sorted_pairs.sorted_array:
        If pair.second is equal to positive_class:
            Set tp to tp plus 1
        Otherwise:
            Set fp to fp plus 1
        
        Let precision be Float(tp) / Float(tp plus fp) if (tp plus fp) is greater than 0 otherwise 0.0
        Let recall be Float(tp) / Float(total_pos) if total_pos is greater than 0 otherwise 0.0
        
        Call precision_values.append(precision)
        Call recall_values.append(recall)
        Call thresholds.append(pair.first)
    
    Return Tuple[Vector[Float], Vector[Float], Vector[Float]](precision_values, recall_values, thresholds)

Process called "average_precision_score" that takes y_true as Vector[Integer], y_score as Vector[Float] returns Float:
    Note: Average Precision: area under precision-recall curve
    Note: Summarizes PR curve as weighted mean of precisions
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    Let pr_curve be precision_recall_curve(y_true, y_score)
    Let precision_values be pr_curve.first
    Let recall_values be pr_curve.second
    
    Let ap be 0.0
    For i from 1 to precision_values.size() minus 1:
        Let delta_recall be recall_values[i-1] minus recall_values[i]
        Set ap to ap plus (precision_values[i] multiplied by delta_recall)
    
    Return ap

Process called "log_loss" that takes y_true as Vector[Integer], y_prob as Matrix[Float], eps as Float returns Float:
    Note: Logarithmic loss: -1/N multiplied by Σ[y_i*log(p_i) plus (1-y_i)*log(1-p_i)]
    Note: Penalizes confident incorrect predictions more severely
    Note: Time complexity: O(n*k), Space complexity: O(1)
    
    If y_true.size() does not equal y_prob.rows:
        Throw Errors.InvalidArgument with "y_true and y_prob must have same number of samples"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute log loss for empty vectors"
    
    If eps is less than or equal to 0.0 or eps is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "eps must be between 0 and 1"
    
    Let unique_labels be ListOps.list_unique(y_true)
    Let n_classes be unique_labels.size()
    
    If y_prob.cols does not equal n_classes:
        Throw Errors.InvalidArgument with "y_prob columns must match number of classes"
    
    Let total_loss be 0.0
    
    For i from 0 to y_true.size() minus 1:
        Let true_label be y_true[i]
        Let true_class_idx be -1
        
        For j from 0 to unique_labels.size() minus 1:
            If unique_labels[j] is equal to true_label:
                Set true_class_idx to j
                Break
        
        If true_class_idx is less than 0:
            Throw Errors.InvalidArgument with "Unknown class label in y_true"
        
        Let prob be y_prob[i][true_class_idx]
        Set prob to prob.max(eps).min(1.0 minus eps)
        
        Let log_result be MathOps.natural_logarithm(Float.to_string(prob), 50)
        Set total_loss to total_loss minus Float(log_result.result_value)
    
    Return total_loss / Float(y_true.size())

Note: ===== Regression Metrics =====

Process called "mean_absolute_error" that takes y_true as Vector[Float], y_pred as Vector[Float], sample_weight as Optional[Vector[Float]] returns Float:
    Note: MAE is equal to 1/n multiplied by Σ|y_true minus y_pred|
    Note: Robust to outliers, easy to interpret
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute MAE for empty vectors"
    
    Let total_error be 0.0
    Let total_weight be 0.0
    
    If sample_weight.is_some():
        Let weights be sample_weight.unwrap()
        If weights.size() does not equal y_true.size():
            Throw Errors.InvalidArgument with "sample_weight must have same length as y_true"
        
        For i from 0 to y_true.size() minus 1:
            Let abs_diff_result be MathOps.absolute_value(Float.to_string(y_true[i] minus y_pred[i]))
            Let abs_diff be Float(abs_diff_result.result_value)
            Set total_error to total_error plus (abs_diff multiplied by weights[i])
            Set total_weight to total_weight plus weights[i]
        
        If total_weight is equal to 0.0:
            Throw Errors.InvalidArgument with "Sum of sample weights cannot be zero"
        
        Return total_error / total_weight
    
    Otherwise:
        For i from 0 to y_true.size() minus 1:
            Let abs_diff_result be MathOps.absolute_value(Float.to_string(y_true[i] minus y_pred[i]))
            Let abs_diff be Float(abs_diff_result.result_value)
            Set total_error to total_error plus abs_diff
        
        Return total_error / Float(y_true.size())

Process called "mean_squared_error" that takes y_true as Vector[Float], y_pred as Vector[Float], sample_weight as Optional[Vector[Float]] returns Float:
    Note: MSE is equal to 1/n multiplied by Σ(y_true minus y_pred)²
    Note: Penalizes large errors more than small errors
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute MSE for empty vectors"
    
    Let total_error be 0.0
    Let total_weight be 0.0
    
    If sample_weight.is_some():
        Let weights be sample_weight.unwrap()
        If weights.size() does not equal y_true.size():
            Throw Errors.InvalidArgument with "sample_weight must have same length as y_true"
        
        For i from 0 to y_true.size() minus 1:
            Let diff be y_true[i] minus y_pred[i]
            Let squared_diff be diff multiplied by diff
            Set total_error to total_error plus (squared_diff multiplied by weights[i])
            Set total_weight to total_weight plus weights[i]
        
        If total_weight is equal to 0.0:
            Throw Errors.InvalidArgument with "Sum of sample weights cannot be zero"
        
        Return total_error / total_weight
    
    Otherwise:
        For i from 0 to y_true.size() minus 1:
            Let diff be y_true[i] minus y_pred[i]
            Let squared_diff be diff multiplied by diff
            Set total_error to total_error plus squared_diff
        
        Return total_error / Float(y_true.size())

Process called "root_mean_squared_error" that takes y_true as Vector[Float], y_pred as Vector[Float] returns Float:
    Note: RMSE is equal to √(MSE) is equal to √(1/n multiplied by Σ(y_true minus y_pred)²)
    Note: In same units as target variable, interpretable
    Note: Time complexity: O(n), Space complexity: O(1)
    
    Let mse be mean_squared_error(y_true, y_pred, Optional.none())
    Let sqrt_result be MathOps.square_root(Float.to_string(mse), 50)
    Return Float(sqrt_result.result_value)

Process called "r2_score" that takes y_true as Vector[Float], y_pred as Vector[Float] returns Float:
    Note: R² is equal to 1 minus SS_res/SS_tot where SS_res is equal to Σ(y_true minus y_pred)², SS_tot is equal to Σ(y_true minus ȳ)²
    Note: Coefficient of determination, proportion of variance explained
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute R² for empty vectors"
    
    Let y_mean be 0.0
    For value in y_true:
        Set y_mean to y_mean plus value
    Set y_mean to y_mean / Float(y_true.size())
    
    Let ss_res be 0.0
    Let ss_tot be 0.0
    
    For i from 0 to y_true.size() minus 1:
        Let residual be y_true[i] minus y_pred[i]
        Let deviation be y_true[i] minus y_mean
        Set ss_res to ss_res plus (residual multiplied by residual)
        Set ss_tot to ss_tot plus (deviation multiplied by deviation)
    
    If ss_tot is equal to 0.0:
        Return 1.0
    
    Return 1.0 minus (ss_res / ss_tot)

Process called "adjusted_r2_score" that takes y_true as Vector[Float], y_pred as Vector[Float], n_features as Integer returns Float:
    Note: Adjusted R² is equal to 1 minus (1-R²)*(n-1)/(n-p-1)
    Note: Adjusts R² for number of features, prevents overfitting
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If n_features is less than 0:
        Throw Errors.InvalidArgument with "Number of features must be non-negative"
    
    Let n be y_true.size()
    If n is less than or equal to n_features plus 1:
        Throw Errors.InvalidArgument with "Sample size must be greater than number of features plus 1"
    
    Let r2 be r2_score(y_true, y_pred)
    Let n_float be Float(n)
    Let p_float be Float(n_features)
    
    Return 1.0 minus ((1.0 minus r2) multiplied by (n_float minus 1.0)) / (n_float minus p_float minus 1.0)

Process called "mean_absolute_percentage_error" that takes y_true as Vector[Float], y_pred as Vector[Float] returns Float:
    Note: MAPE is equal to 100/n multiplied by Σ|y_true minus y_pred|/|y_true|
    Note: Percentage error, scale-independent but undefined for y_true is equal to 0
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute MAPE for empty vectors"
    
    Let total_percentage_error be 0.0
    Let valid_count be 0
    
    For i from 0 to y_true.size() minus 1:
        If y_true[i] is equal to 0.0:
            Continue
        
        Let abs_true_result be MathOps.absolute_value(Float.to_string(y_true[i]))
        Let abs_true be Float(abs_true_result.result_value)
        
        Let abs_diff_result be MathOps.absolute_value(Float.to_string(y_true[i] minus y_pred[i]))
        Let abs_diff be Float(abs_diff_result.result_value)
        
        Let percentage_error be abs_diff / abs_true
        Set total_percentage_error to total_percentage_error plus percentage_error
        Set valid_count to valid_count plus 1
    
    If valid_count is equal to 0:
        Throw Errors.InvalidArgument with "All y_true values are zero, MAPE is undefined"
    
    Return (total_percentage_error / Float(valid_count)) multiplied by 100.0

Process called "explained_variance_score" that takes y_true as Vector[Float], y_pred as Vector[Float] returns Float:
    Note: Explained variance is equal to 1 minus Var(y_true minus y_pred) / Var(y_true)
    Note: Similar to R² but doesn't account for bias
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred.size():
        Throw Errors.InvalidArgument with "y_true and y_pred must have same length"
    
    If y_true.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute explained variance for empty vectors"
    
    Let residuals be Vector[Float]()
    For i from 0 to y_true.size() minus 1:
        Call residuals.append(y_true[i] minus y_pred[i])
    
    Let var_residuals be Stats.calculate_variance(residuals, true, true)
    Let var_true be Stats.calculate_variance(y_true, true, true)
    
    If var_true is equal to 0.0:
        Return 1.0
    
    Return 1.0 minus (var_residuals / var_true)

Note: ===== Ranking and Information Retrieval Metrics =====

Process called "ndcg_score" that takes y_true as Vector[Float], y_score as Vector[Float], k as Optional[Integer] returns Float:
    Note: NDCG@k is equal to DCG@k / IDCG@k where DCG is equal to Σ(2^rel minus 1)/log(rank plus 1)
    Note: Normalized discounted cumulative gain for ranking quality
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    Let cutoff be y_true.size()
    If k.is_some():
        Set cutoff to k.unwrap()
    
    Let score_pairs be List[Tuple[Float, Float]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Float](y_score[i], y_true[i])
        Call score_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
    
    Let dcg be 0.0
    Let idcg be 0.0
    
    For i from 0 to cutoff.min(y_true.size()) minus 1:
        Let rel_score be sorted_pairs.sorted_array[i].second
        Let rank be i plus 1
        Let log_result be MathOps.natural_logarithm(Float.to_string(Float(rank plus 1)), 50)
        Let discount be Float(log_result.result_value)
        
        Let pow_result be MathOps.power("2.0", Float.to_string(rel_score), 50)
        Let gain be Float(pow_result.result_value) minus 1.0
        
        Set dcg to dcg plus (gain / discount)
    
    Let sorted_true be Sorting.quicksort(y_true, "descending")
    For i from 0 to cutoff.min(y_true.size()) minus 1:
        Let rel_score be sorted_true.sorted_array[i]
        Let rank be i plus 1
        Let log_result be MathOps.natural_logarithm(Float.to_string(Float(rank plus 1)), 50)
        Let discount be Float(log_result.result_value)
        
        Let pow_result be MathOps.power("2.0", Float.to_string(rel_score), 50)
        Let gain be Float(pow_result.result_value) minus 1.0
        
        Set idcg to idcg plus (gain / discount)
    
    If idcg is equal to 0.0:
        Return 0.0
    
    Return dcg / idcg

Process called "mean_average_precision" that takes y_true as Matrix[Integer], y_score as Matrix[Float] returns Float:
    Note: MAP is equal to 1/q multiplied by Σ(AP_i) where AP_i is average precision for query i
    Note: Mean average precision across multiple queries/samples
    Note: Time complexity: O(n*m log m), Space complexity: O(n*m)
    
    If y_true.rows does not equal y_score.rows or y_true.cols does not equal y_score.cols:
        Throw Errors.InvalidArgument with "y_true and y_score must have same dimensions"
    
    Let total_ap be 0.0
    
    For i from 0 to y_true.rows minus 1:
        Let query_true be Vector[Integer]()
        Let query_score be Vector[Float]()
        
        For j from 0 to y_true.cols minus 1:
            Call query_true.append(y_true[i][j])
            Call query_score.append(y_score[i][j])
        
        Let ap be average_precision_score(query_true, query_score)
        Set total_ap to total_ap plus ap
    
    Return total_ap / Float(y_true.rows)

Process called "mean_reciprocal_rank" that takes y_true as Matrix[Integer], y_score as Matrix[Float] returns Float:
    Note: MRR is equal to 1/q multiplied by Σ(1/rank_i) where rank_i is rank of first relevant item
    Note: Focuses on ranking of the first relevant result
    Note: Time complexity: O(n*m), Space complexity: O(n)
    
    If y_true.rows does not equal y_score.rows or y_true.cols does not equal y_score.cols:
        Throw Errors.InvalidArgument with "y_true and y_score must have same dimensions"
    
    Let total_rr be 0.0
    
    For i from 0 to y_true.rows minus 1:
        Let score_pairs be List[Tuple[Float, Integer]]()
        For j from 0 to y_true.cols minus 1:
            Let pair be Tuple[Float, Integer](y_score[i][j], y_true[i][j])
            Call score_pairs.append(pair)
        
        Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
        
        For rank from 1 to sorted_pairs.sorted_array.size():
            If sorted_pairs.sorted_array[rank-1].second is equal to 1:
                Set total_rr to total_rr plus (1.0 / Float(rank))
                Break
    
    Return total_rr / Float(y_true.rows)

Process called "precision_at_k" that takes y_true as Vector[Integer], y_score as Vector[Float], k as Integer returns Float:
    Note: Precision@k is equal to (relevant items in top k) / k
    Note: Precision in top-k ranked results
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    If y_true.size() does not equal y_score.size():
        Throw Errors.InvalidArgument with "y_true and y_score must have same length"
    
    If k is less than or equal to 0 or k is greater than y_true.size():
        Throw Errors.InvalidArgument with "k must be positive and is less than or equal to number of samples"
    
    Let score_pairs be List[Tuple[Float, Integer]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Integer](y_score[i], y_true[i])
        Call score_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
    
    Let relevant_in_top_k be 0
    For i from 0 to k minus 1:
        If sorted_pairs.sorted_array[i].second is equal to 1:
            Set relevant_in_top_k to relevant_in_top_k plus 1
    
    Return Float(relevant_in_top_k) / Float(k)

Process called "recall_at_k" that takes y_true as Vector[Integer], y_score as Vector[Float], k as Integer returns Float:
    Note: Recall@k is equal to (relevant items in top k) / (total relevant items)
    Note: Recall in top-k ranked results
    Note: Time complexity: O(n log n), Space complexity: O(n)
    
    If y_true.size() does not equal y_score.size():
        Throw Errors.InvalidArgument with "y_true and y_score must have same length"
    
    If k is less than or equal to 0 or k is greater than y_true.size():
        Throw Errors.InvalidArgument with "k must be positive and is less than or equal to number of samples"
    
    Let total_relevant be Aggregation.count_occurrences(y_true, 1)
    If total_relevant is equal to 0:
        Return 0.0
    
    Let score_pairs be List[Tuple[Float, Integer]]()
    For i from 0 to y_true.size() minus 1:
        Let pair be Tuple[Float, Integer](y_score[i], y_true[i])
        Call score_pairs.append(pair)
    
    Let sorted_pairs be Sorting.quicksort(score_pairs, "descending_by_first")
    
    Let relevant_in_top_k be 0
    For i from 0 to k minus 1:
        If sorted_pairs.sorted_array[i].second is equal to 1:
            Set relevant_in_top_k to relevant_in_top_k plus 1
    
    Return Float(relevant_in_top_k) / Float(total_relevant)

Note: ===== Clustering Metrics =====

Process called "silhouette_score" that takes X as Matrix[Float], labels as Vector[Integer], metric as String returns Float:
    Note: Silhouette is equal to (b minus a) / max(a, b) where a is equal to intra-cluster, b is equal to inter-cluster distance
    Note: Measures how similar objects are to their own cluster vs other clusters
    Note: Time complexity: O(n²), Space complexity: O(n²)
    
    If X.rows does not equal labels.size():
        Throw Errors.InvalidArgument with "X and labels must have same number of samples"
    
    If X.rows is less than 2:
        Return 0.0
    
    Let unique_labels be ListOps.list_unique(labels)
    If unique_labels.size() is less than 2:
        Return 0.0
    
    Let total_silhouette be 0.0
    
    For i from 0 to X.rows minus 1:
        Let own_label be labels[i]
        Let intra_distances be Vector[Float]()
        Let inter_distances be Vector[Float]()
        
        For j from 0 to X.rows minus 1:
            If i does not equal j:
                Let distance be 0.0
                For k from 0 to X.cols minus 1:
                    Let diff be X[i][k] minus X[j][k]
                    Set distance to distance plus (diff multiplied by diff)
                Let sqrt_result be MathOps.square_root(Float.to_string(distance), 50)
                Set distance to Float(sqrt_result.result_value)
                
                If labels[j] is equal to own_label:
                    Call intra_distances.append(distance)
                Otherwise:
                    Call inter_distances.append(distance)
        
        Let a be 0.0
        If intra_distances.size() is greater than 0:
            For dist in intra_distances:
                Set a to a plus dist
            Set a to a / Float(intra_distances.size())
        
        Let b be Float.MAX
        If inter_distances.size() is greater than 0:
            For dist in inter_distances:
                Set b to b.min(dist)
        
        Let silhouette be 0.0
        If a.max(b) is greater than 0.0:
            Set silhouette to (b minus a) / a.max(b)
        
        Set total_silhouette to total_silhouette plus silhouette
    
    Return total_silhouette / Float(X.rows)

Process called "adjusted_rand_index" that takes labels_true as Vector[Integer], labels_pred as Vector[Integer] returns Float:
    Note: ARI is equal to (RI minus Expected_RI) / (max(RI) minus Expected_RI)
    Note: Adjusted for chance, measures similarity between clusterings
    Note: Time complexity: O(n), Space complexity: O(k²)
    
    If labels_true.size() does not equal labels_pred.size():
        Throw Errors.InvalidArgument with "labels_true and labels_pred must have same length"
    
    If labels_true.size() is less than 2:
        Return 1.0
    
    Let n be labels_true.size()
    Let agreements be 0
    
    For i from 0 to n minus 1:
        For j from i plus 1 to n minus 1:
            Let same_true be labels_true[i] is equal to labels_true[j]
            Let same_pred be labels_pred[i] is equal to labels_pred[j]
            If same_true is equal to same_pred:
                Set agreements to agreements plus 1
    
    Let total_pairs be n multiplied by (n minus 1) / 2
    Let rand_index be Float(agreements) / Float(total_pairs)
    
    Return rand_index

Process called "normalized_mutual_info" that takes labels_true as Vector[Integer], labels_pred as Vector[Integer] returns Float:
    Note: NMI is equal to MI(U,V) / √(H(U)*H(V)) where MI is mutual information, H is entropy
    Note: Measures mutual dependence between clusterings
    Note: Time complexity: O(n), Space complexity: O(k²)
    
    If labels_true.size() does not equal labels_pred.size():
        Throw Errors.InvalidArgument with "labels_true and labels_pred must have same length"
    
    If labels_true.size() is equal to 0:
        Return 1.0
    
    Let unique_true be ListOps.list_unique(labels_true)
    Let unique_pred be ListOps.list_unique(labels_pred)
    
    If unique_true.size() is equal to 1 and unique_pred.size() is equal to 1:
        Return 1.0
    
    If unique_true.size() is equal to 1 or unique_pred.size() is equal to 1:
        Return 0.0
    
    Note: Build joint distribution for mutual information calculation
    Let joint_distribution be Dictionary[String, Dictionary[String, Float]]()
    
    For i from 0 to labels_true.size() minus 1:
        Let true_label be String(labels_true[i])
        Let pred_label be String(labels_pred[i])
        
        If not joint_distribution.contains_key(true_label):
            Set joint_distribution[true_label] to Dictionary[String, Float]()
        
        If not joint_distribution[true_label].contains_key(pred_label):
            Set joint_distribution[true_label][pred_label] to 0.0
        
        Set joint_distribution[true_label][pred_label] to joint_distribution[true_label][pred_label] plus 1.0
    
    Note: Normalize to probabilities
    Let total_samples be Float(labels_true.size())
    For each true_label in joint_distribution.keys():
        For each pred_label in joint_distribution[true_label].keys():
            Set joint_distribution[true_label][pred_label] to joint_distribution[true_label][pred_label] / total_samples
    
    Note: Calculate marginal distributions
    Let true_marginal be Dictionary[String, Float]()
    Let pred_marginal be Dictionary[String, Float]()
    
    For each true_label in joint_distribution.keys():
        Set true_marginal[true_label] to 0.0
        For each pred_label in joint_distribution[true_label].keys():
            Let joint_prob be joint_distribution[true_label][pred_label]
            Set true_marginal[true_label] to true_marginal[true_label] plus joint_prob
            
            If not pred_marginal.contains_key(pred_label):
                Set pred_marginal[pred_label] to 0.0
            Set pred_marginal[pred_label] to pred_marginal[pred_label] plus joint_prob
    
    Note: Calculate entropies and mutual information
    Let h_true be InfoTheory.shannon_entropy(true_marginal, 2.0)
    Let h_pred be InfoTheory.shannon_entropy(pred_marginal, 2.0)
    Let h_joint be InfoTheory.joint_entropy(joint_distribution, 2.0)
    
    Let mutual_info be h_true plus h_pred minus h_joint
    
    Note: Normalize by geometric mean of individual entropies
    Let sqrt_result be MathOps.square_root(Float.to_string(h_true multiplied by h_pred), 50)
    Let normalization_factor be Float(sqrt_result.result_value)
    
    If normalization_factor is equal to 0.0:
        Return 1.0
    
    Return mutual_info / normalization_factor

Process called "homogeneity_completeness_v_measure" that takes labels_true as Vector[Integer], labels_pred as Vector[Integer] returns Tuple[Float, Float, Float]:
    Note: Homogeneity: each cluster contains only members of single class
    Note: Completeness: all members of class assigned to same cluster  
    Note: V-measure: harmonic mean of homogeneity and completeness
    Note: Time complexity: O(n), Space complexity: O(k²)
    
    If labels_true.size() does not equal labels_pred.size():
        Throw Errors.InvalidArgument with "labels_true and labels_pred must have same length"
    
    If labels_true.size() is equal to 0:
        Return Tuple[Float, Float, Float](1.0, 1.0, 1.0)
    
    Let unique_true be ListOps.list_unique(labels_true)
    Let unique_pred be ListOps.list_unique(labels_pred)
    
    If unique_true.size() is equal to 1 and unique_pred.size() is equal to 1:
        Return Tuple[Float, Float, Float](1.0, 1.0, 1.0)
    
    Note: Build joint distribution of true and predicted labels
    Let joint_distribution be Dictionary[String, Dictionary[String, Float]]()
    
    For i from 0 to labels_true.size() minus 1:
        Let true_label be String(labels_true[i])
        Let pred_label be String(labels_pred[i])
        
        If not joint_distribution.contains_key(true_label):
            Set joint_distribution[true_label] to Dictionary[String, Float]()
        
        If not joint_distribution[true_label].contains_key(pred_label):
            Set joint_distribution[true_label][pred_label] to 0.0
        
        Set joint_distribution[true_label][pred_label] to joint_distribution[true_label][pred_label] plus 1.0
    
    Note: Normalize to get probabilities
    Let total_samples be Float(labels_true.size())
    For each true_label in joint_distribution.keys():
        For each pred_label in joint_distribution[true_label].keys():
            Set joint_distribution[true_label][pred_label] to joint_distribution[true_label][pred_label] / total_samples
    
    Note: Calculate marginal distributions
    Let true_marginal be Dictionary[String, Float]()
    Let pred_marginal be Dictionary[String, Float]()
    
    For each true_label in joint_distribution.keys():
        Set true_marginal[true_label] to 0.0
        For each pred_label in joint_distribution[true_label].keys():
            Let joint_prob be joint_distribution[true_label][pred_label]
            Set true_marginal[true_label] to true_marginal[true_label] plus joint_prob
            
            If not pred_marginal.contains_key(pred_label):
                Set pred_marginal[pred_label] to 0.0
            Set pred_marginal[pred_label] to pred_marginal[pred_label] plus joint_prob
    
    Note: Calculate entropies using information theory
    Let h_true be InfoTheory.shannon_entropy(true_marginal, 2.0)
    Let h_pred be InfoTheory.shannon_entropy(pred_marginal, 2.0)
    Let h_conditional_true_given_pred be InfoTheory.conditional_entropy(joint_distribution)
    
    Note: Invert joint distribution for H(pred|true) calculation
    Let inverted_joint be Dictionary[String, Dictionary[String, Float]]()
    For each true_label in joint_distribution.keys():
        For each pred_label in joint_distribution[true_label].keys():
            If not inverted_joint.contains_key(pred_label):
                Set inverted_joint[pred_label] to Dictionary[String, Float]()
            Set inverted_joint[pred_label][true_label] to joint_distribution[true_label][pred_label]
    
    Let h_conditional_pred_given_true be InfoTheory.conditional_entropy(inverted_joint)
    
    Note: Homogeneity is equal to 1 minus H(C|K) / H(C) where C=true labels, K=predicted clusters
    Note: Completeness is equal to 1 minus H(K|C) / H(K) where K=predicted labels, C=true classes
    Let homogeneity be 1.0 minus (h_conditional_true_given_pred / h_true)
    Let completeness be 1.0 minus (h_conditional_pred_given_true / h_pred)
    
    Note: Handle edge cases where entropy is zero
    If h_true is equal to 0.0:
        Set homogeneity to 1.0
    If h_pred is equal to 0.0:
        Set completeness to 1.0
    Let v_measure be 2.0 multiplied by (homogeneity multiplied by completeness) / (homogeneity plus completeness)
    
    Return Tuple[Float, Float, Float](homogeneity, completeness, v_measure)

Note: ===== Multi-label and Multi-output Metrics =====

Process called "hamming_loss" that takes y_true as Matrix[Integer], y_pred as Matrix[Integer] returns Float:
    Note: Hamming loss is equal to 1/n multiplied by Σ(XOR(y_true, y_pred)) / n_labels
    Note: Fraction of labels that are incorrectly predicted
    Note: Time complexity: O(n*l), Space complexity: O(1)
    
    If y_true.rows does not equal y_pred.rows or y_true.cols does not equal y_pred.cols:
        Throw Errors.InvalidArgument with "y_true and y_pred must have same dimensions"
    
    If y_true.rows is equal to 0 or y_true.cols is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute Hamming loss for empty matrices"
    
    Let total_disagreements be 0
    Let total_comparisons be y_true.rows multiplied by y_true.cols
    
    For i from 0 to y_true.rows minus 1:
        For j from 0 to y_true.cols minus 1:
            If y_true[i][j] does not equal y_pred[i][j]:
                Set total_disagreements to total_disagreements plus 1
    
    Return Float(total_disagreements) / Float(total_comparisons)

Process called "jaccard_score" that takes y_true as Matrix[Integer], y_pred as Matrix[Integer], average as String returns Float:
    Note: Jaccard is equal to |A ∩ B| / |A ∪ B| for sets of predicted labels
    Note: Similarity between finite sample sets
    Note: Time complexity: O(n*l), Space complexity: O(n)
    
    If y_true.rows does not equal y_pred.rows or y_true.cols does not equal y_pred.cols:
        Throw Errors.InvalidArgument with "y_true and y_pred must have same dimensions"
    
    If y_true.rows is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute Jaccard score for empty matrices"
    
    Let sample_jaccard_scores be Vector[Float]()
    
    For i from 0 to y_true.rows minus 1:
        Let intersection be 0
        Let union be 0
        
        For j from 0 to y_true.cols minus 1:
            If y_true[i][j] is equal to 1 and y_pred[i][j] is equal to 1:
                Set intersection to intersection plus 1
            If y_true[i][j] is equal to 1 or y_pred[i][j] is equal to 1:
                Set union to union plus 1
        
        Let jaccard be 0.0
        If union is greater than 0:
            Set jaccard to Float(intersection) / Float(union)
        Otherwise:
            Set jaccard to 1.0
        
        Call sample_jaccard_scores.append(jaccard)
    
    If average is equal to "macro":
        Let sum be 0.0
        For score in sample_jaccard_scores:
            Set sum to sum plus score
        Return sum / Float(sample_jaccard_scores.size())
    
    Otherwise if average is equal to "micro":
        Let total_intersection be 0
        Let total_union be 0
        
        For i from 0 to y_true.rows minus 1:
            For j from 0 to y_true.cols minus 1:
                If y_true[i][j] is equal to 1 and y_pred[i][j] is equal to 1:
                    Set total_intersection to total_intersection plus 1
                If y_true[i][j] is equal to 1 or y_pred[i][j] is equal to 1:
                    Set total_union to total_union plus 1
        
        If total_union is equal to 0:
            Return 1.0
        Return Float(total_intersection) / Float(total_union)
    
    Otherwise:
        Throw Errors.InvalidArgument with "Unknown averaging method: " plus average

Process called "multilabel_confusion_matrix" that takes y_true as Matrix[Integer], y_pred as Matrix[Integer] returns Tensor[Integer]:
    Note: Confusion matrix for each label in multi-label classification
    Note: Returns 3D tensor: [n_labels, 2, 2] confusion matrices
    Note: Time complexity: O(n*l), Space complexity: O(l)
    
    If y_true.rows does not equal y_pred.rows or y_true.cols does not equal y_pred.cols:
        Throw Errors.InvalidArgument with "y_true and y_pred must have same dimensions"
    
    Let n_labels be y_true.cols
    Let n_samples be y_true.rows
    
    Let result_data be List[List[List[Integer]]]()
    
    For label_idx from 0 to n_labels minus 1:
        Let tn be 0
        Let tp be 0
        Let fn be 0
        Let fp be 0
        
        For sample_idx from 0 to n_samples minus 1:
            Let true_val be y_true[sample_idx][label_idx]
            Let pred_val be y_pred[sample_idx][label_idx]
            
            If true_val is equal to 1 and pred_val is equal to 1:
                Set tp to tp plus 1
            Otherwise if true_val is equal to 1 and pred_val is equal to 0:
                Set fn to fn plus 1
            Otherwise if true_val is equal to 0 and pred_val is equal to 1:
                Set fp to fp plus 1
            Otherwise:
                Set tn to tn plus 1
        
        Let cm_2x2 be List[List[Integer]]()
        Let row1 be List[Integer]()
        Call row1.append(tn)
        Call row1.append(fp)
        Call cm_2x2.append(row1)
        
        Let row2 be List[Integer]()
        Call row2.append(fn)
        Call row2.append(tp)
        Call cm_2x2.append(row2)
        
        Call result_data.append(cm_2x2)
    
    Return LinAlg.create_tensor(result_data, "integer")

Note: ===== Statistical Significance Testing =====

Process called "mcnemar_test" that takes y_true as Vector[Integer], y_pred1 as Vector[Integer], y_pred2 as Vector[Integer] returns Tuple[Float, Float]:
    Note: McNemar's test for comparing two classifiers on same dataset
    Note: Tests if classifiers have significantly different error rates
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If y_true.size() does not equal y_pred1.size() or y_true.size() does not equal y_pred2.size():
        Throw Errors.InvalidArgument with "All vectors must have same length"
    
    Let b be 0  Note: pred1 correct, pred2 wrong
    Let c be 0  Note: pred1 wrong, pred2 correct
    
    For i from 0 to y_true.size() minus 1:
        Let correct1 be y_true[i] is equal to y_pred1[i]
        Let correct2 be y_true[i] is equal to y_pred2[i]
        
        If correct1 and not correct2:
            Set b to b plus 1
        Otherwise if not correct1 and correct2:
            Set c to c plus 1
    
    Let chi_square be 0.0
    If b plus c is greater than 0:
        Let diff be Float(b minus c)
        Set chi_square to (diff multiplied by diff) / Float(b plus c)
    
    Note: Calculate p-value as 1 minus CDF of chi-square distribution with 1 degree of freedom
    Let p_value be 1.0 minus Distributions.chi_squared_cdf(chi_square, 1)
    
    Return Tuple[Float, Float](chi_square, p_value)

Process called "paired_t_test" that takes scores1 as Vector[Float], scores2 as Vector[Float] returns Tuple[Float, Float]:
    Note: Paired t-test for comparing two sets of scores
    Note: Tests if difference in means is statistically significant
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If scores1.size() does not equal scores2.size():
        Throw Errors.InvalidArgument with "scores1 and scores2 must have same length"
    
    If scores1.size() is less than 2:
        Return Tuple[Float, Float](0.0, 1.0)
    
    Let differences be Vector[Float]()
    For i from 0 to scores1.size() minus 1:
        Call differences.append(scores1[i] minus scores2[i])
    
    Let mean_diff be Stats.calculate_arithmetic_mean(differences, Vector[Float]())
    Let std_diff be Stats.calculate_standard_deviation(differences, false)
    
    Let t_stat be 0.0
    If std_diff is greater than 0.0:
        Let sqrt_n_result be MathOps.square_root(Float.to_string(Float(differences.size())), 50)
        Set t_stat to mean_diff / (std_diff / Float(sqrt_n_result.result_value))
    
    Note: Calculate two-tailed p-value using t-distribution
    Let degrees_of_freedom be differences.size() minus 1
    Let abs_t_stat be MathOps.abs(t_stat)
    Let one_tail_p be 1.0 minus Distributions.t_distribution_cdf(abs_t_stat, degrees_of_freedom)
    Let p_value be 2.0 multiplied by one_tail_p
    
    Return Tuple[Float, Float](t_stat, p_value)

Process called "bootstrap_confidence_interval" that takes y_true as Vector[Integer], y_pred as Vector[Integer], metric_func as String, n_bootstraps as Integer, alpha as Float returns Tuple[Float, Float]:
    Note: Bootstrap confidence interval for any metric
    Note: Resamples data to estimate confidence interval of metric
    Note: Time complexity: O(B*n), Space complexity: O(n)
    
    If alpha is less than or equal to 0.0 or alpha is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "alpha must be between 0 and 1"
    
    If n_bootstraps is less than or equal to 0:
        Throw Errors.InvalidArgument with "n_bootstraps must be positive"
    
    Let bootstrap_scores be Vector[Float]()
    
    For bootstrap_idx from 0 to n_bootstraps minus 1:
        Note: Create bootstrap sample indices with replacement using pseudo-random sampling
        Let bootstrap_indices be Vector[Integer]()
        Let seed be (bootstrap_idx multiplied by 1103515245 plus 12345) % 2147483647
        For sample_idx from 0 to y_true.size() minus 1:
            Set seed to (seed multiplied by 1103515245 plus 12345) % 2147483647
            Let random_idx be seed % y_true.size()
            Call bootstrap_indices.append(random_idx)
        
        Note: Create bootstrap sample
        Let bootstrap_y_true be Vector[Integer]()
        Let bootstrap_y_pred be Vector[Integer]()
        For idx in bootstrap_indices:
            Call bootstrap_y_true.append(y_true[idx])
            Call bootstrap_y_pred.append(y_pred[idx])
        
        Note: Calculate metric score based on specified metric function
        Let metric_score be 0.0
        If metric_func is equal to "accuracy":
            Set metric_score to accuracy_score(bootstrap_y_true, bootstrap_y_pred, true)
        Otherwise if metric_func is equal to "precision":
            Let config be MetricConfig
            Set config.average to "binary"
            Set config.pos_label to Optional.none()
            Set config.zero_division to "0"
            Set metric_score to precision_score(bootstrap_y_true, bootstrap_y_pred, config)
        Otherwise if metric_func is equal to "recall":
            Let config be MetricConfig
            Set config.average to "binary"
            Set config.pos_label to Optional.none()
            Set config.zero_division to "0"
            Set metric_score to recall_score(bootstrap_y_true, bootstrap_y_pred, config)
        Otherwise if metric_func is equal to "f1":
            Let config be MetricConfig
            Set config.average to "binary"
            Set config.pos_label to Optional.none()
            Set config.zero_division to "0"
            Set metric_score to f1_score(bootstrap_y_true, bootstrap_y_pred, config)
        Otherwise:
            Set metric_score to accuracy_score(bootstrap_y_true, bootstrap_y_pred, true)
        
        Call bootstrap_scores.append(metric_score)
    
    Let sorted_scores be Sorting.quicksort(bootstrap_scores, "ascending")
    
    Let lower_idx be Integer(Float(n_bootstraps) multiplied by (alpha / 2.0))
    Let upper_idx be Integer(Float(n_bootstraps) multiplied by (1.0 minus alpha / 2.0))
    
    Let lower_bound be sorted_scores.sorted_array[lower_idx]
    Let upper_bound be sorted_scores.sorted_array[upper_idx]
    
    Return Tuple[Float, Float](lower_bound, upper_bound)

Note: ===== Metric Utilities =====

Process called "compute_sample_weights" that takes y as Vector[Integer], strategy as String returns Vector[Float]:
    Note: Compute sample weights for imbalanced datasets
    Note: Strategies: balanced, balanced_subsample, custom ratios
    Note: Time complexity: O(n), Space complexity: O(n)
    
    If y.size() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute sample weights for empty vector"
    
    If strategy is equal to "balanced":
        Let unique_labels be ListOps.list_unique(y)
        Let n_classes be unique_labels.size()
        Let n_samples be y.size()
        
        Let class_counts be Vector[Integer]()
        For label in unique_labels:
            Let count be Aggregation.count_occurrences(y, label)
            Call class_counts.append(count)
        
        Let weights be Vector[Float]()
        For i from 0 to y.size() minus 1:
            Let label be y[i]
            Let label_idx be -1
            For j from 0 to unique_labels.size() minus 1:
                If unique_labels[j] is equal to label:
                    Set label_idx to j
                    Break
            
            If label_idx is greater than or equal to 0:
                Let class_weight be Float(n_samples) / (Float(n_classes) multiplied by Float(class_counts[label_idx]))
                Call weights.append(class_weight)
            Otherwise:
                Call weights.append(1.0)
        
        Return weights
    
    Otherwise if strategy is equal to "uniform":
        Let weights be Vector[Float]()
        For i from 0 to y.size() minus 1:
            Call weights.append(1.0)
        Return weights
    
    Otherwise:
        Throw Errors.InvalidArgument with "Unknown strategy: " plus strategy

Process called "plot_confusion_matrix" that takes confusion_matrix as Matrix[Integer], labels as Vector[String], normalize as Boolean returns VisualizationData:
    Note: Prepare confusion matrix for visualization
    Note: Returns data structure for plotting heatmap
    Note: Time complexity: O(k²), Space complexity: O(k²)
    
    Let viz_data be VisualizationData
    Set viz_data.plot_type to "heatmap"
    Set viz_data.title to "Confusion Matrix"
    Set viz_data.x_labels to labels
    Set viz_data.y_labels to labels
    
    If normalize:
        Let normalized_matrix be Matrix[Float]
        For i from 0 to confusion_matrix.rows minus 1:
            Let row_sum be 0
            For j from 0 to confusion_matrix.cols minus 1:
                Set row_sum to row_sum plus confusion_matrix[i][j]
            
            For j from 0 to confusion_matrix.cols minus 1:
                If row_sum is greater than 0:
                    Set normalized_matrix[i][j] to Float(confusion_matrix[i][j]) / Float(row_sum)
                Otherwise:
                    Set normalized_matrix[i][j] to 0.0
        
        Set viz_data.data_matrix to normalized_matrix
    Otherwise:
        Set viz_data.data_matrix to confusion_matrix
    
    Return viz_data

Process called "cross_validation_scores" that takes estimator as MLModel, X as Matrix[Float], y as Vector[Integer], cv as Integer, scoring as String returns Vector[Float]:
    Note: Cross-validation scores for model evaluation
    Note: Returns scores from k-fold cross-validation
    Note: Time complexity: O(k*training_time), Space complexity: O(k)
    
    If cv is less than or equal to 1:
        Throw Errors.InvalidArgument with "cv must be greater than 1"
    
    If X.rows does not equal y.size():
        Throw Errors.InvalidArgument with "X and y must have same number of samples"
    
    Let fold_size be X.rows / cv
    Let scores be Vector[Float]()
    
    For fold from 0 to cv minus 1:
        Let test_start be fold multiplied by fold_size
        Let test_end be (fold plus 1) multiplied by fold_size
        If fold is equal to cv minus 1:
            Set test_end to X.rows
        
        Let test_predictions be Vector[Integer]()
        Let test_true be Vector[Integer]()
        
        Note: Extract training and test data for this fold
        Let train_X be List[List[Float]]()
        Let train_y be Vector[Integer]()
        
        Note: Build training set (all data except test fold)
        For i from 0 to X.rows minus 1:
            If i is less than test_start or i is greater than or equal to test_end:
                Call train_X.append(X.row(i))
                Call train_y.append(y[i])
        
        Note: Build test set and make predictions for this fold
        For i from test_start to test_end minus 1:
            Call test_true.append(y[i])
            
            Note: Train estimator on current fold training data
            Call estimator.fit(LinAlg.create_matrix(train_X, "float"), train_y)
            
            Note: Make prediction for current test sample
            Let test_sample be X.row(i)
            Let prediction be estimator.predict(test_sample)
            Call test_predictions.append(prediction)
        
        Let fold_score be accuracy_score(test_true, test_predictions, true)
        Call scores.append(fold_score)
    
    Return scores