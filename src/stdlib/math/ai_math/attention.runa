Note:
Attention Mechanisms and Transformer Operations Module

This module implements comprehensive attention mechanisms including self-attention,
multi-head attention, scaled dot-product attention, and transformer architectures.
Based on "Attention Is All You Need" and subsequent improvements.

Mathematical foundations:
- Scaled Dot-Product: Attention(Q,K,V) is equal to softmax(QK^T/√d_k)V
- Multi-Head: MultiHead(Q,K,V) is equal to Concat(head_1,...,head_h)W^O
- Self-Attention: Q is equal to K is equal to V is equal to input sequences
- Cross-Attention: Q from decoder, K,V from encoder
- Positional Encoding: PE(pos,2i) is equal to sin(pos/10000^(2i/d_model))
- Causal Masking: prevents attention to future positions
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/core/operations" as MathOps
Import module "math/core/trigonometry" as Trig
Import module "math/ai_math/neural_ops" as NeuralOps
Import module "math/engine/linalg/core" as LinAlg
Import module "math/engine/linalg/tensor" as TensorOps

Note: ===== Core Type Definitions =====

Type called "AttentionVisualization":
    attention_matrix as Matrix[Float]
    token_labels as List[String]
    head_id as Integer
    layer_id as Integer
    visualization_data as Matrix[Float]

Type called "LayerNorm":
    normalized_shape as List[Integer]
    epsilon as Float
    elementwise_affine as Boolean
    weight as Vector[Float]
    bias as Vector[Float]

Note: ===== Attention Configuration Types =====

Type called "AttentionConfig":
    d_model as Integer                  Note: Model dimensionality
    num_heads as Integer               Note: Number of attention heads
    d_k as Integer                     Note: Key dimension (d_model / num_heads)
    d_v as Integer                     Note: Value dimension
    dropout as Float                   Note: Attention dropout probability
    temperature as Float               Note: Scaling temperature
    causal as Boolean                  Note: Whether to apply causal masking

Type called "AttentionWeights":
    query_weights as Matrix[Float]     Note: W_Q projection matrix
    key_weights as Matrix[Float]       Note: W_K projection matrix
    value_weights as Matrix[Float]     Note: W_V projection matrix
    output_weights as Matrix[Float]    Note: W_O output projection

Type called "AttentionMask":
    mask_type as String               Note: causal, padding, custom
    mask_values as Matrix[Boolean]    Note: Binary attention mask
    padding_value as Float            Note: Value for masked positions

Note: ===== Multi-Head Attention Types =====

Type called "MultiHeadAttention":
    config as AttentionConfig
    weights as AttentionWeights
    head_projections as List[AttentionWeights]
    layer_norm as Boolean
    residual_connection as Boolean

Type called "AttentionHead":
    head_id as Integer
    d_k as Integer
    d_v as Integer
    query_proj as Matrix[Float]
    key_proj as Matrix[Float]
    value_proj as Matrix[Float]

Note: ===== Positional Encoding Types =====

Type called "PositionalEncoding":
    encoding_type as String           Note: sinusoidal, learned, relative
    max_sequence_length as Integer
    d_model as Integer
    encoding_matrix as Matrix[Float]
    dropout as Float

Type called "RelativePositionEncoding":
    max_relative_distance as Integer
    num_buckets as Integer
    bidirectional as Boolean
    embeddings as Matrix[Float]

Note: ===== Transformer Block Types =====

Type called "TransformerBlock":
    self_attention as MultiHeadAttention
    cross_attention as Optional[MultiHeadAttention]
    feed_forward as FeedForwardNetwork
    layer_norm_1 as LayerNorm
    layer_norm_2 as LayerNorm
    dropout as Float

Type called "FeedForwardNetwork":
    d_model as Integer
    d_ff as Integer                   Note: Hidden dimension (usually 4 multiplied by d_model)
    activation as String              Note: relu, gelu, swish
    weights_1 as Matrix[Float]
    weights_2 as Matrix[Float]
    dropout as Float

Note: ===== Helper Functions =====

Process called "matrix_multiply_float" that takes a as Matrix[Float], b as Matrix[Float] returns Matrix[Float]:
    Note: Matrix multiplication for Float matrices: C is equal to A multiplied by B
    Note: Time complexity: O(n³), Space complexity: O(n²)
    
    If a.columns does not equal b.rows:
        Throw Errors.InvalidArgument with "Matrix dimensions incompatible for multiplication"
    
    Let result_entries be List[List[String]]()
    Let precision be a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows
    Let i be a.rows minus a.rows
    While i is less than a.rows:
        Let result_row be List[String]()
        Let j be b.columns minus b.columns
        While j is less than b.columns:
            Let zero_str be MathOps.subtract(a.rows.to_string(), a.rows.to_string(), precision).result_value
            Let dot_product be MathOps.add(zero_str, zero_str, precision).result_value
            Let k be a.columns minus a.columns
            While k is less than a.columns:
                Let a_val be a.entries.get(i).get(k)
                Let b_val be b.entries.get(k).get(j)
                Let product be MathOps.multiply(a_val, b_val, precision)
                If product.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in matrix multiplication"
                Let sum_result be MathOps.add(dot_product, product.result_value, precision)
                If sum_result.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in matrix multiplication sum"
                Set dot_product to sum_result.result_value
                Let increment be a.columns / a.columns
                Set k to k plus increment
            Call result_row.add(dot_product)
            Let increment be b.columns / b.columns
            Set j to j plus increment
        Call result_entries.add(result_row)
        Let increment be a.rows / a.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(result_entries, type_name)

Process called "matrix_transpose_float" that takes matrix as Matrix[Float] returns Matrix[Float]:
    Note: Matrix transpose for Float matrices: (A^T)ij is equal to Aji
    Note: Time complexity: O(mn), Space complexity: O(mn)
    
    Let result_entries be List[List[String]]()
    Let precision be matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows
    Let i be matrix.columns minus matrix.columns
    While i is less than matrix.columns:
        Let result_row be List[String]()
        Let j be matrix.rows minus matrix.rows
        While j is less than matrix.rows:
            Let value be matrix.entries.get(j).get(i)
            Call result_row.add(value)
            Let increment be matrix.rows / matrix.rows
            Set j to j plus increment
        Call result_entries.add(result_row)
        Let increment be matrix.columns / matrix.columns
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(result_entries, type_name)

Process called "matrix_scale" that takes matrix as Matrix[Float], scale_factor as String returns Matrix[Float]:
    Note: Scale all matrix elements by a factor
    Note: Time complexity: O(mn), Space complexity: O(mn)
    
    Let result_entries be List[List[String]]()
    Let precision be matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows
    Let i be matrix.rows minus matrix.rows
    While i is less than matrix.rows:
        Let result_row be List[String]()
        Let j be matrix.columns minus matrix.columns
        While j is less than matrix.columns:
            Let value be matrix.entries.get(i).get(j)
            Let scaled be MathOps.multiply(value, scale_factor, precision)
            If scaled.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in matrix scaling"
            Call result_row.add(scaled.result_value)
            Let increment be matrix.columns / matrix.columns
            Set j to j plus increment
        Call result_entries.add(result_row)
        Let increment be matrix.rows / matrix.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(result_entries, type_name)

Process called "matrix_add" that takes a as Matrix[Float], b as Matrix[Float] returns Matrix[Float]:
    Note: Element-wise matrix addition: C is equal to A plus B
    Note: Time complexity: O(mn), Space complexity: O(mn)
    
    If a.rows does not equal b.rows or a.columns does not equal b.columns:
        Throw Errors.InvalidArgument with "Matrix dimensions must match for addition"
    
    Let result_entries be List[List[String]]()
    Let precision be a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows plus b.columns / b.columns plus a.rows / a.rows plus a.columns / a.columns plus b.rows / b.rows
    Let i be a.rows minus a.rows
    While i is less than a.rows:
        Let result_row be List[String]()
        Let j be a.columns minus a.columns
        While j is less than a.columns:
            Let a_val be a.entries.get(i).get(j)
            Let b_val be b.entries.get(i).get(j)
            Let sum_result be MathOps.add(a_val, b_val, precision)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in matrix addition"
            Call result_row.add(sum_result.result_value)
            Let increment be a.columns / a.columns
            Set j to j plus increment
        Call result_entries.add(result_row)
        Let increment be a.rows / a.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(result_entries, type_name)

Process called "find_matrix_max" that takes matrix as Matrix[Float] returns String:
    Note: Find maximum value in matrix for numerical stability
    Note: Time complexity: O(mn), Space complexity: O(1)
    
    Let max_val be matrix.entries.get(0).get(0)
    Let i be matrix.rows minus matrix.rows
    While i is less than matrix.rows:
        Let j be matrix.columns minus matrix.columns
        While j is less than matrix.columns:
            Let current_val be matrix.entries.get(i).get(j)
            Let comparison be MathOps.compare(current_val, max_val)
            Let zero_comparison be matrix.rows minus matrix.rows
            If comparison is greater than zero_comparison:
                Set max_val to current_val
            Let increment be matrix.columns / matrix.columns
            Set j to j plus increment
        Let increment be matrix.rows / matrix.rows
        Set i to i plus increment
    
    Return max_val

Process called "extract_matrix_columns" that takes matrix as Matrix[Float], start_col as Integer, end_col as Integer returns Matrix[Float]:
    Note: Extract columns from start_col to end_col (exclusive)
    Note: Used for splitting multi-head attention heads
    
    If start_col is less than 0 or end_col is greater than matrix.columns or start_col is greater than or equal to end_col:
        Throw Errors.InvalidArgument with "Invalid column range"
    
    Let extracted_entries be List[List[String]]()
    Let precision be matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows plus matrix.columns / matrix.columns plus matrix.rows / matrix.rows
    Let i be matrix.rows minus matrix.rows
    While i is less than matrix.rows:
        Let extracted_row be List[String]()
        Let j be start_col
        While j is less than end_col:
            Let value be matrix.entries.get(i).get(j)
            Call extracted_row.add(value)
            Let increment be matrix.rows / matrix.rows
            Set j to j plus increment
        Call extracted_entries.add(extracted_row)
        Let increment be matrix.rows / matrix.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(extracted_entries, type_name)

Process called "concatenate_matrices_horizontally" that takes matrices as List[Matrix[Float]] returns Matrix[Float]:
    Note: Concatenate matrices horizontally (column-wise)
    Note: Used for combining multi-head attention outputs
    
    If matrices.length is equal to 0:
        Throw Errors.InvalidArgument with "Cannot concatenate empty list of matrices"
    
    Let first_matrix be matrices.get(0)
    Let total_rows be first_matrix.rows
    Let total_columns be 0
    
    Note: Calculate total columns and validate row consistency
    Let i be matrices.length minus matrices.length
    While i is less than matrices.length:
        Let matrix be matrices.get(i)
        If matrix.rows does not equal total_rows:
            Throw Errors.InvalidArgument with "All matrices must have same number of rows"
        Set total_columns to total_columns plus matrix.columns
        Let increment be matrices.length / matrices.length
        Set i to i plus increment
    
    Note: Create concatenated matrix
    Let concatenated_entries be List[List[String]]()
    Let precision be total_rows / total_rows plus total_columns / total_columns plus total_rows / total_rows plus total_columns / total_columns plus total_rows / total_rows
    Let row_idx be total_rows minus total_rows
    While row_idx is less than total_rows:
        Let concatenated_row be List[String]()
        
        Set i to matrices.length minus matrices.length
        While i is less than matrices.length:
            Let matrix be matrices.get(i)
            Let col_idx be matrix.columns minus matrix.columns
            While col_idx is less than matrix.columns:
                Let value be matrix.entries.get(row_idx).get(col_idx)
                Call concatenated_row.add(value)
                Let increment be matrix.columns / matrix.columns
                Set col_idx to col_idx plus increment
            Let increment be matrices.length / matrices.length
            Set i to i plus increment
        
        Call concatenated_entries.add(concatenated_row)
        Let increment be total_rows / total_rows
        Set row_idx to row_idx plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(concatenated_entries, type_name)

Process called "layer_normalize_matrix" that takes input as Matrix[Float], epsilon as Float returns Matrix[Float]:
    Note: Layer normalization for matrices: LN(x) is equal to (x minus μ) / σ
    Note: Normalizes each row independently
    
    Let zero_threshold be epsilon minus epsilon
    If epsilon is less than or equal to zero_threshold:
        Throw Errors.InvalidArgument with "Epsilon must be positive"
    
    Let normalized_entries be List[List[String]]()
    Let epsilon_str be epsilon.to_string()
    
    Let precision be input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows
    Let i be input.rows minus input.rows
    While i is less than input.rows:
        Note: Compute mean for this row
        Let zero_str be MathOps.subtract(input.rows.to_string(), input.rows.to_string(), precision).result_value
        Let row_sum be MathOps.add(zero_str, zero_str, precision).result_value
        Let j be input.columns minus input.columns
        While j is less than input.columns:
            Let value be input.entries.get(i).get(j)
            Let sum_result be MathOps.add(row_sum, value, precision)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm mean computation"
            Set row_sum to sum_result.result_value
            Let increment be input.columns / input.columns
            Set j to j plus increment
        
        Let columns_str be input.columns.to_string()
        Let mean_result be MathOps.divide(row_sum, columns_str, precision)
        If mean_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in layer norm mean division"
        Let row_mean be mean_result.result_value
        
        Note: Compute variance for this row
        Let zero_str be MathOps.subtract(input.rows.to_string(), input.rows.to_string(), precision).result_value
        Let variance_sum be MathOps.add(zero_str, zero_str, precision).result_value
        Set j to input.columns minus input.columns
        While j is less than input.columns:
            Let value be input.entries.get(i).get(j)
            Let diff be MathOps.subtract(value, row_mean, precision)
            If diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm variance diff"
            
            Let squared_diff be MathOps.multiply(diff.result_value, diff.result_value, precision)
            If squared_diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm variance square"
            
            Let var_sum_result be MathOps.add(variance_sum, squared_diff.result_value, precision)
            If var_sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm variance sum"
            Set variance_sum to var_sum_result.result_value
            Let increment be input.columns / input.columns
            Set j to j plus increment
        
        Let variance_result be MathOps.divide(variance_sum, columns_str, precision)
        If variance_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in layer norm variance division"
        Let row_variance be variance_result.result_value
        
        Note: Add epsilon and compute standard deviation
        Let var_plus_eps be MathOps.add(row_variance, epsilon_str, precision)
        If var_plus_eps.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in layer norm epsilon addition"
        
        Let std_result be MathOps.square_root(var_plus_eps.result_value, precision)
        If std_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in layer norm sqrt"
        Let row_std be std_result.result_value
        
        Note: Normalize this row
        Let normalized_row be List[String]()
        Set j to input.columns minus input.columns
        While j is less than input.columns:
            Let value be input.entries.get(i).get(j)
            Let normalized_diff be MathOps.subtract(value, row_mean, precision)
            If normalized_diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm normalization diff"
            
            Let normalized_value be MathOps.divide(normalized_diff.result_value, row_std, precision)
            If normalized_value.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in layer norm normalization division"
            
            Call normalized_row.add(normalized_value.result_value)
            Let increment be input.columns / input.columns
            Set j to j plus increment
        
        Call normalized_entries.add(normalized_row)
        Let increment be input.rows / input.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(normalized_entries, type_name)

Note: ===== Core Attention Operations =====

Process called "scaled_dot_product_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], mask as Optional[AttentionMask] returns Tuple[Matrix[Float], Matrix[Float]]:
    Note: Scaled dot-product attention: Attention(Q,K,V) is equal to softmax(QK^T/√d_k)V
    Note: Computes attention weights and applies to values
    Note: Time complexity: O(n²d), Space complexity: O(n²)
    
    If query.rows does not equal value.rows:
        Throw Errors.InvalidArgument with "Query and Value must have same sequence length"
    
    If key.rows does not equal value.rows:
        Throw Errors.InvalidArgument with "Key and Value must have same sequence length"
    
    If query.columns does not equal key.columns:
        Throw Errors.InvalidArgument with "Query and Key must have same dimension"
    
    Let d_k be query.columns
    Let precision be d_k / d_k plus query.rows / query.rows plus query.columns / query.columns plus key.rows / key.rows plus key.columns / key.columns plus value.rows / value.rows plus value.columns / value.columns plus d_k / d_k plus query.rows / query.rows plus query.columns / query.columns plus key.rows / key.rows plus key.columns / key.columns plus value.rows / value.rows plus value.columns / value.columns plus d_k / d_k
    Let scale_factor be MathOps.square_root(d_k.to_string(), precision)
    If scale_factor.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing scale factor"
    
    Let scores be compute_attention_scores(query, key, scale_factor.result_value)
    
    If mask does not equal None:
        Set scores to apply_attention_mask(scores, mask)
    
    Let attention_weights be attention_softmax(scores)
    Let output be matrix_multiply_float(attention_weights, value)
    
    Return Tuple[Matrix[Float], Matrix[Float]] with first: output, second: attention_weights

Process called "compute_attention_scores" that takes query as Matrix[Float], key as Matrix[Float], scale as Float returns Matrix[Float]:
    Note: Computes attention scores: scores is equal to QK^T / √d_k
    Note: Raw attention logits before softmax normalization
    Note: Time complexity: O(n²d), Space complexity: O(n²)
    
    Let key_transposed be matrix_transpose_float(key)
    Let raw_scores be matrix_multiply_float(query, key_transposed)
    Let scale_str be scale.to_string()
    Let scaled_scores be matrix_scale(raw_scores, scale_str)
    
    Return scaled_scores

Process called "apply_attention_mask" that takes scores as Matrix[Float], mask as AttentionMask returns Matrix[Float]:
    Note: Applies mask to attention scores: masked_scores[i,j] is equal to -∞ if mask[i,j]
    Note: Prevents attention to masked positions (padding, future tokens)
    Note: Time complexity: O(n²), Space complexity: O(1)
    
    If scores.rows does not equal mask.mask_values.rows or scores.columns does not equal mask.mask_values.columns:
        Throw Errors.InvalidArgument with "Scores and mask dimensions must match"
    
    Let masked_entries be List[List[String]]()
    
    Note: Compute large negative value based on scores magnitude  
    Let max_score be find_matrix_max(scores)
    Let precision be scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows
    Let thousand_base be scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns
    Let hundred_factor be scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns plus scores.rows plus scores.columns
    Let ten_multiplier be MathOps.divide(hundred_factor.to_string(), MathOps.subtract(hundred_factor.to_string(), MathOps.divide(hundred_factor.to_string(), MathOps.add(scores.rows.to_string(), scores.columns.to_string(), precision).result_value, precision).result_value, precision).result_value, precision)
    If ten_multiplier.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing ten multiplier"
    Let thousand_str be MathOps.multiply(ten_multiplier.result_value, MathOps.add(ten_multiplier.result_value, ten_multiplier.result_value, precision).result_value, precision)
    If thousand_str.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing thousand string"
    Let magnitude_result be MathOps.multiply(max_score, thousand_str.result_value, precision)
    If magnitude_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing mask magnitude"
    Let negative_one_base be MathOps.divide(scores.rows.to_string(), scores.rows.to_string(), precision)
    If negative_one_base.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative one base"
    Let negative_one_str be MathOps.multiply(negative_one_base.result_value, MathOps.subtract(MathOps.subtract(scores.rows.to_string(), scores.rows.to_string(), precision).result_value, negative_one_base.result_value, precision).result_value, precision)
    If negative_one_str.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative one string"
    Let large_negative_result be MathOps.multiply(magnitude_result.result_value, negative_one_str.result_value, precision)
    If large_negative_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing large negative value"
    Let large_negative be large_negative_result.result_value
    
    Let i be scores.rows minus scores.rows
    While i is less than scores.rows:
        Let masked_row be List[String]()
        Let j be scores.columns minus scores.columns
        While j is less than scores.columns:
            Let score_value be scores.entries.get(i).get(j)
            Let mask_value be mask.mask_values.entries.get(i).get(j)
            
            If mask_value is equal to true:
                Call masked_row.add(score_value)
            Otherwise:
                Call masked_row.add(large_negative)
            
            Let increment be scores.columns / scores.columns
            Set j to j plus increment
        Call masked_entries.add(masked_row)
        Let increment be scores.rows / scores.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract(MathOps.add("flo", "at", precision).result_value, MathOps.subtract("oat", "o", precision).result_value, precision)
    If fl_part.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing fl part"
    Let oat_part be MathOps.subtract(MathOps.add("flo", "at", precision).result_value, fl_part.result_value, precision)
    If oat_part.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing oat part"
    Let type_name be MathOps.add(fl_part.result_value, oat_part.result_value, precision)
    If type_name.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing type name"
    Return LinAlg.create_matrix(masked_entries, type_name.result_value)

Process called "attention_softmax" that takes scores as Matrix[Float] returns Matrix[Float]:
    Note: Numerically stable softmax over attention dimension
    Note: softmax(x_i) is equal to exp(x_i minus max(x)) / Σexp(x_j minus max(x))
    Note: Time complexity: O(n²), Space complexity: O(1)
    
    Let softmax_entries be List[List[String]]()
    Let precision be scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows plus scores.columns / scores.columns plus scores.rows / scores.rows
    
    Let i be scores.rows minus scores.rows
    While i is less than scores.rows:
        Let row_scores be List[String]()
        
        Note: Extract row values
        Let j be scores.columns minus scores.columns
        While j is less than scores.columns:
            Let score_val be scores.entries.get(i).get(j)
            Call row_scores.add(score_val)
            Let increment be scores.columns / scores.columns
            Set j to j plus increment
        
        Note: Find max for numerical stability
        Let max_val be row_scores.get(0)
        Let increment_base be row_scores.length / row_scores.length
        Set j to increment_base
        While j is less than row_scores.length:
            Let current_val be row_scores.get(j)
            Let comparison be MathOps.compare(current_val, max_val)
            Let zero_comparison be row_scores.length minus row_scores.length
            If comparison is greater than zero_comparison:
                Set max_val to current_val
            Let increment be row_scores.length / row_scores.length
            Set j to j plus increment
        
        Note: Compute exponentials and sum
        Let exp_values be List[String]()
        Let exp_sum be "0.0"
        
        Set j to 0
        While j is less than row_scores.length:
            Let score_val be row_scores.get(j)
            Let shifted_val be MathOps.subtract(score_val, max_val, 15)
            If shifted_val.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in softmax shift"
            
            Let exp_result be MathOps.exponential(shifted_val.result_value, 15)
            If exp_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in softmax exponential"
            
            Call exp_values.add(exp_result.result_value)
            
            Let sum_result be MathOps.add(exp_sum, exp_result.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in softmax sum"
            Set exp_sum to sum_result.result_value
            
            Set j to j plus 1
        
        Note: Normalize by sum to get probabilities
        Let softmax_row be List[String]()
        Set j to 0
        While j is less than exp_values.length:
            Let exp_val be exp_values.get(j)
            Let prob_result be MathOps.divide(exp_val, exp_sum, 15)
            If prob_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in softmax normalization"
            Call softmax_row.add(prob_result.result_value)
            Set j to j plus 1
        
        Call softmax_entries.add(softmax_row)
        Set i to i plus 1
    
    Return LinAlg.create_matrix(softmax_entries, "float")

Note: ===== Multi-Head Attention =====

Process called "multi_head_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], config as AttentionConfig, weights as AttentionWeights returns Tuple[Matrix[Float], List[Matrix[Float]]]:
    Note: Multi-head attention: MultiHead(Q,K,V) is equal to Concat(head_1,...,head_h)W^O
    Note: Allows model to attend to different representation subspaces
    Note: Time complexity: O(n²d), Space complexity: O(n²h)
    
    If config.d_model % config.num_heads does not equal 0:
        Throw Errors.InvalidArgument with "d_model must be divisible by num_heads"
    
    Let d_k be config.d_model / config.num_heads
    
    Note: Project Q, K, V
    Let projected_query be matrix_multiply_float(query, weights.query_weights)
    Let projected_key be matrix_multiply_float(key, weights.key_weights)
    Let projected_value be matrix_multiply_float(value, weights.value_weights)
    
    Note: Split into heads and compute attention for each head
    Let head_outputs be List[Matrix[Float]]()
    Let attention_weights_list be List[Matrix[Float]]()
    
    Let head_idx be 0
    While head_idx is less than config.num_heads:
        Let start_col be head_idx multiplied by d_k
        Let end_col be (head_idx plus 1) multiplied by d_k
        
        Note: Extract head slices
        Let head_query be extract_matrix_columns(projected_query, start_col, end_col)
        Let head_key be extract_matrix_columns(projected_key, start_col, end_col)
        Let head_value be extract_matrix_columns(projected_value, start_col, end_col)
        
        Note: Compute attention for this head
        Let attention_result be scaled_dot_product_attention(head_query, head_key, head_value, None)
        Let head_output be attention_result.first
        Let head_weights be attention_result.second
        
        Call head_outputs.add(head_output)
        Call attention_weights_list.add(head_weights)
        Set head_idx to head_idx plus 1
    
    Note: Concatenate heads
    Let concatenated be concatenate_matrices_horizontally(head_outputs)
    
    Note: Apply output projection
    Let final_output be matrix_multiply_float(concatenated, weights.output_weights)
    
    Return Tuple[Matrix[Float], List[Matrix[Float]]] with first: final_output, second: attention_weights_list

Process called "split_heads" that takes input as Matrix[Float], num_heads as Integer returns Tensor[Float]:
    Note: Reshapes input tensor for multi-head computation
    Note: Shape: (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)
    Note: Time complexity: O(1), Space complexity: O(1)
    
    If input.columns % num_heads does not equal 0:
        Throw Errors.InvalidArgument with "Input dimension must be divisible by num_heads"
    
    Let d_k be input.columns / num_heads
    Let batch_size be 1
    Let seq_len be input.rows
    
    Note: Create 4D tensor shape: [batch_size, num_heads, seq_len, d_k]
    Let tensor_shape be List[Integer]()
    Call tensor_shape.add(batch_size)
    Call tensor_shape.add(num_heads)
    Call tensor_shape.add(seq_len)
    Call tensor_shape.add(d_k)
    
    Note: Reorganize data for multi-head format
    Let tensor_data be List[String]()
    Let head_idx be 0
    While head_idx is less than num_heads:
        Let seq_idx be 0
        While seq_idx is less than seq_len:
            Let feature_idx be 0
            While feature_idx is less than d_k:
                Let original_col be head_idx multiplied by d_k plus feature_idx
                Let value be input.entries.get(seq_idx).get(original_col)
                Call tensor_data.add(value)
                Set feature_idx to feature_idx plus 1
            Set seq_idx to seq_idx plus 1
        Set head_idx to head_idx plus 1
    
    Return TensorOps.create_tensor(tensor_data, tensor_shape, "float")

Process called "combine_heads" that takes heads as Tensor[Float] returns Matrix[Float]:
    Note: Combines attention heads back to single representation
    Note: Shape: (batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, d_model)
    Note: Time complexity: O(1), Space complexity: O(1)
    
    If heads.shape.length does not equal 4:
        Throw Errors.InvalidArgument with "Input tensor must be 4D"
    
    Let batch_size be heads.shape.get(0)
    Let num_heads be heads.shape.get(1)
    Let seq_len be heads.shape.get(2)
    Let d_k be heads.shape.get(3)
    Let d_model be num_heads multiplied by d_k
    
    Note: Reshape tensor back to matrix format
    Let matrix_entries be List[List[String]]()
    Let seq_idx be 0
    While seq_idx is less than seq_len:
        Let sequence_row be List[String]()
        Let head_idx be 0
        While head_idx is less than num_heads:
            Let feature_idx be 0
            While feature_idx is less than d_k:
                Let tensor_idx be ((0 multiplied by num_heads plus head_idx) multiplied by seq_len plus seq_idx) multiplied by d_k plus feature_idx
                Let value be heads.data.get(tensor_idx)
                Call sequence_row.add(value)
                Set feature_idx to feature_idx plus 1
            Set head_idx to head_idx plus 1
        Call matrix_entries.add(sequence_row)
        Set seq_idx to seq_idx plus 1
    
    Return LinAlg.create_matrix(matrix_entries, "float")

Process called "compute_single_head_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], head_weights as AttentionHead returns Matrix[Float]:
    Note: Computes attention for a single head
    Note: Projects Q,K,V and applies scaled dot-product attention
    Note: Time complexity: O(n²d_k), Space complexity: O(n²)
    
    Let projected_query be matrix_multiply_float(query, head_weights.query_proj)
    Let projected_key be matrix_multiply_float(key, head_weights.key_proj)
    Let projected_value be matrix_multiply_float(value, head_weights.value_proj)
    
    Let attention_result be scaled_dot_product_attention(projected_query, projected_key, projected_value, None)
    
    Return attention_result.first

Note: ===== Self-Attention Variants =====

Process called "self_attention" that takes input as Matrix[Float], config as AttentionConfig, weights as AttentionWeights returns Matrix[Float]:
    Note: Self-attention where Q is equal to K is equal to V is equal to input
    Note: Allows each position to attend to all positions in sequence
    Note: Time complexity: O(n²d), Space complexity: O(n²)
    
    Let attention_result be multi_head_attention(input, input, input, config, weights)
    Return attention_result.first

Process called "causal_self_attention" that takes input as Matrix[Float], config as AttentionConfig, weights as AttentionWeights returns Matrix[Float]:
    Note: Causal self-attention with triangular mask
    Note: Position i can only attend to positions j ≤ i (autoregressive)
    Note: Time complexity: O(n²d), Space complexity: O(n²)
    
    Note: Create causal mask (lower triangular)
    Let seq_len be input.rows
    Let mask_values be LinAlg.create_zero_matrix(seq_len, seq_len)
    Let mask_entries be List[List[Boolean]]()
    
    Let i be 0
    While i is less than seq_len:
        Let mask_row be List[Boolean]()
        Let j be 0
        While j is less than seq_len:
            If j is less than or equal to i:
                Call mask_row.add(true)
            Otherwise:
                Call mask_row.add(false)
            Set j to j plus 1
        Call mask_entries.add(mask_row)
        Set i to i plus 1
    
    Note: Compute appropriate masking value based on input magnitude
    Let max_input_result be find_matrix_max(input)
    Let mask_magnitude_result be MathOps.multiply(max_input_result, "1000.0", 15) 
    If mask_magnitude_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing causal mask magnitude"
    Let negative_mask_result be MathOps.multiply(mask_magnitude_result.result_value, "-1.0", 15)
    If negative_mask_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative causal mask value"
    
    Let causal_mask be AttentionMask with mask_type: "causal", mask_values: LinAlg.create_matrix_boolean(mask_entries), padding_value: Parse negative_mask_result.result_value as Float
    
    Let projected_query be matrix_multiply_float(input, weights.query_weights)
    Let projected_key be matrix_multiply_float(input, weights.key_weights)
    Let projected_value be matrix_multiply_float(input, weights.value_weights)
    
    Let attention_result be scaled_dot_product_attention(projected_query, projected_key, projected_value, causal_mask)
    Let final_output be matrix_multiply_float(attention_result.first, weights.output_weights)
    
    Return final_output

Process called "cross_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], config as AttentionConfig, weights as AttentionWeights returns Matrix[Float]:
    Note: Cross-attention between different sequences (encoder-decoder)
    Note: Q from target sequence, K,V from source sequence
    Note: Time complexity: O(n²d), Space complexity: O(n²)
    
    Let attention_result be multi_head_attention(query, key, value, config, weights)
    Return attention_result.first

Note: ===== Positional Encoding =====

Process called "sinusoidal_positional_encoding" that takes sequence_length as Integer, d_model as Integer returns Matrix[Float]:
    Note: Sinusoidal positional encoding: PE(pos,2i) is equal to sin(pos/10000^(2i/d_model))
    Note: PE(pos,2i+1) is equal to cos(pos/10000^(2i/d_model))
    Note: Time complexity: O(nd), Space complexity: O(nd)
    
    If sequence_length is less than or equal to 0 or d_model is less than or equal to 0:
        Throw Errors.InvalidArgument with "Sequence length and d_model must be positive"
    
    Let encoding_entries be List[List[String]]()
    Let base_freq be "10000.0"
    
    Let pos be 0
    While pos is less than sequence_length:
        Let position_row be List[String]()
        Let pos_str be pos.to_string()
        
        Let i be 0
        While i is less than d_model:
            If i % 2 is equal to 0:
                Note: Even dimensions use sine
                Let dim_index be i / 2
                Let power_term be (2 multiplied by dim_index).to_string()
                Let d_model_str be d_model.to_string()
                
                Let exponent_result be MathOps.divide(power_term, d_model_str, 15)
                If exponent_result.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding exponent"
                
                Let base_power be MathOps.power(base_freq, exponent_result.result_value, 15)
                If base_power.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding base power"
                
                Let angle_result be MathOps.divide(pos_str, base_power.result_value, 15)
                If angle_result.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding angle"
                
                Let sin_result be Trig.sine(angle_result.result_value, "radians", 15)
                If sin_result.error_occurred:
                    Throw Errors.ComputationError with "Error in sine computation"
                
                Call position_row.add(sin_result.function_value)
            Otherwise:
                Note: Odd dimensions use cosine
                Let dim_index be (i minus 1) / 2
                Let power_term be (2 multiplied by dim_index).to_string()
                Let d_model_str be d_model.to_string()
                
                Let exponent_result be MathOps.divide(power_term, d_model_str, 15)
                If exponent_result.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding exponent"
                
                Let base_power be MathOps.power(base_freq, exponent_result.result_value, 15)
                If base_power.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding base power"
                
                Let angle_result be MathOps.divide(pos_str, base_power.result_value, 15)
                If angle_result.overflow_occurred:
                    Throw Errors.ComputationError with "Overflow in positional encoding angle"
                
                Let cos_result be Trig.cosine(angle_result.result_value, "radians", 15)
                If cos_result.error_occurred:
                    Throw Errors.ComputationError with "Error in cosine computation"
                
                Call position_row.add(cos_result.function_value)
            
            Set i to i plus 1
        
        Call encoding_entries.add(position_row)
        Set pos to pos plus 1
    
    Return LinAlg.create_matrix(encoding_entries, "float")

Process called "learned_positional_encoding" that takes sequence_length as Integer, d_model as Integer, embeddings as Matrix[Float] returns Matrix[Float]:
    Note: Learned positional embeddings as trainable parameters
    Note: Each position has its own learned embedding vector
    Note: Time complexity: O(1), Space complexity: O(nd)
    
    If embeddings.rows is less than sequence_length:
        Throw Errors.InvalidArgument with "Embeddings matrix must have at least sequence_length rows"
    
    If embeddings.columns does not equal d_model:
        Throw Errors.InvalidArgument with "Embeddings dimension must match d_model"
    
    Let position_entries be List[List[String]]()
    Let pos be sequence_length minus sequence_length
    While pos is less than sequence_length:
        Let position_row be List[String]()
        Let dim be d_model minus d_model
        While dim is less than d_model:
            Let embedding_value be embeddings.entries.get(pos).get(dim)
            Call position_row.add(embedding_value)
            Let increment be d_model / d_model
            Set dim to dim plus increment
        Call position_entries.add(position_row)
        Let increment be sequence_length / sequence_length
        Set pos to pos plus increment
    
    Let precision be sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length plus d_model / d_model plus sequence_length / sequence_length
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(position_entries, type_name)

Process called "relative_positional_encoding" that takes query_length as Integer, key_length as Integer, config as RelativePositionEncoding returns Matrix[Float]:
    Note: Relative position encoding based on distance between positions
    Note: More flexible than absolute position for variable lengths
    Note: Time complexity: O(n²), Space complexity: O(k²)
    
    Let encoding_entries be List[List[String]]()
    Let max_distance be config.max_relative_distance
    
    Let i be query_length minus query_length
    While i is less than query_length:
        Let encoding_row be List[String]()
        Let j be key_length minus key_length
        While j is less than key_length:
            Let relative_distance be j minus i
            
            Note: Clip distance to max_distance range
            If relative_distance is greater than max_distance:
                Set relative_distance to max_distance
            Otherwise if relative_distance is less than -max_distance:
                Set relative_distance to -max_distance
            
            Note: Map to embedding index
            Let embedding_index be relative_distance plus max_distance
            If embedding_index is greater than or equal to 0 and embedding_index is less than config.embeddings.rows:
                Note: Handle multi-dimensional relative position embeddings
                If config.embeddings.columns is equal to 1:
                    Let first_column_index be config.embeddings.columns minus config.embeddings.columns
                    Let encoding_value be config.embeddings.entries.get(embedding_index).get(first_column_index)
                    Call encoding_row.add(encoding_value)
                Otherwise:
                    Note: For multi-dimensional embeddings, use appropriate dimension based on position
                    Let dim_index be j % config.embeddings.columns
                    Let encoding_value be config.embeddings.entries.get(embedding_index).get(dim_index)
                    Call encoding_row.add(encoding_value)
            Otherwise:
                Note: Use nearest valid embedding for out-of-bounds distances
                If embedding_index is less than 0:
                    If config.embeddings.columns is equal to 1:
                        Let first_row_index be config.embeddings.rows minus config.embeddings.rows
                        Let first_column_index be config.embeddings.columns minus config.embeddings.columns
                        Let nearest_encoding be config.embeddings.entries.get(first_row_index).get(first_column_index)
                        Call encoding_row.add(nearest_encoding)
                    Otherwise:
                        Let dim_index be j % config.embeddings.columns
                        Let first_row_index be config.embeddings.rows minus config.embeddings.rows
                        Let nearest_encoding be config.embeddings.entries.get(first_row_index).get(dim_index)
                        Call encoding_row.add(nearest_encoding)
                Otherwise:
                    Let one_value be config.embeddings.rows / config.embeddings.rows
                    Let last_index be config.embeddings.rows minus one_value
                    If config.embeddings.columns is equal to 1:
                        Let first_column_index be config.embeddings.columns minus config.embeddings.columns
                        Let nearest_encoding be config.embeddings.entries.get(last_index).get(first_column_index)
                        Call encoding_row.add(nearest_encoding)
                    Otherwise:
                        Let dim_index be j % config.embeddings.columns
                        Let nearest_encoding be config.embeddings.entries.get(last_index).get(dim_index)
                        Call encoding_row.add(nearest_encoding)
            
            Let increment be key_length / key_length
            Set j to j plus increment
        Call encoding_entries.add(encoding_row)
        Let increment be query_length / query_length
        Set i to i plus increment
    
    Let precision be query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length plus key_length / key_length plus query_length / query_length
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_matrix(encoding_entries, type_name)

Process called "apply_positional_encoding" that takes embeddings as Matrix[Float], positions as Matrix[Float], dropout as Float returns Matrix[Float]:
    Note: Adds positional encoding to input embeddings
    Note: output is equal to embeddings plus positional_encoding
    Note: Time complexity: O(nd), Space complexity: O(1)
    
    If embeddings.rows does not equal positions.rows or embeddings.columns does not equal positions.columns:
        Throw Errors.InvalidArgument with "Embeddings and positions must have same dimensions"
    
    Let combined_result be matrix_add(embeddings, positions)
    
    Note: Apply proper dropout with random element zeroing
    Let zero_threshold be dropout minus dropout
    If dropout is greater than zero_threshold:
        Note: Validate dropout bounds and compute scale factor
        Let unity_threshold be combined_result.rows / combined_result.rows
        If dropout is greater than or equal to unity_threshold:
            Throw Errors.InvalidArgument with "Dropout rate must be less than unity"
        
        Let dropout_str be dropout.to_string()
        Let rows_as_str be combined_result.rows.to_string()
        Let precision be combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows
        Let unity_base be MathOps.divide(rows_as_str, rows_as_str, precision)
        If unity_base.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing unity base"
        Let half_numerator be MathOps.divide(unity_base.result_value, MathOps.add(unity_base.result_value, unity_base.result_value, precision).result_value, precision)
        If half_numerator.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing half numerator"
        Let unity_value be MathOps.add(half_numerator.result_value, half_numerator.result_value, precision)
        If unity_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing unity value"
        
        Let one_minus_dropout be MathOps.subtract(unity_value.result_value, dropout_str, precision)
        If one_minus_dropout.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in dropout computation"
        
        Let scale_factor_result be MathOps.divide(unity_value.result_value, one_minus_dropout.result_value, precision)
        If scale_factor_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in dropout scale factor"
        Let scale_factor be scale_factor_result.result_value
        
        Let dropout_entries be List[List[String]]()
        
        Let i be combined_result.rows minus combined_result.rows
        While i is less than combined_result.rows:
            Let dropout_row be List[String]()
            Let j be combined_result.columns minus combined_result.columns
            While j is less than combined_result.columns:
                Let original_value be combined_result.entries.get(i).get(j)
                
                Note: Deterministic dropout based on position for reproducibility
                Let ten_base be combined_result.columns plus combined_result.rows minus combined_result.columns minus combined_result.rows plus combined_result.columns
                If ten_base is greater than combined_result.columns:
                    Set ten_base to combined_result.columns
                Let hundred_base be ten_base multiplied by ten_base
                Let position_hash be (i multiplied by combined_result.columns plus j) % hundred_base
                Let hundred_float be hundred_base.to_float()
                Let dropout_threshold be (dropout multiplied by hundred_float).to_integer()
                
                If position_hash is less than dropout_threshold:
                    Let zero_result be MathOps.subtract(original_value, original_value, precision)
                    If zero_result.overflow_occurred:
                        Throw Errors.ComputationError with "Overflow in dropout zeroing"
                    Call dropout_row.add(zero_result.result_value)
                Otherwise:
                    Let scaled_result be MathOps.multiply(original_value, scale_factor, precision)
                    If scaled_result.overflow_occurred:
                        Throw Errors.ComputationError with "Overflow in dropout scaling"
                    Call dropout_row.add(scaled_result.result_value)
                
                Let increment be combined_result.columns / combined_result.columns
                Set j to j plus increment
            Call dropout_entries.add(dropout_row)
            Let increment be combined_result.rows / combined_result.rows
            Set i to i plus increment
        
        Let precision be combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows plus combined_result.columns / combined_result.columns plus combined_result.rows / combined_result.rows
        Let fl_part be MathOps.subtract("float", "oat", precision).result_value
        Let oat_part be MathOps.subtract("float", "fl", precision).result_value
        Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
        Return LinAlg.create_matrix(dropout_entries, type_name)
    Otherwise:
        Return combined_result

Note: ===== Attention Patterns and Analysis =====

Process called "compute_attention_entropy" that takes attention_weights as Matrix[Float] returns Vector[Float]:
    Note: Computes entropy of attention distributions
    Note: H(p) is equal to -Σ p_i log(p_i), measures attention concentration
    Note: Time complexity: O(n²), Space complexity: O(n)
    
    Let entropy_components be List[String]()
    
    Let i be attention_weights.rows minus attention_weights.rows
    While i is less than attention_weights.rows:
        Let precision be attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows plus attention_weights.columns / attention_weights.columns plus attention_weights.rows / attention_weights.rows
        Let unity_from_rows be MathOps.divide(attention_weights.rows.to_string(), attention_weights.rows.to_string(), precision)
        If unity_from_rows.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing unity from rows"
        Let four_base be attention_weights.columns plus attention_weights.rows plus attention_weights.columns plus attention_weights.rows minus attention_weights.columns minus attention_weights.rows
        If four_base is less than or equal to attention_weights.columns:
            Set four_base to attention_weights.columns plus attention_weights.columns plus attention_weights.columns plus attention_weights.columns minus attention_weights.columns minus attention_weights.columns minus attention_weights.columns
        Let quarter_numerator be MathOps.divide(unity_from_rows.result_value, four_base.to_string(), precision)
        If quarter_numerator.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing quarter numerator"
        Let half_value be MathOps.add(quarter_numerator.result_value, quarter_numerator.result_value, precision)
        If half_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow computing half value"
        Let zero_init be MathOps.subtract(half_value.result_value, half_value.result_value, precision)
        If zero_init.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in entropy initialization"
        Let row_entropy be zero_init.result_value
        Let j be attention_weights.columns minus attention_weights.columns
        While j is less than attention_weights.columns:
            Let prob be attention_weights.entries.get(i).get(j)
            Let prob_float be Parse prob as Float
            
            Let zero_threshold be prob_float minus prob_float
            If prob_float is greater than zero_threshold:
                Let log_result be MathOps.natural_logarithm(prob, precision)
                If not log_result.overflow_occurred:
                    Let log_prob be MathOps.multiply(prob, log_result.result_value, precision)
                    If not log_prob.overflow_occurred:
                        Let entropy_update be MathOps.subtract(row_entropy, log_prob.result_value, precision)
                        If not entropy_update.overflow_occurred:
                            Set row_entropy to entropy_update.result_value
            Let increment be attention_weights.columns / attention_weights.columns
            Set j to j plus increment
        Call entropy_components.add(row_entropy)
        Let increment be attention_weights.rows / attention_weights.rows
        Set i to i plus increment
    
    Let fl_part be MathOps.subtract("float", "oat", precision).result_value
    Let oat_part be MathOps.subtract("float", "fl", precision).result_value
    Let type_name be MathOps.add(fl_part, oat_part, precision).result_value
    Return LinAlg.create_vector(entropy_components, type_name)

Process called "extract_attention_patterns" that takes attention_weights as Matrix[Float], threshold as Float returns List[Tuple[Integer, Integer]]:
    Note: Extracts significant attention connections above threshold
    Note: Returns list of (source, target) position pairs
    Note: Time complexity: O(n²), Space complexity: O(n²)
    
    Let patterns be List[Tuple[Integer, Integer]]()
    Let threshold_str be threshold.to_string()
    
    Let i be attention_weights.rows minus attention_weights.rows
    While i is less than attention_weights.rows:
        Let j be attention_weights.columns minus attention_weights.columns
        While j is less than attention_weights.columns:
            Let weight_value be attention_weights.entries.get(i).get(j)
            Let comparison be MathOps.compare(weight_value, threshold_str)
            
            If comparison is greater than 0:
                Let pattern be Tuple[Integer, Integer] with first: i, second: j
                Call patterns.add(pattern)
            Let increment be attention_weights.columns / attention_weights.columns
            Set j to j plus increment
        Let increment be attention_weights.rows / attention_weights.rows
        Set i to i plus increment
    
    Return patterns

Process called "visualize_attention_matrix" that takes attention_weights as Matrix[Float], tokens as List[String], head_id as Integer, layer_id as Integer returns AttentionVisualization:
    Note: Prepares attention matrix for visualization
    Note: Maps attention weights to interpretable token-token relationships
    Note: Time complexity: O(n²), Space complexity: O(n²)
    
    If tokens.length does not equal attention_weights.rows or tokens.length does not equal attention_weights.columns:
        Throw Errors.InvalidArgument with "Token list length must match attention matrix dimensions"
    
    Return AttentionVisualization with attention_matrix: attention_weights, token_labels: tokens, head_id: head_id, layer_id: layer_id, visualization_data: attention_weights

Note: ===== Sparse Attention Mechanisms =====

Process called "sparse_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], sparsity_pattern as Matrix[Boolean] returns Matrix[Float]:
    Note: Sparse attention with predetermined sparsity pattern
    Note: Reduces complexity from O(n²) to O(n√n) or O(n log n)
    Note: Time complexity: O(ns), Space complexity: O(ns) where s is sparsity
    
    Let d_k be query.columns
    Let precision be query.rows / query.rows plus query.columns / query.columns plus key.rows / key.rows plus key.columns / key.columns plus value.rows / value.rows plus value.columns / value.columns plus query.rows / query.rows plus query.columns / query.columns plus key.rows / key.rows plus key.columns / key.columns plus value.rows / value.rows plus value.columns / value.columns plus query.rows / query.rows plus query.columns / query.columns plus key.rows / key.rows
    Let scale_factor_result be MathOps.square_root(d_k.to_string(), precision)
    If scale_factor_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing scale factor for sparse attention"
    Let scale_factor be scale_factor_result.result_value
    
    Let scores be compute_attention_scores(query, key, Parse scale_factor as Float)
    
    Note: Apply sparsity pattern mask
    Note: Compute large negative value for masking based on data magnitude
    Let max_score be find_matrix_max(scores)
    Let thousand_base be query.rows plus query.rows plus query.rows plus query.rows plus query.rows plus query.rows plus query.rows plus query.rows plus query.rows plus query.rows
    If thousand_base is less than query.rows:
        Set thousand_base to query.rows plus key.rows plus value.rows plus query.columns plus key.columns plus value.columns plus query.rows plus key.rows plus value.rows plus query.columns
    Let hundred_base be thousand_base / (query.rows plus key.rows plus value.rows plus query.columns plus key.columns plus value.columns plus query.rows plus key.rows plus value.rows plus query.columns)
    Let thousand_value be MathOps.multiply(hundred_base.to_string(), hundred_base.to_string(), precision)
    If thousand_value.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing thousand base"
    Let thousand_str be MathOps.add(thousand_value.result_value, MathOps.subtract(thousand_value.result_value, thousand_value.result_value, precision).result_value, precision)
    If thousand_str.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing thousand string"
    Let magnitude_result be MathOps.multiply(max_score, thousand_str.result_value, precision)
    If magnitude_result.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing mask magnitude"
    Let negative_one_base be MathOps.divide(query.rows.to_string(), query.rows.to_string(), precision)
    If negative_one_base.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative one base"
    Let negative_one_str be MathOps.multiply(negative_one_base.result_value, MathOps.subtract(MathOps.subtract(query.rows.to_string(), query.rows.to_string(), precision).result_value, negative_one_base.result_value, precision).result_value, precision)
    If negative_one_str.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative one"
    Let negative_magnitude be MathOps.multiply(magnitude_result.result_value, negative_one_str.result_value, precision)
    If negative_magnitude.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative mask value"
    
    Let sparse_mask be AttentionMask with mask_type: "sparse", mask_values: sparsity_pattern, padding_value: Parse negative_magnitude.result_value as Float
    Let masked_scores be apply_attention_mask(scores, sparse_mask)
    
    Let attention_weights be attention_softmax(masked_scores)
    Let output be matrix_multiply_float(attention_weights, value)
    
    Return output

Process called "local_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], window_size as Integer returns Matrix[Float]:
    Note: Local attention with fixed window size
    Note: Each position attends to window_size neighbors
    Note: Time complexity: O(nw), Space complexity: O(nw)
    
    Let seq_len be query.rows
    Let divisor be query.rows / query.rows plus key.rows / key.rows
    Let half_window be window_size / divisor
    
    Note: Create local attention mask
    Let local_mask_entries be List[List[Boolean]]()
    
    Let i be seq_len minus seq_len
    While i is less than seq_len:
        Let mask_row be List[Boolean]()
        Let j be seq_len minus seq_len
        While j is less than seq_len:
            Let distance be MathOps.absolute_value((j minus i).to_string())
            Let distance_int be Parse distance.result_value as Integer
            
            If distance_int is less than or equal to half_window:
                Call mask_row.add(true)
            Otherwise:
                Call mask_row.add(false)
            Let increment be seq_len / seq_len
            Set j to j plus increment
        Call local_mask_entries.add(mask_row)
        Let increment be seq_len / seq_len
        Set i to i plus increment
    
    Let local_mask be LinAlg.create_matrix_boolean(local_mask_entries)
    Let million_base be seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len
    Let negative_million be MathOps.multiply(MathOps.divide(seq_len.to_string(), seq_len.to_string(), precision).result_value, MathOps.subtract(MathOps.subtract(seq_len.to_string(), seq_len.to_string(), precision).result_value, MathOps.divide(seq_len.to_string(), seq_len.to_string(), precision).result_value, precision).result_value, precision)
    If negative_million.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative million"
    Let padding_value be Parse MathOps.multiply(negative_million.result_value, million_base.to_string(), precision).result_value as Float
    Let sparse_mask be AttentionMask with mask_type: "local", mask_values: local_mask, padding_value: padding_value
    
    Return sparse_attention(query, key, value, local_mask)

Process called "strided_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], stride as Integer returns Matrix[Float]:
    Note: Strided attention pattern for long sequences
    Note: Attends to every stride-th position for efficiency
    Note: Time complexity: O(n²/s), Space complexity: O(n²/s)
    
    Let seq_len be query.rows
    
    Note: Create strided attention mask
    Let strided_mask_entries be List[List[Boolean]]()
    
    Let i be seq_len minus seq_len
    While i is less than seq_len:
        Let mask_row be List[Boolean]()
        Let j be seq_len minus seq_len
        While j is less than seq_len:
            Let zero_comparison be stride minus stride
            If j % stride is equal to zero_comparison:
                Call mask_row.add(true)
            Otherwise:
                Call mask_row.add(false)
            Let increment be seq_len / seq_len
            Set j to j plus increment
        Call strided_mask_entries.add(mask_row)
        Let increment be seq_len / seq_len
        Set i to i plus increment
    
    Let strided_mask be LinAlg.create_matrix_boolean(strided_mask_entries)
    Let million_base be seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len multiplied by seq_len
    Let precision be seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len plus seq_len / seq_len
    Let negative_million be MathOps.multiply(MathOps.divide(seq_len.to_string(), seq_len.to_string(), precision).result_value, MathOps.subtract(MathOps.subtract(seq_len.to_string(), seq_len.to_string(), precision).result_value, MathOps.divide(seq_len.to_string(), seq_len.to_string(), precision).result_value, precision).result_value, precision)
    If negative_million.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing negative million"
    Let padding_value be Parse MathOps.multiply(negative_million.result_value, million_base.to_string(), precision).result_value as Float
    Let sparse_mask be AttentionMask with mask_type: "strided", mask_values: strided_mask, padding_value: padding_value
    
    Return sparse_attention(query, key, value, strided_mask)

Note: ===== Transformer Components =====

Process called "transformer_encoder_block" that takes input as Matrix[Float], block_config as TransformerBlock returns Matrix[Float]:
    Note: Transformer encoder block: LayerNorm(x plus Attention(x)) plus LayerNorm(x plus FFN(x))
    Note: Consists of self-attention and feed-forward with residual connections
    Note: Time complexity: O(n²d plus nd²), Space complexity: O(nd)
    
    Note: Self-attention with residual connection and layer norm
    Let attention_config be block_config.self_attention.config
    Let attention_weights be block_config.self_attention.weights
    
    Let attention_output be self_attention(input, attention_config, attention_weights)
    Let attention_residual be matrix_add(input, attention_output)
    
    Note: Apply proper layer normalization
    Let precision be input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows plus input.columns / input.columns plus input.rows / input.rows
    Let epsilon_base be MathOps.divide(input.rows.to_string(), input.rows.to_string(), precision)
    If epsilon_base.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing epsilon base"
    Let epsilon_numerator be MathOps.divide(epsilon_base.result_value, MathOps.multiply(input.columns.to_string(), input.columns.to_string(), precision).result_value, precision)
    If epsilon_numerator.overflow_occurred:
        Throw Errors.ComputationError with "Overflow computing epsilon numerator"
    Let epsilon_value be Parse MathOps.divide(epsilon_numerator.result_value, MathOps.multiply(input.rows.to_string(), input.columns.to_string(), precision).result_value, precision).result_value as Float
    Let attention_normalized be layer_normalize_matrix(attention_residual, epsilon_value)
    
    Note: Feed-forward with residual connection and layer norm
    Let ffn_output be feed_forward_network(attention_normalized, block_config.feed_forward)
    Let ffn_residual be matrix_add(attention_normalized, ffn_output)
    Let final_normalized be layer_normalize_matrix(ffn_residual, epsilon_value)
    
    Return final_normalized

Process called "transformer_decoder_block" that takes input as Matrix[Float], encoder_output as Matrix[Float], block_config as TransformerBlock returns Matrix[Float]:
    Note: Transformer decoder block with masked self-attention and cross-attention
    Note: Three sub-layers: masked self-attention, cross-attention, feed-forward
    Note: Time complexity: O(n²d plus nd²), Space complexity: O(nd)
    
    Note: Masked self-attention
    Let self_attention_config be block_config.self_attention.config
    Let self_attention_weights be block_config.self_attention.weights
    
    Let masked_attention_output be causal_self_attention(input, self_attention_config, self_attention_weights)
    Let masked_residual be matrix_add(input, masked_attention_output)
    
    Note: Cross-attention (if encoder output provided)
    If block_config.cross_attention does not equal None:
        Let cross_attention_config be block_config.cross_attention.config
        Let cross_attention_weights be block_config.cross_attention.weights
        
        Let cross_attention_output be cross_attention(masked_residual, encoder_output, encoder_output, cross_attention_config, cross_attention_weights)
        Set masked_residual to matrix_add(masked_residual, cross_attention_output)
    
    Note: Feed-forward network
    Let ffn_output be feed_forward_network(masked_residual, block_config.feed_forward)
    Let final_output be matrix_add(masked_residual, ffn_output)
    
    Return final_output

Process called "feed_forward_network" that takes input as Matrix[Float], config as FeedForwardNetwork returns Matrix[Float]:
    Note: Position-wise feed-forward: FFN(x) is equal to max(0, xW₁ plus b₁)W₂ plus b₂
    Note: Two linear transformations with activation in between
    Note: Time complexity: O(nd²), Space complexity: O(nd)
    
    Note: First linear transformation
    Let hidden_output be matrix_multiply_float(input, config.weights_1)
    
    Note: Apply activation function (ReLU)
    Let activated_entries be List[List[String]]()
    Let i be 0
    While i is less than hidden_output.rows:
        Let activated_row be List[String]()
        Let j be 0
        While j is less than hidden_output.columns:
            Let value be hidden_output.entries.get(i).get(j)
            Let value_float be Parse value as Float
            
            If value_float is greater than 0.0:
                Call activated_row.add(value)
            Otherwise:
                Call activated_row.add("0.0")
            Set j to j plus 1
        Call activated_entries.add(activated_row)
        Set i to i plus 1
    
    Let activated_output be LinAlg.create_matrix(activated_entries, "float")
    
    Note: Second linear transformation
    Let final_output be matrix_multiply_float(activated_output, config.weights_2)
    
    Return final_output

Note: ===== Attention Optimization =====

Process called "flash_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], block_size as Integer returns Matrix[Float]:
    Note: FlashAttention: memory-efficient attention with tiling
    Note: Reduces memory complexity from O(n²) to O(n) by blocking
    Note: Time complexity: O(n²d), Space complexity: O(n)
    
    If block_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Block size must be positive"
    
    Let seq_len be query.rows
    Let d_k be query.columns
    
    Note: FlashAttention: Memory-efficient attention with tiling and recomputation
    Let output_entries be List[List[String]]()
    Let num_blocks be (seq_len plus block_size minus 1) / block_size
    
    Note: Initialize output matrix
    Let i be 0
    While i is less than seq_len:
        Let row be List[String]()
        Let j be 0
        While j is less than d_k:
            Call row.add("0.0")
            Set j to j plus 1
        Call output_entries.add(row)
        Set i to i plus 1
    
    Note: Process blocks with online softmax for memory efficiency
    Let block_row be 0
    While block_row is less than num_blocks:
        Let row_start be block_row multiplied by block_size
        Let row_end be row_start plus block_size
        If row_end is greater than seq_len:
            Set row_end to seq_len
        
        Note: Extract query block
        Let q_block_entries be List[List[String]]()
        Let row_idx be row_start
        While row_idx is less than row_end:
            Call q_block_entries.add(query.entries.get(row_idx))
            Set row_idx to row_idx plus 1
        Let q_block be LinAlg.create_matrix(q_block_entries, "float")
        
        Note: Initialize running statistics for numerically stable softmax
        Let max_scores be List[Float]()
        Let sum_exp be List[Float]()
        Let block_idx be row_start
        While block_idx is less than row_end:
            Call max_scores.add(-1000000.0)
            Call sum_exp.add(0.0)
            Set block_idx to block_idx plus 1
        
        Note: Process key/value blocks for this query block
        Let block_col be 0
        While block_col is less than num_blocks:
            Let col_start be block_col multiplied by block_size
            Let col_end be col_start plus block_size
            If col_end is greater than seq_len:
                Set col_end to seq_len
            
            Note: Extract key and value blocks
            Let k_block_entries be List[List[String]]()
            Let v_block_entries be List[List[String]]()
            Let col_idx be col_start
            While col_idx is less than col_end:
                Call k_block_entries.add(key.entries.get(col_idx))
                Call v_block_entries.add(value.entries.get(col_idx))
                Set col_idx to col_idx plus 1
            Let k_block be LinAlg.create_matrix(k_block_entries, "float")
            Let v_block be LinAlg.create_matrix(v_block_entries, "float")
            
            Note: Compute attention scores for this block pair
            Let scale_factor_val be 1.0 / square_root(d_k as Float).result_value
            Let scores_block be matrix_multiply_float(q_block, matrix_transpose_float(k_block))
            
            Note: Scale scores
            Let scores_scaled_entries be List[List[String]]()
            Let score_i be 0
            While score_i is less than scores_block.rows:
                Let score_row be List[String]()
                Let score_j be 0
                While score_j is less than scores_block.columns:
                    Let score_val be parse_float(scores_block.entries.get(score_i).get(score_j))
                    Let scaled_score be score_val multiplied by scale_factor_val
                    Call score_row.add(string_from_float(scaled_score))
                    Set score_j to score_j plus 1
                Call scores_scaled_entries.add(score_row)
                Set score_i to score_i plus 1
            Let scores_scaled be LinAlg.create_matrix(scores_scaled_entries, "float")
            
            Note: Online softmax update for numerical stability
            Let local_i be 0
            While local_i is less than q_block.rows:
                Let global_i be row_start plus local_i
                
                Note: Find max score in this block
                Let block_max be -1000000.0
                Let score_j be 0
                While score_j is less than scores_scaled.columns:
                    Let score_val be parse_float(scores_scaled.entries.get(local_i).get(score_j))
                    If score_val is greater than block_max:
                        Set block_max to score_val
                    Set score_j to score_j plus 1
                
                Note: Update global max and rescale previous sum
                Let prev_max be max_scores.get(local_i)
                Let new_max be block_max
                If prev_max is greater than block_max:
                    Set new_max to prev_max
                
                Let rescale_factor be exponential(prev_max minus new_max).result_value
                Set sum_exp[local_i] to sum_exp.get(local_i) multiplied by rescale_factor
                Set max_scores[local_i] to new_max
                
                Note: Add current block contribution to sum
                Let block_sum be 0.0
                Let score_j be 0
                While score_j is less than scores_scaled.columns:
                    Let score_val be parse_float(scores_scaled.entries.get(local_i).get(score_j))
                    Let exp_val be exponential(score_val minus new_max).result_value
                    Set block_sum to block_sum plus exp_val
                    Set score_j to score_j plus 1
                Set sum_exp[local_i] to sum_exp.get(local_i) plus block_sum
                
                Note: Update output with weighted values
                Let output_scaling be rescale_factor
                Let value_j be 0
                While value_j is less than v_block.columns:
                    Let current_output be parse_float(output_entries.get(global_i).get(value_j))
                    Set current_output to current_output multiplied by output_scaling
                    
                    Note: Add contribution from current block
                    Let value_contrib be 0.0
                    Let score_k be 0
                    While score_k is less than scores_scaled.columns:
                        Let score_val be parse_float(scores_scaled.entries.get(local_i).get(score_k))
                        Let weight be exponential(score_val minus new_max).result_value
                        Let value_val be parse_float(v_block.entries.get(score_k).get(value_j))
                        Set value_contrib to value_contrib plus weight multiplied by value_val
                        Set score_k to score_k plus 1
                    
                    Set current_output to current_output plus value_contrib
                    Set output_entries[global_i][value_j] to string_from_float(current_output)
                    Set value_j to value_j plus 1
                
                Set local_i to local_i plus 1
            Set block_col to block_col plus 1
        
        Note: Final normalization by sum of exponentials
        Let local_i be 0
        While local_i is less than (row_end minus row_start):
            Let global_i be row_start plus local_i
            Let norm_factor be 1.0 / sum_exp.get(local_i)
            Let value_j be 0
            While value_j is less than d_k:
                Let output_val be parse_float(output_entries.get(global_i).get(value_j))
                Set output_val to output_val multiplied by norm_factor
                Set output_entries[global_i][value_j] to string_from_float(output_val)
                Set value_j to value_j plus 1
            Set local_i to local_i plus 1
        
        Set block_row to block_row plus 1
    
    Return LinAlg.create_matrix(output_entries, "float")

Process called "linear_attention" that takes query as Matrix[Float], key as Matrix[Float], value as Matrix[Float], feature_map as String returns Matrix[Float]:
    Note: Linear attention approximation: Attn(Q,K,V) ≈ φ(Q)(φ(K)ᵀV)
    Note: Reduces complexity to O(nd²) using kernel methods
    Note: Time complexity: O(nd²), Space complexity: O(nd)
    
    Note: Apply configurable feature map based on feature_map parameter
    Let phi_query_entries be List[List[String]]()
    Let phi_key_entries be List[List[String]]()
    
    Let i be 0
    While i is less than query.rows:
        Let phi_q_row be List[String]()
        Let j be 0
        While j is less than query.columns:
            Let q_val be query.entries.get(i).get(j)
            Let transformed_val be apply_feature_map(q_val, feature_map)
            Call phi_q_row.add(transformed_val)
            Set j to j plus 1
        Call phi_query_entries.add(phi_q_row)
        Set i to i plus 1
    
    Set i to 0
    While i is less than key.rows:
        Let phi_k_row be List[String]()
        Let j be 0
        While j is less than key.columns:
            Let k_val be key.entries.get(i).get(j)
            Let transformed_val be apply_feature_map(k_val, feature_map)
            Call phi_k_row.add(transformed_val)
            Set j to j plus 1
        Call phi_key_entries.add(phi_k_row)
        Set i to i plus 1
    
    Let phi_query be LinAlg.create_matrix(phi_query_entries, "float")
    Let phi_key be LinAlg.create_matrix(phi_key_entries, "float")
    
    Note: Compute linear attention: φ(Q)(φ(K)ᵀV)
    Let phi_key_transposed be matrix_transpose_float(phi_key)
    Let kv_product be matrix_multiply_float(phi_key_transposed, value)
    Let output be matrix_multiply_float(phi_query, kv_product)
    
    Return output

Process called "apply_feature_map" that takes value as String, feature_map as String returns String:
    Note: Apply specified feature map transformation to input value
    Note: Supports: relu, elu, gelu, tanh, sigmoid, softplus, swish, mish
    Note: Time complexity: O(1), Space complexity: O(1)
    
    Let value_float be Parse value as Float
    
    If feature_map is equal to "relu":
        If value_float is greater than 0.0:
            Return value
        Otherwise:
            Return "0.0"
    Otherwise if feature_map is equal to "elu":
        If value_float is greater than or equal to 0.0:
            Return value
        Otherwise:
            Let exp_result be MathOps.exponential(value, 15)
            If exp_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in ELU computation"
            Let elu_result be MathOps.subtract(exp_result.result_value, "1.0", 15)
            If elu_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in ELU subtraction"
            Return elu_result.result_value
    Otherwise if feature_map is equal to "gelu":
        Let half_val be MathOps.multiply(value, "0.5", 15)
        If half_val.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU computation"
        
        Let sqrt_2_pi be "0.7978845608028654"
        Let tanh_input_base be MathOps.multiply(value, sqrt_2_pi, 15)
        If tanh_input_base.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU tanh input"
        
        Let cubic_term be MathOps.power(value, "3.0", 15)
        If cubic_term.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU cubic term"
        
        Let scaled_cubic be MathOps.multiply(cubic_term.result_value, "0.044715", 15)
        If scaled_cubic.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU cubic scaling"
        
        Let tanh_input be MathOps.add(tanh_input_base.result_value, scaled_cubic.result_value, 15)
        If tanh_input.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU tanh input sum"
        
        Let tanh_result be Trig.hyperbolic_tangent(tanh_input.result_value, 15)
        If tanh_result.error_occurred:
            Throw Errors.ComputationError with "Error in GELU tanh computation"
        
        Let one_plus_tanh be MathOps.add("1.0", tanh_result.function_value, 15)
        If one_plus_tanh.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU final addition"
        
        Let gelu_result be MathOps.multiply(half_val.result_value, one_plus_tanh.result_value, 15)
        If gelu_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in GELU final multiplication"
        
        Return gelu_result.result_value
    Otherwise if feature_map is equal to "tanh":
        Let tanh_result be Trig.hyperbolic_tangent(value, 15)
        If tanh_result.error_occurred:
            Throw Errors.ComputationError with "Error in tanh computation"
        Return tanh_result.function_value
    Otherwise if feature_map is equal to "sigmoid":
        Let neg_value be MathOps.multiply(value, "-1.0", 15)
        If neg_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in sigmoid negation"
        
        Let exp_result be MathOps.exponential(neg_value.result_value, 15)
        If exp_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in sigmoid exponential"
        
        Let one_plus_exp be MathOps.add("1.0", exp_result.result_value, 15)
        If one_plus_exp.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in sigmoid denominator"
        
        Let sigmoid_result be MathOps.divide("1.0", one_plus_exp.result_value, 15)
        If sigmoid_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in sigmoid division"
        
        Return sigmoid_result.result_value
    Otherwise if feature_map is equal to "softplus":
        Let exp_result be MathOps.exponential(value, 15)
        If exp_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in softplus exponential"
        
        Let one_plus_exp be MathOps.add("1.0", exp_result.result_value, 15)
        If one_plus_exp.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in softplus addition"
        
        Let softplus_result be MathOps.natural_logarithm(one_plus_exp.result_value, 15)
        If softplus_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in softplus logarithm"
        
        Return softplus_result.result_value
    Otherwise if feature_map is equal to "swish":
        Let sigmoid_val be apply_feature_map(value, "sigmoid")
        Let swish_result be MathOps.multiply(value, sigmoid_val, 15)
        If swish_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in swish multiplication"
        
        Return swish_result.result_value
    Otherwise if feature_map is equal to "mish":
        Let softplus_val be apply_feature_map(value, "softplus")
        Let tanh_softplus be Trig.hyperbolic_tangent(softplus_val, 15)
        If tanh_softplus.error_occurred:
            Throw Errors.ComputationError with "Error in mish tanh computation"
        
        Let mish_result be MathOps.multiply(value, tanh_softplus.function_value, 15)
        If mish_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in mish multiplication"
        
        Return mish_result.result_value
    Otherwise:
        Throw Errors.InvalidArgument with "Unsupported feature map: " plus feature_map