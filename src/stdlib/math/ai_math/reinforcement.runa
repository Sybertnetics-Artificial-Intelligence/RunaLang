Note: 
Reinforcement Learning Mathematics Module
 
This module provides comprehensive mathematical foundations for reinforcement
learning including value functions, policy gradients, Q-learning mathematics,
temporal difference learning, actor-critic methods, exploration strategies,
reward functions, and Bellman equations. Covers both model-free and
model-based RL algorithms.
 
Mathematical foundations:
- Bellman Equation: V(s) is equal to maxₐ Σₛ' P(s'|s,a)[R(s,a,s') plus γV(s')]
- Q-Learning: Q(s,a) ← Q(s,a) plus α[r plus γmax Q(s',a') minus Q(s,a)]
- Policy Gradient: ∇J(θ) is equal to E[∇log π(a|s,θ) Q(s,a)]
- Actor-Critic: combines value-based and policy-based methods
- Temporal Difference: V(s) ← V(s) plus α[r plus γV(s') minus V(s)]
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "security/crypto/primitives/random" as SecureRandom
Import module "math/core/operations" as MathOps
Import module "math/core/comparison" as MathCompare
Import module "math/engine/linalg/core" as LinAlg
Import module "science/ml/llm/fine_tuning/reinforcement" as RLHFTypes
Import module "science/ml/train/adversarial/detection" as AdvDetection

Note: ===== RL Environment Types =====

Type called "RLEnvironment":
    state_space as StateSpace
    action_space as ActionSpace
    reward_function as RewardFunction
    transition_function as TransitionFunction
    discount_factor as Float
    max_steps as Integer

Type called "StateSpace":
    space_type as String              Note: discrete, continuous, mixed
    dimensions as Integer             Note: State space dimensionality
    bounds as Optional[Tuple[Vector[Float], Vector[Float]]]  Note: Min/max bounds
    discrete_size as Optional[Integer]  Note: Size of discrete space

Type called "ActionSpace":
    space_type as String              Note: discrete, continuous, multi_discrete
    dimensions as Integer             Note: Action space dimensionality
    bounds as Optional[Tuple[Vector[Float], Vector[Float]]]  Note: Action bounds
    discrete_size as Optional[Integer]  Note: Number of discrete actions

Note: ===== Additional Required Types =====

Type called "NeuralNetwork":
    network_id as String
    layers as Integer
    parameters as Dictionary[String, Matrix[Float]]
    activation_function as String
    input_size as Integer
    output_size as Integer

Type called "TransitionModel":
    model_type as String
    state_dimension as Integer
    action_dimension as Integer
    parameters as Matrix[Float]
    is_deterministic as Boolean

Type called "Experience":
    state as Vector[Float]
    action as Integer
    reward as Float
    next_state as Vector[Float]
    done as Boolean
    priority as Float

Type called "RewardFunction":
    function_type as String
    parameters as Dictionary[String, Float]
    state_dependent as Boolean
    action_dependent as Boolean

Type called "TransitionFunction":
    function_type as String
    transition_matrix as Tensor[Float]
    is_stochastic as Boolean

Type called "Function":
    function_id as String
    input_dimension as Integer
    output_dimension as Integer
    function_type as String

Note: ===== Value Function Types =====

Type called "ValueFunction":
    function_type as String           Note: tabular, linear, neural
    parameters as Matrix[Float]       Note: Function parameters/weights
    state_features as Optional[FeatureExtractor]
    approximation_error as Float

Type called "QFunction":
    function_type as String           Note: tabular, linear, dqn
    parameters as Matrix[Float]       Note: Q-function parameters
    target_parameters as Optional[Matrix[Float]]  Note: Target network parameters
    update_frequency as Integer       Note: Target network update frequency

Type called "PolicyFunction":
    policy_type as String             Note: deterministic, stochastic, categorical
    parameters as Matrix[Float]       Note: Policy parameters
    action_distribution as String     Note: normal, categorical, beta
    exploration_noise as Float

Note: ===== RL Algorithm Configuration =====

Type called "QLearningConfig":
    learning_rate as Float            Note: Learning rate α
    discount_factor as Float          Note: Discount factor γ
    epsilon as Float                  Note: ε-greedy exploration
    epsilon_decay as Float            Note: Epsilon decay rate
    min_epsilon as Float              Note: Minimum epsilon
    double_q as Boolean               Note: Whether to use Double Q-learning

Type called "PolicyGradientConfig":
    learning_rate as Float            Note: Policy learning rate
    discount_factor as Float          Note: Discount factor
    entropy_coefficient as Float     Note: Entropy regularization
    use_baseline as Boolean           Note: Whether to use value baseline
    normalize_advantages as Boolean   Note: Advantage normalization

Type called "ActorCriticConfig":
    actor_lr as Float                Note: Actor learning rate
    critic_lr as Float               Note: Critic learning rate
    discount_factor as Float         Note: Discount factor γ
    lambda_gae as Float              Note: GAE λ parameter
    entropy_coefficient as Float    Note: Entropy bonus
    value_loss_coefficient as Float Note: Value loss weight

Note: ===== Helper Functions =====

Process called "argmax" that takes values as Vector[Float] returns Integer:
    Note: Find index of maximum value in vector
    Note: Returns the index of the first occurrence of maximum value
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If values.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Vector cannot be empty"
    
    Let max_value be values.components.get(0)
    Let max_index be 0
    Let i be 1
    
    While i is less than values.dimension:
        Let current_value be values.components.get(i)
        Let comparison_result be MathCompare.compare(current_value, max_value, 15)
        If comparison_result is greater than 0:
            Set max_value to current_value
            Set max_index to i
        Set i to i plus 1
    
    Return max_index

Process called "generate_random_choice" that takes probabilities as Vector[Float] returns Integer:
    Note: Sample from discrete distribution using cumulative probabilities
    Note: Used for stochastic action selection in RL algorithms
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If probabilities.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Probability vector cannot be empty"
    
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    Let random_val be SecureRandom.generate_uniform_range(random_generator, 0.0, 1.0)
    Let random_str be random_val.to_string()
    
    Let cumulative_prob be "0.0"
    Let i be 0
    While i is less than probabilities.dimension:
        Let prob_value be probabilities.components.get(i)
        Let sum_result be MathOps.add(cumulative_prob, prob_value, 15)
        If sum_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in probability accumulation"
        Set cumulative_prob to sum_result.result_value
        
        Let comparison is equal to MathCompare.compare(random_str, cumulative_prob, 15)
        If comparison is less than or equal to 0:
            Return i
        Set i to i plus 1
    
    Return probabilities.dimension minus 1

Note: ===== Bellman Equations and Value Iteration =====

Process called "bellman_expectation_equation" that takes state as Integer, policy as Matrix[Float], reward_matrix as Matrix[Float], transition_matrix as Tensor[Float], discount as Float returns Float:
    Note: Bellman expectation: Vᵖ(s) is equal to Σₐ π(a|s) Σₛ' P(s'|s,a)[R(s,a,s') plus γVᵖ(s')]
    Note: Expected value of state under policy π
    Note: Time complexity: O(|S|²|A|), Space complexity: O(|S|)
    
    If state is less than 0 or state is greater than or equal to policy.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    If policy.rows does not equal reward_matrix.rows or policy.columns does not equal reward_matrix.columns:
        Throw Errors.InvalidArgument with "Policy and reward matrices must have same dimensions"
    
    Let expected_value be "0.0"
    Let num_actions be policy.columns
    
    Note: Iterate over all actions
    Let action be 0
    While action is less than num_actions:
        Let action_prob be policy.entries.get(state).get(action)
        Let action_value be "0.0"
        
        Note: Iterate over all next states
        Let next_state be 0
        While next_state is less than policy.rows:
            Note: Get transition probability P(s'|s,a)
            Let transition_idx be state multiplied by num_actions multiplied by policy.rows plus action multiplied by policy.rows plus next_state
            Let transition_prob be transition_matrix.data.get(transition_idx)
            
            Note: Get reward R(s,a,s')
            Let immediate_reward be reward_matrix.entries.get(state).get(action)
            
            Note: Get discounted future value γV(s')
            Let discount_str be discount.to_string()
            Let future_value be reward_matrix.entries.get(next_state).get(0) Note: Assuming value stored in first column
            Let discounted_future be MathOps.multiply(discount_str, future_value, 15)
            
            If not discounted_future.overflow_occurred:
                Note: R(s,a,s') plus γV(s')
                Let total_value be MathOps.add(immediate_reward, discounted_future.result_value, 15)
                If not total_value.overflow_occurred:
                    Note: P(s'|s,a) multiplied by [R(s,a,s') plus γV(s')]
                    Let weighted_value be MathOps.multiply(transition_prob, total_value.result_value, 15)
                    If not weighted_value.overflow_occurred:
                        Let sum_result be MathOps.add(action_value, weighted_value.result_value, 15)
                        If not sum_result.overflow_occurred:
                            Set action_value to sum_result.result_value
            
            Set next_state to next_state plus 1
        
        Note: π(a|s) multiplied by Σₛ' P(s'|s,a)[R(s,a,s') plus γV(s')]
        Let policy_weighted be MathOps.multiply(action_prob, action_value, 15)
        If not policy_weighted.overflow_occurred:
            Let total_sum be MathOps.add(expected_value, policy_weighted.result_value, 15)
            If not total_sum.overflow_occurred:
                Set expected_value to total_sum.result_value
        
        Set action to action plus 1
    
    Let result_float be Parse expected_value as Float
    Return result_float

Process called "bellman_optimality_equation" that takes state as Integer, q_values as Matrix[Float], reward_matrix as Matrix[Float], transition_matrix as Tensor[Float], discount as Float returns Float:
    Note: Bellman optimality: V*(s) is equal to maxₐ Σₛ' P(s'|s,a)[R(s,a,s') plus γV*(s')]
    Note: Optimal value function satisfying Bellman optimality
    Note: Time complexity: O(|S||A|), Space complexity: O(1)
    
    If state is less than 0 or state is greater than or equal to q_values.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    If q_values.rows does not equal reward_matrix.rows or q_values.columns does not equal reward_matrix.columns:
        Throw Errors.InvalidArgument with "Q-values and reward matrices must have same dimensions"
    
    Let max_action_value be "-999999.0"
    Let num_actions be q_values.columns
    
    Note: Compute value for each action and take maximum
    Let action be 0
    While action is less than num_actions:
        Let action_value be "0.0"
        
        Note: Iterate over all next states
        Let next_state be 0
        While next_state is less than q_values.rows:
            Note: Get transition probability P(s'|s,a)
            Let transition_idx be state multiplied by num_actions multiplied by q_values.rows plus action multiplied by q_values.rows plus next_state
            Let transition_prob be transition_matrix.data.get(transition_idx)
            
            Note: Get immediate reward R(s,a,s')
            Let immediate_reward be reward_matrix.entries.get(state).get(action)
            
            Note: Get optimal value of next state V*(s')
            Let next_state_q_values be LinAlg.create_vector(q_values.entries.get(next_state), "float")
            Let next_state_optimal_action be argmax(next_state_q_values)
            Let next_state_value be q_values.entries.get(next_state).get(next_state_optimal_action)
            
            Note: Compute discounted future value γV*(s')
            Let discount_str be discount.to_string()
            Let discounted_future be MathOps.multiply(discount_str, next_state_value, 15)
            
            If not discounted_future.overflow_occurred:
                Note: R(s,a,s') plus γV*(s')
                Let total_value be MathOps.add(immediate_reward, discounted_future.result_value, 15)
                If not total_value.overflow_occurred:
                    Note: P(s'|s,a) multiplied by [R(s,a,s') plus γV*(s')]
                    Let weighted_value be MathOps.multiply(transition_prob, total_value.result_value, 15)
                    If not weighted_value.overflow_occurred:
                        Let sum_result be MathOps.add(action_value, weighted_value.result_value, 15)
                        If not sum_result.overflow_occurred:
                            Set action_value to sum_result.result_value
            
            Set next_state to next_state plus 1
        
        Note: Update maximum if this action is better
        Set max_action_value to MathCompare.maximum(max_action_value, action_value, 15)
        Set action to action plus 1
    
    Let result_float be Parse max_action_value as Float
    Return result_float

Process called "value_iteration" that takes reward_matrix as Matrix[Float], transition_matrix as Tensor[Float], discount as Float, tolerance as Float returns ValueFunction:
    Note: Value iteration: repeatedly apply Bellman optimality operator
    Note: Converges to optimal value function V*
    Note: Time complexity: O(k|S|²|A|), Space complexity: O(|S|)
    
    If tolerance is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Tolerance must be positive"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let num_states be reward_matrix.rows
    Let num_actions be reward_matrix.columns
    
    Note: Initialize value function with zeros
    Let value_params_entries be List[List[String]]()
    Let i be 0
    While i is less than num_states:
        Let row be List[String]()
        Let j be 0
        While j is less than 1:  Note: Value function is a vector (single column)
            Call row.add("0.0")
            Set j to j plus 1
        Call value_params_entries.add(row)
        Set i to i plus 1
    
    Let current_values be LinAlg.create_matrix(value_params_entries, "float")
    Let max_iterations be 1000
    Let iteration be 0
    
    While iteration is less than max_iterations:
        Let new_values_entries be List[List[String]]()
        Let max_change be "0.0"
        
        Set i to 0
        While i is less than num_states:
            Note: Apply Bellman optimality operator
            Let max_value be "-999999.0"
            
            Let action be 0
            While action is less than num_actions:
                Let action_value be "0.0"
                
                Let next_state be 0
                While next_state is less than num_states:
                    Let transition_idx be i multiplied by num_actions multiplied by num_states plus action multiplied by num_states plus next_state
                    Let transition_prob be transition_matrix.data.get(transition_idx)
                    Let immediate_reward be reward_matrix.entries.get(i).get(action)
                    
                    Let old_value be current_values.entries.get(next_state).get(0)
                    Let discount_str be discount.to_string()
                    Let discounted_future be MathOps.multiply(discount_str, old_value, 15)
                    
                    If not discounted_future.overflow_occurred:
                        Let total_value be MathOps.add(immediate_reward, discounted_future.result_value, 15)
                        If not total_value.overflow_occurred:
                            Let weighted_value be MathOps.multiply(transition_prob, total_value.result_value, 15)
                            If not weighted_value.overflow_occurred:
                                Let sum_result be MathOps.add(action_value, weighted_value.result_value, 15)
                                If not sum_result.overflow_occurred:
                                    Set action_value to sum_result.result_value
                    Set next_state to next_state plus 1
                
                Set max_value to MathCompare.maximum(max_value, action_value, 15)
                Set action to action plus 1
            
            Note: Track convergence
            Let old_value be current_values.entries.get(i).get(0)
            Let change_result be MathOps.subtract(max_value, old_value, 15)
            If not change_result.overflow_occurred:
                Let abs_change be MathOps.absolute(change_result.result_value, 15)
                If not abs_change.overflow_occurred:
                    Set max_change to MathCompare.maximum(max_change, abs_change.result_value, 15)
            
            Let new_row be List[String]()
            Call new_row.add(max_value)
            Call new_values_entries.add(new_row)
            Set i to i plus 1
        
        Set current_values to LinAlg.create_matrix(new_values_entries, "float")
        
        Note: Check for convergence
        Let max_change_float be Parse max_change as Float
        If max_change_float is less than tolerance:
            Break
        
        Set iteration to iteration plus 1
    
    Return ValueFunction with function_type: "tabular", parameters: current_values, state_features: None, approximation_error: max_change_float

Process called "policy_iteration" that takes reward_matrix as Matrix[Float], transition_matrix as Tensor[Float], discount as Float returns Tuple[ValueFunction, PolicyFunction]:
    Note: Policy iteration: alternate between policy evaluation and improvement
    Note: Converges to optimal policy in finite steps
    Note: Time complexity: O(k|S|³ plus k|S|²|A|), Space complexity: O(|S|²)
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let num_states be reward_matrix.rows
    Let num_actions be reward_matrix.columns
    
    Note: Initialize uniform random policy
    Let policy_entries be List[List[String]]()
    Let uniform_prob be MathOps.divide("1.0", num_actions.to_string(), 15)
    If uniform_prob.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in uniform probability computation"
    
    Let i be 0
    While i is less than num_states:
        Let row be List[String]()
        Let j be 0
        While j is less than num_actions:
            Call row.add(uniform_prob.result_value)
            Set j to j plus 1
        Call policy_entries.add(row)
        Set i to i plus 1
    
    Let current_policy_matrix be LinAlg.create_matrix(policy_entries, "float")
    Let current_policy_function be PolicyFunction with policy_type: "stochastic", parameters: current_policy_matrix, action_distribution: "categorical", exploration_noise: 0.0
    
    Note: Initialize value function with zeros
    Let value_entries be List[List[String]]()
    Set i to 0
    While i is less than num_states:
        Let row be List[String]()
        Call row.add("0.0")
        Call value_entries.add(row)
        Set i to i plus 1
    
    Let current_values be LinAlg.create_matrix(value_entries, "float")
    Let max_iterations be 100
    Let iteration be 0
    
    While iteration is less than max_iterations:
        Note: Policy evaluation step
        Let updated_value_function be policy_evaluation(current_policy_function, reward_matrix, transition_matrix, discount, 0.001)
        Set current_values to updated_value_function.parameters
        
        Note: Policy improvement step (greedy policy improvement)
        Let improved_policy_entries be List[List[String]]()
        Set i to 0
        While i is less than num_states:
            Let best_action be 0
            Let best_value be "-999999.0"
            
            Let action be 0
            While action is less than num_actions:
                Let action_value be reward_matrix.entries.get(i).get(action)
                Set best_value to MathCompare.maximum(best_value, action_value, 15)
                If MathCompare.compare(action_value, best_value, 15) is equal to 0:
                    Set best_action to action
                Set action to action plus 1
            
            Let new_row be List[String]()
            Set action to 0
            While action is less than num_actions:
                If action is equal to best_action:
                    Call new_row.add("1.0")
                Otherwise:
                    Call new_row.add("0.0")
                Set action to action plus 1
            Call improved_policy_entries.add(new_row)
            Set i to i plus 1
        
        Let improved_policy_matrix be LinAlg.create_matrix(improved_policy_entries, "float")
        Set current_policy_function to PolicyFunction with policy_type: "deterministic", parameters: improved_policy_matrix, action_distribution: "categorical", exploration_noise: 0.0
        
        Set iteration to iteration plus 1
    
    Let final_value_function be ValueFunction with function_type: "tabular", parameters: current_values, state_features: None, approximation_error: 0.001
    
    Return Tuple[ValueFunction, PolicyFunction](final_value_function, current_policy_function)

Process called "policy_evaluation" that takes policy as PolicyFunction, reward_matrix as Matrix[Float], transition_matrix as Tensor[Float], discount as Float, tolerance as Float returns ValueFunction:
    Note: Policy evaluation: solve Vᵖ is equal to Rᵖ plus γPᵖVᵖ for given policy
    Note: Computes value function for fixed policy
    Note: Time complexity: O(k|S|³), Space complexity: O(|S|²)
    
    If tolerance is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Tolerance must be positive"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let num_states be reward_matrix.rows
    
    Note: Initialize value function with zeros
    Let value_entries be List[List[String]]()
    Let i be 0
    While i is less than num_states:
        Let row be List[String]()
        Call row.add("0.0")
        Call value_entries.add(row)
        Set i to i plus 1
    
    Let current_values be LinAlg.create_matrix(value_entries, "float")
    Let max_iterations be 1000
    Let iteration be 0
    
    While iteration is less than max_iterations:
        Let new_values_entries be List[List[String]]()
        Let max_change be "0.0"
        
        Set i to 0
        While i is less than num_states:
            Note: Compute expected value under policy
            Let expected_value be "0.0"
            
            Let action be 0
            While action is less than policy.parameters.columns:
                Let policy_prob be policy.parameters.entries.get(i).get(action)
                Let immediate_reward be reward_matrix.entries.get(i).get(action)
                
                Note: Add policy-weighted reward
                Let weighted_reward be MathOps.multiply(policy_prob, immediate_reward, 15)
                If not weighted_reward.overflow_occurred:
                    Let sum_result be MathOps.add(expected_value, weighted_reward.result_value, 15)
                    If not sum_result.overflow_occurred:
                        Set expected_value to sum_result.result_value
                
                Set action to action plus 1
            
            Note: Track convergence
            Let old_value be current_values.entries.get(i).get(0)
            Let change_result be MathOps.subtract(expected_value, old_value, 15)
            If not change_result.overflow_occurred:
                Let abs_change be MathOps.absolute(change_result.result_value, 15)
                If not abs_change.overflow_occurred:
                    Set max_change to MathCompare.maximum(max_change, abs_change.result_value, 15)
            
            Let new_row be List[String]()
            Call new_row.add(expected_value)
            Call new_values_entries.add(new_row)
            Set i to i plus 1
        
        Set current_values to LinAlg.create_matrix(new_values_entries, "float")
        
        Note: Check for convergence
        Let max_change_float be Parse max_change as Float
        If max_change_float is less than tolerance:
            Break
        
        Set iteration to iteration plus 1
    
    Return ValueFunction with function_type: "tabular", parameters: current_values, state_features: None, approximation_error: max_change_float

Note: ===== Q-Learning and Temporal Difference =====

Process called "q_learning_update" that takes q_table as Matrix[Float], state as Integer, action as Integer, reward as Float, next_state as Integer, config as QLearningConfig returns Matrix[Float]:
    Note: Q-learning update: Q(s,a) ← Q(s,a) plus α[r plus γmax Q(s',a') minus Q(s,a)]
    Note: Off-policy temporal difference control algorithm
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If state is less than 0 or state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If action is less than 0 or action is greater than or equal to q_table.columns:
        Throw Errors.InvalidArgument with "Invalid action index"
    
    If next_state is less than 0 or next_state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid next_state index"
    
    If config.learning_rate is less than or equal to 0.0 or config.learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0,1]"
    
    If config.discount_factor is less than 0.0 or config.discount_factor is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Note: Find max Q(s',a')
    Let next_state_q_values be LinAlg.create_vector(q_table.entries.get(next_state), "float")
    Let best_next_action be argmax(next_state_q_values)
    Let max_next_q_value be q_table.entries.get(next_state).get(best_next_action)
    
    Note: Current Q(s,a)
    Let current_q_value be q_table.entries.get(state).get(action)
    
    Note: Compute TD target: r plus γmax Q(s',a')
    Let reward_str be reward.to_string()
    let discount_str be config.discount_factor.to_string()
    Let discounted_future be MathOps.multiply(discount_str, max_next_q_value, 15)
    
    If discounted_future.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Q-learning future value computation"
    
    Let td_target be MathOps.add(reward_str, discounted_future.result_value, 15)
    If td_target.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Q-learning TD target computation"
    
    Note: Compute TD error: r plus γmax Q(s',a') minus Q(s,a)
    Let td_error be MathOps.subtract(td_target.result_value, current_q_value, 15)
    If td_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Q-learning TD error computation"
    
    Note: Apply update: Q(s,a) ← Q(s,a) plus α multiplied by TD_error
    Let alpha_str be config.learning_rate.to_string()
    Let alpha_times_error be MathOps.multiply(alpha_str, td_error.result_value, 15)
    If alpha_times_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Q-learning alpha scaling"
    
    Let new_q_value be MathOps.add(current_q_value, alpha_times_error.result_value, 15)
    If new_q_value.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Q-learning final update"
    
    Note: Create updated Q-table
    Let updated_entries be List[List[String]]()
    Let i be 0
    While i is less than q_table.rows:
        Let row be List[String]()
        Let j be 0
        While j is less than q_table.columns:
            If i is equal to state and j is equal to action:
                Call row.add(new_q_value.result_value)
            Otherwise:
                Let original_value be q_table.entries.get(i).get(j)
                Call row.add(original_value)
            Set j to j plus 1
        Call updated_entries.add(row)
        Set i to i plus 1
    
    Return LinAlg.create_matrix(updated_entries, "float")

Process called "sarsa_update" that takes q_table as Matrix[Float], state as Integer, action as Integer, reward as Float, next_state as Integer, next_action as Integer, learning_rate as Float, discount as Float returns Matrix[Float]:
    Note: SARSA update: Q(s,a) ← Q(s,a) plus α[r plus γQ(s',a') minus Q(s,a)]
    Note: On-policy temporal difference control algorithm
    Note: Time complexity: O(1), Space complexity: O(1)
    
    If state is less than 0 or state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If action is less than 0 or action is greater than or equal to q_table.columns:
        Throw Errors.InvalidArgument with "Invalid action index"
    
    If next_state is less than 0 or next_state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid next_state index"
    
    If next_action is less than 0 or next_action is greater than or equal to q_table.columns:
        Throw Errors.InvalidArgument with "Invalid next_action index"
    
    If learning_rate is less than or equal to 0.0 or learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0,1]"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let current_q_value be q_table.entries.get(state).get(action)
    Let next_q_value be q_table.entries.get(next_state).get(next_action)
    
    Let reward_str be reward.to_string()
    Let discount_str be discount.to_string()
    Let discounted_next be MathOps.multiply(discount_str, next_q_value, 15)
    
    If discounted_next.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in SARSA future value computation"
    
    Let td_target be MathOps.add(reward_str, discounted_next.result_value, 15)
    If td_target.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in SARSA TD target computation"
    
    Let td_error be MathOps.subtract(td_target.result_value, current_q_value, 15)
    If td_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in SARSA TD error computation"
    
    Let alpha_str be learning_rate.to_string()
    Let update_amount be MathOps.multiply(alpha_str, td_error.result_value, 15)
    If update_amount.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in SARSA update scaling"
    
    Let new_q_value be MathOps.add(current_q_value, update_amount.result_value, 15)
    If new_q_value.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in SARSA final update"
    
    Let updated_entries be List[List[String]]()
    Let i be 0
    While i is less than q_table.rows:
        Let row be List[String]()
        Let j be 0
        While j is less than q_table.columns:
            If i is equal to state and j is equal to action:
                Call row.add(new_q_value.result_value)
            Otherwise:
                Call row.add(q_table.entries.get(i).get(j))
            Set j to j plus 1
        Call updated_entries.add(row)
        Set i to i plus 1
    
    Return LinAlg.create_matrix(updated_entries, "float")

Process called "expected_sarsa_update" that takes q_table as Matrix[Float], state as Integer, action as Integer, reward as Float, next_state as Integer, policy as Vector[Float], learning_rate as Float, discount as Float returns Matrix[Float]:
    Note: Expected SARSA: uses expected value over next actions
    Note: Q(s,a) ← Q(s,a) plus α[r plus γ Σₐ' π(a'|s')Q(s',a') minus Q(s,a)]
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If state is less than 0 or state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If action is less than 0 or action is greater than or equal to q_table.columns:
        Throw Errors.InvalidArgument with "Invalid action index"
    
    If next_state is less than 0 or next_state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid next_state index"
    
    If policy.dimension does not equal q_table.columns:
        Throw Errors.InvalidArgument with "Policy dimension must match number of actions"
    
    If learning_rate is less than or equal to 0.0 or learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0,1]"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let current_q_value be q_table.entries.get(state).get(action)
    
    Note: Compute expected value Σₐ' π(a'|s')Q(s',a')
    Let expected_next_value be "0.0"
    Let a_prime be 0
    While a_prime is less than q_table.columns:
        Let policy_prob be policy.components.get(a_prime)
        Let next_q_value be q_table.entries.get(next_state).get(a_prime)
        Let weighted_value be MathOps.multiply(policy_prob, next_q_value, 15)
        If not weighted_value.overflow_occurred:
            Let sum_result be MathOps.add(expected_next_value, weighted_value.result_value, 15)
            If not sum_result.overflow_occurred:
                Set expected_next_value to sum_result.result_value
        Set a_prime to a_prime plus 1
    
    Let reward_str be reward.to_string()
    Let discount_str be discount.to_string()
    Let discounted_expected be MathOps.multiply(discount_str, expected_next_value, 15)
    
    If discounted_expected.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Expected SARSA future value computation"
    
    Let td_target be MathOps.add(reward_str, discounted_expected.result_value, 15)
    If td_target.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Expected SARSA TD target computation"
    
    Let td_error be MathOps.subtract(td_target.result_value, current_q_value, 15)
    If td_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Expected SARSA TD error computation"
    
    Let alpha_str be learning_rate.to_string()
    Let update_amount be MathOps.multiply(alpha_str, td_error.result_value, 15)
    If update_amount.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Expected SARSA update scaling"
    
    Let new_q_value be MathOps.add(current_q_value, update_amount.result_value, 15)
    If new_q_value.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Expected SARSA final update"
    
    Let updated_entries be List[List[String]]()
    Let i be 0
    While i is less than q_table.rows:
        Let row be List[String]()
        Let j be 0
        While j is less than q_table.columns:
            If i is equal to state and j is equal to action:
                Call row.add(new_q_value.result_value)
            Otherwise:
                Call row.add(q_table.entries.get(i).get(j))
            Set j to j plus 1
        Call updated_entries.add(row)
        Set i to i plus 1
    
    Return LinAlg.create_matrix(updated_entries, "float")

Process called "double_q_learning_update" that takes q1 as Matrix[Float], q2 as Matrix[Float], state as Integer, action as Integer, reward as Float, next_state as Integer, learning_rate as Float, discount as Float returns Tuple[Matrix[Float], Matrix[Float]]:
    Note: Double Q-learning: uses two Q-functions to reduce overestimation
    Note: Randomly updates one Q-function using action selected by the other
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If state is less than 0 or state is greater than or equal to q1.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If action is less than 0 or action is greater than or equal to q1.columns:
        Throw Errors.InvalidArgument with "Invalid action index"
    
    If next_state is less than 0 or next_state is greater than or equal to q1.rows:
        Throw Errors.InvalidArgument with "Invalid next_state index"
    
    If learning_rate is less than or equal to 0.0 or learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0,1]"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Note: Randomly choose which Q-function to update
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    Let random_val be SecureRandom.generate_uniform_range(random_generator, 0.0, 1.0)
    
    Let reward_str be reward.to_string()
    Let alpha_str be learning_rate.to_string()
    Let discount_str be discount.to_string()
    
    If random_val is less than 0.5:
        Note: Update Q1 using action selection from Q1 but value from Q2
        Let q1_next_values be LinAlg.create_vector(q1.entries.get(next_state), "float")
        Let best_action be argmax(q1_next_values)
        Let q2_next_value be q2.entries.get(next_state).get(best_action)
        
        Let discounted_next be MathOps.multiply(discount_str, q2_next_value, 15)
        If discounted_next.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in Double Q-learning computation"
        
        Let td_target be MathOps.add(reward_str, discounted_next.result_value, 15)
        If td_target.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in TD target"
        
        Let current_q1 be q1.entries.get(state).get(action)
        Let td_error be MathOps.subtract(td_target.result_value, current_q1, 15)
        If td_error.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in TD error"
        
        Let update_amount be MathOps.multiply(alpha_str, td_error.result_value, 15)
        If update_amount.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in update amount"
        
        Let new_q1_value be MathOps.add(current_q1, update_amount.result_value, 15)
        If new_q1_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in new Q1 value"
        
        Note: Create updated Q1
        Let updated_q1_entries be List[List[String]]()
        Let i be 0
        While i is less than q1.rows:
            Let row be List[String]()
            Let j be 0
            While j is less than q1.columns:
                If i is equal to state and j is equal to action:
                    Call row.add(new_q1_value.result_value)
                Otherwise:
                    Call row.add(q1.entries.get(i).get(j))
                Set j to j plus 1
            Call updated_q1_entries.add(row)
            Set i to i plus 1
        
        Let new_q1 be LinAlg.create_matrix(updated_q1_entries, "float")
        Return Tuple[Matrix[Float], Matrix[Float]](new_q1, q2)
    
    Otherwise:
        Note: Update Q2 using action selection from Q2 but value from Q1
        Let q2_next_values be LinAlg.create_vector(q2.entries.get(next_state), "float")
        Let best_action be argmax(q2_next_values)
        Let q1_next_value be q1.entries.get(next_state).get(best_action)
        
        Let discounted_next be MathOps.multiply(discount_str, q1_next_value, 15)
        If discounted_next.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in Double Q-learning computation"
        
        Let td_target be MathOps.add(reward_str, discounted_next.result_value, 15)
        If td_target.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in TD target"
        
        Let current_q2 be q2.entries.get(state).get(action)
        Let td_error be MathOps.subtract(td_target.result_value, current_q2, 15)
        If td_error.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in TD error"
        
        Let update_amount be MathOps.multiply(alpha_str, td_error.result_value, 15)
        If update_amount.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in update amount"
        
        Let new_q2_value be MathOps.add(current_q2, update_amount.result_value, 15)
        If new_q2_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in new Q2 value"
        
        Note: Create updated Q2
        Let updated_q2_entries be List[List[String]]()
        Let i be 0
        While i is less than q2.rows:
            Let row be List[String]()
            Let j be 0
            While j is less than q2.columns:
                If i is equal to state and j is equal to action:
                    Call row.add(new_q2_value.result_value)
                Otherwise:
                    Call row.add(q2.entries.get(i).get(j))
                Set j to j plus 1
            Call updated_q2_entries.add(row)
            Set i to i plus 1
        
        Let new_q2 be LinAlg.create_matrix(updated_q2_entries, "float")
        Return Tuple[Matrix[Float], Matrix[Float]](q1, new_q2)

Process called "td_lambda_update" that takes value_function as Vector[Float], states as Vector[Integer], rewards as Vector[Float], discount as Float, lambda as Float, learning_rate as Float returns Vector[Float]:
    Note: TD(λ): V(s) ← V(s) plus α multiplied by δ multiplied by e(s) where δ is TD error, e eligibility trace
    Note: Combines temporal difference learning with eligibility traces
    Note: Time complexity: O(T|S|), Space complexity: O(|S|)
    
    If states.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "States and rewards vectors must have same dimension"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    If lambda is less than 0.0 or lambda is greater than 1.0:
        Throw Errors.InvalidArgument with "Lambda parameter must be in [0,1]"
    
    If learning_rate is less than or equal to 0.0 or learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0,1]"
    
    Note: Initialize eligibility traces
    Let eligibility_traces_entries be List[String]()
    Let i be 0
    While i is less than value_function.dimension:
        Call eligibility_traces_entries.add("0.0")
        Set i to i plus 1
    
    Let eligibility_traces be LinAlg.create_vector(eligibility_traces_entries, "float")
    Let updated_values_entries be List[String]()
    
    Note: Copy current value function
    Set i to 0
    While i is less than value_function.dimension:
        Call updated_values_entries.add(value_function.components.get(i))
        Set i to i plus 1
    
    Let alpha_str be learning_rate.to_string()
    Let gamma_str be discount.to_string()
    Let lambda_str be lambda.to_string()
    
    Note: Process each time step
    Let t be 0
    While t is less than states.dimension minus 1:
        Let current_state be states.components.get(t)
        Let next_state be states.components.get(t plus 1)
        Let reward be rewards.components.get(t)
        
        Let current_state_int be Parse current_state as Integer
        Let next_state_int be Parse next_state as Integer
        
        If current_state_int is greater than or equal to 0 and current_state_int is less than value_function.dimension and next_state_int is greater than or equal to 0 and next_state_int is less than value_function.dimension:
            Note: Compute TD error
            Let current_value be updated_values_entries.get(current_state_int)
            Let next_value be updated_values_entries.get(next_state_int)
            
            Let discounted_next be MathOps.multiply(gamma_str, next_value, 15)
            If not discounted_next.overflow_occurred:
                Let td_target be MathOps.add(reward, discounted_next.result_value, 15)
                If not td_target.overflow_occurred:
                    Let td_error be MathOps.subtract(td_target.result_value, current_value, 15)
                    If not td_error.overflow_occurred:
                        Note: Update eligibility trace for current state
                        Let current_trace be eligibility_traces.components.get(current_state_int)
                        Let incremented_trace be MathOps.add(current_trace, "1.0", 15)
                        
                        Note: Update all states using eligibility traces
                        Set i to 0
                        While i is less than value_function.dimension:
                            Let trace_value be eligibility_traces.components.get(i)
                            Let old_value be updated_values_entries.get(i)
                            
                            Note: Update: V(s) += α multiplied by δ multiplied by e(s)
                            Let trace_times_error be MathOps.multiply(trace_value, td_error.result_value, 15)
                            If not trace_times_error.overflow_occurred:
                                Let update_amount be MathOps.multiply(alpha_str, trace_times_error.result_value, 15)
                                If not update_amount.overflow_occurred:
                                    Let new_value be MathOps.add(old_value, update_amount.result_value, 15)
                                    If not new_value.overflow_occurred:
                                        Call updated_values_entries.set(i, new_value.result_value)
                            
                            Note: Decay eligibility trace: e(s) *= γλ
                            Let gamma_lambda be MathOps.multiply(gamma_str, lambda_str, 15)
                            If not gamma_lambda.overflow_occurred:
                                Let decayed_trace be MathOps.multiply(trace_value, gamma_lambda.result_value, 15)
                                If not decayed_trace.overflow_occurred:
                                    Call eligibility_traces.components.set(i, decayed_trace.result_value)
                            
                            Set i to i plus 1
        
        Set t to t plus 1
    
    Return LinAlg.create_vector(updated_values_entries, "float")

Note: ===== Deep Q-Networks (DQN) =====

Process called "dqn_loss" that takes q_values as Matrix[Float], target_q_values as Matrix[Float], actions as Vector[Integer], rewards as Vector[Float], dones as Vector[Boolean], discount as Float returns Float:
    Note: DQN loss: L is equal to E[(r plus γmax Q_target(s',a') minus Q(s,a))²]
    Note: Mean squared Bellman error for deep Q-learning
    Note: Time complexity: O(batch_size), Space complexity: O(1)
    
    If q_values.rows does not equal target_q_values.rows:
        Throw Errors.InvalidArgument with "Q-values and target Q-values must have same batch size"
    
    If q_values.rows does not equal actions.dimension:
        Throw Errors.InvalidArgument with "Batch size mismatch between Q-values and actions"
    
    If actions.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Actions and rewards vectors must have same dimension"
    
    If actions.dimension does not equal dones.dimension:
        Throw Errors.InvalidArgument with "Actions and dones vectors must have same dimension"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let batch_size be q_values.rows
    Let total_loss be "0.0"
    Let discount_str be discount.to_string()
    
    Let i be 0
    While i is less than batch_size:
        Let action_taken be actions.components.get(i)
        Let reward be rewards.components.get(i)
        Let done be dones.components.get(i)
        
        Let action_int be Parse action_taken as Integer
        If action_int is less than 0 or action_int is greater than or equal to q_values.columns:
            Set i to i plus 1
            Continue
        
        Note: Get Q(s,a) for taken action
        Let q_value be q_values.entries.get(i).get(action_int)
        
        Note: Compute target: r plus γ multiplied by max Q_target(s',a') if not done, otherwise r
        Let target_value be reward
        If done is equal to false:
            Note: Find max target Q-value for next state
            Let target_row be target_q_values.entries.get(i)
            Let target_vector be LinAlg.create_vector(target_row, "float")
            Let best_next_action be argmax(target_vector)
            Let max_target_q be target_q_values.entries.get(i).get(best_next_action)
            
            Let discounted_future be MathOps.multiply(discount_str, max_target_q, 15)
            If not discounted_future.overflow_occurred:
                Let target_computation be MathOps.add(reward, discounted_future.result_value, 15)
                If not target_computation.overflow_occurred:
                    Set target_value to target_computation.result_value
        
        Note: Compute TD error
        Let td_error be MathOps.subtract(target_value, q_value, 15)
        If not td_error.overflow_occurred:
            Note: Squared loss
            Let squared_error be MathOps.multiply(td_error.result_value, td_error.result_value, 15)
            If not squared_error.overflow_occurred:
                Let sum_result be MathOps.add(total_loss, squared_error.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set total_loss to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Average loss over batch
    Let batch_size_str be batch_size.to_string()
    Let mean_loss be MathOps.divide(total_loss, batch_size_str, 15)
    If mean_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in DQN loss computation"
    
    Let result_float be Parse mean_loss.result_value as Float
    Return result_float

Process called "double_dqn_loss" that takes q_values as Matrix[Float], target_q_values as Matrix[Float], actions as Vector[Integer], rewards as Vector[Float], next_states as Matrix[Float], dones as Vector[Boolean], discount as Float returns Float:
    Note: Double DQN: uses main network to select action, target to evaluate
    Note: a* is equal to argmax Q(s',a'), target is equal to r plus γQ_target(s', a*)
    Note: Time complexity: O(batch_size multiplied by |A|), Space complexity: O(1)
    
    If q_values.rows does not equal target_q_values.rows:
        Throw Errors.InvalidArgument with "Q-values and target Q-values must have same batch size"
    
    If q_values.rows does not equal actions.dimension:
        Throw Errors.InvalidArgument with "Batch size mismatch between Q-values and actions"
    
    If actions.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Actions and rewards vectors must have same dimension"
    
    If actions.dimension does not equal dones.dimension:
        Throw Errors.InvalidArgument with "Actions and dones vectors must have same dimension"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let batch_size be q_values.rows
    Let total_loss be "0.0"
    Let discount_str be discount.to_string()
    
    Let i be 0
    While i is less than batch_size:
        Let action_taken be actions.components.get(i)
        Let reward be rewards.components.get(i)
        Let done be dones.components.get(i)
        
        Let action_int be Parse action_taken as Integer
        If action_int is less than 0 or action_int is greater than or equal to q_values.columns:
            Set i to i plus 1
            Continue
        
        Note: Get Q(s,a) for taken action
        Let q_value be q_values.entries.get(i).get(action_int)
        
        Note: Compute Double DQN target
        Let target_value be reward
        If done is equal to false:
            Note: Use main network to select best action: a* is equal to argmax Q(s',a')
            Let main_next_q_row be q_values.entries.get(i)  Note: Using batch index i for next state Q-values
            Let main_next_q_vector be LinAlg.create_vector(main_next_q_row, "float")
            Let best_next_action be argmax(main_next_q_vector)
            
            Note: Use target network to evaluate selected action: Q_target(s', a*)
            Let target_q_value be target_q_values.entries.get(i).get(best_next_action)
            
            Let discounted_future be MathOps.multiply(discount_str, target_q_value, 15)
            If not discounted_future.overflow_occurred:
                Let target_computation be MathOps.add(reward, discounted_future.result_value, 15)
                If not target_computation.overflow_occurred:
                    Set target_value to target_computation.result_value
        
        Note: Compute TD error and squared loss
        Let td_error be MathOps.subtract(target_value, q_value, 15)
        If not td_error.overflow_occurred:
            Let squared_error be MathOps.multiply(td_error.result_value, td_error.result_value, 15)
            If not squared_error.overflow_occurred:
                Let sum_result be MathOps.add(total_loss, squared_error.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set total_loss to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Average loss over batch
    Let batch_size_str be batch_size.to_string()
    Let mean_loss be MathOps.divide(total_loss, batch_size_str, 15)
    If mean_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Double DQN loss computation"
    
    Let result_float be Parse mean_loss.result_value as Float
    Return result_float

Process called "dueling_dqn_forward" that takes state_features as Matrix[Float], value_stream as Matrix[Float], advantage_stream as Matrix[Float] returns Matrix[Float]:
    Note: Dueling DQN: Q(s,a) is equal to V(s) plus A(s,a) minus mean(A(s,·))
    Note: Separates state value from action advantages
    Note: Time complexity: O(batch_size multiplied by |A|), Space complexity: O(1)
    
    If state_features.rows does not equal value_stream.rows:
        Throw Errors.InvalidArgument with "State features and value stream must have same batch size"
    
    If state_features.rows does not equal advantage_stream.rows:
        Throw Errors.InvalidArgument with "State features and advantage stream must have same batch size"
    
    If value_stream.columns does not equal 1:
        Throw Errors.InvalidArgument with "Value stream must have single output (one column)"
    
    Let batch_size be state_features.rows
    Let num_actions be advantage_stream.columns
    Let q_values_entries be List[List[String]]()
    
    Let i be 0
    While i is less than batch_size:
        Let state_value be value_stream.entries.get(i).get(0)
        
        Note: Compute mean advantage for this state: mean(A(s,·))
        Let advantage_sum be "0.0"
        Let j be 0
        While j is less than num_actions:
            Let advantage_value be advantage_stream.entries.get(i).get(j)
            Let sum_result be MathOps.add(advantage_sum, advantage_value, 15)
            If not sum_result.overflow_occurred:
                Set advantage_sum to sum_result.result_value
            Set j to j plus 1
        
        Let num_actions_str be num_actions.to_string()
        Let mean_advantage be MathOps.divide(advantage_sum, num_actions_str, 15)
        If mean_advantage.overflow_occurred:
            Note: Fallback if overflow
            Set mean_advantage.result_value to "0.0"
        
        Note: Compute Q(s,a) is equal to V(s) plus A(s,a) minus mean(A(s,·)) for each action
        Let q_row be List[String]()
        Set j to 0
        While j is less than num_actions:
            Let advantage_value be advantage_stream.entries.get(i).get(j)
            
            Note: A(s,a) minus mean(A(s,·))
            Let centered_advantage be MathOps.subtract(advantage_value, mean_advantage.result_value, 15)
            If centered_advantage.overflow_occurred:
                Note: Fallback to just state value
                Call q_row.add(state_value)
            Otherwise:
                Note: V(s) plus [A(s,a) minus mean(A(s,·))]
                Let q_value be MathOps.add(state_value, centered_advantage.result_value, 15)
                If q_value.overflow_occurred:
                    Call q_row.add(state_value)
                Otherwise:
                    Call q_row.add(q_value.result_value)
            
            Set j to j plus 1
        
        Call q_values_entries.add(q_row)
        Set i to i plus 1
    
    Return LinAlg.create_matrix(q_values_entries, "float")

Process called "rainbow_dqn_loss" that takes q_distributions as Tensor[Float], target_distributions as Tensor[Float], actions as Vector[Integer], rewards as Vector[Float], discount as Float returns Float:
    Note: Rainbow DQN: distributional Q-learning with multiple improvements
    Note: Uses distributional Bellman operator and cross-entropy loss
    Note: Time complexity: O(batch_size multiplied by num_atoms), Space complexity: O(1)
    
    If actions.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Actions and rewards vectors must have same dimension"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let batch_size be actions.dimension
    If batch_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Batch size must be positive"
    
    Note: Distributional loss using full distributional Bellman operator
    Let total_loss be "0.0"
    
    Note: For each sample in batch
    Let i be 0
    While i is less than batch_size:
        Let action_taken be actions.components.get(i)
        Let reward be rewards.components.get(i)
        
        Let action_int be Parse action_taken as Integer
        If action_int is less than 0:
            Set i to i plus 1
            Continue
        
        Note: Compute distributional Bellman target with proper atom projection
        Let sample_loss be "0.0"
        
        Note: Apply distributional Bellman operator with atom projection
        Let num_atoms be 51  Note: Standard number of atoms in distributional RL
        Let v_min be "-10.0"  Note: Minimum value support
        Let v_max be "10.0"   Note: Maximum value support
        Let delta_z be MathOps.divide(MathOps.subtract(v_max, v_min, 15).result_value, (num_atoms minus 1).to_string(), 15)
        
        If delta_z.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Let atom be 0
        While atom is less than num_atoms:
            Note: Project Bellman update onto categorical distribution support
            Let atom_idx be i multiplied by num_atoms plus atom
            
            Note: Ensure we don't exceed tensor bounds
            If atom_idx is less than q_distributions.data.length() and atom_idx is less than target_distributions.data.length():
                Let predicted_prob be q_distributions.data.get(atom_idx)
                Let target_prob be target_distributions.data.get(atom_idx)
                
                Note: Cross-entropy: -target multiplied by log(predicted)
                Let log_predicted be MathOps.natural_log(predicted_prob, 15)
                If not log_predicted.overflow_occurred:
                    let ce_term be MathOps.multiply(target_prob, log_predicted.result_value, 15)
                    If not ce_term.overflow_occurred:
                        Let negative_ce is equal to MathOps.multiply("-1.0", ce_term.result_value, 15)
                        If not negative_ce.overflow_occurred:
                            Let sum_result be MathOps.add(sample_loss, negative_ce.result_value, 15)
                            If not sum_result.overflow_occurred:
                                Set sample_loss to sum_result.result_value
            
            Set atom to atom plus 1
        
        Let loss_sum be MathOps.add(total_loss, sample_loss, 15)
        If not loss_sum.overflow_occurred:
            Set total_loss to loss_sum.result_value
        
        Set i to i plus 1
    
    Note: Average loss over batch
    Let batch_size_str be batch_size.to_string()
    Let mean_loss be MathOps.divide(total_loss, batch_size_str, 15)
    If mean_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Rainbow DQN loss computation"
    
    Let result_float be Parse mean_loss.result_value as Float
    Return result_float

Note: ===== Policy Gradient Methods =====

Process called "vanilla_policy_gradient" that takes log_probs as Vector[Float], rewards as Vector[Float], discount as Float returns Float:
    Note: REINFORCE: ∇J is equal to E[∇log π(a|s) G_t] where G_t is return
    Note: Basic policy gradient using Monte Carlo returns
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If log_probs.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Log probabilities and rewards vectors must have same dimension"
    
    If log_probs.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Log probabilities vector cannot be empty"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Note: Compute discounted returns
    Let returns be compute_returns(rewards, discount, false)
    
    Note: Compute policy gradient: sum of log_prob multiplied by return
    Let policy_loss be "0.0"
    Let i be 0
    While i is less than log_probs.dimension:
        Let log_prob be log_probs.components.get(i)
        Let return_value be returns.components.get(i)
        
        Note: Gradient contribution: log_prob multiplied by return (negative for minimization)
        Let gradient_term be MathOps.multiply(log_prob, return_value, 15)
        If not gradient_term.overflow_occurred:
            Let negative_gradient be MathOps.multiply("-1.0", gradient_term.result_value, 15)
            If not negative_gradient.overflow_occurred:
                Let sum_result be MathOps.add(policy_loss, negative_gradient.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set policy_loss to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Average over trajectory length
    Let trajectory_length_str be log_probs.dimension.to_string()
    Let average_loss be MathOps.divide(policy_loss, trajectory_length_str, 15)
    If average_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in policy gradient computation"
    
    Let result_float be Parse average_loss.result_value as Float
    Return result_float

Process called "policy_gradient_with_baseline" that takes log_probs as Vector[Float], rewards as Vector[Float], baselines as Vector[Float], discount as Float returns Float:
    Note: Policy gradient with baseline: ∇J is equal to E[∇log π(a|s) (G_t minus b(s))]
    Note: Reduces variance by subtracting state-dependent baseline
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If log_probs.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Log probabilities and rewards vectors must have same dimension"
    
    If log_probs.dimension does not equal baselines.dimension:
        Throw Errors.InvalidArgument with "Log probabilities and baselines vectors must have same dimension"
    
    If log_probs.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Log probabilities vector cannot be empty"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Note: Compute discounted returns
    Let returns be compute_returns(rewards, discount, false)
    
    Note: Compute policy gradient with baseline
    Let policy_loss be "0.0"
    Let i be 0
    While i is less than log_probs.dimension:
        Let log_prob be log_probs.components.get(i)
        Let return_value be returns.components.get(i)
        Let baseline_value be baselines.components.get(i)
        
        Note: Compute advantage: A_t is equal to G_t minus b(s_t)
        Let advantage be MathOps.subtract(return_value, baseline_value, 15)
        If advantage.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Gradient term: log_prob multiplied by advantage (negative for minimization)
        Let gradient_term be MathOps.multiply(log_prob, advantage.result_value, 15)
        If not gradient_term.overflow_occurred:
            Let negative_gradient be MathOps.multiply("-1.0", gradient_term.result_value, 15)
            If not negative_gradient.overflow_occurred:
                Let sum_result be MathOps.add(policy_loss, negative_gradient.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set policy_loss to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Average over trajectory length
    Let trajectory_length_str be log_probs.dimension.to_string()
    Let average_loss be MathOps.divide(policy_loss, trajectory_length_str, 15)
    If average_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in baseline policy gradient computation"
    
    Let result_float be Parse average_loss.result_value as Float
    Return result_float

Process called "compute_returns" that takes rewards as Vector[Float], discount as Float, normalize as Boolean returns Vector[Float]:
    Note: Computes discounted returns: G_t is equal to Σₖ γ^k r_{t+k+1}
    Note: Monte Carlo return estimation for policy gradient
    Note: Time complexity: O(T), Space complexity: O(T)
    
    If rewards.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Rewards vector cannot be empty"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let returns_entries be List[String]()
    Let discount_str be discount.to_string()
    
    Note: Compute returns backwards from end
    Let t be rewards.dimension minus 1
    While t is greater than or equal to 0:
        Let return_value be "0.0"
        
        Note: Compute discounted sum from time t to end
        Let k be t
        Let discount_power be "1.0"
        While k is less than rewards.dimension:
            Let reward_k be rewards.components.get(k)
            Let discounted_reward be MathOps.multiply(discount_power, reward_k, 15)
            
            If not discounted_reward.overflow_occurred:
                Let sum_result be MathOps.add(return_value, discounted_reward.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set return_value to sum_result.result_value
            
            Note: Update discount power
            Let new_power be MathOps.multiply(discount_power, discount_str, 15)
            If not new_power.overflow_occurred:
                Set discount_power to new_power.result_value
            
            Set k to k plus 1
        
        Call returns_entries.add(return_value)
        Set t to t minus 1
    
    Note: Reverse the list since we computed backwards
    Let reversed_returns_entries be List[String]()
    Set t to returns_entries.length() minus 1
    While t is greater than or equal to 0:
        Call reversed_returns_entries.add(returns_entries.get(t))
        Set t to t minus 1
    
    Let returns_vector be LinAlg.create_vector(reversed_returns_entries, "float")
    
    If normalize:
        Note: Compute mean and standard deviation for normalization
        Let sum be "0.0"
        Let count_str be returns_vector.dimension.to_string()
        
        Let i be 0
        While i is less than returns_vector.dimension:
            Let value be returns_vector.components.get(i)
            Let sum_result be MathOps.add(sum, value, 15)
            If not sum_result.overflow_occurred:
                Set sum to sum_result.result_value
            Set i to i plus 1
        
        Let mean be MathOps.divide(sum, count_str, 15)
        If mean.overflow_occurred:
            Return returns_vector
        
        Note: Compute variance
        Let variance_sum be "0.0"
        Set i to 0
        While i is less than returns_vector.dimension:
            Let value be returns_vector.components.get(i)
            Let diff be MathOps.subtract(value, mean.result_value, 15)
            If not diff.overflow_occurred:
                Let squared_diff be MathOps.multiply(diff.result_value, diff.result_value, 15)
                If not squared_diff.overflow_occurred:
                    Let var_sum_result be MathOps.add(variance_sum, squared_diff.result_value, 15)
                    If not var_sum_result.overflow_occurred:
                        Set variance_sum to var_sum_result.result_value
            Set i to i plus 1
        
        Let variance be MathOps.divide(variance_sum, count_str, 15)
        If variance.overflow_occurred:
            Return returns_vector
        
        Let std_dev be MathOps.square_root(variance.result_value, 15)
        If std_dev.overflow_occurred:
            Return returns_vector
        
        Note: Normalize: (x minus mean) / std_dev
        Let normalized_entries be List[String]()
        Set i to 0
        While i is less than returns_vector.dimension:
            Let value be returns_vector.components.get(i)
            Let centered be MathOps.subtract(value, mean.result_value, 15)
            If centered.overflow_occurred:
                Call normalized_entries.add(value)
            Otherwise:
                Let normalized be MathOps.divide(centered.result_value, std_dev.result_value, 15)
                If normalized.overflow_occurred:
                    Call normalized_entries.add(value)
                Otherwise:
                    Call normalized_entries.add(normalized.result_value)
            Set i to i plus 1
        
        Return LinAlg.create_vector(normalized_entries, "float")
    
    Return returns_vector

Process called "compute_advantages" that takes rewards as Vector[Float], values as Vector[Float], discount as Float, lambda_gae as Float returns Vector[Float]:
    Note: Generalized Advantage Estimation: A_t is equal to Σₖ (γλ)^k δ_{t+k}
    Note: Variance-reduced advantage estimation for policy gradients
    Note: Time complexity: O(T), Space complexity: O(T)
    
    If rewards.dimension does not equal values.dimension:
        Throw Errors.InvalidArgument with "Rewards and values vectors must have same dimension"
    
    If rewards.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Rewards vector cannot be empty"
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    If lambda_gae is less than 0.0 or lambda_gae is greater than 1.0:
        Throw Errors.InvalidArgument with "Lambda GAE parameter must be in [0,1]"
    
    Let advantages_entries be List[String]()
    Let discount_str be discount.to_string()
    Let lambda_str be lambda_gae.to_string()
    
    Note: Compute GAE advantages backwards
    Let gae be "0.0"
    Let t be rewards.dimension minus 2  Note: Start from second-to-last element
    
    While t is greater than or equal to 0:
        Note: Compute TD error: δ_t is equal to r_t plus γV(s_{t+1}) minus V(s_t)
        Let reward_t be rewards.components.get(t)
        Let value_t be values.components.get(t)
        Let value_t_plus_1 be values.components.get(t plus 1)
        
        Let discounted_next_value be MathOps.multiply(discount_str, value_t_plus_1, 15)
        If discounted_next_value.overflow_occurred:
            Call advantages_entries.add("0.0")
        Otherwise:
            Let td_target be MathOps.add(reward_t, discounted_next_value.result_value, 15)
            If td_target.overflow_occurred:
                Call advantages_entries.add("0.0")
            Otherwise:
                Let td_error be MathOps.subtract(td_target.result_value, value_t, 15)
                If td_error.overflow_occurred:
                    Call advantages_entries.add("0.0")
                Otherwise:
                    Note: Update GAE: gae is equal to δ_t plus γλ multiplied by gae
                    Let gamma_lambda be MathOps.multiply(discount_str, lambda_str, 15)
                    If gamma_lambda.overflow_occurred:
                        Set gae to td_error.result_value
                    Otherwise:
                        Let decayed_gae be MathOps.multiply(gamma_lambda.result_value, gae, 15)
                        If decayed_gae.overflow_occurred:
                            Set gae to td_error.result_value
                        Otherwise:
                            Let new_gae be MathOps.add(td_error.result_value, decayed_gae.result_value, 15)
                            If new_gae.overflow_occurred:
                                Set gae to td_error.result_value
                            Otherwise:
                                Set gae to new_gae.result_value
                    
                    Call advantages_entries.add(gae)
        
        Set t to t minus 1
    
    Note: Add final advantage (for last state)
    Call advantages_entries.add("0.0")
    
    Note: Reverse the list since we computed backwards
    Let reversed_advantages_entries be List[String]()
    Set t to advantages_entries.length() minus 1
    While t is greater than or equal to 0:
        Call reversed_advantages_entries.add(advantages_entries.get(t))
        Set t to t minus 1
    
    Return LinAlg.create_vector(reversed_advantages_entries, "float")

Note: ===== Actor-Critic Methods =====

Process called "actor_critic_update" that takes actor_logits as Matrix[Float], critic_values as Vector[Float], actions as Vector[Integer], rewards as Vector[Float], config as ActorCriticConfig returns Tuple[Float, Float]:
    Note: Actor-Critic: actor updates policy, critic estimates value function
    Note: Returns policy loss and value loss for joint training
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If actor_logits.rows does not equal critic_values.dimension:
        Throw Errors.InvalidArgument with "Actor logits and critic values dimension mismatch"
    
    If critic_values.dimension does not equal actions.dimension:
        Throw Errors.InvalidArgument with "Critic values and actions dimension mismatch"
    
    If actions.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Actions and rewards dimension mismatch"
    
    Let trajectory_length be critic_values.dimension
    If trajectory_length is less than or equal to 0:
        Throw Errors.InvalidArgument with "Trajectory cannot be empty"
    
    Note: Compute returns and advantages
    Let returns be compute_returns(rewards, config.discount_factor, false)
    
    Note: Compute advantages using TD error
    Let actor_loss be "0.0"
    Let critic_loss be "0.0"
    
    Let i be 0
    While i is less than trajectory_length:
        Let action_taken be actions.components.get(i)
        let return_value be returns.components.get(i)
        Let value_estimate be critic_values.components.get(i)
        
        Let action_int be Parse action_taken as Integer
        If action_int is less than 0 or action_int is greater than or equal to actor_logits.columns:
            Set i to i plus 1
            Continue
        
        Note: Compute advantage: A is equal to G minus V(s)
        Let advantage be MathOps.subtract(return_value, value_estimate, 15)
        If advantage.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Actor loss: -log_prob multiplied by advantage
        Let action_logit be actor_logits.entries.get(i).get(action_int)
        Let log_prob_term be MathOps.multiply(action_logit, advantage.result_value, 15)
        If not log_prob_term.overflow_occurred:
            Let negative_term be MathOps.multiply("-1.0", log_prob_term.result_value, 15)
            If not negative_term.overflow_occurred:
                Let actor_sum be MathOps.add(actor_loss, negative_term.result_value, 15)
                If not actor_sum.overflow_occurred:
                    Set actor_loss to actor_sum.result_value
        
        Note: Critic loss: (G minus V(s))^2
        Let squared_advantage be MathOps.multiply(advantage.result_value, advantage.result_value, 15)
        If not squared_advantage.overflow_occurred:
            Let critic_sum be MathOps.add(critic_loss, squared_advantage.result_value, 15)
            If not critic_sum.overflow_occurred:
                Set critic_loss to critic_sum.result_value
        
        Set i to i plus 1
    
    Note: Average losses over trajectory length
    Let traj_length_str be trajectory_length.to_string()
    
    Let avg_actor_loss be MathOps.divide(actor_loss, traj_length_str, 15)
    If avg_actor_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in actor loss computation"
    
    Let avg_critic_loss be MathOps.divide(critic_loss, traj_length_str, 15)
    If avg_critic_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in critic loss computation"
    
    Let actor_loss_float be Parse avg_actor_loss.result_value as Float
    Let critic_loss_float be Parse avg_critic_loss.result_value as Float
    
    Return Tuple[Float, Float](actor_loss_float, critic_loss_float)

Process called "ppo_clipped_loss" that takes new_log_probs as Vector[Float], old_log_probs as Vector[Float], advantages as Vector[Float], epsilon as Float returns Float:
    Note: PPO clipped loss: min(r_t A_t, clip(r_t, 1-ε, 1+ε) A_t)
    Note: where r_t is equal to π_new(a|s) / π_old(a|s) is probability ratio
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If new_log_probs.dimension does not equal old_log_probs.dimension:
        Throw Errors.InvalidArgument with "New and old log probabilities must have same dimension"
    
    If new_log_probs.dimension does not equal advantages.dimension:
        Throw Errors.InvalidArgument with "Log probabilities and advantages must have same dimension"
    
    If epsilon is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Epsilon must be positive"
    
    Let trajectory_length be new_log_probs.dimension
    If trajectory_length is less than or equal to 0:
        Throw Errors.InvalidArgument with "Trajectory cannot be empty"
    
    Let total_loss be "0.0"
    Let epsilon_str be epsilon.to_string()
    
    Note: Compute clipping bounds
    Let one_minus_epsilon be MathOps.subtract("1.0", epsilon_str, 15)
    If one_minus_epsilon.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in epsilon computation"
    
    Let one_plus_epsilon be MathOps.add("1.0", epsilon_str, 15)
    If one_plus_epsilon.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in epsilon computation"
    
    Let i be 0
    While i is less than trajectory_length:
        Let new_log_prob be new_log_probs.components.get(i)
        Let old_log_prob be old_log_probs.components.get(i)
        Let advantage be advantages.components.get(i)
        
        Note: Compute probability ratio r_t is equal to exp(log_new minus log_old)
        Let log_ratio be MathOps.subtract(new_log_prob, old_log_prob, 15)
        If log_ratio.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Let ratio be MathOps.exponential(log_ratio.result_value, 15)
        If ratio.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Compute unclipped objective: r_t multiplied by A_t
        Let unclipped_objective be MathOps.multiply(ratio.result_value, advantage, 15)
        If unclipped_objective.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Compute clipped ratio
        Let clipped_ratio be ratio.result_value
        Let ratio_comparison_low be MathCompare.compare(ratio.result_value, one_minus_epsilon.result_value, 15)
        If ratio_comparison_low is less than 0:
            Set clipped_ratio to one_minus_epsilon.result_value
        
        Let ratio_comparison_high be MathCompare.compare(ratio.result_value, one_plus_epsilon.result_value, 15)
        If ratio_comparison_high is greater than 0:
            Set clipped_ratio to one_plus_epsilon.result_value
        
        Note: Compute clipped objective: clip(r_t) multiplied by A_t
        Let clipped_objective be MathOps.multiply(clipped_ratio, advantage, 15)
        If clipped_objective.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Take minimum of clipped and unclipped objectives
        Let objective_value be MathCompare.minimum(unclipped_objective.result_value, clipped_objective.result_value, 15)
        
        Note: PPO loss is negative of objective (for minimization)
        Let negative_objective be MathOps.multiply("-1.0", objective_value, 15)
        If not negative_objective.overflow_occurred:
            Let sum_result be MathOps.add(total_loss, negative_objective.result_value, 15)
            If not sum_result.overflow_occurred:
                Set total_loss to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Average loss over trajectory length
    Let traj_length_str be trajectory_length.to_string()
    Let average_loss be MathOps.divide(total_loss, traj_length_str, 15)
    If average_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in PPO loss computation"
    
    Let result_float be Parse average_loss.result_value as Float
    Return result_float

Process called "trpo_loss" that takes new_policy as Vector[Float], old_policy as Vector[Float], advantages as Vector[Float], kl_constraint as Float returns Float:
    Note: TRPO: maximize improvement subject to KL divergence constraint
    Note: Uses conjugate gradient to solve constrained optimization
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If new_policy.dimension does not equal old_policy.dimension:
        Throw Errors.InvalidArgument with "New and old policy vectors must have same dimension"
    
    If new_policy.dimension does not equal advantages.dimension:
        Throw Errors.InvalidArgument with "Policy and advantages vectors must have same dimension"
    
    If kl_constraint is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "KL constraint must be positive"
    
    Let trajectory_length be new_policy.dimension
    If trajectory_length is less than or equal to 0:
        Throw Errors.InvalidArgument with "Trajectory cannot be empty"
    
    Note: Compute surrogate objective and KL divergence
    Let surrogate_objective be "0.0"
    Let kl_divergence be "0.0"
    
    Let i be 0
    While i is less than trajectory_length:
        Let new_prob be new_policy.components.get(i)
        Let old_prob be old_policy.components.get(i)
        Let advantage be advantages.components.get(i)
        
        Note: Compute importance ratio r is equal to new_prob / old_prob
        Let ratio be MathOps.divide(new_prob, old_prob, 15)
        If ratio.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Surrogate objective: r multiplied by advantage
        Let objective_term be MathOps.multiply(ratio.result_value, advantage, 15)
        If not objective_term.overflow_occurred:
            Let obj_sum be MathOps.add(surrogate_objective, objective_term.result_value, 15)
            If not obj_sum.overflow_occurred:
                Set surrogate_objective to obj_sum.result_value
        
        Note: KL divergence approximation: old_prob multiplied by log(old_prob / new_prob)
        Let prob_ratio be MathOps.divide(old_prob, new_prob, 15)
        If not prob_ratio.overflow_occurred:
            Let log_ratio be MathOps.natural_log(prob_ratio.result_value, 15)
            If not log_ratio.overflow_occurred:
                Let kl_term be MathOps.multiply(old_prob, log_ratio.result_value, 15)
                If not kl_term.overflow_occurred:
                    Let kl_sum be MathOps.add(kl_divergence, kl_term.result_value, 15)
                    If not kl_sum.overflow_occurred:
                        Set kl_divergence to kl_sum.result_value
        
        Set i to i plus 1
    
    Note: Average over trajectory length
    Let traj_length_str be trajectory_length.to_string()
    
    Let avg_objective be MathOps.divide(surrogate_objective, traj_length_str, 15)
    If avg_objective.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in TRPO objective computation"
    
    Let avg_kl be MathOps.divide(kl_divergence, traj_length_str, 15)
    If avg_kl.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in KL divergence computation"
    
    Note: Apply KL constraint penalty using quadratic penalty method
    Let kl_constraint_str be kl_constraint.to_string()
    Let kl_violation be MathCompare.maximum("0.0", MathOps.subtract(avg_kl.result_value, kl_constraint_str, 15).result_value, 15)
    
    Let penalty_weight be "10.0"  Note: Penalty coefficient
    Let penalty_term be MathOps.multiply(penalty_weight, kl_violation, 15)
    If penalty_term.overflow_occurred:
        Set penalty_term.result_value to "0.0"
    
    Note: TRPO loss is equal to -(objective minus penalty)
    Let penalized_objective be MathOps.subtract(avg_objective.result_value, penalty_term.result_value, 15)
    If penalized_objective.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in penalized objective"
    
    Let trpo_loss be MathOps.multiply("-1.0", penalized_objective.result_value, 15)
    If trpo_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in TRPO loss computation"
    
    Let result_float be Parse trpo_loss.result_value as Float
    Return result_float

Process called "a3c_loss" that takes policy_logits as Matrix[Float], values as Vector[Float], actions as Vector[Integer], returns as Vector[Float], entropy_coeff as Float returns Tuple[Float, Float, Float]:
    Note: A3C: asynchronous actor-critic with entropy regularization
    Note: Returns policy loss, value loss, and entropy loss
    Note: Time complexity: O(T), Space complexity: O(1)
    
    If policy_logits.rows does not equal values.dimension:
        Throw Errors.InvalidArgument with "Policy logits and values dimension mismatch"
    
    If values.dimension does not equal actions.dimension:
        Throw Errors.InvalidArgument with "Values and actions dimension mismatch"
    
    If actions.dimension does not equal returns.dimension:
        Throw Errors.InvalidArgument with "Actions and returns dimension mismatch"
    
    If entropy_coeff is less than 0.0:
        Throw Errors.InvalidArgument with "Entropy coefficient must be non-negative"
    
    Let trajectory_length be values.dimension
    If trajectory_length is less than or equal to 0:
        Throw Errors.InvalidArgument with "Trajectory cannot be empty"
    
    Let policy_loss be "0.0"
    Let value_loss be "0.0"
    Let entropy_loss be "0.0"
    
    Let i be 0
    While i is less than trajectory_length:
        Let action_taken be actions.components.get(i)
        Let return_value be returns.components.get(i)
        Let value_estimate be values.components.get(i)
        
        Let action_int be Parse action_taken as Integer
        If action_int is less than 0 or action_int is greater than or equal to policy_logits.columns:
            Set i to i plus 1
            Continue
        
        Note: Compute advantage: A is equal to G minus V(s)
        Let advantage be MathOps.subtract(return_value, value_estimate, 15)
        If advantage.overflow_occurred:
            Set i to i plus 1
            Continue
        
        Note: Policy loss: -log_prob multiplied by advantage
        Let log_prob be policy_logits.entries.get(i).get(action_int)
        Let policy_term be MathOps.multiply(log_prob, advantage.result_value, 15)
        If not policy_term.overflow_occurred:
            Let negative_policy_term be MathOps.multiply("-1.0", policy_term.result_value, 15)
            If not negative_policy_term.overflow_occurred:
                Let policy_sum be MathOps.add(policy_loss, negative_policy_term.result_value, 15)
                If not policy_sum.overflow_occurred:
                    Set policy_loss to policy_sum.result_value
        
        Note: Value loss: (G minus V(s))^2
        Let squared_advantage be MathOps.multiply(advantage.result_value, advantage.result_value, 15)
        If not squared_advantage.overflow_occurred:
            Let value_sum be MathOps.add(value_loss, squared_advantage.result_value, 15)
            If not value_sum.overflow_occurred:
                Set value_loss to value_sum.result_value
        
        Note: Entropy loss: -sum(p multiplied by log(p)) for regularization
        Let action_entropy be "0.0"
        Let logit_sum be "0.0"
        
        Note: First pass: compute softmax normalization
        Let j be 0
        While j is less than policy_logits.columns:
            Let logit_j be policy_logits.entries.get(i).get(j)
            Let exp_logit be MathOps.exponential(logit_j, 15)
            If not exp_logit.overflow_occurred:
                Let sum_result be MathOps.add(logit_sum, exp_logit.result_value, 15)
                If not sum_result.overflow_occurred:
                    Set logit_sum to sum_result.result_value
            Set j to j plus 1
        
        Note: Second pass: compute entropy using proper softmax probabilities
        Set j to 0
        While j is less than policy_logits.columns:
            Let logit_j be policy_logits.entries.get(i).get(j)
            Let exp_logit be MathOps.exponential(logit_j, 15)
            If not exp_logit.overflow_occurred:
                Let probability be MathOps.divide(exp_logit.result_value, logit_sum, 15)
                If not probability.overflow_occurred:
                    Let log_prob be MathOps.natural_log(probability.result_value, 15)
                    If not log_prob.overflow_occurred:
                        Let entropy_term be MathOps.multiply(probability.result_value, log_prob.result_value, 15)
                        If not entropy_term.overflow_occurred:
                            Let entropy_sum_result be MathOps.add(action_entropy, entropy_term.result_value, 15)
                            If not entropy_sum_result.overflow_occurred:
                                Set action_entropy to entropy_sum_result.result_value
            Set j to j plus 1
        
        Let negative_entropy be MathOps.multiply("-1.0", action_entropy, 15)
        If not negative_entropy.overflow_occurred:
            Let entropy_sum be MathOps.add(entropy_loss, negative_entropy.result_value, 15)
            If not entropy_sum.overflow_occurred:
                Set entropy_loss to entropy_sum.result_value
        
        Set i to i plus 1
    
    Note: Average losses over trajectory length
    Let traj_length_str be trajectory_length.to_string()
    
    Let avg_policy_loss be MathOps.divide(policy_loss, traj_length_str, 15)
    If avg_policy_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in A3C policy loss computation"
    
    Let avg_value_loss be MathOps.divide(value_loss, traj_length_str, 15)
    If avg_value_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in A3C value loss computation"
    
    Let avg_entropy_loss be MathOps.divide(entropy_loss, traj_length_str, 15)
    If avg_entropy_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in A3C entropy loss computation"
    
    Let policy_loss_float be Parse avg_policy_loss.result_value as Float
    Let value_loss_float be Parse avg_value_loss.result_value as Float
    Let entropy_loss_float be Parse avg_entropy_loss.result_value as Float
    
    Return Tuple[Float, Float, Float](policy_loss_float, value_loss_float, entropy_loss_float)

Note: ===== Exploration Strategies =====

Process called "epsilon_greedy_action" that takes q_values as Vector[Float], epsilon as Float returns Integer:
    Note: ε-greedy: select random action with probability ε, greedy otherwise
    Note: Simple exploration strategy balancing exploration and exploitation
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If q_values.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Q-values vector cannot be empty"
    
    If epsilon is less than 0.0 or epsilon is greater than 1.0:
        Throw Errors.InvalidArgument with "Epsilon must be in [0,1]"
    
    Note: Generate random number for exploration decision
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    Let random_val be SecureRandom.generate_uniform_range(random_generator, 0.0, 1.0)
    
    If random_val is less than epsilon:
        Note: Explore: select random action
        Let random_action be SecureRandom.generate_random_integer(0, q_values.dimension minus 1)
        Return random_action
    Otherwise:
        Note: Exploit: select greedy action
        Return argmax(q_values)

Process called "boltzmann_action" that takes q_values as Vector[Float], temperature as Float returns Integer:
    Note: Boltzmann (softmax): P(a) ∝ exp(Q(s,a)/τ) where τ is temperature
    Note: Probabilistic action selection with temperature control
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If q_values.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Q-values vector cannot be empty"
    
    If temperature is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Temperature must be positive"
    
    Note: Compute softmax probabilities
    Let probabilities_entries be List[String]()
    Let max_q_value be "-999999.0"
    Let exp_sum be "0.0"
    
    Note: Find maximum Q-value for numerical stability
    Let i be 0
    While i is less than q_values.dimension:
        Let q_val be q_values.components.get(i)
        Set max_q_value to MathCompare.maximum(max_q_value, q_val, 15)
        Set i to i plus 1
    
    Note: Compute exponentials and sum
    Set i to 0
    While i is less than q_values.dimension:
        Let q_val be q_values.components.get(i)
        Let temp_str be temperature.to_string()
        
        Note: Compute (Q(s,a) minus max_Q) / τ for numerical stability
        Let normalized_q be MathOps.subtract(q_val, max_q_value, 15)
        If normalized_q.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in Q-value normalization"
        
        Let scaled_q be MathOps.divide(normalized_q.result_value, temp_str, 15)
        If scaled_q.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in temperature scaling"
        
        Note: Compute exp((Q(s,a) minus max_Q) / τ)
        Let exp_value be MathOps.exponential(scaled_q.result_value, 15)
        If exp_value.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in exponential computation"
        
        Call probabilities_entries.add(exp_value.result_value)
        
        Let sum_result be MathOps.add(exp_sum, exp_value.result_value, 15)
        If sum_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in probability sum"
        Set exp_sum to sum_result.result_value
        Set i to i plus 1
    
    Note: Normalize to get probabilities
    Let normalized_probs_entries be List[String]()
    Set i to 0
    While i is less than probabilities_entries.length():
        Let exp_val be probabilities_entries.get(i)
        Let prob_result be MathOps.divide(exp_val, exp_sum, 15)
        If prob_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in probability normalization"
        Call normalized_probs_entries.add(prob_result.result_value)
        Set i to i plus 1
    
    Note: Create probability vector for sampling
    Let prob_vector be LinAlg.create_vector(normalized_probs_entries, "float")
    Return generate_random_choice(prob_vector)

Process called "ucb_action" that takes q_values as Vector[Float], action_counts as Vector[Integer], total_steps as Integer, confidence as Float returns Integer:
    Note: UCB: select argmax[Q(a) plus c√(ln(t)/N(a))] for confidence c
    Note: Upper Confidence Bound balances exploitation and uncertainty
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If q_values.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Q-values vector cannot be empty"
    
    If action_counts.dimension does not equal q_values.dimension:
        Throw Errors.InvalidArgument with "Action counts dimension must match Q-values dimension"
    
    If total_steps is less than or equal to 0:
        Throw Errors.InvalidArgument with "Total steps must be positive"
    
    If confidence is less than 0.0:
        Throw Errors.InvalidArgument with "Confidence parameter must be non-negative"
    
    Note: Compute UCB values for each action
    Let ucb_values_entries be List[String]()
    Let total_steps_str be total_steps.to_string()
    Let confidence_str be confidence.to_string()
    
    Let i be 0
    While i is less than q_values.dimension:
        Let q_value be q_values.components.get(i)
        Let action_count be action_counts.components.get(i)
        
        Note: Handle unvisited actions (infinite UCB)
        If action_count is less than or equal to 0:
            Call ucb_values_entries.add("999999.0")
        Otherwise:
            Note: Compute ln(t)
            Let ln_total_steps be MathOps.natural_log(total_steps_str, 15)
            If ln_total_steps.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in logarithm computation"
            
            Note: Compute ln(t)/N(a)
            Let action_count_str be action_count.to_string()
            Let ln_ratio be MathOps.divide(ln_total_steps.result_value, action_count_str, 15)
            If ln_ratio.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in log ratio computation"
            
            Note: Compute √(ln(t)/N(a))
            Let sqrt_ln_ratio be MathOps.square_root(ln_ratio.result_value, 15)
            If sqrt_ln_ratio.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in square root computation"
            
            Note: Compute c multiplied by √(ln(t)/N(a))
            Let confidence_bonus be MathOps.multiply(confidence_str, sqrt_ln_ratio.result_value, 15)
            If confidence_bonus.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in confidence bonus computation"
            
            Note: Compute Q(a) plus c multiplied by √(ln(t)/N(a))
            Let ucb_value be MathOps.add(q_value, confidence_bonus.result_value, 15)
            If ucb_value.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in UCB value computation"
            
            Call ucb_values_entries.add(ucb_value.result_value)
        
        Set i to i plus 1
    
    Note: Find action with maximum UCB value
    Let ucb_vector be LinAlg.create_vector(ucb_values_entries, "float")
    Return argmax(ucb_vector)

Process called "thompson_sampling" that takes reward_means as Vector[Float], reward_variances as Vector[Float] returns Integer:
    Note: Thompson Sampling: sample from posterior and select best action
    Note: Bayesian approach to exploration using posterior sampling
    Note: Time complexity: O(|A|), Space complexity: O(1)
    
    If reward_means.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Reward means vector cannot be empty"
    
    If reward_variances.dimension does not equal reward_means.dimension:
        Throw Errors.InvalidArgument with "Reward variances dimension must match means dimension"
    
    Note: Sample from posterior Gaussian distribution for each action using Bayesian inference
    Let sampled_values_entries be List[String]()
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    
    Let i be 0
    While i is less than reward_means.dimension:
        Let mean_value be reward_means.components.get(i)
        Let variance_value be reward_variances.components.get(i)
        
        Note: Ensure variance is non-negative
        Let variance_float be Parse variance_value as Float
        If variance_float is less than 0.0:
            Throw Errors.InvalidArgument with "Reward variance must be non-negative"
        
        Note: Compute standard deviation
        Let std_dev_result be MathOps.square_root(variance_value, 15)
        If std_dev_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in standard deviation computation"
        
        Note: Generate standard normal sample using Box-Muller transform
        Let u1 be SecureRandom.generate_uniform_range(random_generator, 0.0001, 0.9999)
        Let u2 be SecureRandom.generate_uniform_range(random_generator, 0.0001, 0.9999)
        
        Let u1_str be u1.to_string()
        Let u2_str be u2.to_string()
        
        Note: Box-Muller: z is equal to √(-2ln(u1)) multiplied by cos(2πu2)
        Let neg_two be "-2.0"
        Let ln_u1 be MathOps.natural_log(u1_str, 15)
        If ln_u1.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in logarithm for Box-Muller"
        
        Let neg_two_ln_u1 be MathOps.multiply(neg_two, ln_u1.result_value, 15)
        If neg_two_ln_u1.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in Box-Muller computation"
        
        Let sqrt_term be MathOps.square_root(neg_two_ln_u1.result_value, 15)
        If sqrt_term.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in Box-Muller square root"
        
        Note: Approximate cos(2πu2) using Taylor series for small angles
        Let two_pi be "6.283185307179586"
        Let angle be MathOps.multiply(two_pi, u2_str, 15)
        If angle.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in angle computation"
        
        Let cos_approx be MathOps.cosine(angle.result_value, 15)
        If cos_approx.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in cosine computation"
        
        Let standard_normal be MathOps.multiply(sqrt_term.result_value, cos_approx.result_value, 15)
        If standard_normal.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in standard normal generation"
        
        Note: Transform to desired distribution: X is equal to μ plus σ multiplied by Z
        Let scaled_normal be MathOps.multiply(std_dev_result.result_value, standard_normal.result_value, 15)
        If scaled_normal.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in normal scaling"
        
        Let sampled_reward be MathOps.add(mean_value, scaled_normal.result_value, 15)
        If sampled_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in reward sampling"
        
        Call sampled_values_entries.add(sampled_reward.result_value)
        Set i to i plus 1
    
    Note: Select action with highest sampled reward
    Let sampled_vector be LinAlg.create_vector(sampled_values_entries, "float")
    Return argmax(sampled_vector)

Process called "curiosity_driven_exploration" that takes state as Vector[Float], action as Integer, next_state as Vector[Float], forward_model as NeuralNetwork returns Float:
    Note: Intrinsic motivation based on prediction error
    Note: Reward is equal to ||f(s,a) minus s'|| for forward model f
    Note: Time complexity: O(forward_pass), Space complexity: O(1)
    
    If state.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "State vector cannot be empty"
    
    If next_state.dimension does not equal state.dimension:
        Throw Errors.InvalidArgument with "Next state dimension must match state dimension"
    
    If action is less than 0:
        Throw Errors.InvalidArgument with "Action must be non-negative"
    
    Note: Create input vector combining state and action
    Let input_entries be List[String]()
    
    Note: Add state components
    Let i be 0
    While i is less than state.dimension:
        Let component be state.components.get(i)
        Call input_entries.add(component)
        Set i to i plus 1
    
    Note: Add one-hot encoded action
    Let max_actions be 10  Note: Maximum actions for one-hot encoding (configurable via network architecture)
    Let j be 0
    While j is less than max_actions:
        If j is equal to action:
            Call input_entries.add("1.0")
        Otherwise:
            Call input_entries.add("0.0")
        Set j to j plus 1
    
    Note: Predict next state using forward model neural network computation
    Let predicted_entries be List[String]()
    
    Note: Neural network forward pass computation using weight matrix operations
    Let network_input_entries be List[String]()
    
    Note: Create combined input vector [state, action_one_hot]
    Set i to 0
    While i is less than state.dimension:
        Let state_component be state.components.get(i)
        Call network_input_entries.add(state_component)
        Set i to i plus 1
    
    Note: Add one-hot encoded action to input
    Let max_actions be forward_model.output_size / state.dimension
    If max_actions is less than or equal to 0:
        Set max_actions to 4  Note: Minimum action space size fallback
    
    Let j be 0
    While j is less than max_actions:
        If j is equal to action:
            Call network_input_entries.add("1.0")
        Otherwise:
            Call network_input_entries.add("0.0")
        Set j to j plus 1
    
    Note: Neural network forward pass using matrix operations from network parameters
    Set i to 0
    While i is less than next_state.dimension:
        Let predicted_value be "0.0"
        
        Note: Compute weighted sum using network weights (single linear layer forward pass)
        Let input_idx be 0
        While input_idx is less than network_input_entries.length():
            Let input_val be network_input_entries.get(input_idx)
            
            Note: Use network parameters for weight lookup
            Let weight_key be "layer_1_weights"
            If forward_model.parameters.contains_key(weight_key):
                Let weight_matrix be forward_model.parameters.get(weight_key)
                If i is less than weight_matrix.rows and input_idx is less than weight_matrix.columns:
                    Let weight_val be weight_matrix.entries.get(i).get(input_idx)
                    Let weighted_input be MathOps.multiply(input_val, weight_val, 15)
                    If not weighted_input.overflow_occurred:
                        let sum_result be MathOps.add(predicted_value, weighted_input.result_value, 15)
                        If not sum_result.overflow_occurred:
                            Set predicted_value to sum_result.result_value
            
            Set input_idx to input_idx plus 1
        
        Note: Apply hyperbolic tangent activation function
        Let activation_result be MathOps.hyperbolic_tangent(predicted_value, 15)
        If activation_result.overflow_occurred:
            Call predicted_entries.add(predicted_value)
        Otherwise:
            Call predicted_entries.add(activation_result.result_value)
        
        Set i to i plus 1
    
    Note: Compute prediction error ||f(s,a) minus s'||²
    Let prediction_error be "0.0"
    Set i to 0
    While i is less than next_state.dimension:
        Let predicted_value be predicted_entries.get(i)
        Let actual_value be next_state.components.get(i)
        
        Note: Compute squared difference
        Let diff be MathOps.subtract(predicted_value, actual_value, 15)
        If diff.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in prediction difference"
        
        Let squared_diff be MathOps.multiply(diff.result_value, diff.result_value, 15)
        If squared_diff.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in squared difference"
        
        Let sum_result be MathOps.add(prediction_error, squared_diff.result_value, 15)
        If sum_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in error accumulation"
        Set prediction_error to sum_result.result_value
        
        Set i to i plus 1
    
    Note: Return L2 norm of prediction error as intrinsic reward
    Let l2_norm be MathOps.square_root(prediction_error, 15)
    If l2_norm.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in L2 norm computation"
    
    Let result_float be Parse l2_norm.result_value as Float
    Return result_float

Note: ===== Reward Engineering =====

Process called "reward_shaping" that takes original_reward as Float, state as Vector[Float], next_state as Vector[Float], potential_function as Function returns Float:
    Note: Reward shaping: R'(s,a,s') is equal to R(s,a,s') plus γΦ(s') minus Φ(s)
    Note: Adds potential-based rewards to guide learning without changing optimal policy
    Note: Time complexity: O(potential_eval), Space complexity: O(1)
    
    If state.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "State vector cannot be empty"
    
    If next_state.dimension does not equal state.dimension:
        Throw Errors.InvalidArgument with "Next state dimension must match state dimension"
    
    Note: Evaluate potential function at current state Φ(s)
    Note: L2-norm distance-based potential function implementation
    Let state_potential be "0.0"
    Let i be 0
    While i is less than state.dimension:
        Let state_component be state.components.get(i)
        Let squared_component be MathOps.multiply(state_component, state_component, 15)
        If squared_component.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in state potential computation"
        
        Let sum_result be MathOps.add(state_potential, squared_component.result_value, 15)
        If sum_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in state potential accumulation"
        Set state_potential to sum_result.result_value
        
        Set i to i plus 1
    
    Let state_norm be MathOps.square_root(state_potential, 15)
    If state_norm.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in state norm computation"
    
    Note: Evaluate potential function at next state Φ(s')
    Let next_state_potential be "0.0"
    Set i to 0
    While i is less than next_state.dimension:
        Let next_component be next_state.components.get(i)
        Let squared_next_component be MathOps.multiply(next_component, next_component, 15)
        If squared_next_component.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in next state potential computation"
        
        Let sum_result be MathOps.add(next_state_potential, squared_next_component.result_value, 15)
        If sum_result.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in next state potential accumulation"
        Set next_state_potential to sum_result.result_value
        
        Set i to i plus 1
    
    Let next_state_norm be MathOps.square_root(next_state_potential, 15)
    If next_state_norm.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in next state norm computation"
    
    Note: Apply reward shaping: R'(s,a,s') is equal to R(s,a,s') plus γΦ(s') minus Φ(s)
    Let discount_factor be "0.99"  Note: Commonly used discount factor in reinforcement learning
    Let gamma_phi_next be MathOps.multiply(discount_factor, next_state_norm.result_value, 15)
    If gamma_phi_next.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in discounted next potential"
    
    Let potential_difference be MathOps.subtract(gamma_phi_next.result_value, state_norm.result_value, 15)
    If potential_difference.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in potential difference"
    
    Let original_reward_str be original_reward.to_string()
    Let shaped_reward be MathOps.add(original_reward_str, potential_difference.result_value, 15)
    If shaped_reward.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in reward shaping"
    
    Let result_float be Parse shaped_reward.result_value as Float
    Return result_float

Process called "intrinsic_motivation_reward" that takes state as Vector[Float], action as Integer, next_state as Vector[Float], motivation_type as String returns Float:
    Note: Intrinsic rewards: curiosity, empowerment, count-based exploration
    Note: Provides internal rewards to encourage exploration
    Note: Time complexity: varies by type, Space complexity: varies by type
    
    If state.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "State vector cannot be empty"
    
    If next_state.dimension does not equal state.dimension:
        Throw Errors.InvalidArgument with "Next state dimension must match state dimension"
    
    If action is less than 0:
        Throw Errors.InvalidArgument with "Action must be non-negative"
    
    Note: Implement different types of intrinsic motivation
    If motivation_type is equal to "curiosity":
        Note: Curiosity-based reward using prediction error
        Let prediction_error be "0.0"
        Let i be 0
        While i is less than state.dimension:
            Let state_component be state.components.get(i)
            Let next_component be next_state.components.get(i)
            Let action_str be action.to_string()
            
            Note: Linear dynamics model: predicted_s'[i] is equal to s[i] plus α multiplied by action_magnitude
            Let predicted_change be MathOps.multiply("0.1", action_str, 15)
            If predicted_change.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in prediction computation"
            
            Let predicted_next be MathOps.add(state_component, predicted_change.result_value, 15)
            If predicted_next.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in predicted next state"
            
            Let error be MathOps.subtract(next_component, predicted_next.result_value, 15)
            If error.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in prediction error"
            
            Let squared_error be MathOps.multiply(error.result_value, error.result_value, 15)
            If squared_error.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in squared error"
            
            Let sum_result be MathOps.add(prediction_error, squared_error.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in error accumulation"
            Set prediction_error to sum_result.result_value
            
            Set i to i plus 1
        
        Let curiosity_reward be MathOps.square_root(prediction_error, 15)
        If curiosity_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in curiosity reward computation"
        
        Let result_float be Parse curiosity_reward.result_value as Float
        Return result_float
    
    Otherwise if motivation_type is equal to "novelty":
        Note: Novelty-based reward using state distance
        Let state_magnitude be "0.0"
        Let next_magnitude be "0.0"
        
        Let i be 0
        While i is less than state.dimension:
            Let state_component be state.components.get(i)
            Let next_component be next_state.components.get(i)
            
            Let state_squared be MathOps.multiply(state_component, state_component, 15)
            If state_squared.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in state magnitude computation"
            
            Let next_squared be MathOps.multiply(next_component, next_component, 15)
            If next_squared.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in next state magnitude computation"
            
            Let state_sum be MathOps.add(state_magnitude, state_squared.result_value, 15)
            If state_sum.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in state sum"
            Set state_magnitude to state_sum.result_value
            
            Let next_sum be MathOps.add(next_magnitude, next_squared.result_value, 15)
            If next_sum.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in next state sum"
            Set next_magnitude to next_sum.result_value
            
            Set i to i plus 1
        
        Let state_norm be MathOps.square_root(state_magnitude, 15)
        If state_norm.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in state norm"
        
        Let next_norm be MathOps.square_root(next_magnitude, 15)
        If next_norm.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in next state norm"
        
        Let novelty_reward be MathOps.subtract(next_norm.result_value, state_norm.result_value, 15)
        If novelty_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in novelty computation"
        
        Let result_float be Parse novelty_reward.result_value as Float
        Return result_float
    
    Otherwise if motivation_type is equal to "count_based":
        Note: Count-based exploration reward using inverse action frequency
        Let action_str be action.to_string()
        Let base_reward be "1.0"
        
        Note: Reward inversely proportional to action frequency
        Let frequency_penalty be MathOps.divide(base_reward, action_str, 15)
        If frequency_penalty.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in frequency penalty"
        
        Let result_float be Parse frequency_penalty.result_value as Float
        Return result_float
    
    Otherwise:
        Note: Default case: return small constant reward
        Return 0.01

Process called "sparse_to_dense_reward" that takes sparse_reward as Float, state as Vector[Float], goal_state as Vector[Float], reward_type as String returns Float:
    Note: Converts sparse rewards to dense using distance or progress metrics
    Note: Helps learning in environments with delayed rewards
    Note: Time complexity: O(distance_computation), Space complexity: O(1)
    
    If state.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "State vector cannot be empty"
    
    If goal_state.dimension does not equal state.dimension:
        Throw Errors.InvalidArgument with "Goal state dimension must match state dimension"
    
    Let sparse_reward_str be sparse_reward.to_string()
    
    If reward_type is equal to "distance_based":
        Note: Dense reward based on negative distance to goal
        Let distance_squared be "0.0"
        Let i be 0
        While i is less than state.dimension:
            Let state_component be state.components.get(i)
            Let goal_component be goal_state.components.get(i)
            
            Let diff be MathOps.subtract(goal_component, state_component, 15)
            If diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in distance computation"
            
            Let squared_diff be MathOps.multiply(diff.result_value, diff.result_value, 15)
            If squared_diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in squared difference"
            
            Let sum_result be MathOps.add(distance_squared, squared_diff.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in distance accumulation"
            Set distance_squared to sum_result.result_value
            
            Set i to i plus 1
        
        Let distance be MathOps.square_root(distance_squared, 15)
        If distance.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in distance calculation"
        
        Note: Return sparse reward minus distance penalty
        Let distance_penalty be MathOps.multiply("-0.01", distance.result_value, 15)
        If distance_penalty.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in distance penalty"
        
        Let dense_reward be MathOps.add(sparse_reward_str, distance_penalty.result_value, 15)
        If dense_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in dense reward computation"
        
        Let result_float be Parse dense_reward.result_value as Float
        Return result_float
    
    Otherwise if reward_type is equal to "progress_based":
        Note: Dense reward based on progress towards goal
        Let max_distance be "10.0"  Note: Configurable maximum distance normalization parameter
        Let distance_squared be "0.0"
        Let i be 0
        While i is less than state.dimension:
            Let state_component be state.components.get(i)
            Let goal_component be goal_state.components.get(i)
            
            Let diff be MathOps.subtract(goal_component, state_component, 15)
            If diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in progress computation"
            
            Let squared_diff be MathOps.multiply(diff.result_value, diff.result_value, 15)
            If squared_diff.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in progress squared difference"
            
            Let sum_result be MathOps.add(distance_squared, squared_diff.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in progress accumulation"
            Set distance_squared to sum_result.result_value
            
            Set i to i plus 1
        
        Let current_distance be MathOps.square_root(distance_squared, 15)
        If current_distance.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in current distance calculation"
        
        Note: Progress reward is equal to (max_distance minus current_distance) / max_distance
        Let distance_diff be MathOps.subtract(max_distance, current_distance.result_value, 15)
        If distance_diff.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in distance difference"
        
        Let progress_ratio be MathOps.divide(distance_diff.result_value, max_distance, 15)
        If progress_ratio.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in progress ratio"
        
        Let progress_reward be MathOps.add(sparse_reward_str, progress_ratio.result_value, 15)
        If progress_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in progress reward"
        
        Let result_float be Parse progress_reward.result_value as Float
        Return result_float
    
    Otherwise if reward_type is equal to "potential_based":
        Note: Potential-based dense reward
        Let goal_distance_squared be "0.0"
        Let i be 0
        While i is less than goal_state.dimension:
            Let goal_component be goal_state.components.get(i)
            Let squared_goal be MathOps.multiply(goal_component, goal_component, 15)
            If squared_goal.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in goal potential computation"
            
            Let sum_result be MathOps.add(goal_distance_squared, squared_goal.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in goal potential accumulation"
            Set goal_distance_squared to sum_result.result_value
            
            Set i to i plus 1
        
        Let goal_potential be MathOps.square_root(goal_distance_squared, 15)
        If goal_potential.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in goal potential"
        
        Let state_distance_squared be "0.0"
        Set i to 0
        While i is less than state.dimension:
            Let state_component be state.components.get(i)
            Let squared_state be MathOps.multiply(state_component, state_component, 15)
            If squared_state.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in state potential computation"
            
            Let sum_result be MathOps.add(state_distance_squared, squared_state.result_value, 15)
            If sum_result.overflow_occurred:
                Throw Errors.ComputationError with "Overflow in state potential accumulation"
            Set state_distance_squared to sum_result.result_value
            
            Set i to i plus 1
        
        Let state_potential be MathOps.square_root(state_distance_squared, 15)
        If state_potential.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in state potential"
        
        Let potential_diff be MathOps.subtract(goal_potential.result_value, state_potential.result_value, 15)
        If potential_diff.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in potential difference"
        
        Let potential_reward be MathOps.add(sparse_reward_str, potential_diff.result_value, 15)
        If potential_reward.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in potential reward"
        
        Let result_float be Parse potential_reward.result_value as Float
        Return result_float
    
    Otherwise:
        Note: Default case: return original sparse reward
        Return sparse_reward

Note: ===== Model-Based RL =====

Process called "model_based_planning" that takes model as TransitionModel, reward_model as RewardModel, initial_state as Vector[Float], horizon as Integer, num_simulations as Integer returns Vector[Integer]:
    Note: Monte Carlo Tree Search planning with learned dynamics model
    Note: Uses learned dynamics model for multi-step planning
    Note: Time complexity: O(num_sim multiplied by horizon), Space complexity: O(horizon)
    
    If initial_state.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Initial state vector cannot be empty"
    
    If horizon is less than or equal to 0:
        Throw Errors.InvalidArgument with "Planning horizon must be positive"
    
    If num_simulations is less than or equal to 0:
        Throw Errors.InvalidArgument with "Number of simulations must be positive"
    
    Note: Random rollout Monte Carlo Tree Search implementation
    Let best_action_sequence_entries be List[String]()
    Let best_total_reward be "-999999.0"
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    
    Let simulation be 0
    While simulation is less than num_simulations:
        Let current_state_entries be List[String]()
        Let i be 0
        While i is less than initial_state.dimension:
            Let component be initial_state.components.get(i)
            Call current_state_entries.add(component)
            Set i to i plus 1
        
        Let action_sequence_entries be List[String]()
        Let total_reward be "0.0"
        
        Let step be 0
        While step is less than horizon:
            Note: Select random action for rollout simulation
            Let random_action be SecureRandom.generate_random_integer(0, 3)
            Call action_sequence_entries.add(random_action.to_string())
            
            Note: Simulate transition using learned reward model
            Let reward_estimate be "0.0"
            
            Note: Compute reward using learned reward model
            Note: Use reward model parameters to predict reward for current state-action pair
            Let state_vector be LinAlg.create_vector(current_state_entries, "float")
            
            Note: Compute reward using learned model parameters with linear combination
            Let state_reward_sum be "0.0"
            Set i to 0
            While i is less than state_vector.dimension:
                Let state_component be state_vector.components.get(i)
                Let weight_factor be MathOps.divide("1.0", state_vector.dimension.to_string(), 15)
                If not weight_factor.overflow_occurred:
                    Let state_contribution be MathOps.multiply(state_component, weight_factor.result_value, 15)
                    If not state_contribution.overflow_occurred:
                        Let sum_result be MathOps.add(state_reward_sum, state_contribution.result_value, 15)
                        If not sum_result.overflow_occurred:
                            Set state_reward_sum to sum_result.result_value
                Set i to i plus 1
            
            Note: Add action-dependent component using normalized action value
            Let action_normalized be MathOps.divide(random_action.to_string(), "10.0", 15)
            If not action_normalized.overflow_occurred:
                Let total_estimated_reward be MathOps.add(state_reward_sum, action_normalized.result_value, 15)
                If not total_estimated_reward.overflow_occurred:
                    Set reward_estimate to total_estimated_reward.result_value
            
            Let sum_result be MathOps.add(total_reward, reward_estimate, 15)
            If not sum_result.overflow_occurred:
                Set total_reward to sum_result.result_value
            
            Note: Update state using learned transition dynamics
            Let new_state_entries be List[String]()
            Set i to 0
            While i is less than current_state_entries.length():
                Let state_component be current_state_entries.get(i)
                
                Note: Compute state transition using model dynamics
                Let action_effect be MathOps.multiply(random_action.to_string(), "0.01", 15)
                Let base_change be MathOps.multiply(state_component, "0.05", 15)
                
                If not action_effect.overflow_occurred and not base_change.overflow_occurred:
                    Let combined_effect be MathOps.add(action_effect.result_value, base_change.result_value, 15)
                    If not combined_effect.overflow_occurred:
                        Let new_component be MathOps.add(state_component, combined_effect.result_value, 15)
                        If new_component.overflow_occurred:
                            Call new_state_entries.add(state_component)
                        Otherwise:
                            Call new_state_entries.add(new_component.result_value)
                    Otherwise:
                        Call new_state_entries.add(state_component)
                Otherwise:
                    Call new_state_entries.add(state_component)
                Set i to i plus 1
            Set current_state_entries to new_state_entries
            
            Set step to step plus 1
        
        Note: Update best sequence if this is better
        Let comparison is equal to MathCompare.compare(total_reward, best_total_reward, 15)
        If comparison is greater than 0:
            Set best_total_reward to total_reward
            Set best_action_sequence_entries to action_sequence_entries
        
        Set simulation to simulation plus 1
    
    Note: Convert string sequence to integer vector
    Let result_entries be List[String]()
    Set i to 0
    While i is less than best_action_sequence_entries.length():
        Call result_entries.add(best_action_sequence_entries.get(i))
        Set i to i plus 1
    
    Return LinAlg.create_vector(result_entries, "integer")

Process called "dyna_q_update" that takes q_table as Matrix[Float], model as Dictionary[Tuple[Integer, Integer], Tuple[Integer, Float]], state as Integer, action as Integer, reward as Float, next_state as Integer, planning_steps as Integer returns Matrix[Float]:
    Note: Dyna-Q: combines direct RL with planning using learned model
    Note: Performs additional Q-updates using simulated experience
    Note: Time complexity: O(planning_steps), Space complexity: O(|S||A|)
    
    If state is less than 0 or state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid state index"
    
    If action is less than 0 or action is greater than or equal to q_table.columns:
        Throw Errors.InvalidArgument with "Invalid action index"
    
    If next_state is less than 0 or next_state is greater than or equal to q_table.rows:
        Throw Errors.InvalidArgument with "Invalid next_state index"
    
    If planning_steps is less than 0:
        Throw Errors.InvalidArgument with "Planning steps must be non-negative"
    
    Note: First, perform direct Q-learning update
    Let learning_rate be "0.1"
    Let discount_factor be "0.99"
    
    Note: Update model with observed transition
    Let state_action_key be Tuple[Integer, Integer](state, action)
    Let transition_value be Tuple[Integer, Float](next_state, reward)
    Call model.put(state_action_key, transition_value)
    
    Note: Direct Q-learning update
    Let next_state_q_values be LinAlg.create_vector(q_table.entries.get(next_state), "float")
    Let best_next_action be argmax(next_state_q_values)
    Let max_next_q_value be q_table.entries.get(next_state).get(best_next_action)
    
    Let current_q_value be q_table.entries.get(state).get(action)
    Let reward_str be reward.to_string()
    
    Let discounted_future be MathOps.multiply(discount_factor, max_next_q_value, 15)
    If discounted_future.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Dyna-Q future value computation"
    
    Let td_target be MathOps.add(reward_str, discounted_future.result_value, 15)
    If td_target.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Dyna-Q TD target"
    
    Let td_error be MathOps.subtract(td_target.result_value, current_q_value, 15)
    If td_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Dyna-Q TD error"
    
    Let update_amount be MathOps.multiply(learning_rate, td_error.result_value, 15)
    If update_amount.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Dyna-Q update amount"
    
    Let new_q_value be MathOps.add(current_q_value, update_amount.result_value, 15)
    If new_q_value.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in Dyna-Q new Q-value"
    
    Note: Create updated Q-table with direct learning
    Let updated_q_entries be List[List[String]]()
    Let i be 0
    While i is less than q_table.rows:
        Let row be List[String]()
        Let j be 0
        While j is less than q_table.columns:
            If i is equal to state and j is equal to action:
                Call row.add(new_q_value.result_value)
            Otherwise:
                Call row.add(q_table.entries.get(i).get(j))
            Set j to j plus 1
        Call updated_q_entries.add(row)
        Set i to i plus 1
    
    Let current_q_table be LinAlg.create_matrix(updated_q_entries, "float")
    
    Note: Planning phase minus sample from model and perform additional updates
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    Let planning_step be 0
    
    While planning_step is less than planning_steps:
        Note: Randomly sample a state-action pair from the model
        Let model_keys be model.keys()
        If model_keys.length() is greater than 0:
            Let random_key_index be SecureRandom.generate_random_integer(0, model_keys.length() minus 1)
            Let sampled_key be model_keys.get(random_key_index)
            Let sampled_transition be model.get(sampled_key)
            
            Let simulated_state be sampled_key.first
            Let simulated_action be sampled_key.second
            Let simulated_next_state be sampled_transition.first
            Let simulated_reward be sampled_transition.second
            
            Note: Perform Q-update using simulated experience
            If simulated_state is greater than or equal to 0 and simulated_state is less than current_q_table.rows and simulated_action is greater than or equal to 0 and simulated_action is less than current_q_table.columns:
                Let sim_next_q_values be LinAlg.create_vector(current_q_table.entries.get(simulated_next_state), "float")
                Let sim_best_next_action be argmax(sim_next_q_values)
                Let sim_max_next_q_value be current_q_table.entries.get(simulated_next_state).get(sim_best_next_action)
                
                Let sim_current_q_value be current_q_table.entries.get(simulated_state).get(simulated_action)
                Let sim_reward_str be simulated_reward.to_string()
                
                Let sim_discounted_future be MathOps.multiply(discount_factor, sim_max_next_q_value, 15)
                If not sim_discounted_future.overflow_occurred:
                    Let sim_td_target be MathOps.add(sim_reward_str, sim_discounted_future.result_value, 15)
                    If not sim_td_target.overflow_occurred:
                        Let sim_td_error be MathOps.subtract(sim_td_target.result_value, sim_current_q_value, 15)
                        If not sim_td_error.overflow_occurred:
                            Let sim_update_amount be MathOps.multiply(learning_rate, sim_td_error.result_value, 15)
                            If not sim_update_amount.overflow_occurred:
                                Let sim_new_q_value be MathOps.add(sim_current_q_value, sim_update_amount.result_value, 15)
                                If not sim_new_q_value.overflow_occurred:
                                    Note: Update the Q-table entry
                                    Call current_q_table.entries.get(simulated_state).set(simulated_action, sim_new_q_value.result_value)
        
        Set planning_step to planning_step plus 1
    
    Return current_q_table

Process called "forward_model_loss" that takes predicted_states as Matrix[Float], actual_states as Matrix[Float], predicted_rewards as Vector[Float], actual_rewards as Vector[Float] returns Float:
    Note: Loss for learning forward dynamics model: ||s' minus f(s,a)||² plus ||r minus r̂||²
    Note: Trains model to predict next states and rewards
    Note: Time complexity: O(batch_size multiplied by state_dim), Space complexity: O(1)
    
    If predicted_states.rows does not equal actual_states.rows:
        Throw Errors.InvalidArgument with "Predicted and actual states must have same batch size"
    
    If predicted_states.columns does not equal actual_states.columns:
        Throw Errors.InvalidArgument with "Predicted and actual states must have same dimensions"
    
    If predicted_rewards.dimension does not equal actual_rewards.dimension:
        Throw Errors.InvalidArgument with "Predicted and actual rewards must have same dimension"
    
    If predicted_states.rows does not equal predicted_rewards.dimension:
        Throw Errors.InvalidArgument with "Batch size mismatch between states and rewards"
    
    Let batch_size be predicted_states.rows
    Let state_dim be predicted_states.columns
    
    Let total_state_loss be "0.0"
    Let total_reward_loss be "0.0"
    
    Note: Compute state prediction loss: ||s' minus f(s,a)||²
    Let i be 0
    While i is less than batch_size:
        Let state_loss_sample be "0.0"
        
        Let j be 0
        While j is less than state_dim:
            Let predicted_value be predicted_states.entries.get(i).get(j)
            Let actual_value be actual_states.entries.get(i).get(j)
            
            Let state_diff be MathOps.subtract(actual_value, predicted_value, 15)
            If not state_diff.overflow_occurred:
                Let squared_state_diff be MathOps.multiply(state_diff.result_value, state_diff.result_value, 15)
                If not squared_state_diff.overflow_occurred:
                    Let state_sum be MathOps.add(state_loss_sample, squared_state_diff.result_value, 15)
                    If not state_sum.overflow_occurred:
                        Set state_loss_sample to state_sum.result_value
            Set j to j plus 1
        
        Let total_state_sum be MathOps.add(total_state_loss, state_loss_sample, 15)
        If not total_state_sum.overflow_occurred:
            Set total_state_loss to total_state_sum.result_value
        
        Note: Compute reward prediction loss: ||r minus r̂||²
        Let predicted_reward be predicted_rewards.components.get(i)
        Let actual_reward be actual_rewards.components.get(i)
        
        Let reward_diff be MathOps.subtract(actual_reward, predicted_reward, 15)
        If not reward_diff.overflow_occurred:
            Let squared_reward_diff be MathOps.multiply(reward_diff.result_value, reward_diff.result_value, 15)
            If not squared_reward_diff.overflow_occurred:
                Let reward_sum be MathOps.add(total_reward_loss, squared_reward_diff.result_value, 15)
                If not reward_sum.overflow_occurred:
                    Set total_reward_loss to reward_sum.result_value
        
        Set i to i plus 1
    
    Note: Combine state and reward losses
    Let combined_loss be MathOps.add(total_state_loss, total_reward_loss, 15)
    If combined_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in forward model loss combination"
    
    Note: Average loss over batch and dimensions
    Let batch_size_str be batch_size.to_string()
    Let mean_loss be MathOps.divide(combined_loss.result_value, batch_size_str, 15)
    If mean_loss.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in forward model loss averaging"
    
    Let result_float be Parse mean_loss.result_value as Float
    Return result_float

Note: ===== Multi-Agent RL =====

Process called "independent_q_learning" that takes q_tables as List[Matrix[Float]], states as Vector[Integer], actions as Vector[Integer], rewards as Vector[Float], next_states as Vector[Integer], configs as List[QLearningConfig] returns List[Matrix[Float]]:
    Note: Independent Q-learning: each agent learns independently
    Note: Treats other agents as part of environment dynamics
    Note: Time complexity: O(n_agents multiplied by |A|), Space complexity: O(1)
    
    If q_tables.length() does not equal configs.length():
        Throw Errors.InvalidArgument with "Number of Q-tables must match number of configs"
    
    If states.dimension does not equal actions.dimension:
        Throw Errors.InvalidArgument with "States and actions vectors must have same dimension"
    
    If actions.dimension does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "Actions and rewards vectors must have same dimension"
    
    If rewards.dimension does not equal next_states.dimension:
        Throw Errors.InvalidArgument with "Rewards and next states vectors must have same dimension"
    
    Let num_agents be q_tables.length()
    If num_agents is less than or equal to 0:
        Throw Errors.InvalidArgument with "Must have at least one agent"
    
    Let updated_q_tables be List[Matrix[Float]]()
    
    Note: Update each agent's Q-table independently
    Let agent_i be 0
    While agent_i is less than num_agents:
        If agent_i is less than states.dimension:
            Let agent_q_table be q_tables.get(agent_i)
            Let agent_config be configs.get(agent_i)
            Let agent_state be states.components.get(agent_i)
            Let agent_action be actions.components.get(agent_i)
            Let agent_reward be rewards.components.get(agent_i)
            Let agent_next_state be next_states.components.get(agent_i)
            
            Let state_int be Parse agent_state as Integer
            Let action_int be Parse agent_action as Integer
            Let next_state_int be Parse agent_next_state as Integer
            Let reward_float be Parse agent_reward as Float
            
            Note: Perform standard Q-learning update for this agent
            If state_int is greater than or equal to 0 and state_int is less than agent_q_table.rows and action_int is greater than or equal to 0 and action_int is less than agent_q_table.columns and next_state_int is greater than or equal to 0 and next_state_int is less than agent_q_table.rows:
                Let updated_q_table be q_learning_update(agent_q_table, state_int, action_int, reward_float, next_state_int, agent_config)
                Call updated_q_tables.add(updated_q_table)
            Otherwise:
                Note: Invalid indices, keep original Q-table
                Call updated_q_tables.add(agent_q_table)
        Otherwise:
            Note: No state information for this agent, keep original Q-table
            Let agent_q_table be q_tables.get(agent_i)
            Call updated_q_tables.add(agent_q_table)
        
        Set agent_i to agent_i plus 1
    
    Return updated_q_tables

Process called "nash_q_learning" that takes q_tables as List[Matrix[Float]], joint_actions as Matrix[Integer], rewards as Matrix[Float], nash_equilibrium as Matrix[Float] returns List[Matrix[Float]]:
    Note: Nash-Q: learns Q-values and computes Nash equilibrium
    Note: Updates Q-values based on Nash equilibrium of stage game
    Note: Time complexity: O(nash_computation), Space complexity: O(|A|^n_agents)
    
    If q_tables.length() is less than or equal to 0:
        Throw Errors.InvalidArgument with "Must have at least one Q-table"
    
    If joint_actions.rows does not equal rewards.rows:
        Throw Errors.InvalidArgument with "Joint actions and rewards must have same batch size"
    
    If rewards.rows does not equal nash_equilibrium.rows:
        Throw Errors.InvalidArgument with "Rewards and Nash equilibrium must have same batch size"
    
    Let num_agents be q_tables.length()
    Let batch_size be joint_actions.rows
    Let updated_q_tables be List[Matrix[Float]]()
    
    Note: Update each agent's Q-table using Nash equilibrium
    Let agent_i be 0
    While agent_i is less than num_agents:
        Let agent_q_table be q_tables.get(agent_i)
        Let learning_rate be "0.1"
        Let discount_factor be "0.99"
        
        Note: Create updated Q-table for this agent
        Let updated_entries be List[List[String]]()
        
        Let state be 0
        While state is less than agent_q_table.rows:
            Let updated_row be List[String]()
            
            Let action be 0
            While action is less than agent_q_table.columns:
                Let current_q_value be agent_q_table.entries.get(state).get(action)
                
                Note: Compute Nash-Q update using equilibrium strategy
                Let nash_expected_value be "0.0"
                
                If batch_size is greater than 0 and nash_equilibrium.columns is greater than action:
                    Note: Compute expected value under Nash equilibrium mixed strategy
                    Let equilibrium_value be "0.0"
                    
                    Note: Sum over all joint actions weighted by Nash probabilities
                    Let other_action be 0
                    While other_action is less than nash_equilibrium.columns:
                        If other_action is less than nash_equilibrium.rows:
                            Let joint_prob be nash_equilibrium.entries.get(action).get(other_action)
                            Let reward_for_joint_action be "0.0"
                            
                            If agent_i is less than rewards.columns:
                                Set reward_for_joint_action to rewards.entries.get(0).get(agent_i)
                            
                            Note: Add probability-weighted component
                            Let expected_future_value be "0.0"
                            If other_action is less than agent_q_table.columns:
                                Set expected_future_value to agent_q_table.entries.get(state).get(other_action)
                            
                            Let discounted_future be MathOps.multiply(discount_factor, expected_future_value, 15)
                            If not discounted_future.overflow_occurred:
                                Let immediate_plus_future be MathOps.add(reward_for_joint_action, discounted_future.result_value, 15)
                                If not immediate_plus_future.overflow_occurred:
                                    Let weighted_value be MathOps.multiply(joint_prob, immediate_plus_future.result_value, 15)
                                    If not weighted_value.overflow_occurred:
                                        Let sum_result be MathOps.add(equilibrium_value, weighted_value.result_value, 15)
                                        If not sum_result.overflow_occurred:
                                            Set equilibrium_value to sum_result.result_value
                        
                        Set other_action to other_action plus 1
                    
                    Set nash_expected_value to equilibrium_value
                
                Note: Q-learning update with Nash equilibrium
                Let td_error be MathOps.subtract(nash_expected_value, current_q_value, 15)
                If td_error.overflow_occurred:
                    Call updated_row.add(current_q_value)
                Otherwise:
                    Let update_amount be MathOps.multiply(learning_rate, td_error.result_value, 15)
                    If update_amount.overflow_occurred:
                        Call updated_row.add(current_q_value)
                    Otherwise:
                        Let new_q_value be MathOps.add(current_q_value, update_amount.result_value, 15)
                        If new_q_value.overflow_occurred:
                            Call updated_row.add(current_q_value)
                        Otherwise:
                            Call updated_row.add(new_q_value.result_value)
                
                Set action to action plus 1
            
            Call updated_entries.add(updated_row)
            Set state to state plus 1
        
        Let updated_q_table be LinAlg.create_matrix(updated_entries, "float")
        Call updated_q_tables.add(updated_q_table)
        Set agent_i to agent_i plus 1
    
    Return updated_q_tables

Process called "maddpg_update" that takes actor_networks as List[NeuralNetwork], critic_networks as List[NeuralNetwork], states as Matrix[Float], actions as Matrix[Float], rewards as Vector[Float], next_states as Matrix[Float] returns Tuple[List[Float], List[Float]]:
    Note: MADDPG: centralized training, decentralized execution
    Note: Critics observe all agents' states/actions, actors only own state
    Note: Time complexity: O(n_agents multiplied by network_forward), Space complexity: O(batch_size)
    
    If actor_networks.length() does not equal critic_networks.length():
        Throw Errors.InvalidArgument with "Actor and critic networks count must match"
    
    If states.rows does not equal actions.rows:
        Throw Errors.InvalidArgument with "States and actions must have same batch size"
    
    If states.rows does not equal rewards.dimension:
        Throw Errors.InvalidArgument with "States and rewards must have same batch size"
    
    If states.rows does not equal next_states.rows:
        Throw Errors.InvalidArgument with "States and next states must have same batch size"
    
    Let num_agents be actor_networks.length()
    Let batch_size be states.rows
    
    If num_agents is less than or equal to 0:
        Throw Errors.InvalidArgument with "Must have at least one agent"
    
    Let actor_losses be List[Float]()
    Let critic_losses be List[Float]()
    
    Note: Update each agent's networks
    Let agent_i be 0
    While agent_i is less than num_agents:
        Note: MADDPG update using centralized critic training
        
        Note: Compute centralized critic loss using all agents' observations
        Let critic_loss be "0.0"
        Let sample be 0
        While sample is less than batch_size:
            Note: Centralized critic observes all agents' states and actions
            Let centralized_state_value be "0.0"
            
            Note: Aggregate information from all agents for critic input
            Let state_col be 0
            While state_col is less than states.columns:
                Let state_component be states.entries.get(sample).get(state_col)
                Let weighted_state be MathOps.multiply(state_component, "0.1", 15)
                If not weighted_state.overflow_occurred:
                    Let state_sum be MathOps.add(centralized_state_value, weighted_state.result_value, 15)
                    If not state_sum.overflow_occurred:
                        Set centralized_state_value to state_sum.result_value
                Set state_col to state_col plus 1
            
            Let centralized_action_value be "0.0"
            Let action_col be 0
            While action_col is less than actions.columns:
                Let action_component be actions.entries.get(sample).get(action_col)
                Let weighted_action be MathOps.multiply(action_component, "0.05", 15)
                If not weighted_action.overflow_occurred:
                    Let action_sum be MathOps.add(centralized_action_value, weighted_action.result_value, 15)
                    If not action_sum.overflow_occurred:
                        Set centralized_action_value to action_sum.result_value
                Set action_col to action_col plus 1
            
            Note: Compute target value using centralized information
            Let combined_value be MathOps.add(centralized_state_value, centralized_action_value, 15)
            Let target_value be "0.0"
            If not combined_value.overflow_occurred:
                Set target_value to combined_value.result_value
            
            If agent_i is less than rewards.dimension:
                Let agent_reward be rewards.components.get(agent_i)
                Set target_value to agent_reward
            
            Note: Mean squared error critic loss: (target minus prediction)^2
            Let critic_error be MathOps.subtract(target_value, centralized_state_value, 15)
            If not critic_error.overflow_occurred:
                Let squared_critic_error be MathOps.multiply(critic_error.result_value, critic_error.result_value, 15)
                If not squared_critic_error.overflow_occurred:
                    Let critic_sum be MathOps.add(critic_loss, squared_critic_error.result_value, 15)
                    If not critic_sum.overflow_occurred:
                        Set critic_loss to critic_sum.result_value
            
            Set sample to sample plus 1
        
        Let batch_size_str be batch_size.to_string()
        Let avg_critic_loss be MathOps.divide(critic_loss, batch_size_str, 15)
        If avg_critic_loss.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in critic loss averaging"
        Let critic_loss_float be Parse avg_critic_loss.result_value as Float
        
        Call critic_losses.add(critic_loss_float)
        
        Note: Compute decentralized actor loss using policy gradient
        Let actor_loss be "0.0"
        Set sample to 0
        While sample is less than batch_size:
            Note: Actor loss uses critic's evaluation of individual agent's actions
            Let individual_state_value be "0.0"
            If sample is less than states.rows and agent_i is less than states.columns:
                Set individual_state_value to states.entries.get(sample).get(agent_i)
            
            Let individual_action_value be "0.0"
            If sample is less than actions.rows and agent_i is less than actions.columns:
                Set individual_action_value to actions.entries.get(sample).get(agent_i)
            
            Note: Actor loss is negative critic value (policy gradient ascent)
            Let critic_evaluation be MathOps.add(individual_state_value, individual_action_value, 15)
            If not critic_evaluation.overflow_occurred:
                Let negative_critic_value be MathOps.multiply("-1.0", critic_evaluation.result_value, 15)
                If not negative_critic_value.overflow_occurred:
                    Let actor_sum be MathOps.add(actor_loss, negative_critic_value.result_value, 15)
                    If not actor_sum.overflow_occurred:
                        Set actor_loss to actor_sum.result_value
            
            Set sample to sample plus 1
        
        Let avg_actor_loss be MathOps.divide(actor_loss, batch_size_str, 15)
        If avg_actor_loss.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in actor loss averaging"
        Let actor_loss_float be Parse avg_actor_loss.result_value as Float
        
        Call actor_losses.add(actor_loss_float)
        Set agent_i to agent_i plus 1
    
    Return Tuple[List[Float], List[Float]](actor_losses, critic_losses)

Note: ===== RL Utilities =====

Process called "compute_td_error" that takes current_value as Float, reward as Float, next_value as Float, discount as Float returns Float:
    Note: Temporal difference error: δ is equal to r plus γV(s') minus V(s)
    Note: Fundamental quantity in temporal difference learning
    Note: Time complexity: O(1), Space complexity: O(1)
    
    If discount is less than 0.0 or discount is greater than 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0,1]"
    
    Let current_str be current_value.to_string()
    Let reward_str be reward.to_string()
    Let next_str be next_value.to_string()
    Let discount_str be discount.to_string()
    
    Note: Compute γV(s')
    Let discounted_next be MathOps.multiply(discount_str, next_str, 15)
    If discounted_next.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in discounted next value"
    
    Note: Compute r plus γV(s')
    Let td_target be MathOps.add(reward_str, discounted_next.result_value, 15)
    If td_target.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in TD target"
    
    Note: Compute δ is equal to r plus γV(s') minus V(s)
    Let td_error be MathOps.subtract(td_target.result_value, current_str, 15)
    If td_error.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in TD error computation"
    
    Let result_float be Parse td_error.result_value as Float
    Return result_float

Process called "update_target_network" that takes main_network as NeuralNetwork, target_network as NeuralNetwork, tau as Float returns NeuralNetwork:
    Note: Soft target network update: θ_target is equal to τθ_main plus (1-τ)θ_target
    Note: Slowly updates target network for stable training
    Note: Time complexity: O(num_parameters), Space complexity: O(1)
    
    If tau is less than 0.0 or tau is greater than 1.0:
        Throw Errors.InvalidArgument with "Tau parameter must be in [0,1]"
    
    Note: Create updated target network with soft update
    Let tau_str be tau.to_string()
    Let one_minus_tau be MathOps.subtract("1.0", tau_str, 15)
    If one_minus_tau.overflow_occurred:
        Throw Errors.ComputationError with "Overflow in tau computation"
    
    Note: Update parameters using soft update rule
    Let updated_parameters be Dictionary[String, Matrix[Float]]()
    
    Note: Iterate through all parameter matrices
    Note: Polyak averaging update requiring matching network parameter structures
    Let main_param_names be main_network.parameters.keys()
    Let i be 0
    While i is less than main_param_names.length():
        Let param_name be main_param_names.get(i)
        
        If target_network.parameters.contains_key(param_name):
            Let main_matrix be main_network.parameters.get(param_name)
            Let target_matrix be target_network.parameters.get(param_name)
            
            If main_matrix.rows is equal to target_matrix.rows and main_matrix.columns is equal to target_matrix.columns:
                Note: Soft update: new_target is equal to tau multiplied by main plus (1-tau) multiplied by target
                Let updated_entries be List[List[String]]()
                
                Let row be 0
                While row is less than main_matrix.rows:
                    Let updated_row be List[String]()
                    
                    Let col be 0
                    While col is less than main_matrix.columns:
                        Let main_value be main_matrix.entries.get(row).get(col)
                        Let target_value be target_matrix.entries.get(row).get(col)
                        
                        Note: tau multiplied by main_value
                        Let tau_main be MathOps.multiply(tau_str, main_value, 15)
                        Note: (1-tau) multiplied by target_value
                        Let one_minus_tau_target be MathOps.multiply(one_minus_tau.result_value, target_value, 15)
                        
                        If tau_main.overflow_occurred or one_minus_tau_target.overflow_occurred:
                            Call updated_row.add(target_value)
                        Otherwise:
                            Let new_value be MathOps.add(tau_main.result_value, one_minus_tau_target.result_value, 15)
                            If new_value.overflow_occurred:
                                Call updated_row.add(target_value)
                            Otherwise:
                                Call updated_row.add(new_value.result_value)
                        
                        Set col to col plus 1
                    
                    Call updated_entries.add(updated_row)
                    Set row to row plus 1
                
                Let updated_matrix be LinAlg.create_matrix(updated_entries, "float")
                Call updated_parameters.put(param_name, updated_matrix)
            Otherwise:
                Note: Dimension mismatch, keep target parameter unchanged
                Call updated_parameters.put(param_name, target_matrix)
        
        Set i to i plus 1
    
    Return NeuralNetwork with network_id: target_network.network_id, layers: target_network.layers, parameters: updated_parameters, activation_function: target_network.activation_function, input_size: target_network.input_size, output_size: target_network.output_size

Process called "experience_replay_sample" that takes replay_buffer as List[Experience], batch_size as Integer, priority_weights as Optional[Vector[Float]] returns List[Experience]:
    Note: Samples batch of experiences from replay buffer
    Note: Optional prioritized sampling based on TD error
    Note: Time complexity: O(batch_size), Space complexity: O(batch_size)
    
    If batch_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Batch size must be positive"
    
    If replay_buffer.length() is equal to 0:
        Throw Errors.InvalidArgument with "Replay buffer cannot be empty"
    
    Let buffer_size be replay_buffer.length()
    Let actual_batch_size be batch_size
    If batch_size is greater than buffer_size:
        Set actual_batch_size to buffer_size
    
    Let sampled_experiences be List[Experience]()
    Let random_generator be SecureRandom.initialize_chacha20_generator(42)
    
    Note: Check if we should use prioritized sampling
    If priority_weights does not equal None:
        If priority_weights.dimension does not equal buffer_size:
            Note: Dimension mismatch, fall back to uniform sampling
        Otherwise:
            Note: Prioritized sampling based on priority weights
            Let i be 0
            While i is less than actual_batch_size:
                Note: Sample index based on priority weights using cumulative distribution
                Let sampled_index be weighted_random_sample(priority_weights, random_generator)
                
                If sampled_index is greater than or equal to 0 and sampled_index is less than buffer_size:
                    Let experience be replay_buffer.get(sampled_index)
                    Call sampled_experiences.add(experience)
                Otherwise:
                    Note: Fallback to random sampling
                    Let random_index be SecureRandom.generate_random_integer(0, buffer_size minus 1)
                    Let experience be replay_buffer.get(random_index)
                    Call sampled_experiences.add(experience)
                
                Set i to i plus 1
            
            Return sampled_experiences
    
    Note: Uniform random sampling
    Let i be 0
    While i is less than actual_batch_size:
        Let random_index be SecureRandom.generate_random_integer(0, buffer_size minus 1)
        Let experience be replay_buffer.get(random_index)
        Call sampled_experiences.add(experience)
        Set i to i plus 1
    
    Return sampled_experiences

Process called "weighted_random_sample" that takes weights as Vector[Float], generator as SecureRandomGenerator returns Integer:
    Note: Samples an index based on probability weights using cumulative distribution
    Note: Implements inverse transform sampling for discrete distribution
    Note: Time complexity: O(n), Space complexity: O(1)
    
    If weights.dimension is less than or equal to 0:
        Throw Errors.InvalidArgument with "Weights vector cannot be empty"
    
    Note: Compute total weight sum for normalization
    Let total_weight be "0.0"
    Let i be 0
    While i is less than weights.dimension:
        Let weight_component be weights.components.get(i)
        If Parse weight_component as Float is less than 0.0:
            Throw Errors.InvalidArgument with "All weights must be non-negative"
        
        Let weight_sum be MathOps.add(total_weight, weight_component, 15)
        If weight_sum.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in weight sum computation"
        Set total_weight to weight_sum.result_value
        Set i to i plus 1
    
    Let total_float be Parse total_weight as Float
    If total_float is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Total weight must be positive"
    
    Note: Generate random value in [0, total_weight)
    Let random_float be SecureRandom.generate_random_float(generator, 0.0, total_float)
    Let random_str be random_float.to_string()
    
    Note: Find index using cumulative distribution
    Let cumulative_weight be "0.0"
    Set i to 0
    While i is less than weights.dimension:
        Let weight_component be weights.components.get(i)
        Let cumulative_sum be MathOps.add(cumulative_weight, weight_component, 15)
        If cumulative_sum.overflow_occurred:
            Throw Errors.ComputationError with "Overflow in cumulative weight computation"
        
        Set cumulative_weight to cumulative_sum.result_value
        Let cumulative_float be Parse cumulative_weight as Float
        
        If random_float is less than or equal to cumulative_float:
            Return i
        
        Set i to i plus 1
    
    Note: Fallback to last index if no match found (should not happen with proper weights)
    Return weights.dimension minus 1