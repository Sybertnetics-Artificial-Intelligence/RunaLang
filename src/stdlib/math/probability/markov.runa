Note:
math/probability/markov.runa
Markov Chains and Monte Carlo Markov Chain Methods

This module provides comprehensive Markov chain analysis including state
transition modeling, steady-state analysis, absorption probabilities,
first passage times, and Monte Carlo Markov Chain methods for sampling
from complex probability distributions.

Mathematical foundations include stochastic process theory, linear algebra,
and convergence analysis for Markov chain Monte Carlo algorithms.
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/engine/linalg/core" as LinAlgCore
Import module "math/algebra/linear" as LinearAlgebra
Import module "math/engine/linalg/decomposition" as Decomposition
Import module "math/statistics/inferential" as Statistics
Import module "math/probability/sampling" as Sampling
Import module "math/core/operations" as MathOps

Note: =====================================================================
Note: MARKOV CHAIN DATA STRUCTURES
Note: =====================================================================

Type called "MarkovChain":
    state_space as List[String]
    transition_matrix as List[List[Float]]
    initial_distribution as List[Float]
    state_labels as Dictionary[Integer, String]
    chain_type as String
    periodicity as Dictionary[String, Integer]
    communicating_classes as List[List[Integer]]

Type called "ContinuousTimeMarkovChain":
    state_space as List[String]
    generator_matrix as List[List[Float]]
    initial_distribution as List[Float]
    holding_times as Dictionary[Integer, Float]
    jump_probabilities as List[List[Float]]
    embedded_chain as MarkovChain

Type called "MarkovChainAnalysis":
    chain_id as String
    stationary_distribution as List[Float]
    absorption_probabilities as Dictionary[Integer, Dictionary[Integer, Float]]
    expected_hitting_times as Dictionary[Integer, Dictionary[Integer, Float]]
    fundamental_matrix as List[List[Float]]
    mixing_time as Float
    spectral_gap as Float

Type called "MCMCConfig":
    algorithm_type as String
    chain_length as Integer
    burn_in_period as Integer
    thinning_interval as Integer
    target_acceptance_rate as Float
    adaptation_window as Integer
    convergence_diagnostics as List[String]
    parallel_chains as Integer

Note: =====================================================================
Note: MARKOV CHAIN CONSTRUCTION OPERATIONS
Note: =====================================================================

Process called "create_markov_chain" that takes state_space as List[String], transition_matrix as List[List[Float]] returns MarkovChain:
    Note: Construct discrete-time Markov chain from transition matrix
    Note: Validates stochastic matrix properties and irreducibility
    Note: Computational complexity: O(nÂ²) for n states
    
    If state_space.length is equal to 0:
        Throw Errors.InvalidArgument with "State space cannot be empty"
    
    If transition_matrix.length does not equal state_space.length:
        Throw Errors.InvalidArgument with "Transition matrix rows must match state space size"
    
    Note: Validate matrix is square
    Let n be state_space.length
    Let i be 0
    While i is less than n:
        If transition_matrix.get(i).length does not equal n:
            Throw Errors.InvalidArgument with "Transition matrix must be square"
        Set i to i plus 1
    
    Note: Validate stochastic property (rows sum to 1)
    Set i to 0
    While i is less than n:
        Let row_sum be 0.0
        Let j be 0
        While j is less than n:
            Let entry be transition_matrix.get(i).get(j)
            If entry is less than 0.0:
                Throw Errors.InvalidArgument with "Transition probabilities must be non-negative"
            Set row_sum to row_sum plus entry
            Set j to j plus 1
        If row_sum is less than 0.999 or row_sum is greater than 1.001:
            Throw Errors.InvalidArgument with "Transition matrix rows must sum to 1"
        Set i to i plus 1
    
    Note: Create state labels dictionary
    Let state_labels be Dictionary[Integer, String]()
    Set i to 0
    While i is less than n:
        Call state_labels.set(i, state_space.get(i))
        Set i to i plus 1
    
    Note: Determine chain type by checking aperiodicity and irreducibility
    Let chain_type be "regular"
    Let communicating_classes be analyze_communicating_classes(transition_matrix)
    If communicating_classes.length is greater than 1:
        Set chain_type to "reducible"
    
    Note: Analyze periodicity for each state
    Let periodicity be Dictionary[String, Integer]()
    Set i to 0
    While i is less than n:
        Let period be compute_state_period(transition_matrix, i)
        Call periodicity.set(state_space.get(i), period)
        If period is greater than 1:
            Set chain_type to "periodic"
        Set i to i plus 1
    
    Note: Compute initial uniform distribution
    Let initial_distribution be List[Float]()
    Let uniform_prob be 1.0 / n.to_float()
    Set i to 0
    While i is less than n:
        Call initial_distribution.add(uniform_prob)
        Set i to i plus 1
    
    Return MarkovChain with
        state_space: state_space,
        transition_matrix: transition_matrix,
        initial_distribution: initial_distribution,
        state_labels: state_labels,
        chain_type: chain_type,
        periodicity: periodicity,
        communicating_classes: communicating_classes

Process called "estimate_transition_matrix" that takes state_sequence as List[String], smoothing_parameter as Float returns List[List[Float]]:
    Note: Estimate transition probabilities from observed state sequence
    Note: Uses maximum likelihood with optional Laplace smoothing
    Note: Computational complexity: O(T multiplied by n) for sequence length T
    
    If state_sequence.length is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 observations to estimate transitions"
    
    Note: Find unique states
    Let unique_states be List[String]()
    Let i be 0
    While i is less than state_sequence.length:
        Let current_state be state_sequence.get(i)
        If not unique_states.contains(current_state):
            Call unique_states.add(current_state)
        Set i to i plus 1
    
    Let n be unique_states.length
    
    Note: Create transition count matrix
    Let transition_counts be List[List[Float]]()
    Set i to 0
    While i is less than n:
        Let row be List[Float]()
        Let j be 0
        While j is less than n:
            Call row.add(smoothing_parameter)
            Set j to j plus 1
        Call transition_counts.add(row)
        Set i to i plus 1
    
    Note: Count transitions
    Set i to 0
    While i is less than state_sequence.length minus 1:
        Let from_state be state_sequence.get(i)
        Let to_state be state_sequence.get(i plus 1)
        
        Let from_index be unique_states.index_of(from_state)
        Let to_index be unique_states.index_of(to_state)
        
        Let current_count be transition_counts.get(from_index).get(to_index)
        Set transition_counts.get(from_index)[to_index] to current_count plus 1.0
        Set i to i plus 1
    
    Note: Normalize to get probabilities
    Let transition_matrix be List[List[Float]]()
    Set i to 0
    While i is less than n:
        Let row be List[Float]()
        Let row_sum be 0.0
        Let j be 0
        While j is less than n:
            Set row_sum to row_sum plus transition_counts.get(i).get(j)
            Set j to j plus 1
        
        Set j to 0
        While j is less than n:
            If row_sum is greater than 0.0:
                Call row.add(transition_counts.get(i).get(j) / row_sum)
            Otherwise:
                Call row.add(1.0 / n.to_float())
            Set j to j plus 1
        
        Call transition_matrix.add(row)
        Set i to i plus 1
    
    Return transition_matrix

Process called "higher_order_markov_chain" that takes state_sequence as List[String], order as Integer returns MarkovChain:
    Note: Construct higher-order Markov chain with memory length is greater than 1
    Note: Creates expanded state space for k-th order dependencies
    Note: Computational complexity: O(n^k) for state space expansion
    
    If order is less than or equal to 0:
        Throw Errors.InvalidArgument with "Order must be positive"
    
    If state_sequence.length is less than or equal to order:
        Throw Errors.InvalidArgument with "Sequence too short for specified order"
    
    Note: Create compound states representing k-tuples
    Let compound_states be List[String]()
    Let compound_state_map be Dictionary[String, Integer]()
    
    Note: Generate all observed k-tuples
    Let i be 0
    While i is less than or equal to state_sequence.length minus order:
        Let compound_state be ""
        Let j be 0
        While j is less than order:
            If j is greater than 0:
                Set compound_state to compound_state plus ","
            Set compound_state to compound_state plus state_sequence.get(i plus j)
            Set j to j plus 1
        
        If not compound_state_map.contains_key(compound_state):
            Call compound_states.add(compound_state)
            Call compound_state_map.set(compound_state, compound_states.length minus 1)
        Set i to i plus 1
    
    Let n be compound_states.length
    
    Note: Build transition matrix for compound states
    Let transition_counts be List[List[Float]]()
    Set i to 0
    While i is less than n:
        Let row be List[Float]()
        Let j be 0
        While j is less than n:
            Call row.add(0.0)
            Set j to j plus 1
        Call transition_counts.add(row)
        Set i to i plus 1
    
    Note: Count transitions between compound states
    Set i to 0
    While i is less than state_sequence.length minus order:
        Note: Create current compound state
        Let current_compound be ""
        Let j be 0
        While j is less than order:
            If j is greater than 0:
                Set current_compound to current_compound plus ","
            Set current_compound to current_compound plus state_sequence.get(i plus j)
            Set j to j plus 1
        
        Note: Create next compound state (shifted by one position)
        Let next_compound be ""
        Set j to 1
        While j is less than or equal to order:
            If j is greater than 1:
                Set next_compound to next_compound plus ","
            Set next_compound to next_compound plus state_sequence.get(i plus j)
            Set j to j plus 1
        
        Note: Only count transition if both states exist in our map
        If compound_state_map.contains_key(current_compound) and compound_state_map.contains_key(next_compound):
            Let from_idx be compound_state_map.get(current_compound)
            Let to_idx be compound_state_map.get(next_compound)
            Let current_count be transition_counts.get(from_idx).get(to_idx)
            Set transition_counts.get(from_idx)[to_idx] to current_count plus 1.0
        Set i to i plus 1
    
    Note: Normalize to probabilities
    Let transition_matrix be List[List[Float]]()
    Set i to 0
    While i is less than n:
        Let row be List[Float]()
        Let row_sum be 0.0
        Let j be 0
        While j is less than n:
            Set row_sum to row_sum plus transition_counts.get(i).get(j)
            Set j to j plus 1
        
        Set j to 0
        While j is less than n:
            If row_sum is greater than 0.0:
                Call row.add(transition_counts.get(i).get(j) / row_sum)
            Otherwise:
                Call row.add(0.0)
            Set j to j plus 1
        Call transition_matrix.add(row)
        Set i to i plus 1
    
    Note: Create Markov chain with expanded state space
    Return create_markov_chain(compound_states, transition_matrix)

Process called "continuous_time_markov_chain" that takes generator_matrix as List[List[Float]] returns ContinuousTimeMarkovChain:
    Note: Construct continuous-time Markov chain from generator matrix
    Note: Validates generator matrix properties and constructs embedded chain
    Note: Computational complexity: O(nÂ²) for matrix operations
    
    If generator_matrix.length is equal to 0:
        Throw Errors.InvalidArgument with "Generator matrix cannot be empty"
    
    Let n be generator_matrix.length
    
    Note: Validate generator matrix is square
    Let i be 0
    While i is less than n:
        If generator_matrix.get(i).length does not equal n:
            Throw Errors.InvalidArgument with "Generator matrix must be square"
        Set i to i plus 1
    
    Note: Validate generator matrix properties (rows sum to 0, off-diagonal non-negative)
    Let holding_times be Dictionary[Integer, Float]()
    Set i to 0
    While i is less than n:
        Let row_sum be 0.0
        Let diagonal_entry be generator_matrix.get(i).get(i)
        
        Let j be 0
        While j is less than n:
            Let entry be generator_matrix.get(i).get(j)
            If i does not equal j and entry is less than 0.0:
                Throw Errors.InvalidArgument with "Off-diagonal entries must be non-negative"
            Set row_sum to row_sum plus entry
            Set j to j plus 1
        
        If row_sum.abs() is greater than 0.001:
            Throw Errors.InvalidArgument with "Generator matrix rows must sum to zero"
        
        Note: Calculate holding time as 1/|q_ii|
        If diagonal_entry.abs() is greater than 0.0:
            Call holding_times.set(i, 1.0 / diagonal_entry.abs())
        Otherwise:
            Note: Absorbing state minus infinite holding time
            Call holding_times.set(i, Float.positive_infinity())
        Set i to i plus 1
    
    Note: Construct embedded discrete-time chain
    Let jump_probabilities be List[List[Float]]()
    Set i to 0
    While i is less than n:
        Let row be List[Float]()
        Let diagonal_rate be generator_matrix.get(i).get(i).abs()
        
        Let j be 0
        While j is less than n:
            If i is equal to j:
                Call row.add(0.0)
            Otherwise:
                If diagonal_rate is greater than 0.0:
                    Call row.add(generator_matrix.get(i).get(j) / diagonal_rate)
                Otherwise:
                    Call row.add(0.0)
            Set j to j plus 1
        Call jump_probabilities.add(row)
        Set i to i plus 1
    
    Note: Create state space with integer labels
    Let state_space be List[String]()
    Set i to 0
    While i is less than n:
        Call state_space.add("State_" plus i.to_string())
        Set i to i plus 1
    
    Note: Create embedded discrete-time Markov chain
    Let embedded_chain be create_markov_chain(state_space, jump_probabilities)
    
    Note: Compute uniform initial distribution
    Let initial_distribution be List[Float]()
    Let uniform_prob be 1.0 / n.to_float()
    Set i to 0
    While i is less than n:
        Call initial_distribution.add(uniform_prob)
        Set i to i plus 1
    
    Return ContinuousTimeMarkovChain with
        state_space: state_space,
        generator_matrix: generator_matrix,
        initial_distribution: initial_distribution,
        holding_times: holding_times,
        jump_probabilities: jump_probabilities,
        embedded_chain: embedded_chain

Note: =====================================================================
Note: MARKOV CHAIN ANALYSIS OPERATIONS
Note: =====================================================================

Process called "compute_stationary_distribution" that takes transition_matrix as List[List[Float]] returns List[Float]:
    Note: Calculate stationary distribution using eigenvector decomposition
    Note: Finds left eigenvector corresponding to eigenvalue 1
    Note: Computational complexity: O(nÂ³) for dense matrix eigendecomposition
    
    If transition_matrix.length is equal to 0:
        Throw Errors.InvalidArgument with "Transition matrix cannot be empty"
    
    Let n be transition_matrix.length
    
    Note: Convert to string matrix for linear algebra operations
    Let string_matrix be List[List[String]]()
    Let i be 0
    While i is less than n:
        Let row be List[String]()
        Let j be 0
        While j is less than n:
            Call row.add(transition_matrix.get(i).get(j).to_string())
            Set j to j plus 1
        Call string_matrix.add(row)
        Set i to i plus 1
    
    Note: Create matrix object for eigenvalue computation
    Let matrix_obj be LinAlgCore.create_matrix(string_matrix, "float")
    
    Note: Compute eigenvalues and eigenvectors
    Let eigenvalues be LinearAlgebra.find_eigenvalues(matrix_obj)
    
    Note: Find eigenvalue closest to 1.0
    Let best_eigenvalue be ""
    Let best_distance be Float.positive_infinity()
    Let k be 0
    While k is less than eigenvalues.length:
        Let eigenval be eigenvalues.get(k).to_float()
        Let distance be (eigenval minus 1.0).abs()
        If distance is less than best_distance:
            Set best_distance to distance
            Set best_eigenvalue to eigenvalues.get(k)
        Set k to k plus 1
    
    Note: Get eigenvector corresponding to eigenvalue 1
    Let eigenvectors be LinearAlgebra.find_eigenvectors(matrix_obj, best_eigenvalue)
    If eigenvectors.length is equal to 0:
        Note: Fallback to power iteration method
        Return compute_stationary_power_iteration(transition_matrix)
    
    Note: Take first eigenvector and normalize
    Let stationary_vector be eigenvectors.get(0)
    Let sum be 0.0
    Set i to 0
    While i is less than stationary_vector.length:
        Let val be stationary_vector.get(i).to_float().abs()
        Set sum to sum plus val
        Set i to i plus 1
    
    Note: Normalize to probability distribution
    Let stationary_distribution be List[Float]()
    Set i to 0
    While i is less than stationary_vector.length:
        Let normalized_val be stationary_vector.get(i).to_float().abs() / sum
        Call stationary_distribution.add(normalized_val)
        Set i to i plus 1
    
    Return stationary_distribution

Process called "classify_markov_chain_states" that takes transition_matrix as List[List[Float]] returns Dictionary[String, List[Integer]]:
    Note: Classify states as transient, recurrent, absorbing, or periodic
    Note: Uses graph algorithms to find strongly connected components
    Note: Computational complexity: O(nÂ²) for adjacency matrix analysis
    
    Let n be transition_matrix.length
    Let result be Dictionary[String, List[Integer]]()
    Call result.set("transient", List[Integer]())
    Call result.set("recurrent", List[Integer]())
    Call result.set("absorbing", List[Integer]())
    Call result.set("periodic", List[Integer]())
    
    Note: Find communicating classes
    Let communicating_classes be analyze_communicating_classes(transition_matrix)
    
    Note: Classify each state based on its communicating class properties
    Let i be 0
    While i is less than communicating_classes.length:
        Let current_class be communicating_classes.get(i)
        
        Note: Check if class is absorbing (self-loops with probability 1)
        Let is_absorbing_class be true
        If current_class.length is equal to 1:
            Let state be current_class.get(0)
            If transition_matrix.get(state).get(state) is less than 0.999:
                Set is_absorbing_class to false
        Otherwise:
            Set is_absorbing_class to false
        
        Note: Check if class is recurrent (strongly connected and closed)
        Let is_recurrent_class be true
        Let j be 0
        While j is less than current_class.length:
            Let state_j be current_class.get(j)
            
            Note: Check if state can reach states outside the class
            Let k be 0
            While k is less than n:
                If transition_matrix.get(state_j).get(k) is greater than 0.0:
                    Let k_in_class be false
                    Let l be 0
                    While l is less than current_class.length:
                        If current_class.get(l) is equal to k:
                            Set k_in_class to true
                            Break
                        Set l to l plus 1
                    
                    If not k_in_class:
                        Set is_recurrent_class to false
                        Break
                Set k to k plus 1
            
            If not is_recurrent_class:
                Break
            Set j to j plus 1
        
        Note: Check periodicity of the class
        Let is_periodic_class be false
        If current_class.length is greater than 0:
            Let representative_state be current_class.get(0)
            Let period be compute_state_period(transition_matrix, representative_state)
            If period is greater than 1:
                Set is_periodic_class to true
        
        Note: Classify all states in this class
        Set j to 0
        While j is less than current_class.length:
            Let state be current_class.get(j)
            
            If is_absorbing_class:
                Call result.get("absorbing").add(state)
            Otherwise if is_recurrent_class:
                If is_periodic_class:
                    Call result.get("periodic").add(state)
                Otherwise:
                    Call result.get("recurrent").add(state)
            Otherwise:
                Call result.get("transient").add(state)
            Set j to j plus 1
        Set i to i plus 1
    
    Return result

Process called "compute_absorption_probabilities" that takes transition_matrix as List[List[Float]], transient_states as List[Integer], absorbing_states as List[Integer] returns Dictionary[Integer, Dictionary[Integer, Float]]:
    Note: Calculate absorption probabilities from transient to absorbing states
    Note: Solves fundamental matrix equation (I minus Q)^(-1) multiplied by R
    Note: Computational complexity: O(tÂ³ plus tÂ²a) for t transient, a absorbing states
    
    If transient_states.length is equal to 0:
        Return Dictionary[Integer, Dictionary[Integer, Float]]()
    
    Let t be transient_states.length
    Let a be absorbing_states.length
    
    Note: Extract Q matrix (transient to transient transitions)
    Let q_matrix be List[List[String]]()
    Let i be 0
    While i is less than t:
        Let row be List[String]()
        Let j be 0
        While j is less than t:
            Let trans_i be transient_states.get(i)
            Let trans_j be transient_states.get(j)
            Call row.add(transition_matrix.get(trans_i).get(trans_j).to_string())
            Set j to j plus 1
        Call q_matrix.add(row)
        Set i to i plus 1
    
    Note: Create (I minus Q) matrix
    Let i_minus_q be List[List[String]]()
    Set i to 0
    While i is less than t:
        Let row be List[String]()
        Let j be 0
        While j is less than t:
            If i is equal to j:
                Let diagonal_entry be 1.0 minus q_matrix.get(i).get(j).to_float()
                Call row.add(diagonal_entry.to_string())
            Otherwise:
                Let entry be -q_matrix.get(i).get(j).to_float()
                Call row.add(entry.to_string())
            Set j to j plus 1
        Call i_minus_q.add(row)
        Set i to i plus 1
    
    Note: Compute fundamental matrix N is equal to (I minus Q)^(-1)
    Let i_minus_q_matrix be LinAlgCore.create_matrix(i_minus_q, "float")
    Let fundamental_matrix be LinAlgCore.matrix_inverse(i_minus_q_matrix, "gauss_jordan")
    
    Note: Extract R matrix (transient to absorbing transitions)
    Let r_matrix be List[List[String]]()
    Set i to 0
    While i is less than t:
        Let row be List[String]()
        Let j be 0
        While j is less than a:
            Let trans_i be transient_states.get(i)
            Let absorb_j be absorbing_states.get(j)
            Call row.add(transition_matrix.get(trans_i).get(absorb_j).to_string())
            Set j to j plus 1
        Call r_matrix.add(row)
        Set i to i plus 1
    
    Let r_matrix_obj be LinAlgCore.create_matrix(r_matrix, "float")
    
    Note: Compute absorption probabilities B is equal to N multiplied by R
    Let b_matrix be LinAlgCore.matrix_multiply_objects(fundamental_matrix, r_matrix_obj)
    
    Note: Convert to result dictionary format
    Let absorption_probs be Dictionary[Integer, Dictionary[Integer, Float]]()
    Set i to 0
    While i is less than t:
        Let trans_state be transient_states.get(i)
        Let absorb_dict be Dictionary[Integer, Float]()
        
        Let j be 0
        While j is less than a:
            Let absorb_state be absorbing_states.get(j)
            Let prob be b_matrix.entries.get(i).get(j).to_float()
            Call absorb_dict.set(absorb_state, prob)
            Set j to j plus 1
        
        Call absorption_probs.set(trans_state, absorb_dict)
        Set i to i plus 1
    
    Return absorption_probs

Process called "expected_hitting_times" that takes transition_matrix as List[List[Float]], source_states as List[Integer], target_states as List[Integer] returns Dictionary[Integer, Dictionary[Integer, Float]]:
    Note: Calculate expected first passage times between states
    Note: Solves system of linear equations for hitting time expectations
    Note: Computational complexity: O(nÂ³) for matrix inversion
    
    Let n be transition_matrix.length
    Let result be Dictionary[Integer, Dictionary[Integer, Float]]()
    
    Note: For each target state, solve hitting time system
    Let target_idx be 0
    While target_idx is less than target_states.length:
        Let target_state be target_states.get(target_idx)
        
        Note: Create system (I minus P plus e_t multiplied by 1^T) multiplied by h is equal to 1
        Note: where e_t is unit vector for target state
        Let system_matrix be List[List[String]]()
        Let rhs_vector be List[String]()
        
        Let i be 0
        While i is less than n:
            Let row be List[String]()
            Let j be 0
            While j is less than n:
                If i is equal to j:
                    If i is equal to target_state:
                        Call row.add("1.0")
                    Otherwise:
                        Let diagonal_entry be 1.0 minus transition_matrix.get(i).get(j)
                        Call row.add(diagonal_entry.to_string())
                Otherwise:
                    If i is equal to target_state:
                        Call row.add("1.0")
                    Otherwise:
                        Let off_diagonal be -transition_matrix.get(i).get(j)
                        Call row.add(off_diagonal.to_string())
                Set j to j plus 1
            Call system_matrix.add(row)
            
            Note: RHS vector is 1 for non-target states, 0 for target state
            If i is equal to target_state:
                Call rhs_vector.add("0.0")
            Otherwise:
                Call rhs_vector.add("1.0")
            Set i to i plus 1
        
        Note: Solve the linear system
        Let system_matrix_obj be LinAlgCore.create_matrix(system_matrix, "float")
        Let rhs_vector_obj be LinAlgCore.create_vector(rhs_vector, "float")
        
        Let hitting_times_vector be LinAlgCore.solve_linear_system(system_matrix_obj, rhs_vector_obj, "lu")
        
        Note: Extract hitting times for source states
        Let source_idx be 0
        While source_idx is less than source_states.length:
            Let source_state be source_states.get(source_idx)
            
            If not result.contains_key(source_state):
                Call result.set(source_state, Dictionary[Integer, Float]())
            
            Let hitting_time be hitting_times_vector.components.get(source_state).to_float()
            Call result.get(source_state).set(target_state, hitting_time)
            Set source_idx to source_idx plus 1
        Set target_idx to target_idx plus 1
    
    Return result

Note: =====================================================================
Note: MARKOV CHAIN SIMULATION OPERATIONS
Note: =====================================================================

Process called "simulate_markov_chain" that takes chain as MarkovChain, chain_length as Integer, initial_state as Integer returns List[Integer]:
    Note: Simulate discrete-time Markov chain trajectory
    Note: Uses random number generation with transition probabilities
    Note: Computational complexity: O(T) for simulation length T
    
    If chain_length is less than or equal to 0:
        Throw Errors.InvalidArgument with "Chain length must be positive"
    
    If initial_state is less than 0 or initial_state is greater than or equal to chain.state_space.length:
        Throw Errors.InvalidArgument with "Initial state index out of bounds"
    
    Note: Initialize high-quality random generator
    Let rng be Sampling.initialize_mersenne_twister(12345)
    
    Let trajectory be List[Integer]()
    Let current_state be initial_state
    Call trajectory.add(current_state)
    
    Note: Generate trajectory
    Let step be 1
    While step is less than chain_length:
        Note: Get transition probabilities for current state
        Let transition_probs be chain.transition_matrix.get(current_state)
        
        Note: Generate random number for state selection
        Let random_value be Sampling.generate_mt_random_float(rng)
        
        Note: Select next state based on transition probabilities
        Let cumulative_prob be 0.0
        Let next_state be 0
        Let j be 0
        While j is less than transition_probs.length:
            Set cumulative_prob to cumulative_prob plus transition_probs.get(j)
            If random_value is less than or equal to cumulative_prob:
                Set next_state to j
                Break
            Set j to j plus 1
        
        Set current_state to next_state
        Call trajectory.add(current_state)
        Set step to step plus 1
    
    Return trajectory

Process called "simulate_continuous_time_chain" that takes ctmc as ContinuousTimeMarkovChain, time_horizon as Float returns List[Dictionary[String, Float]]:
    Note: Simulate continuous-time Markov chain using jump process
    Note: Uses exponential holding times and embedded chain transitions
    Note: Computational complexity: O(expected_jumps) depending on rates
    
    If time_horizon is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Time horizon must be positive"
    
    Let rng be Sampling.initialize_mersenne_twister(54321)
    Let trajectory be List[Dictionary[String, Float]]()
    
    Note: Start from initial state (choose randomly from initial distribution)
    Let cumulative_prob be 0.0
    Let random_val be Sampling.generate_mt_random_float(rng)
    Let current_state be 0
    Let i be 0
    While i is less than ctmc.initial_distribution.length:
        Set cumulative_prob to cumulative_prob plus ctmc.initial_distribution.get(i)
        If random_val is less than or equal to cumulative_prob:
            Set current_state to i
            Break
        Set i to i plus 1
    
    Let current_time be 0.0
    
    Note: Add initial event
    Let initial_event be Dictionary[String, Float]()
    Call initial_event.set("time", current_time)
    Call initial_event.set("state", current_state.to_float())
    Call trajectory.add(initial_event)
    
    Note: Simulate jump process until time horizon
    While current_time is less than time_horizon:
        Note: Generate exponential holding time
        Let holding_time be ctmc.holding_times.get(current_state)
        Let exponential_random be Sampling.generate_mt_random_float(rng)
        If exponential_random is less than or equal to 0.0:
            Set exponential_random to 0.000001
        Let waiting_time be -holding_time multiplied by exponential_random.ln()
        
        Set current_time to current_time plus waiting_time
        
        If current_time is greater than or equal to time_horizon:
            Break
        
        Note: Select next state using embedded chain transition probabilities
        Let transition_probs be ctmc.jump_probabilities.get(current_state)
        Set random_val to Sampling.generate_mt_random_float(rng)
        Set cumulative_prob to 0.0
        Let next_state be current_state
        
        Set i to 0
        While i is less than transition_probs.length:
            Set cumulative_prob to cumulative_prob plus transition_probs.get(i)
            If random_val is less than or equal to cumulative_prob:
                Set next_state to i
                Break
            Set i to i plus 1
        
        Note: Add jump event
        Let jump_event be Dictionary[String, Float]()
        Call jump_event.set("time", current_time)
        Call jump_event.set("state", next_state.to_float())
        Call trajectory.add(jump_event)
        
        Set current_state to next_state
    
    Note: Add final state at time horizon if needed
    If trajectory.length is greater than 0:
        Let last_event be trajectory.get(trajectory.length minus 1)
        If last_event.get("time") is less than time_horizon:
            Let final_event be Dictionary[String, Float]()
            Call final_event.set("time", time_horizon)
            Call final_event.set("state", current_state.to_float())
            Call trajectory.add(final_event)
    
    Return trajectory

Process called "markov_chain_monte_carlo_simulation" that takes chain as MarkovChain, sample_size as Integer, statistic_function as Dictionary[String, String] returns List[Float]:
    Note: Use Markov chain for Monte Carlo estimation of expectations
    Note: Generates correlated samples from stationary distribution
    Note: Computational complexity: O(n multiplied by T) for n states and T samples
    
    If sample_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Sample size must be positive"
    
    Note: Run long Markov chain to generate samples
    Let burn_in_length be sample_size / 10
    Let total_length be sample_size plus burn_in_length
    
    Let trajectory be simulate_markov_chain(chain, total_length, 0)
    
    Note: Apply statistic function to each sample from trajectory
    Let results be List[Float]()
    Let i be burn_in_length
    While i is less than trajectory.length:
        Let state_index be trajectory.get(i)
        Let computed_statistic be 0.0
        
        Note: Evaluate the statistic function based on type
        Let function_type be statistic_function.get("type")
        If function_type is equal to "identity":
            Set computed_statistic to state_index.to_float()
        Otherwise if function_type is equal to "indicator":
            Let target_state be statistic_function.get("target_state").to_integer()
            If state_index is equal to target_state:
                Set computed_statistic to 1.0
            Otherwise:
                Set computed_statistic to 0.0
        Otherwise if function_type is equal to "polynomial":
            Let degree be statistic_function.get("degree").to_float()
            Set computed_statistic to MathOps.power(state_index.to_string(), degree.to_string(), 10).result_value.to_float()
        Otherwise if function_type is equal to "exponential":
            Let rate be statistic_function.get("rate").to_float()
            Set computed_statistic to MathOps.exponential((rate multiplied by state_index.to_float()).to_string(), 10).result_value.to_float()
        Otherwise:
            Note: Default to identity function if type not recognized
            Set computed_statistic to state_index.to_float()
        
        Call results.add(computed_statistic)
        Set i to i plus 1
    
    Return results

Note: =====================================================================
Note: MCMC CONVERGENCE DIAGNOSTICS
Note: =====================================================================

Process called "gelman_rubin_diagnostic" that takes multiple_chains as List[List[Float]] returns Float:
    Note: Calculate Gelman-Rubin potential scale reduction factor
    Note: Compares within-chain and between-chain variance for convergence
    Note: Computational complexity: O(m multiplied by n) for m chains of length n
    
    If multiple_chains.length is less than 2:
        Throw Errors.InvalidArgument with "Need at least 2 chains for Gelman-Rubin diagnostic"
    
    Let m be multiple_chains.length
    Let n be multiple_chains.get(0).length
    
    Note: Validate all chains have same length
    Let i be 1
    While i is less than m:
        If multiple_chains.get(i).length does not equal n:
            Throw Errors.InvalidArgument with "All chains must have same length"
        Set i to i plus 1
    
    If n is less than 2:
        Throw Errors.InvalidArgument with "Chains must have at least 2 samples"
    
    Note: Calculate chain means
    Let chain_means be List[Float]()
    Set i to 0
    While i is less than m:
        Let chain_sum be 0.0
        Let j be 0
        While j is less than n:
            Set chain_sum to chain_sum plus multiple_chains.get(i).get(j)
            Set j to j plus 1
        Call chain_means.add(chain_sum / n.to_float())
        Set i to i plus 1
    
    Note: Calculate overall mean
    Let overall_mean be 0.0
    Set i to 0
    While i is less than m:
        Set overall_mean to overall_mean plus chain_means.get(i)
        Set i to i plus 1
    Set overall_mean to overall_mean / m.to_float()
    
    Note: Calculate within-chain variances
    Let within_chain_variances be List[Float]()
    Set i to 0
    While i is less than m:
        Let chain_variance be 0.0
        Let j be 0
        While j is less than n:
            Let diff be multiple_chains.get(i).get(j) minus chain_means.get(i)
            Set chain_variance to chain_variance plus (diff multiplied by diff)
            Set j to j plus 1
        Set chain_variance to chain_variance / (n minus 1).to_float()
        Call within_chain_variances.add(chain_variance)
        Set i to i plus 1
    
    Note: Calculate pooled within-chain variance W
    Let w_variance be 0.0
    Set i to 0
    While i is less than m:
        Set w_variance to w_variance plus within_chain_variances.get(i)
        Set i to i plus 1
    Set w_variance to w_variance / m.to_float()
    
    Note: Calculate between-chain variance B
    Let b_variance be 0.0
    Set i to 0
    While i is less than m:
        Let diff be chain_means.get(i) minus overall_mean
        Set b_variance to b_variance plus (diff multiplied by diff)
        Set i to i plus 1
    Set b_variance to (n.to_float() multiplied by b_variance) / (m minus 1).to_float()
    
    Note: Calculate pooled variance estimate
    Let pooled_variance be ((n minus 1).to_float() multiplied by w_variance plus b_variance) / n.to_float()
    
    Note: Calculate potential scale reduction factor
    Let psrf be pooled_variance / w_variance
    
    If w_variance is less than or equal to 0.0:
        Return Float.positive_infinity()
    
    Return psrf.sqrt()

Process called "geweke_diagnostic" that takes chain as List[Float], first_fraction as Float, last_fraction as Float returns Float:
    Note: Calculate Geweke convergence diagnostic using spectral density
    Note: Compares means of early and late portions of single chain
    Note: Computational complexity: O(n log n) for spectral density estimation
    
    If first_fraction is less than or equal to 0.0 or first_fraction is greater than or equal to 0.5:
        Throw Errors.InvalidArgument with "First fraction must be in (0, 0.5)"
    
    If last_fraction is less than or equal to 0.0 or last_fraction is greater than or equal to 0.5:
        Throw Errors.InvalidArgument with "Last fraction must be in (0, 0.5)"
    
    Let n be chain.length
    If n is less than 10:
        Return Float.positive_infinity()
    
    Note: Extract first and last portions
    Let first_length be (n.to_float() multiplied by first_fraction).to_integer()
    Let last_length be (n.to_float() multiplied by last_fraction).to_integer()
    
    Let first_portion be List[Float]()
    Let last_portion be List[Float]()
    
    Let i be 0
    While i is less than first_length:
        Call first_portion.add(chain.get(i))
        Set i to i plus 1
    
    Set i to n minus last_length
    While i is less than n:
        Call last_portion.add(chain.get(i))
        Set i to i plus 1
    
    Note: Calculate means
    Let first_mean be 0.0
    Let last_mean be 0.0
    
    Set i to 0
    While i is less than first_portion.length:
        Set first_mean to first_mean plus first_portion.get(i)
        Set i to i plus 1
    Set first_mean to first_mean / first_portion.length.to_float()
    
    Set i to 0
    While i is less than last_portion.length:
        Set last_mean to last_mean plus last_portion.get(i)
        Set i to i plus 1
    Set last_mean to last_mean / last_portion.length.to_float()
    
    Note: Calculate spectral density estimates using autocovariance function
    Let max_lag be Minimum(first_portion.length / 4, 20)
    Let first_spectral_density be compute_spectral_density_at_zero(first_portion, first_mean, max_lag)
    Let last_spectral_density be compute_spectral_density_at_zero(last_portion, last_mean, max_lag)
    
    Note: Calculate Geweke statistic using spectral density estimates
    Let first_asymptotic_variance be first_spectral_density / first_portion.length.to_float()
    Let last_asymptotic_variance be last_spectral_density / last_portion.length.to_float()
    Let combined_variance be first_asymptotic_variance plus last_asymptotic_variance
    
    If combined_variance is less than or equal to 0.0:
        Return 0.0
    
    Let standard_error be combined_variance.sqrt()
    Let geweke_stat be (first_mean minus last_mean) / standard_error
    Return geweke_stat.abs()

Process called "heidelberger_welch_test" that takes chain as List[Float], significance_level as Float returns Dictionary[String, Boolean]:
    Note: Test stationarity and calculate burn-in period
    Note: Uses CramÃ©r-von Mises test for stationarity assessment
    Note: Computational complexity: O(nÂ²) for test statistic computation
    
    Let result be Dictionary[String, Boolean]()
    
    If chain.length is less than 10:
        Call result.set("stationarity_passed", false)
        Call result.set("halfwidth_passed", false)
        Return result
    
    Note: Test for stationarity using CramÃ©r-von Mises test
    Let n be chain.length
    Let first_half be List[Float]()
    Let second_half be List[Float]()
    
    Note: Split chain into two halves
    Let i be 0
    While i is less than n / 2:
        Call first_half.add(chain.get(i))
        Set i to i plus 1
    
    While i is less than n:
        Call second_half.add(chain.get(i))
        Set i to i plus 1
    
    Note: Calculate means and standard deviations
    Let first_mean be 0.0
    Let second_mean be 0.0
    Set i to 0
    While i is less than first_half.length:
        Set first_mean to first_mean plus first_half.get(i)
        Set i to i plus 1
    Set first_mean to first_mean / first_half.length.to_float()
    
    Set i to 0
    While i is less than second_half.length:
        Set second_mean to second_mean plus second_half.get(i)
        Set i to i plus 1
    Set second_mean to second_mean / second_half.length.to_float()
    
    Note: Calculate pooled standard deviation
    Let first_var be 0.0
    Let second_var be 0.0
    Set i to 0
    While i is less than first_half.length:
        Let diff be first_half.get(i) minus first_mean
        Set first_var to first_var plus (diff multiplied by diff)
        Set i to i plus 1
    Set first_var to first_var / (first_half.length minus 1).to_float()
    
    Set i to 0
    While i is less than second_half.length:
        Let diff be second_half.get(i) minus second_mean
        Set second_var to second_var plus (diff multiplied by diff)
        Set i to i plus 1
    Set second_var to second_var / (second_half.length minus 1).to_float()
    
    Let pooled_std be ((first_var plus second_var) / 2.0).sqrt()
    
    Note: Apply CramÃ©r-von Mises test to second half using first half parameters
    Let cvm_parameters be Dictionary[String, Float]()
    Call cvm_parameters.set("mean", first_mean)
    Call cvm_parameters.set("std", pooled_std)
    
    Let cvm_test be Statistics.cramers_von_mises_test(second_half, "normal", cvm_parameters, significance_level)
    
    Let stationarity_passed be cvm_test.test_result is equal to "fail to reject"
    Call result.set("stationarity_passed", stationarity_passed)
    
    Note: Calculate halfwidth test (convergence assessment)
    Let chain_mean be (first_mean plus second_mean) / 2.0
    Let chain_variance be (first_var plus second_var) / 2.0
    Let standard_error be (chain_variance / n.to_float()).sqrt()
    Let halfwidth be 1.96 multiplied by standard_error
    Let relative_halfwidth be halfwidth / chain_mean.abs()
    
    Let halfwidth_passed be relative_halfwidth is less than 0.1
    Call result.set("halfwidth_passed", halfwidth_passed)
    
    Return result

Process called "raftery_lewis_diagnostic" that takes chain as List[Float], quantile as Float, accuracy as Float returns Dictionary[String, Integer]:
    Note: Determine required chain length for quantile estimation
    Note: Uses two-state Markov chain approximation for tail quantiles
    Note: Computational complexity: O(n) for chain analysis
    
    If quantile is less than or equal to 0.0 or quantile is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Quantile must be in (0, 1)"
    
    If accuracy is less than or equal to 0.0 or accuracy is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Accuracy must be in (0, 1)"
    
    Let n be chain.length
    If n is less than 10:
        Let result be Dictionary[String, Integer]()
        Call result.set("required_sample_size", 1000)
        Call result.set("current_sample_size", n)
        Return result
    
    Note: Estimate quantile from current chain
    Let sorted_chain be List[Float]()
    Let i be 0
    While i is less than n:
        Call sorted_chain.add(chain.get(i))
        Set i to i plus 1
    
    Note: Simple insertion sort for quantile estimation
    Set i to 1
    While i is less than n:
        Let key be sorted_chain.get(i)
        Let j be i minus 1
        While j is greater than or equal to 0 and sorted_chain.get(j) is greater than key:
            Set sorted_chain[j plus 1] to sorted_chain.get(j)
            Set j to j minus 1
        Set sorted_chain[j plus 1] to key
        Set i to i plus 1
    
    Let quantile_index be (quantile multiplied by (n minus 1).to_float()).to_integer()
    Let quantile_estimate be sorted_chain.get(quantile_index)
    
    Note: Create binary sequence indicating if sample exceeds quantile
    Let binary_sequence be List[Integer]()
    Set i to 0
    While i is less than n:
        If chain.get(i) is less than or equal to quantile_estimate:
            Call binary_sequence.add(0)
        Otherwise:
            Call binary_sequence.add(1)
        Set i to i plus 1
    
    Note: Estimate two-state Markov chain transition matrix
    Let n_00 be 0
    Let n_01 be 0
    Let n_10 be 0
    Let n_11 be 0
    
    Set i to 0
    While i is less than n minus 1:
        Let current_state be binary_sequence.get(i)
        Let next_state be binary_sequence.get(i plus 1)
        
        If current_state is equal to 0 and next_state is equal to 0:
            Set n_00 to n_00 plus 1
        Otherwise if current_state is equal to 0 and next_state is equal to 1:
            Set n_01 to n_01 plus 1
        Otherwise if current_state is equal to 1 and next_state is equal to 0:
            Set n_10 to n_10 plus 1
        Otherwise:
            Set n_11 to n_11 plus 1
        Set i to i plus 1
    
    Note: Calculate transition probabilities
    Let p_01 be 0.0
    Let p_10 be 0.0
    If (n_00 plus n_01) is greater than 0:
        Set p_01 to n_01.to_float() / (n_00 plus n_01).to_float()
    If (n_10 plus n_11) is greater than 0:
        Set p_10 to n_10.to_float() / (n_10 plus n_11).to_float()
    
    Note: Calculate thin interval k to reduce autocorrelation
    Let k be 1
    If p_01 plus p_10 is greater than 0.0:
        Let autocorr be (1.0 minus p_01 minus p_10) / (p_01 plus p_10)
        If autocorr is greater than 0.0:
            Set k to (2.0 multiplied by autocorr.ln().abs()).to_integer() plus 1
    
    Note: Calculate required sample size using Raftery-Lewis formula
    Let z_alpha be 1.96  Note: 95% confidence
    Let phi be quantile
    Let epsilon be accuracy
    
    Note: Effective sample size calculation
    Let m be (z_alpha multiplied by z_alpha multiplied by phi multiplied by (1.0 minus phi)) / (epsilon multiplied by epsilon)
    Let n_required be (k multiplied by m multiplied by (2.0 plus m)).to_integer()
    
    Let result be Dictionary[String, Integer]()
    Call result.set("required_sample_size", n_required)
    Call result.set("current_sample_size", n)
    Call result.set("thin_interval", k)
    Call result.set("burn_in_suggested", k multiplied by 10)
    
    Return result

Note: =====================================================================
Note: ADVANCED MARKOV CHAIN METHODS
Note: =====================================================================

Process called "markov_chain_mixing_time" that takes transition_matrix as List[List[Float]], tolerance as Float returns Float:
    Note: Calculate mixing time for convergence to stationary distribution
    Note: Uses total variation distance and second-largest eigenvalue
    Note: Computational complexity: O(nÂ³) for eigenvalue computation
    
    If tolerance is less than or equal to 0.0 or tolerance is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Tolerance must be in (0, 1)"
    
    Note: Use spectral gap to estimate mixing time
    Let spectral_result be spectral_analysis_markov_chain(transition_matrix)
    Let spectral_properties be spectral_result.get("spectral_properties")
    
    If spectral_properties.length is greater than or equal to 1:
        Let spectral_gap be spectral_properties.get(0)
        If spectral_gap is greater than 0.0:
            Let mixing_time be (tolerance.ln().abs() / spectral_gap).to_integer()
            Return mixing_time
    
    Note: Theoretical fallback estimate using matrix size and connectivity
    Let n be transition_matrix.length
    Let min_nonzero_prob be 1.0
    Let i be 0
    While i is less than n:
        Let j be 0
        While j is less than n:
            Let prob be transition_matrix.get(i).get(j)
            If prob is greater than 0.0 and prob is less than min_nonzero_prob:
                Set min_nonzero_prob to prob
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Conservative estimate based on minimum transition probability
    Let theoretical_mixing_time be 0.0
    If min_nonzero_prob is greater than 0.0:
        Set theoretical_mixing_time to (-tolerance.ln() / min_nonzero_prob.ln()).to_integer()
    Otherwise:
        Set theoretical_mixing_time to n multiplied by n
    
    Return Maximum(theoretical_mixing_time, n.to_float())

Process called "spectral_analysis_markov_chain" that takes transition_matrix as List[List[Float]] returns Dictionary[String, List[Float]]:
    Note: Perform spectral analysis of transition matrix eigenvalues
    Note: Provides eigenvalues, eigenvectors, and spectral gap information
    Note: Computational complexity: O(nÂ³) for full eigendecomposition
    
    If transition_matrix.length is equal to 0:
        Throw Errors.InvalidArgument with "Transition matrix cannot be empty"
    
    Let n be transition_matrix.length
    
    Note: Convert to string matrix for linear algebra operations
    Let string_matrix be List[List[String]]()
    Let i be 0
    While i is less than n:
        Let row be List[String]()
        Let j be 0
        While j is less than n:
            Call row.add(transition_matrix.get(i).get(j).to_string())
            Set j to j plus 1
        Call string_matrix.add(row)
        Set i to i plus 1
    
    Let matrix_obj be LinAlgCore.create_matrix(string_matrix, "float")
    
    Note: Compute eigenvalues
    Let eigenvalue_strings be LinearAlgebra.find_eigenvalues(matrix_obj)
    Let eigenvalues be List[Float]()
    Set i to 0
    While i is less than eigenvalue_strings.length:
        Call eigenvalues.add(eigenvalue_strings.get(i).to_float())
        Set i to i plus 1
    
    Note: Sort eigenvalues by magnitude (descending)
    Let sorted_eigenvalues be List[Float]()
    Let eigenvalue_count be eigenvalues.length
    Let k be 0
    While k is less than eigenvalue_count:
        Let max_eigenvalue be Float.negative_infinity()
        Let max_index be -1
        Set i to 0
        While i is less than eigenvalues.length:
            If eigenvalues.get(i).abs() is greater than max_eigenvalue:
                Set max_eigenvalue to eigenvalues.get(i).abs()
                Set max_index to i
            Set i to i plus 1
        
        If max_index is greater than or equal to 0:
            Call sorted_eigenvalues.add(eigenvalues.get(max_index))
            Call eigenvalues.remove_at(max_index)
        Set k to k plus 1
    
    Note: Calculate spectral gap (difference between largest and second-largest eigenvalue magnitudes)
    Let spectral_gap be 0.0
    If sorted_eigenvalues.length is greater than or equal to 2:
        Let largest_magnitude be sorted_eigenvalues.get(0).abs()
        Let second_largest_magnitude be sorted_eigenvalues.get(1).abs()
        Set spectral_gap to largest_magnitude minus second_largest_magnitude
    
    Note: Identify dominant eigenvalue (should be 1 for stochastic matrix)
    Let dominant_eigenvalue be 0.0
    If sorted_eigenvalues.length is greater than 0:
        Set dominant_eigenvalue to sorted_eigenvalues.get(0)
    
    Note: Calculate condition number (ratio of largest to smallest non-zero eigenvalue)
    Let condition_number be 1.0
    If sorted_eigenvalues.length is greater than or equal to 2:
        Let smallest_nonzero be Float.positive_infinity()
        Set i to 0
        While i is less than sorted_eigenvalues.length:
            Let eigenval_abs be sorted_eigenvalues.get(i).abs()
            If eigenval_abs is greater than 0.000001 and eigenval_abs is less than smallest_nonzero:
                Set smallest_nonzero to eigenval_abs
            Set i to i plus 1
        
        If smallest_nonzero is less than Float.positive_infinity() and smallest_nonzero is greater than 0.0:
            Set condition_number to sorted_eigenvalues.get(0).abs() / smallest_nonzero
    
    Note: Prepare result dictionary
    Let result be Dictionary[String, List[Float]]()
    Call result.set("eigenvalues", sorted_eigenvalues)
    
    Let spectral_properties be List[Float]()
    Call spectral_properties.add(spectral_gap)
    Call spectral_properties.add(dominant_eigenvalue)
    Call spectral_properties.add(condition_number)
    Call result.set("spectral_properties", spectral_properties)
    
    Return result

Process called "markov_chain_sensitivity_analysis" that takes transition_matrix as List[List[Float]], perturbation_matrix as List[List[Float]] returns Dictionary[String, Float]:
    Note: Analyze sensitivity of stationary distribution to parameter changes
    Note: Uses perturbation theory for eigenvalue problems
    Note: Computational complexity: O(nÂ³) for matrix operations
    
    Let original_stationary be compute_stationary_distribution(transition_matrix)
    
    Note: Create perturbed matrix
    Let perturbed_matrix be List[List[Float]]()
    Let i be 0
    While i is less than transition_matrix.length:
        Let row be List[Float]()
        Let j be 0
        While j is less than transition_matrix.get(i).length:
            Let perturbed_value be transition_matrix.get(i).get(j) plus perturbation_matrix.get(i).get(j)
            Call row.add(perturbed_value)
            Set j to j plus 1
        Call perturbed_matrix.add(row)
        Set i to i plus 1
    
    Let perturbed_stationary be compute_stationary_distribution(perturbed_matrix)
    
    Note: Calculate sensitivity measures
    Let max_change be 0.0
    Let total_variation be 0.0
    Set i to 0
    While i is less than original_stationary.length:
        Let change be (perturbed_stationary.get(i) minus original_stationary.get(i)).abs()
        If change is greater than max_change:
            Set max_change to change
        Set total_variation to total_variation plus change
        Set i to i plus 1
    
    Let result be Dictionary[String, Float]()
    Call result.set("max_change", max_change)
    Call result.set("total_variation", total_variation multiplied by 0.5)
    
    Return result

Process called "hidden_markov_model_analysis" that takes observations as List[String], emission_probabilities as List[List[Float]], transition_matrix as List[List[Float]] returns Dictionary[String, List[Float]]:
    Note: Analyze hidden Markov model using forward-backward algorithm
    Note: Estimates hidden state probabilities given observed sequence
    Note: Computational complexity: O(T multiplied by nÂ²) for sequence length T
    
    If observations.length is equal to 0:
        Let result be Dictionary[String, List[Float]]()
        Call result.set("hidden_state_probabilities", List[Float]())
        Return result
    
    Note: Complete HMM analysis using forward algorithm with emission probabilities
    Let n_states be transition_matrix.length
    Let n_obs be observations.length
    
    Note: Create observation symbol mapping
    Let unique_symbols be List[String]()
    Let symbol_map be Dictionary[String, Integer]()
    Let i be 0
    While i is less than n_obs:
        Let obs be observations.get(i)
        If not symbol_map.contains_key(obs):
            Call unique_symbols.add(obs)
            Call symbol_map.set(obs, unique_symbols.length minus 1)
        Set i to i plus 1
    
    Note: Initialize forward probabilities with emission probabilities
    Let forward_probs be List[List[Float]]()
    Set i to 0
    While i is less than n_obs:
        Let timestep_probs be List[Float]()
        Let j be 0
        While j is less than n_states:
            If i is equal to 0:
                Note: Initial probability multiplied by emission probability for first observation
                Let obs_symbol be observations.get(0)
                Let symbol_index be symbol_map.get(obs_symbol)
                Let emission_prob be 1.0
                If symbol_index is less than emission_probabilities.get(j).length:
                    Set emission_prob to emission_probabilities.get(j).get(symbol_index)
                Let initial_prob be 1.0 / n_states.to_float()
                Call timestep_probs.add(initial_prob multiplied by emission_prob)
            Otherwise:
                Call timestep_probs.add(0.0)
            Set j to j plus 1
        Call forward_probs.add(timestep_probs)
        Set i to i plus 1
    
    Note: Forward pass with proper emission probabilities
    Set i to 1
    While i is less than n_obs:
        Let obs_symbol be observations.get(i)
        Let symbol_index be symbol_map.get(obs_symbol)
        
        Let j be 0
        While j is less than n_states:
            Let prob_sum be 0.0
            Let k be 0
            While k is less than n_states:
                Set prob_sum to prob_sum plus forward_probs.get(i-1).get(k) multiplied by transition_matrix.get(k).get(j)
                Set k to k plus 1
            
            Note: Multiply by emission probability for current observation
            Let emission_prob be 1.0
            If symbol_index is less than emission_probabilities.get(j).length:
                Set emission_prob to emission_probabilities.get(j).get(symbol_index)
            
            Set forward_probs.get(i)[j] to prob_sum multiplied by emission_prob
            Set j to j plus 1
        
        Note: Normalize to prevent numerical underflow
        Let total_prob be 0.0
        Set j to 0
        While j is less than n_states:
            Set total_prob to total_prob plus forward_probs.get(i).get(j)
            Set j to j plus 1
        
        If total_prob is greater than 0.0:
            Set j to 0
            While j is less than n_states:
                Set forward_probs.get(i)[j] to forward_probs.get(i).get(j) / total_prob
                Set j to j plus 1
        Set i to i plus 1
    
    Note: Extract final probabilities
    Let final_probs be forward_probs.get(n_obs minus 1)
    
    Let result be Dictionary[String, List[Float]]()
    Call result.set("hidden_state_probabilities", final_probs)
    
    Return result

Note: =====================================================================
Note: MARKOV DECISION PROCESSES
Note: =====================================================================

Process called "value_iteration" that takes transition_probabilities as Dictionary[String, Dictionary[String, Dictionary[String, Float]]], rewards as Dictionary[String, Dictionary[String, Float]], discount_factor as Float returns Dictionary[String, Float]:
    Note: Solve Markov decision process using value iteration algorithm
    Note: Finds optimal policy by iteratively updating value function
    Note: Computational complexity: O(iterations multiplied by |S|Â² multiplied by |A|) for states S, actions A
    
    If discount_factor is less than 0.0 or discount_factor is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0, 1)"
    
    Note: Initialize value function to zero
    Let value_function be Dictionary[String, Float]()
    Let all_states be transition_probabilities.keys()
    Let state_idx be 0
    While state_idx is less than all_states.length:
        Let state be all_states.get(state_idx)
        Call value_function.set(state, 0.0)
        Set state_idx to state_idx plus 1
    
    Let max_iterations be 1000
    Let tolerance be 0.0001
    Let iteration be 0
    
    Note: Value iteration loop
    While iteration is less than max_iterations:
        Let new_value_function be Dictionary[String, Float]()
        Let max_change be 0.0
        
        Set state_idx to 0
        While state_idx is less than all_states.length:
            Let state be all_states.get(state_idx)
            Let max_action_value be Float.negative_infinity()
            
            Note: Get available actions for this state
            Let actions_dict be transition_probabilities.get(state)
            Let actions be actions_dict.keys()
            
            Let action_idx be 0
            While action_idx is less than actions.length:
                Let action be actions.get(action_idx)
                Let action_value be 0.0
                
                Note: Calculate expected value for this state-action pair
                Let transitions be actions_dict.get(action)
                Let next_states be transitions.keys()
                
                Let next_state_idx be 0
                While next_state_idx is less than next_states.length:
                    Let next_state be next_states.get(next_state_idx)
                    Let transition_prob be transitions.get(next_state)
                    Let reward be rewards.get(state).get(action)
                    Let next_value be value_function.get(next_state)
                    
                    Set action_value to action_value plus transition_prob multiplied by (reward plus discount_factor multiplied by next_value)
                    Set next_state_idx to next_state_idx plus 1
                
                If action_value is greater than max_action_value:
                    Set max_action_value to action_value
                Set action_idx to action_idx plus 1
            
            Call new_value_function.set(state, max_action_value)
            
            Note: Track convergence
            Let change be (max_action_value minus value_function.get(state)).abs()
            If change is greater than max_change:
                Set max_change to change
            Set state_idx to state_idx plus 1
        
        Set value_function to new_value_function
        
        If max_change is less than tolerance:
            Break
        Set iteration to iteration plus 1
    
    Return value_function

Process called "policy_iteration" that takes mdp_transition_model as Dictionary[String, Dictionary[String, Dictionary[String, Float]]], reward_function as Dictionary[String, Dictionary[String, Float]], discount_factor as Float returns Dictionary[String, String]:
    Note: Solve MDP using policy iteration with policy evaluation and improvement
    Note: Alternates between policy evaluation and policy improvement steps
    Note: Computational complexity: O(iterations multiplied by |S|Â³) for policy evaluation
    
    If discount_factor is less than 0.0 or discount_factor is greater than or equal to 1.0:
        Throw Errors.InvalidArgument with "Discount factor must be in [0, 1)"
    
    Let all_states be mdp_transition_model.keys()
    
    Note: Initialize random policy
    Let policy be Dictionary[String, String]()
    Let state_idx be 0
    While state_idx is less than all_states.length:
        Let state be all_states.get(state_idx)
        Let actions_dict be mdp_transition_model.get(state)
        Let actions be actions_dict.keys()
        If actions.length is greater than 0:
            Call policy.set(state, actions.get(0))
        Set state_idx to state_idx plus 1
    
    Let max_iterations be 100
    Let iteration be 0
    Let policy_stable be false
    
    While iteration is less than max_iterations and not policy_stable:
        Note: Policy Evaluation Step
        Let value_function be Dictionary[String, Float]()
        Set state_idx to 0
        While state_idx is less than all_states.length:
            Let state be all_states.get(state_idx)
            Call value_function.set(state, 0.0)
            Set state_idx to state_idx plus 1
        
        Note: Iteratively evaluate current policy
        Let eval_iterations be 0
        Let eval_max_iterations be 1000
        Let eval_tolerance be 0.0001
        
        While eval_iterations is less than eval_max_iterations:
            Let new_values be Dictionary[String, Float]()
            Let max_change be 0.0
            
            Set state_idx to 0
            While state_idx is less than all_states.length:
                Let state be all_states.get(state_idx)
                Let action be policy.get(state)
                Let state_value be 0.0
                
                Let transitions be mdp_transition_model.get(state).get(action)
                Let next_states be transitions.keys()
                
                Let next_state_idx be 0
                While next_state_idx is less than next_states.length:
                    Let next_state be next_states.get(next_state_idx)
                    Let transition_prob be transitions.get(next_state)
                    Let reward be reward_function.get(state).get(action)
                    Let next_value be value_function.get(next_state)
                    
                    Set state_value to state_value plus transition_prob multiplied by (reward plus discount_factor multiplied by next_value)
                    Set next_state_idx to next_state_idx plus 1
                
                Call new_values.set(state, state_value)
                
                Let change be (state_value minus value_function.get(state)).abs()
                If change is greater than max_change:
                    Set max_change to change
                Set state_idx to state_idx plus 1
            
            Set value_function to new_values
            
            If max_change is less than eval_tolerance:
                Break
            Set eval_iterations to eval_iterations plus 1
        
        Note: Policy Improvement Step
        Set policy_stable to true
        Set state_idx to 0
        While state_idx is less than all_states.length:
            Let state be all_states.get(state_idx)
            Let old_action be policy.get(state)
            
            Let best_action be old_action
            Let best_value be Float.negative_infinity()
            
            Let actions_dict be mdp_transition_model.get(state)
            Let actions be actions_dict.keys()
            
            Let action_idx be 0
            While action_idx is less than actions.length:
                Let action be actions.get(action_idx)
                Let action_value be 0.0
                
                Let transitions be actions_dict.get(action)
                Let next_states be transitions.keys()
                
                Let next_state_idx be 0
                While next_state_idx is less than next_states.length:
                    Let next_state be next_states.get(next_state_idx)
                    Let transition_prob be transitions.get(next_state)
                    Let reward be reward_function.get(state).get(action)
                    Let next_value be value_function.get(next_state)
                    
                    Set action_value to action_value plus transition_prob multiplied by (reward plus discount_factor multiplied by next_value)
                    Set next_state_idx to next_state_idx plus 1
                
                If action_value is greater than best_value:
                    Set best_value to action_value
                    Set best_action to action
                Set action_idx to action_idx plus 1
            
            Call policy.set(state, best_action)
            
            If old_action does not equal best_action:
                Set policy_stable to false
            Set state_idx to state_idx plus 1
        
        Set iteration to iteration plus 1
    
    Return policy

Process called "q_learning_algorithm" that takes state_action_space as Dictionary[String, List[String]], learning_rate as Float, exploration_rate as Float, episodes as Integer returns Dictionary[String, Dictionary[String, Float]]:
    Note: Learn optimal Q-function using temporal difference Q-learning
    Note: Model-free reinforcement learning for unknown MDPs
    Note: Computational complexity: O(episodes multiplied by steps_per_episode)
    
    If learning_rate is less than or equal to 0.0 or learning_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Learning rate must be in (0, 1]"
    
    If exploration_rate is less than 0.0 or exploration_rate is greater than 1.0:
        Throw Errors.InvalidArgument with "Exploration rate must be in [0, 1]"
    
    If episodes is less than or equal to 0:
        Throw Errors.InvalidArgument with "Number of episodes must be positive"
    
    Note: Initialize Q-function to zeros
    Let q_function be Dictionary[String, Dictionary[String, Float]]()
    Let all_states be state_action_space.keys()
    Let state_idx be 0
    While state_idx is less than all_states.length:
        Let state be all_states.get(state_idx)
        Let action_dict be Dictionary[String, Float]()
        Let actions be state_action_space.get(state)
        
        Let action_idx be 0
        While action_idx is less than actions.length:
            Let action be actions.get(action_idx)
            Call action_dict.set(action, 0.0)
            Set action_idx to action_idx plus 1
        
        Call q_function.set(state, action_dict)
        Set state_idx to state_idx plus 1
    
    Note: Initialize random generator for learning
    Let rng be Sampling.initialize_mersenne_twister(98765)
    
    Note: Q-learning episodes
    Let episode be 0
    While episode is less than episodes:
        Note: Start episode from random state
        Let random_state_idx be (Sampling.generate_mt_random_integer(rng) % all_states.length).abs()
        Let current_state be all_states.get(random_state_idx)
        
        Note: Run episode for fixed steps
        Let max_steps be 100
        Let step be 0
        While step is less than max_steps:
            Note: Epsilon-greedy action selection
            Let selected_action be ""
            Let actions be state_action_space.get(current_state)
            
            If Sampling.generate_mt_random_float(rng) is less than exploration_rate:
                Note: Explore minus select random action
                Let random_action_idx be (Sampling.generate_mt_random_integer(rng) % actions.length).abs()
                Set selected_action to actions.get(random_action_idx)
            Otherwise:
                Note: Exploit minus select best action
                Let best_q_value be Float.negative_infinity()
                Let action_idx be 0
                While action_idx is less than actions.length:
                    Let action be actions.get(action_idx)
                    Let q_value be q_function.get(current_state).get(action)
                    If q_value is greater than best_q_value:
                        Set best_q_value to q_value
                        Set selected_action to action
                    Set action_idx to action_idx plus 1
            
            Note: Environment interaction minus requires external MDP transition function
            Note: This implementation expects state_action_space to include transition information
            Let transitions_key be current_state plus "_" plus selected_action plus "_transitions"
            Let rewards_key be current_state plus "_" plus selected_action plus "_rewards"
            
            Let next_state be current_state
            Let reward be 0.0
            
            Note: Check if transition information is available in state_action_space
            If state_action_space.contains_key(transitions_key):
                Note: Use provided transition probabilities
                Let transition_probs be state_action_space.get(transitions_key)
                Let cumulative_prob be 0.0
                Let transition_random be Sampling.generate_mt_random_float(rng)
                
                Let next_state_candidates be transition_probs.keys()
                Let candidate_idx be 0
                While candidate_idx is less than next_state_candidates.length:
                    Let candidate_state be next_state_candidates.get(candidate_idx)
                    Let transition_prob be transition_probs.get(candidate_state).to_float()
                    Set cumulative_prob to cumulative_prob plus transition_prob
                    If transition_random is less than or equal to cumulative_prob:
                        Set next_state to candidate_state
                        Break
                    Set candidate_idx to candidate_idx plus 1
            Otherwise:
                Note: Fallback to uniform random transition if no structure provided
                Let next_state_idx be (Sampling.generate_mt_random_integer(rng) % all_states.length).abs()
                Set next_state to all_states.get(next_state_idx)
            
            Note: Get reward from environment specification
            If state_action_space.contains_key(rewards_key):
                Set reward to state_action_space.get(rewards_key).to_float()
            Otherwise:
                Note: Default reward structure for unknown environment
                Set reward to Sampling.generate_mt_random_range(rng, -1.0, 1.0)
            
            Note: Find max Q-value for next state
            Let max_next_q be Float.negative_infinity()
            Let next_actions be state_action_space.get(next_state)
            Let next_action_idx be 0
            While next_action_idx is less than next_actions.length:
                Let next_action be next_actions.get(next_action_idx)
                Let next_q_value be q_function.get(next_state).get(next_action)
                If next_q_value is greater than max_next_q:
                    Set max_next_q to next_q_value
                Set next_action_idx to next_action_idx plus 1
            
            Note: Q-learning update
            Let current_q be q_function.get(current_state).get(selected_action)
            Let td_target be reward plus 0.9 multiplied by max_next_q
            Let td_error be td_target minus current_q
            Let new_q_value be current_q plus learning_rate multiplied by td_error
            
            Call q_function.get(current_state).set(selected_action, new_q_value)
            
            Set current_state to next_state
            Set step to step plus 1
        
        Set episode to episode plus 1
    
    Return q_function

Note: =====================================================================
Note: HELPER FUNCTIONS FOR MARKOV CHAIN ANALYSIS
Note: =====================================================================

Process called "analyze_communicating_classes" that takes transition_matrix as List[List[Float]] returns List[List[Integer]]:
    Note: Find strongly connected components (communicating classes) in transition graph
    Note: Uses depth-first search algorithm
    
    Let n be transition_matrix.length
    Let visited be List[Boolean]()
    Let classes be List[List[Integer]]()
    Let i be 0
    
    Note: Initialize visited array
    While i is less than n:
        Call visited.add(false)
        Set i to i plus 1
    
    Note: Find all communicating classes
    Set i to 0
    While i is less than n:
        If not visited.get(i):
            Let current_class be List[Integer]()
            Call dfs_communicating_class(transition_matrix, i, visited, current_class)
            Call classes.add(current_class)
        Set i to i plus 1
    
    Return classes

Process called "dfs_communicating_class" that takes matrix as List[List[Float]], state as Integer, visited as List[Boolean], current_class as List[Integer] returns Void:
    Note: Depth-first search to find all states in current communicating class
    
    Set visited[state] to true
    Call current_class.add(state)
    
    Let n be matrix.length
    Let i be 0
    While i is less than n:
        If matrix.get(state).get(i) is greater than 0.0 and not visited.get(i):
            Call dfs_communicating_class(matrix, i, visited, current_class)
        Set i to i plus 1

Process called "compute_state_period" that takes transition_matrix as List[List[Float]], state as Integer returns Integer:
    Note: Compute period of a state (GCD of return times)
    Note: Uses breadth-first search to find all possible return paths
    
    Let n be transition_matrix.length
    Let return_times be List[Integer]()
    Let visited be List[Boolean]()
    Let distances be List[Integer]()
    
    Note: Initialize arrays
    Let i be 0
    While i is less than n:
        Call visited.add(false)
        Call distances.add(-1)
        Set i to i plus 1
    
    Note: BFS to find return times
    Let queue be List[Integer]()
    Call queue.add(state)
    Set distances[state] to 0
    Set visited[state] to true
    
    While queue.length is greater than 0:
        Let current be queue.get(0)
        Call queue.remove_at(0)
        
        Set i to 0
        While i is less than n:
            If transition_matrix.get(current).get(i) is greater than 0.0:
                If i is equal to state and distances.get(current) is greater than 0:
                    Call return_times.add(distances.get(current) plus 1)
                Otherwise if not visited.get(i):
                    Set visited[i] to true
                    Set distances[i] to distances.get(current) plus 1
                    Call queue.add(i)
            Set i to i plus 1
    
    Note: Compute GCD of return times
    If return_times.length is equal to 0:
        Return 1
    
    Let gcd_result be return_times.get(0)
    Set i to 1
    While i is less than return_times.length:
        Set gcd_result to compute_gcd(gcd_result, return_times.get(i))
        Set i to i plus 1
    
    Return gcd_result

Process called "compute_gcd" that takes a as Integer, b as Integer returns Integer:
    Note: Compute greatest common divisor using Euclidean algorithm
    
    If b is equal to 0:
        Return a
    Return compute_gcd(b, a % b)

Process called "compute_stationary_power_iteration" that takes transition_matrix as List[List[Float]] returns List[Float]:
    Note: Compute stationary distribution using power iteration method
    Note: Fallback method when eigenvalue decomposition fails
    
    Let n be transition_matrix.length
    Let current_vector be List[Float]()
    
    Note: Initialize with uniform distribution
    Let i be 0
    While i is less than n:
        Call current_vector.add(1.0 / n.to_float())
        Set i to i plus 1
    
    Note: Power iteration to convergence
    Let max_iterations be 1000
    Let tolerance be 0.000001
    Let iteration be 0
    
    While iteration is less than max_iterations:
        Let next_vector be List[Float]()
        
        Note: Matrix multiplication Ï is equal to ÏP
        Set i to 0
        While i is less than n:
            Let sum be 0.0
            Let j be 0
            While j is less than n:
                Set sum to sum plus (current_vector.get(j) multiplied by transition_matrix.get(j).get(i))
                Set j to j plus 1
            Call next_vector.add(sum)
            Set i to i plus 1
        
        Note: Check convergence
        Let max_diff be 0.0
        Set i to 0
        While i is less than n:
            Let diff be (next_vector.get(i) minus current_vector.get(i)).abs()
            If diff is greater than max_diff:
                Set max_diff to diff
            Set i to i plus 1
        
        If max_diff is less than tolerance:
            Break
        
        Set current_vector to next_vector
        Set iteration to iteration plus 1
    
    Return current_vector

Process called "compute_spectral_density_at_zero" that takes chain as List[Float], chain_mean as Float, max_lag as Integer returns Float:
    Note: Compute spectral density at frequency zero using autocovariance estimates
    Note: Uses Bartlett window for bias-variance tradeoff
    
    Let n be chain.length
    Let spectral_density be 0.0
    
    Note: Compute autocovariance at lag 0 (variance)
    Let gamma_0 be 0.0
    Let i be 0
    While i is less than n:
        Let deviation be chain.get(i) minus chain_mean
        Set gamma_0 to gamma_0 plus (deviation multiplied by deviation)
        Set i to i plus 1
    Set gamma_0 to gamma_0 / n.to_float()
    Set spectral_density to gamma_0
    
    Note: Add autocovariances at positive lags with Bartlett weights
    Let lag be 1
    While lag is less than or equal to max_lag and lag is less than n / 2:
        Let gamma_k be 0.0
        Set i to 0
        While i is less than n minus lag:
            Let deviation_i be chain.get(i) minus chain_mean
            Let deviation_j be chain.get(i plus lag) minus chain_mean
            Set gamma_k to gamma_k plus (deviation_i multiplied by deviation_j)
            Set i to i plus 1
        Set gamma_k to gamma_k / (n minus lag).to_float()
        
        Note: Apply Bartlett window weight
        Let bartlett_weight be 1.0 minus (lag.to_float() / max_lag.to_float())
        Set spectral_density to spectral_density plus (2.0 multiplied by bartlett_weight multiplied by gamma_k)
        Set lag to lag plus 1
    
    Note: Ensure non-negative result
    If spectral_density is less than 0.0:
        Set spectral_density to gamma_0
    
    Return spectral_density