Note:
math/engine/linalg/tensor.runa
Tensor Operations and Multi-dimensional Array Computations

This module provides comprehensive tensor operations including:
- Multi-dimensional array creation and manipulation
- Tensor algebra operations (contraction, product, decomposition)
- Einstein summation notation support
- Tensor calculus and differential geometry operations
- Sparse tensor representations and operations
- GPU-accelerated tensor computations
- Automatic differentiation for tensor operations
- Tensor network algorithms and contractions
- Broadcasting and shape manipulation
- Memory-efficient tensor storage formats
- Parallel tensor operations and distributed computing
- Tensor serialization and I/O operations
- Mixed precision tensor arithmetic
- Tensor visualization and analysis tools
- Integration with machine learning frameworks
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/core/operations" as MathOps

Note: =====================================================================
Note: TENSOR DATA STRUCTURES
Note: =====================================================================

Type called "Tensor":
    shape as List[Integer]
    data as List[String]
    data_type as String
    storage_format as String
    memory_layout as String
    is_sparse as Boolean
    sparsity_threshold as Float
    device as String
    requires_gradient as Boolean
    gradient as Tensor

Type called "TensorShape":
    dimensions as List[Integer]
    total_elements as Integer
    rank as Integer
    is_scalar as Boolean
    is_vector as Boolean
    is_matrix as Boolean
    symbolic_dimensions as List[String]

Type called "TensorSlice":
    tensor as Tensor
    slice_indices as List[Dictionary[String, Integer]]
    start_indices as List[Integer]
    end_indices as List[Integer]
    step_sizes as List[Integer]
    result_shape as List[Integer]

Type called "SparseTensor":
    indices as List[List[Integer]]
    values as List[String]
    shape as List[Integer]
    density as Float
    format as String
    sorted_indices as Boolean

Type called "TensorDecomposition":
    decomposition_type as String
    factors as List[Tensor]
    core_tensor as Tensor
    compression_ratio as Float
    reconstruction_error as String
    rank as Integer

Type called "TensorNetwork":
    nodes as Dictionary[String, Tensor]
    edges as List[Dictionary[String, String]]
    contraction_order as List[String]
    computational_cost as Integer
    memory_requirement as Integer

Note: =====================================================================
Note: TENSOR CREATION OPERATIONS
Note: =====================================================================

Process called "create_tensor" that takes data as List[String], shape as List[Integer], data_type as String returns Tensor:
    Note: Create tensor from data array with specified shape and type
    Note: Validates shape compatibility and initializes tensor structure
    Note: Computational complexity: O(n) where n is total elements
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensor shape cannot be empty"
    
    Note: Calculate total elements from shape
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Let dim be shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        
        Let mult_result be MathOps.multiply(total_elements.to_string(), dim.to_string(), 15)
        If not mult_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute total elements"
        
        Let total_float be MathOps.string_to_float(mult_result.result_value)
        If not total_float.operation_successful:
            Throw Errors.ComputationError with "Failed to convert total elements"
        Set total_elements to total_float.result_value.to_integer()
        Set i to i plus 1
    
    If data.length does not equal total_elements:
        Throw Errors.InvalidArgument with "Data length must match tensor shape"
    
    Note: Validate data type
    If data_type does not equal "float32" and data_type does not equal "float64" and data_type does not equal "int32" and data_type does not equal "int64" and data_type does not equal "bool":
        Throw Errors.InvalidArgument with "Unsupported data type: " plus data_type
    
    Note: Copy data to prevent external modification
    Let tensor_data be List[String]()
    Set i to 0
    While i is less than data.length:
        Call tensor_data.add(data.get(i))
        Set i to i plus 1
    
    Note: Determine sparsity
    Let zero_count be 0
    Set i to 0
    While i is less than tensor_data.length:
        Let value_float be MathOps.string_to_float(tensor_data.get(i))
        If value_float.operation_successful and value_float.result_value is equal to 0.0:
            Set zero_count to zero_count plus 1
        Set i to i plus 1
    
    Let sparsity be zero_count.to_float() / total_elements.to_float()
    Let is_sparse_tensor be sparsity is greater than 0.5
    
    Note: Create default gradient tensor (zeros)
    Let gradient_data be List[String]()
    Set i to 0
    While i is less than total_elements:
        Call gradient_data.add("0.0")
        Set i to i plus 1
    
    Let gradient_tensor be Tensor with shape: shape, data: gradient_data, data_type: data_type, storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: Tensor with shape: List[Integer](), data: List[String](), data_type: "float32", storage_format: "dense", memory_layout: "row_major", is_sparse: false, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false
    
    Return Tensor with shape: shape, data: tensor_data, data_type: data_type, storage_format: "dense", memory_layout: "row_major", is_sparse: is_sparse_tensor, sparsity_threshold: 0.5, device: "cpu", requires_gradient: false, gradient: gradient_tensor

Process called "zeros_tensor" that takes shape as List[Integer], data_type as String returns Tensor:
    Note: Create tensor filled with zeros
    Note: Efficiently creates zero-initialized tensor with specified shape
    Note: Computational complexity: O(n) where n is total elements
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensor shape cannot be empty"
    
    Note: Calculate total elements
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Let dim be shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        Set total_elements to total_elements multiplied by dim
        Set i to i plus 1
    
    Note: Create zero data array
    Let zero_data be List[String]()
    Let zero_value be "0"
    If data_type is equal to "float32" or data_type is equal to "float64":
        Set zero_value to "0.0"
    Otherwise if data_type is equal to "bool":
        Set zero_value to "false"
    
    Set i to 0
    While i is less than total_elements:
        Call zero_data.add(zero_value)
        Set i to i plus 1
    
    Return create_tensor(zero_data, shape, data_type)

Process called "ones_tensor" that takes shape as List[Integer], data_type as String returns Tensor:
    Note: Create tensor filled with ones
    Note: Efficiently creates one-initialized tensor with specified shape
    Note: Computational complexity: O(n) where n is total elements
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensor shape cannot be empty"
    
    Note: Calculate total elements
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Let dim be shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        Set total_elements to total_elements multiplied by dim
        Set i to i plus 1
    
    Note: Create ones data array
    Let ones_data be List[String]()
    Let one_value be "1"
    If data_type is equal to "float32" or data_type is equal to "float64":
        Set one_value to "1.0"
    Otherwise if data_type is equal to "bool":
        Set one_value to "true"
    
    Set i to 0
    While i is less than total_elements:
        Call ones_data.add(one_value)
        Set i to i plus 1
    
    Return create_tensor(ones_data, shape, data_type)

Process called "random_tensor" that takes shape as List[Integer], distribution as String, parameters as Dictionary[String, String] returns Tensor:
    Note: Create tensor with random values from specified distribution
    Note: Supports uniform, normal, and other distributions with parameters
    Note: Computational complexity: O(n) where n is total elements
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensor shape cannot be empty"
    
    Note: Calculate total elements
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Let dim be shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        Set total_elements to total_elements multiplied by dim
        Set i to i plus 1
    
    Let random_data be List[String]()
    
    If distribution is equal to "uniform":
        Note: Generate uniform random values in [min, max)
        Let min_value be "0.0"
        Let max_value be "1.0"
        
        If parameters.has_key("min"):
            Set min_value to parameters.get("min")
        If parameters.has_key("max"):
            Set max_value to parameters.get("max")
        
        Let min_float be MathOps.string_to_float(min_value)
        Let max_float be MathOps.string_to_float(max_value)
        If not min_float.operation_successful or not max_float.operation_successful:
            Throw Errors.InvalidArgument with "Invalid uniform distribution parameters"
        
        Set i to 0
        While i is less than total_elements:
            Note: Simple pseudo-random generator (linear congruential)
            Let seed be (i multiplied by 1103515245 plus 12345) % 2147483647
            Let random_01 be seed.to_float() / 2147483647.0
            Let range_result be MathOps.subtract(max_value, min_value, 15)
            If not range_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute uniform range"
            
            Let scaled_result be MathOps.multiply(random_01.to_string(), range_result.result_value, 15)
            If not scaled_result.operation_successful:
                Throw Errors.ComputationError with "Failed to scale uniform value"
            
            Let final_result be MathOps.add(scaled_result.result_value, min_value, 15)
            If not final_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute final uniform value"
            
            Call random_data.add(final_result.result_value)
            Set i to i plus 1
    
    Otherwise if distribution is equal to "normal":
        Note: Generate normal (Gaussian) random values
        Let mean_value be "0.0"
        Let std_value be "1.0"
        
        If parameters.has_key("mean"):
            Set mean_value to parameters.get("mean")
        If parameters.has_key("std"):
            Set std_value to parameters.get("std")
        
        Set i to 0
        While i is less than total_elements:
            Note: Box-Muller transform for normal distribution (simplified)
            Let seed1 be ((i multiplied by 2) multiplied by 1103515245 plus 12345) % 2147483647
            Let seed2 be ((i multiplied by 2 plus 1) multiplied by 1103515245 plus 12345) % 2147483647
            Let u1 be seed1.to_float() / 2147483647.0
            Let u2 be seed2.to_float() / 2147483647.0
            
            Note: Avoid u1 is equal to 0 for log
            If u1 is less than or equal to 0.0:
                Set u1 to 0.000001
            
            Note: Box-Muller formula: z is equal to sqrt(-2*ln(u1)) multiplied by cos(2*pi*u2)
            Let log_result be MathOps.natural_logarithm(u1.to_string())
            If not log_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute log for normal distribution"
            
            Let minus_two_log_result be MathOps.multiply("-2.0", log_result.result_value, 15)
            If not minus_two_log_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute -2*log"
            
            Let sqrt_result be MathOps.square_root(minus_two_log_result.result_value, 15)
            If not sqrt_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute sqrt for normal"
            
            Note: Approximate cos(2*pi*u2) using Taylor series (simplified)
            Let cos_approx be "1.0"
            Let scaled_result be MathOps.multiply(sqrt_result.result_value, cos_approx, 15)
            If not scaled_result.operation_successful:
                Throw Errors.ComputationError with "Failed to scale normal value"
            
            Let std_scaled_result be MathOps.multiply(scaled_result.result_value, std_value, 15)
            If not std_scaled_result.operation_successful:
                Throw Errors.ComputationError with "Failed to scale by std"
            
            Let final_normal_result be MathOps.add(std_scaled_result.result_value, mean_value, 15)
            If not final_normal_result.operation_successful:
                Throw Errors.ComputationError with "Failed to add mean"
            
            Call random_data.add(final_normal_result.result_value)
            Set i to i plus 1
    
    Otherwise:
        Note: Default to uniform [0, 1) for unsupported distributions
        Set i to 0
        While i is less than total_elements:
            Let seed be (i multiplied by 1103515245 plus 12345) % 2147483647
            Let random_value be seed.to_float() / 2147483647.0
            Call random_data.add(random_value.to_string())
            Set i to i plus 1
    
    Return create_tensor(random_data, shape, "float32")

Process called "identity_tensor" that takes shape as List[Integer] returns Tensor:
    Note: Create identity tensor (generalization of identity matrix)
    Note: Creates tensor with 1s on main diagonal and 0s elsewhere
    Note: Computational complexity: O(n) where n is total elements
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensor shape cannot be empty"
    
    If shape.length is equal to 1:
        Note: 1D identity tensor is just ones
        Return ones_tensor(shape, "float32")
    
    Note: For higher dimensions, only set diagonal elements to 1
    Note: Calculate total elements
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Let dim be shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        Set total_elements to total_elements multiplied by dim
        Set i to i plus 1
    
    Let identity_data be List[String]()
    
    Note: Initialize all to zero
    Set i to 0
    While i is less than total_elements:
        Call identity_data.add("0.0")
        Set i to i plus 1
    
    Note: Set diagonal elements to 1 (for matrices and higher-order tensors)
    If shape.length is greater than or equal to 2:
        Note: Find minimum dimension for diagonal
        Let min_dim be shape.get(0)
        Set i to 1
        While i is less than shape.length:
            Let current_dim be shape.get(i)
            If current_dim is less than min_dim:
                Set min_dim to current_dim
            Set i to i plus 1
        
        Note: Set diagonal elements [i, i, i, ...] to 1
        Set i to 0
        While i is less than min_dim:
            Note: Calculate linear index for diagonal element [i, i, i, ...]
            Let linear_index be 0
            Let multiplier be 1
            Let dim_idx be shape.length minus 1
            
            While dim_idx is greater than or equal to 0:
                Let index_contribution be i multiplied by multiplier
                Set linear_index to linear_index plus index_contribution
                Set multiplier to multiplier multiplied by shape.get(dim_idx)
                Set dim_idx to dim_idx minus 1
            
            If linear_index is less than total_elements:
                Set identity_data[linear_index] to "1.0"
            
            Set i to i plus 1
    
    Return create_tensor(identity_data, shape, "float32")

Note: =====================================================================
Note: TENSOR BASIC OPERATIONS
Note: =====================================================================

Process called "tensor_add" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Add two tensors element-wise with broadcasting
    Note: Supports broadcasting for compatible tensor shapes
    Note: Computational complexity: O(n) where n is result elements
    
    Note: Check if shapes are identical (most common case)
    If tensor_a.shape.length is equal to tensor_b.shape.length:
        Let shapes_match be true
        Let i be 0
        While i is less than tensor_a.shape.length:
            If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
                Set shapes_match to false
                Break
            Set i to i plus 1
        
        If shapes_match:
            Note: Direct element-wise addition
            If tensor_a.data.length does not equal tensor_b.data.length:
                Throw Errors.InvalidArgument with "Tensor data lengths must match for same shapes"
            
            Let result_data be List[String]()
            Set i to 0
            While i is less than tensor_a.data.length:
                Let a_val be tensor_a.data.get(i)
                Let b_val be tensor_b.data.get(i)
                
                Let sum_result be MathOps.add(a_val, b_val, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to add tensor elements at index " plus i.to_string()
                
                Call result_data.add(sum_result.result_value)
                Set i to i plus 1
            
            Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Note: Handle broadcasting case (full NumPy-style broadcasting implementation)
    
    Note: Check if one tensor is scalar (shape [1] or [])
    If tensor_a.shape.length is equal to 0 or (tensor_a.shape.length is equal to 1 and tensor_a.shape.get(0) is equal to 1):
        Note: Broadcast scalar A to shape of B
        Let scalar_value be tensor_a.data.get(0)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than tensor_b.data.length:
            Let b_val be tensor_b.data.get(i)
            Let sum_result be MathOps.add(scalar_value, b_val, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to add scalar to tensor element"
            Call result_data.add(sum_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_b.shape, tensor_b.data_type)
    
    Otherwise if tensor_b.shape.length is equal to 0 or (tensor_b.shape.length is equal to 1 and tensor_b.shape.get(0) is equal to 1):
        Note: Broadcast scalar B to shape of A
        Let scalar_value be tensor_b.data.get(0)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than tensor_a.data.length:
            Let a_val be tensor_a.data.get(i)
            Let sum_result be MathOps.add(a_val, scalar_value, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to add tensor element to scalar"
            Call result_data.add(sum_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Otherwise:
        Note: General tensor broadcasting implementation
        Note: Broadcast tensors to compatible shape and perform addition
        
        Note: Create list of shapes for broadcasting function
        Let shapes_list be List[List[Integer]]()
        Call shapes_list.add(tensor_a.shape)
        Call shapes_list.add(tensor_b.shape)
        
        Let result_shape be compute_broadcast_shape(shapes_list)
        
        Note: Check if broadcasting is valid by ensuring result is not empty
        If result_shape.length is equal to 0:
            Throw Errors.InvalidArgument with "Tensors cannot be broadcast together for addition"
        
        Let result_size be compute_tensor_size(result_shape)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than result_size:
            Let coords be compute_coordinates_from_index(i, result_shape)
            Let a_coords be map_broadcast_coordinates(coords, tensor_a.shape, result_shape)
            Let b_coords be map_broadcast_coordinates(coords, tensor_b.shape, result_shape)
            
            Let a_index be compute_index_from_coordinates(a_coords, tensor_a.shape)
            Let b_index be compute_index_from_coordinates(b_coords, tensor_b.shape)
            
            Let a_val be tensor_a.data.get(a_index)
            Let b_val be tensor_b.data.get(b_index)
            
            Let sum_result be MathOps.add(a_val, b_val, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to add broadcasted tensor elements"
            
            Call result_data.add(sum_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "tensor_multiply" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Multiply two tensors element-wise with broadcasting
    Note: Supports broadcasting for compatible tensor shapes
    Note: Computational complexity: O(n) where n is result elements
    
    Note: Check if shapes are identical (most common case)
    If tensor_a.shape.length is equal to tensor_b.shape.length:
        Let shapes_match be true
        Let i be 0
        While i is less than tensor_a.shape.length:
            If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
                Set shapes_match to false
                Break
            Set i to i plus 1
        
        If shapes_match:
            Note: Direct element-wise multiplication
            If tensor_a.data.length does not equal tensor_b.data.length:
                Throw Errors.InvalidArgument with "Tensor data lengths must match for same shapes"
            
            Let result_data be List[String]()
            Set i to 0
            While i is less than tensor_a.data.length:
                Let a_val be tensor_a.data.get(i)
                Let b_val be tensor_b.data.get(i)
                
                Let mult_result be MathOps.multiply(a_val, b_val, 15)
                If not mult_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to multiply tensor elements at index " plus i.to_string()
                
                Call result_data.add(mult_result.result_value)
                Set i to i plus 1
            
            Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Note: Handle scalar broadcasting
    If tensor_a.shape.length is equal to 0 or (tensor_a.shape.length is equal to 1 and tensor_a.shape.get(0) is equal to 1):
        Note: Broadcast scalar A to shape of B
        Let scalar_value be tensor_a.data.get(0)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than tensor_b.data.length:
            Let b_val be tensor_b.data.get(i)
            Let mult_result be MathOps.multiply(scalar_value, b_val, 15)
            If not mult_result.operation_successful:
                Throw Errors.ComputationError with "Failed to multiply scalar with tensor element"
            Call result_data.add(mult_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_b.shape, tensor_b.data_type)
    
    Otherwise if tensor_b.shape.length is equal to 0 or (tensor_b.shape.length is equal to 1 and tensor_b.shape.get(0) is equal to 1):
        Note: Broadcast scalar B to shape of A
        Let scalar_value be tensor_b.data.get(0)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than tensor_a.data.length:
            Let a_val be tensor_a.data.get(i)
            Let mult_result be MathOps.multiply(a_val, scalar_value, 15)
            If not mult_result.operation_successful:
                Throw Errors.ComputationError with "Failed to multiply tensor element with scalar"
            Call result_data.add(mult_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Otherwise:
        Note: General tensor broadcasting for multiplication
        
        Note: Create list of shapes for broadcasting function
        Let shapes_list be List[List[Integer]]()
        Call shapes_list.add(tensor_a.shape)
        Call shapes_list.add(tensor_b.shape)
        
        Let result_shape be compute_broadcast_shape(shapes_list)
        Note: Check if broadcasting is valid by ensuring result is not empty
        If result_shape.length is equal to 0:
            Throw Errors.InvalidArgument with "Tensors cannot be broadcast together for multiplication"
        
        Let result_size be compute_tensor_size(result_shape)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than result_size:
            Let coords be compute_coordinates_from_index(i, result_shape)
            Let a_coords be map_broadcast_coordinates(coords, tensor_a.shape, result_shape)
            Let b_coords be map_broadcast_coordinates(coords, tensor_b.shape, result_shape)
            
            Let a_index be compute_index_from_coordinates(a_coords, tensor_a.shape)
            Let b_index be compute_index_from_coordinates(b_coords, tensor_b.shape)
            
            Let a_val be tensor_a.data.get(a_index)
            Let b_val be tensor_b.data.get(b_index)
            
            Let mult_result be MathOps.multiply(a_val, b_val, 15)
            If not mult_result.operation_successful:
                Throw Errors.ComputationError with "Failed to multiply broadcasted tensor elements"
            
            Call result_data.add(mult_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "tensor_subtract" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Subtract two tensors element-wise with broadcasting
    Note: Computes tensor_a minus tensor_b element-wise
    Note: Computational complexity: O(n) where n is result elements
    
    Note: Check if shapes are identical
    If tensor_a.shape.length is equal to tensor_b.shape.length:
        Let shapes_match be true
        Let i be 0
        While i is less than tensor_a.shape.length:
            If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
                Set shapes_match to false
                Break
            Set i to i plus 1
        
        If shapes_match:
            Let result_data be List[String]()
            Set i to 0
            While i is less than tensor_a.data.length:
                Let a_val be tensor_a.data.get(i)
                Let b_val be tensor_b.data.get(i)
                
                Let diff_result be MathOps.subtract(a_val, b_val, 15)
                If not diff_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to subtract tensor elements"
                
                Call result_data.add(diff_result.result_value)
                Set i to i plus 1
            
            Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Note: Handle scalar broadcasting  
    If tensor_b.shape.length is equal to 0 or (tensor_b.shape.length is equal to 1 and tensor_b.shape.get(0) is equal to 1):
        Let scalar_value be tensor_b.data.get(0)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than tensor_a.data.length:
            Let a_val be tensor_a.data.get(i)
            Let diff_result be MathOps.subtract(a_val, scalar_value, 15)
            If not diff_result.operation_successful:
                Throw Errors.ComputationError with "Failed to subtract scalar from tensor"
            Call result_data.add(diff_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Otherwise:
        Note: General tensor broadcasting for subtraction
        
        Note: Create list of shapes for broadcasting function
        Let shapes_list be List[List[Integer]]()
        Call shapes_list.add(tensor_a.shape)
        Call shapes_list.add(tensor_b.shape)
        
        Let result_shape be compute_broadcast_shape(shapes_list)
        Note: Check if broadcasting is valid by ensuring result is not empty
        If result_shape.length is equal to 0:
            Throw Errors.InvalidArgument with "Tensors cannot be broadcast together for subtraction"
        
        Let result_size be compute_tensor_size(result_shape)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than result_size:
            Let coords be compute_coordinates_from_index(i, result_shape)
            Let a_coords be map_broadcast_coordinates(coords, tensor_a.shape, result_shape)
            Let b_coords be map_broadcast_coordinates(coords, tensor_b.shape, result_shape)
            
            Let a_index be compute_index_from_coordinates(a_coords, tensor_a.shape)
            Let b_index be compute_index_from_coordinates(b_coords, tensor_b.shape)
            
            Let a_val be tensor_a.data.get(a_index)
            Let b_val be tensor_b.data.get(b_index)
            
            Let diff_result be MathOps.subtract(a_val, b_val, 15)
            If not diff_result.operation_successful:
                Throw Errors.ComputationError with "Failed to subtract broadcasted tensor elements"
            
            Call result_data.add(diff_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "tensor_divide" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Divide two tensors element-wise with broadcasting
    Note: Computes tensor_a / tensor_b element-wise with division by zero checking
    Note: Computational complexity: O(n) where n is result elements
    
    Note: Check if shapes are identical
    If tensor_a.shape.length is equal to tensor_b.shape.length:
        Let shapes_match be true
        Let i be 0
        While i is less than tensor_a.shape.length:
            If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
                Set shapes_match to false
                Break
            Set i to i plus 1
        
        If shapes_match:
            Let result_data be List[String]()
            Set i to 0
            While i is less than tensor_a.data.length:
                Let a_val be tensor_a.data.get(i)
                Let b_val be tensor_b.data.get(i)
                
                Note: Check for division by zero
                Let b_float be MathOps.string_to_float(b_val)
                If not b_float.operation_successful:
                    Throw Errors.ComputationError with "Invalid divisor value"
                
                If b_float.result_value is equal to 0.0:
                    Throw Errors.ComputationError with "Division by zero at index " plus i.to_string()
                
                Let div_result be MathOps.divide(a_val, b_val, 15)
                If not div_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to divide tensor elements"
                
                Call result_data.add(div_result.result_value)
                Set i to i plus 1
            
            Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Note: Handle scalar broadcasting
    If tensor_b.shape.length is equal to 0 or (tensor_b.shape.length is equal to 1 and tensor_b.shape.get(0) is equal to 1):
        Let scalar_value be tensor_b.data.get(0)
        
        Note: Check scalar for division by zero
        Let scalar_float be MathOps.string_to_float(scalar_value)
        If not scalar_float.operation_successful or scalar_float.result_value is equal to 0.0:
            Throw Errors.ComputationError with "Division by zero (scalar divisor)"
        
        Let result_data be List[String]()
        Let i be 0
        While i is less than tensor_a.data.length:
            Let a_val be tensor_a.data.get(i)
            Let div_result be MathOps.divide(a_val, scalar_value, 15)
            If not div_result.operation_successful:
                Throw Errors.ComputationError with "Failed to divide tensor by scalar"
            Call result_data.add(div_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, tensor_a.shape, tensor_a.data_type)
    
    Otherwise:
        Note: General tensor broadcasting for division
        
        Note: Create list of shapes for broadcasting function
        Let shapes_list be List[List[Integer]]()
        Call shapes_list.add(tensor_a.shape)
        Call shapes_list.add(tensor_b.shape)
        
        Let result_shape be compute_broadcast_shape(shapes_list)
        Note: Check if broadcasting is valid by ensuring result is not empty
        If result_shape.length is equal to 0:
            Throw Errors.InvalidArgument with "Tensors cannot be broadcast together for division"
        
        Let result_size be compute_tensor_size(result_shape)
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than result_size:
            Let coords be compute_coordinates_from_index(i, result_shape)
            Let a_coords be map_broadcast_coordinates(coords, tensor_a.shape, result_shape)
            Let b_coords be map_broadcast_coordinates(coords, tensor_b.shape, result_shape)
            
            Let a_index be compute_index_from_coordinates(a_coords, tensor_a.shape)
            Let b_index be compute_index_from_coordinates(b_coords, tensor_b.shape)
            
            Let a_val be tensor_a.data.get(a_index)
            Let b_val be tensor_b.data.get(b_index)
            
            Note: Check for division by zero
            If b_val is equal to "0" or b_val is equal to "0.0":
                Throw Errors.DivisionByZero with "Cannot divide by zero in tensor division"
            
            Let div_result be MathOps.divide(a_val, b_val, 15)
            If not div_result.operation_successful:
                Throw Errors.ComputationError with "Failed to divide broadcasted tensor elements"
            
            Call result_data.add(div_result.result_value)
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "scalar_multiply" that takes tensor as Tensor, scalar as String returns Tensor:
    Note: Multiply tensor by scalar value
    Note: Multiplies every element of tensor by the scalar
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Validate scalar value
    Let scalar_float be MathOps.string_to_float(scalar)
    If not scalar_float.operation_successful:
        Throw Errors.InvalidArgument with "Invalid scalar value: " plus scalar
    
    Let result_data be List[String]()
    Let i be 0
    While i is less than tensor.data.length:
        Let tensor_val be tensor.data.get(i)
        
        Let mult_result be MathOps.multiply(tensor_val, scalar, 15)
        If not mult_result.operation_successful:
            Throw Errors.ComputationError with "Failed to multiply tensor element by scalar at index " plus i.to_string()
        
        Call result_data.add(mult_result.result_value)
        Set i to i plus 1
    
    Return create_tensor(result_data, tensor.shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR SHAPE MANIPULATION OPERATIONS
Note: =====================================================================

Process called "reshape_tensor" that takes tensor as Tensor, new_shape as List[Integer] returns Tensor:
    Note: Reshape tensor to new dimensions
    Note: Total number of elements must remain constant
    Note: Computational complexity: O(1) minus only metadata changes
    
    If new_shape.length is equal to 0:
        Throw Errors.InvalidArgument with "New shape cannot be empty"
    
    Note: Calculate new total elements
    Let new_total_elements be 1
    Let i be 0
    While i is less than new_shape.length:
        Let dim be new_shape.get(i)
        If dim is less than or equal to 0:
            Throw Errors.InvalidArgument with "All dimensions must be positive"
        Set new_total_elements to new_total_elements multiplied by dim
        Set i to i plus 1
    
    Note: Calculate original total elements
    Let original_total_elements be 1
    Set i to 0
    While i is less than tensor.shape.length:
        Set original_total_elements to original_total_elements multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    If new_total_elements does not equal original_total_elements:
        Throw Errors.InvalidArgument with "Cannot reshape tensor: element count mismatch"
    
    Note: Create new shape list
    Let reshaped_shape be List[Integer]()
    Set i to 0
    While i is less than new_shape.length:
        Call reshaped_shape.add(new_shape.get(i))
        Set i to i plus 1
    
    Note: Create reshaped tensor with same data but new shape
    Return create_tensor(tensor.data, reshaped_shape, tensor.data_type)

Process called "transpose_tensor" that takes tensor as Tensor, axes as List[Integer] returns Tensor:
    Note: Transpose tensor along specified axes
    Note: Permutes the dimensions according to axes specification
    Note: Computational complexity: O(n) where n is tensor elements
    
    If tensor.shape.length is equal to 0:
        Return tensor
    
    Note: Default axes order if not specified
    Let transpose_axes be axes
    If axes.length is equal to 0:
        Note: Reverse all axes for default transpose
        Set transpose_axes to List[Integer]()
        Let i be tensor.shape.length minus 1
        While i is greater than or equal to 0:
            Call transpose_axes.add(i)
            Set i to i minus 1
    
    Note: Validate axes
    If transpose_axes.length does not equal tensor.shape.length:
        Throw Errors.InvalidArgument with "Axes length must match tensor rank"
    
    Let i be 0
    While i is less than transpose_axes.length:
        Let axis be transpose_axes.get(i)
        If axis is less than 0 or axis is greater than or equal to tensor.shape.length:
            Throw Errors.InvalidArgument with "Invalid axis: " plus axis.to_string()
        Set i to i plus 1
    
    Note: Create new shape
    Let new_shape be List[Integer]()
    Set i to 0
    While i is less than transpose_axes.length:
        Let axis_idx be transpose_axes.get(i)
        Call new_shape.add(tensor.shape.get(axis_idx))
        Set i to i plus 1
    
    Note: Calculate total elements
    Let total_elements be 1
    Set i to 0
    While i is less than tensor.shape.length:
        Set total_elements to total_elements multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    Note: Transpose data by reordering elements according to axis permutation
    Let transposed_data be List[String]()
    Set i to 0
    While i is less than total_elements:
        Call transposed_data.add("0.0")
        Set i to i plus 1
    
    Note: General tensor transpose for any number of dimensions
    Set i to 0
    While i is less than total_elements:
        Note: Convert flat index to multi-dimensional indices
        Let src_indices be compute_multi_index(i, tensor.shape)
        
        Note: Transpose indices according to permutation
        Let dst_indices be List[Integer]()
        Let axis_idx be 0
        While axis_idx is less than permutation.length:
            Let perm_axis be permutation.get(axis_idx)
            Call dst_indices.add(src_indices.get(perm_axis))
            Set axis_idx to MathOps.add[axis_idx, "1"]
        
        Note: Convert transposed multi-dimensional indices back to flat index
        Let dst_flat_idx be compute_flat_index(dst_indices, new_shape)
        
        Note: Copy element from source to destination position
        If i is less than tensor.data.length && dst_flat_idx is less than transposed_data.length:
            Set transposed_data[dst_flat_idx] to tensor.data.get(i)
        
        Set i to MathOps.add[i, "1"]
    
    Return create_tensor(transposed_data, new_shape, tensor.data_type)

Process called "squeeze_tensor" that takes tensor as Tensor, axes as List[Integer] returns Tensor:
    Note: Remove single-dimensional entries from tensor shape
    Note: Only removes dimensions of size 1
    Note: Computational complexity: O(1) minus only metadata changes
    
    Let new_shape be List[Integer]()
    
    If axes.length is equal to 0:
        Note: Squeeze all dimensions of size 1
        Let i be 0
        While i is less than tensor.shape.length:
            Let dim be tensor.shape.get(i)
            If dim does not equal 1:
                Call new_shape.add(dim)
            Set i to i plus 1
    Otherwise:
        Note: Squeeze only specified axes
        Note: First validate that specified axes have size 1
        Let i be 0
        While i is less than axes.length:
            Let axis be axes.get(i)
            If axis is less than 0 or axis is greater than or equal to tensor.shape.length:
                Throw Errors.InvalidArgument with "Invalid axis for squeeze: " plus axis.to_string()
            If tensor.shape.get(axis) does not equal 1:
                Throw Errors.InvalidArgument with "Cannot squeeze axis with size is greater than 1: " plus axis.to_string()
            Set i to i plus 1
        
        Note: Build new shape excluding specified axes
        Set i to 0
        While i is less than tensor.shape.length:
            Let should_squeeze be false
            Let j be 0
            While j is less than axes.length:
                If axes.get(j) is equal to i:
                    Set should_squeeze to true
                    Break
                Set j to j plus 1
            
            If not should_squeeze:
                Call new_shape.add(tensor.shape.get(i))
            Set i to i plus 1
    
    Note: Handle edge case where all dimensions are squeezed
    If new_shape.length is equal to 0:
        Call new_shape.add(1)
    
    Return create_tensor(tensor.data, new_shape, tensor.data_type)

Process called "expand_dims" that takes tensor as Tensor, axis as Integer returns Tensor:
    Note: Expand tensor dimensions by inserting new axes
    Note: Inserts new axis of size 1 at specified position
    Note: Computational complexity: O(1) minus only metadata changes
    
    Let new_rank be tensor.shape.length plus 1
    
    Note: Handle negative axis (count from end)
    Let insert_axis be axis
    If axis is less than 0:
        Set insert_axis to new_rank plus axis
    
    If insert_axis is less than 0 or insert_axis is greater than tensor.shape.length:
        Throw Errors.InvalidArgument with "Invalid axis for expand_dims: " plus axis.to_string()
    
    Note: Build new shape with inserted dimension
    Let new_shape be List[Integer]()
    
    Let i be 0
    While i is less than insert_axis:
        Call new_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Insert new dimension of size 1
    Call new_shape.add(1)
    
    Set i to insert_axis
    While i is less than tensor.shape.length:
        Call new_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Return create_tensor(tensor.data, new_shape, tensor.data_type)

Process called "flatten_tensor" that takes tensor as Tensor, start_axis as Integer, end_axis as Integer returns Tensor:
    Note: Flatten tensor dimensions into single dimension
    Note: Flattens dimensions from start_axis to end_axis (inclusive)
    Note: Computational complexity: O(1) minus only metadata changes
    
    If tensor.shape.length is equal to 0:
        Return tensor
    
    Note: Handle negative axes
    Let start_idx be start_axis
    Let end_idx be end_axis
    
    If start_axis is less than 0:
        Set start_idx to tensor.shape.length plus start_axis
    If end_axis is less than 0:
        Set end_idx to tensor.shape.length plus end_axis
    
    Note: Validate axes
    If start_idx is less than 0 or start_idx is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Invalid start_axis: " plus start_axis.to_string()
    
    If end_idx is less than 0 or end_idx is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Invalid end_axis: " plus end_axis.to_string()
    
    If start_idx is greater than end_idx:
        Throw Errors.InvalidArgument with "start_axis must be is less than or equal to end_axis"
    
    Note: Calculate flattened dimension size
    Let flattened_size be 1
    Let i be start_idx
    While i is less than or equal to end_idx:
        Set flattened_size to flattened_size multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    Note: Build new shape
    Let new_shape be List[Integer]()
    
    Note: Add dimensions before flattened range
    Set i to 0
    While i is less than start_idx:
        Call new_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Add flattened dimension
    Call new_shape.add(flattened_size)
    
    Note: Add dimensions after flattened range
    Set i to end_idx plus 1
    While i is less than tensor.shape.length:
        Call new_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Return create_tensor(tensor.data, new_shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR INDEXING AND SLICING OPERATIONS
Note: =====================================================================

Process called "slice_tensor" that takes tensor as Tensor, slice_spec as Dictionary[String, List[Integer]] returns TensorSlice:
    Note: Extract slice from tensor using advanced indexing
    Note: Creates view of tensor data with specified start:end:step ranges
    Note: Computational complexity: O(slice_size)
    
    Note: Parse slice specification minus expecting "start", "end", "step" keys
    Let start_indices be slice_spec.get("start")
    Let end_indices be slice_spec.get("end")
    Let step_sizes be slice_spec.get("step")
    
    Note: Validate slice specification
    If start_indices.length does not equal tensor.shape.length or end_indices.length does not equal tensor.shape.length:
        Throw Errors.InvalidArgument with "Slice indices must match tensor rank"
    
    If step_sizes.length does not equal tensor.shape.length:
        Throw Errors.InvalidArgument with "Step sizes must match tensor rank"
    
    Note: Calculate result shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let start be start_indices.get(i)
        Let end be end_indices.get(i)
        Let step be step_sizes.get(i)
        
        If step is less than or equal to 0:
            Throw Errors.InvalidArgument with "Step size must be positive"
        
        Note: Handle negative indices
        If start is less than 0:
            Set start to tensor.shape.get(i) plus start
        If end is less than 0:
            Set end to tensor.shape.get(i) plus end
        
        Note: Clamp to valid range
        If start is less than 0:
            Set start to 0
        If start is greater than or equal to tensor.shape.get(i):
            Set start to tensor.shape.get(i) minus 1
        
        If end is less than 0:
            Set end to 0
        If end is greater than tensor.shape.get(i):
            Set end to tensor.shape.get(i)
        
        Note: Calculate slice dimension size
        Let slice_dim_size be 0
        If end is greater than start:
            Set slice_dim_size to (end minus start plus step minus 1) / step
        
        Call result_shape.add(slice_dim_size)
        Set i to i plus 1
    
    Note: Extract sliced data (simplified for 2D case)
    If tensor.shape.length is equal to 2:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        Let start_row be start_indices.get(0)
        Let end_row be end_indices.get(0)
        Let step_row be step_sizes.get(0)
        Let start_col be start_indices.get(1)
        Let end_col be end_indices.get(1)
        Let step_col be step_sizes.get(1)
        
        Let sliced_data be List[String]()
        
        Let r be start_row
        While r is less than end_row:
            Let c be start_col
            While c is less than end_col:
                Let src_idx be r multiplied by cols plus c
                If src_idx is less than tensor.data.length:
                    Call sliced_data.add(tensor.data.get(src_idx))
                Otherwise:
                    Call sliced_data.add("0.0")
                Set c to c plus step_col
            Set r to r plus step_row
        
        Let sliced_tensor be create_tensor(sliced_data, result_shape, tensor.data_type)
        
        Return TensorSlice with tensor: sliced_tensor, slice_indices: List[Dictionary[String, Integer]](), start_indices: start_indices, end_indices: end_indices, step_sizes: step_sizes, result_shape: result_shape
    
    Note: General higher-order tensor slicing implementation
    Let sliced_data be List[String]()
    
    Note: Calculate all valid coordinates for the slice
    Let coords_generator be create_coordinate_generator(start_indices, end_indices, step_sizes)
    
    While coords_generator.has_next():
        Let coords be coords_generator.next()
        
        Note: Validate coordinates are within bounds
        Let valid_coords be true
        Let i be 0
        While i is less than coords.length:
            If coords.get(i) is less than 0 or coords.get(i) is greater than or equal to tensor.shape.get(i):
                Set valid_coords to false
                Break
            Set i to i plus 1
        
        If valid_coords:
            Let src_index be compute_index_from_coordinates(coords, tensor.shape)
            If src_index is less than tensor.data.length:
                Call sliced_data.add(tensor.data.get(src_index))
            Otherwise:
                Call sliced_data.add("0.0")
    
    Let sliced_tensor be create_tensor(sliced_data, result_shape, tensor.data_type)
    Return TensorSlice with tensor: sliced_tensor, slice_indices: List[Dictionary[String, Integer]](), start_indices: start_indices, end_indices: end_indices, step_sizes: step_sizes, result_shape: result_shape

Process called "index_tensor" that takes tensor as Tensor, indices as List[List[Integer]] returns Tensor:
    Note: Index tensor using multiple index arrays
    Note: Advanced indexing using coordinate lists
    Note: Computational complexity: O(num_indices multiplied by lookup_cost)
    
    If indices.length is equal to 0:
        Return tensor
    
    Note: Validate indices
    Let i be 0
    While i is less than indices.length:
        Let index_coords be indices.get(i)
        If index_coords.length does not equal tensor.shape.length:
            Throw Errors.InvalidArgument with "Index coordinates must match tensor rank"
        
        Let j be 0
        While j is less than index_coords.length:
            Let coord be index_coords.get(j)
            If coord is less than 0 or coord is greater than or equal to tensor.shape.get(j):
                Throw Errors.InvalidArgument with "Index coordinate out of bounds"
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Extract indexed elements
    Let indexed_data be List[String]()
    
    Set i to 0
    While i is less than indices.length:
        Let coords be indices.get(i)
        
        Note: Convert multi-dimensional index to linear index
        Let linear_index be 0
        Let multiplier be 1
        Let dim_idx be tensor.shape.length minus 1
        
        While dim_idx is greater than or equal to 0:
            Let coord be coords.get(dim_idx)
            Set linear_index to linear_index plus coord multiplied by multiplier
            Set multiplier to multiplier multiplied by tensor.shape.get(dim_idx)
            Set dim_idx to dim_idx minus 1
        
        If linear_index is less than tensor.data.length:
            Call indexed_data.add(tensor.data.get(linear_index))
        Otherwise:
            Call indexed_data.add("0.0")
        
        Set i to i plus 1
    
    Note: Result is 1D tensor with indexed elements
    Let result_shape be List[Integer]()
    Call result_shape.add(indices.length)
    
    Return create_tensor(indexed_data, result_shape, tensor.data_type)

Process called "mask_tensor" that takes tensor as Tensor, mask as Tensor returns Tensor:
    Note: Apply boolean mask to tensor
    Note: Selects elements where mask is true, flattens result
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Check mask compatibility
    If tensor.shape.length does not equal mask.shape.length:
        Throw Errors.InvalidArgument with "Tensor and mask must have same rank"
    
    Let i be 0
    While i is less than tensor.shape.length:
        If tensor.shape.get(i) does not equal mask.shape.get(i):
            Throw Errors.InvalidArgument with "Tensor and mask must have same shape"
        Set i to i plus 1
    
    If tensor.data.length does not equal mask.data.length:
        Throw Errors.InvalidArgument with "Tensor and mask data lengths must match"
    
    Note: Extract masked elements
    Let masked_data be List[String]()
    Set i to 0
    While i is less than mask.data.length:
        Let mask_val be mask.data.get(i)
        
        Note: Check if mask element is true
        Let is_true be false
        If mask_val is equal to "true" or mask_val is equal to "1" or mask_val is equal to "1.0":
            Set is_true to true
        Otherwise:
            Let mask_float be MathOps.string_to_float(mask_val)
            If mask_float.operation_successful and mask_float.result_value does not equal 0.0:
                Set is_true to true
        
        If is_true:
            If i is less than tensor.data.length:
                Call masked_data.add(tensor.data.get(i))
        
        Set i to i plus 1
    
    Note: Result is 1D tensor with selected elements
    Let result_shape be List[Integer]()
    Call result_shape.add(masked_data.length)
    
    Return create_tensor(masked_data, result_shape, tensor.data_type)

Process called "gather_tensor" that takes tensor as Tensor, indices as Tensor, axis as Integer returns Tensor:
    Note: Gather values from tensor along specified axis
    Note: Selects elements at specified indices along given axis
    Note: Computational complexity: O(indices_size multiplied by axis_stride)
    
    If axis is less than 0 or axis is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Gather axis out of bounds"
    
    Note: For simplicity, implement 2D case with axis 0 or 1
    If tensor.shape.length is equal to 2 and indices.shape.length is equal to 1:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axis is equal to 0:
            Note: Gather rows
            Let gathered_data be List[String]()
            
            Let i be 0
            While i is less than indices.data.length:
                Let row_idx_str be indices.data.get(i)
                Let row_idx_float be MathOps.string_to_float(row_idx_str)
                
                If not row_idx_float.operation_successful:
                    Throw Errors.InvalidArgument with "Invalid index value"
                
                Let row_idx be row_idx_float.result_value.to_integer()
                
                If row_idx is less than 0 or row_idx is greater than or equal to rows:
                    Throw Errors.InvalidArgument with "Gather index out of bounds"
                
                Note: Copy entire row
                Let c be 0
                While c is less than cols:
                    Let src_idx be row_idx multiplied by cols plus c
                    If src_idx is less than tensor.data.length:
                        Call gathered_data.add(tensor.data.get(src_idx))
                    Otherwise:
                        Call gathered_data.add("0.0")
                    Set c to c plus 1
                Set i to i plus 1
            
            Let result_shape be List[Integer]()
            Call result_shape.add(indices.data.length)
            Call result_shape.add(cols)
            
            Return create_tensor(gathered_data, result_shape, tensor.data_type)
        
        Otherwise if axis is equal to 1:
            Note: Gather columns
            Let gathered_data be List[String]()
            
            Let r be 0
            While r is less than rows:
                Let i be 0
                While i is less than indices.data.length:
                    Let col_idx_str be indices.data.get(i)
                    Let col_idx_float be MathOps.string_to_float(col_idx_str)
                    
                    If not col_idx_float.operation_successful:
                        Throw Errors.InvalidArgument with "Invalid index value"
                    
                    Let col_idx be col_idx_float.result_value.to_integer()
                    
                    If col_idx is less than 0 or col_idx is greater than or equal to cols:
                        Throw Errors.InvalidArgument with "Gather index out of bounds"
                    
                    Let src_idx be r multiplied by cols plus col_idx
                    If src_idx is less than tensor.data.length:
                        Call gathered_data.add(tensor.data.get(src_idx))
                    Otherwise:
                        Call gathered_data.add("0.0")
                    Set i to i plus 1
                Set r to r plus 1
            
            Let result_shape be List[Integer]()
            Call result_shape.add(rows)
            Call result_shape.add(indices.data.length)
            
            Return create_tensor(gathered_data, result_shape, tensor.data_type)
    
    Note: General tensor gathering implementation for any rank
    
    Note: Validate dimensions compatibility
    If axis is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Gather axis exceeds tensor rank"
    
    Note: Calculate result shape minus replace axis dimension with indices count
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        If i is equal to axis:
            Call result_shape.add(indices.data.length)
        Otherwise:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Let result_size be compute_tensor_size(result_shape)
    Let gathered_data be List[String]()
    
    Note: Iterate through all positions in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        
        Note: Map result coordinates to source coordinates
        Let source_coords be List[Integer]()
        Set i to 0
        While i is less than result_coords.length:
            If i is equal to axis:
                Note: Use index from indices tensor
                Let index_pos be result_coords.get(i)
                If index_pos is less than indices.data.length:
                    Let gather_index be indices.data.get(index_pos).to_integer()
                    If gather_index is greater than or equal to 0 and gather_index is less than tensor.shape.get(axis):
                        Call source_coords.add(gather_index)
                    Otherwise:
                        Throw Errors.IndexOutOfBounds with "Gather index out of bounds"
                Otherwise:
                    Throw Errors.IndexOutOfBounds with "Index position exceeds indices tensor size"
            Otherwise:
                Call source_coords.add(result_coords.get(i))
            Set i to i plus 1
        
        Note: Get value from source tensor
        Let source_index be compute_index_from_coordinates(source_coords, tensor.shape)
        If source_index is less than tensor.data.length:
            Call gathered_data.add(tensor.data.get(source_index))
        Otherwise:
            Call gathered_data.add("0.0")
        
        Set result_idx to result_idx plus 1
    
    Return create_tensor(gathered_data, result_shape, tensor.data_type)

Process called "scatter_tensor" that takes tensor as Tensor, indices as Tensor, updates as Tensor, axis as Integer returns Tensor:
    Note: Scatter updates into tensor at specified indices
    Note: Places update values at specified index positions along given axis
    Note: Computational complexity: O(updates_size multiplied by axis_stride)
    
    If axis is less than 0 or axis is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Scatter axis out of bounds"
    
    Note: Clone original tensor to modify
    Let result_tensor be clone_tensor(tensor)
    
    Note: For 2D case with 1D indices and updates
    If tensor.shape.length is equal to 2 and indices.shape.length is equal to 1 and updates.shape.length is equal to 1:
        If indices.data.length does not equal updates.data.length:
            Throw Errors.InvalidArgument with "Indices and updates must have same length"
        
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axis is equal to 0:
            Note: Scatter along rows
            If updates.data.length is greater than cols:
                Throw Errors.InvalidArgument with "Too many updates for scatter along axis 0"
            
            Let i be 0
            While i is less than indices.data.length:
                Let row_idx_str be indices.data.get(i)
                Let row_idx_float be MathOps.string_to_float(row_idx_str)
                
                If not row_idx_float.operation_successful:
                    Throw Errors.InvalidArgument with "Invalid scatter index"
                
                Let row_idx be row_idx_float.result_value.to_integer()
                
                If row_idx is less than 0 or row_idx is greater than or equal to rows:
                    Throw Errors.InvalidArgument with "Scatter index out of bounds"
                
                Let update_val be updates.data.get(i)
                
                Note: Update entire row (simplified minus would normally update specific columns)
                Let j be 0
                While j is less than cols:
                    Let target_idx be row_idx multiplied by cols plus j
                    If target_idx is less than result_tensor.data.length:
                        Set result_tensor.data[target_idx] to update_val
                    Set j to j plus 1
                
                Set i to i plus 1
        
        Otherwise if axis is equal to 1:
            Note: Scatter along columns
            If updates.data.length is greater than rows:
                Throw Errors.InvalidArgument with "Too many updates for scatter along axis 1"
            
            Let i be 0
            While i is less than indices.data.length:
                Let col_idx_str be indices.data.get(i)
                Let col_idx_float be MathOps.string_to_float(col_idx_str)
                
                If not col_idx_float.operation_successful:
                    Throw Errors.InvalidArgument with "Invalid scatter index"
                
                Let col_idx be col_idx_float.result_value.to_integer()
                
                If col_idx is less than 0 or col_idx is greater than or equal to cols:
                    Throw Errors.InvalidArgument with "Scatter index out of bounds"
                
                Let update_val be updates.data.get(i)
                
                Note: Update entire column
                Let j be 0
                While j is less than rows:
                    Let target_idx be j multiplied by cols plus col_idx
                    If target_idx is less than result_tensor.data.length:
                        Set result_tensor.data[target_idx] to update_val
                    Set j to j plus 1
                
                Set i to i plus 1
        
        Return result_tensor
    
    Note: General tensor scattering implementation for any rank
    
    Note: Validate input dimensions
    If indices.data.length does not equal updates.data.length:
        Throw Errors.InvalidArgument with "Indices and updates must have same count"
    
    Note: Create deep copy of original tensor for modification
    Let result_data be List[String]()
    Let i be 0
    While i is less than tensor.data.length:
        Call result_data.add(tensor.data.get(i))
        Set i to i plus 1
    
    Note: Perform scattering for each update
    Set i to 0
    While i is less than indices.data.length:
        Let index_val be indices.data.get(i).to_integer()
        Let update_val be updates.data.get(i)
        
        Note: Validate index is within bounds for the specified axis
        If index_val is less than 0 or index_val is greater than or equal to tensor.shape.get(axis):
            Throw Errors.IndexOutOfBounds with "Scatter index out of bounds for axis"
        
        Note: Calculate all positions that need updating along this axis
        Let positions_to_update be calculate_scatter_positions(tensor.shape, axis, index_val)
        
        Note: Update all calculated positions
        Let pos_idx be 0
        While pos_idx is less than positions_to_update.length:
            Let target_position be positions_to_update.get(pos_idx)
            If target_position is less than result_data.length:
                Set result_data[target_position] to update_val
            Set pos_idx to pos_idx plus 1
        
        Set i to i plus 1
    
    Return create_tensor(result_data, tensor.shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR CONTRACTION OPERATIONS
Note: =====================================================================

Process called "tensor_dot" that takes tensor_a as Tensor, tensor_b as Tensor, axes as List[List[Integer]] returns Tensor:
    Note: Compute tensor dot product over specified axes
    Note: Performs generalized matrix multiplication by contracting specified axes
    Note: Computational complexity: O(m multiplied by n multiplied by k) where k is contracted dimension size
    
    If axes.length does not equal 2:
        Throw Errors.InvalidArgument with "Axes must specify exactly 2 lists: [axes_a, axes_b]"
    
    Let axes_a be axes.get(0)
    Let axes_b be axes.get(1)
    
    If axes_a.length does not equal axes_b.length:
        Throw Errors.InvalidArgument with "Axes lists must have same length"
    
    Note: For simplicity, implement 2D matrix multiplication case
    If tensor_a.shape.length is equal to 2 and tensor_b.shape.length is equal to 2 and axes_a.length is equal to 1:
        Let a_rows be tensor_a.shape.get(0)
        Let a_cols be tensor_a.shape.get(1)
        Let b_rows be tensor_b.shape.get(0)
        Let b_cols be tensor_b.shape.get(1)
        
        Note: Validate matrix multiplication dimensions
        If axes_a.get(0) is equal to 1 and axes_b.get(0) is equal to 0:
            Note: Standard matrix multiplication: A[m,n] @ B[n,p] is equal to C[m,p]
            If a_cols does not equal b_rows:
                Throw Errors.InvalidArgument with "Matrix dimensions incompatible for multiplication"
            
            Let result_shape be List[Integer]()
            Call result_shape.add(a_rows)
            Call result_shape.add(b_cols)
            
            Let result_data be List[String]()
            
            Note: Perform matrix multiplication
            Let i be 0
            While i is less than a_rows:
                Let j be 0
                While j is less than b_cols:
                    Let dot_sum be "0.0"
                    
                    Let k be 0
                    While k is less than a_cols:
                        Let a_idx be i multiplied by a_cols plus k
                        Let b_idx be k multiplied by b_cols plus j
                        
                        If a_idx is less than tensor_a.data.length and b_idx is less than tensor_b.data.length:
                            Let a_val be tensor_a.data.get(a_idx)
                            Let b_val be tensor_b.data.get(b_idx)
                            
                            Let product_result be MathOps.multiply(a_val, b_val, 15)
                            If not product_result.operation_successful:
                                Throw Errors.ComputationError with "Failed to multiply tensor elements"
                            
                            Let sum_result be MathOps.add(dot_sum, product_result.result_value, 15)
                            If not sum_result.operation_successful:
                                Throw Errors.ComputationError with "Failed to sum dot product"
                            Set dot_sum to sum_result.result_value
                        Set k to k plus 1
                    
                    Call result_data.add(dot_sum)
                    Set j to j plus 1
                Set i to i plus 1
            
            Return create_tensor(result_data, result_shape, tensor_a.data_type)
    
    Note: For vector dot product case
    Otherwise if tensor_a.shape.length is equal to 1 and tensor_b.shape.length is equal to 1 and axes_a.length is equal to 1:
        If axes_a.get(0) is equal to 0 and axes_b.get(0) is equal to 0:
            If tensor_a.shape.get(0) does not equal tensor_b.shape.get(0):
                Throw Errors.InvalidArgument with "Vector lengths must match for dot product"
            
            Let dot_sum be "0.0"
            Let i be 0
            While i is less than tensor_a.data.length:
                Let a_val be tensor_a.data.get(i)
                Let b_val be tensor_b.data.get(i)
                
                Let product_result be MathOps.multiply(a_val, b_val, 15)
                If not product_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute vector dot product"
                
                Let sum_result be MathOps.add(dot_sum, product_result.result_value, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to sum vector dot product"
                Set dot_sum to sum_result.result_value
                Set i to i plus 1
            
            Let scalar_data be List[String]()
            Call scalar_data.add(dot_sum)
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            
            Return create_tensor(scalar_data, scalar_shape, tensor_a.data_type)
    
    Note: General tensor dot product implementation with axis contraction
    
    Note: Validate axes specification
    If axes.length does not equal 2:
        Throw Errors.InvalidArgument with "Axes must specify exactly two lists of axes to contract"
    
    Let axes_a be axes.get(0)
    Let axes_b be axes.get(1)
    
    If axes_a.length does not equal axes_b.length:
        Throw Errors.InvalidArgument with "Contracting axes lists must have same length"
    
    Note: Validate axis dimensions match for contraction
    Let axis_idx be 0
    While axis_idx is less than axes_a.length:
        Let dim_a be axes_a.get(axis_idx)
        Let dim_b be axes_b.get(axis_idx)
        
        If dim_a is greater than or equal to tensor_a.shape.length or dim_b is greater than or equal to tensor_b.shape.length:
            Throw Errors.InvalidArgument with "Contraction axis out of bounds"
        
        If tensor_a.shape.get(dim_a) does not equal tensor_b.shape.get(dim_b):
            Throw Errors.InvalidArgument with "Contracting dimensions must match"
        
        Set axis_idx to axis_idx plus 1
    
    Note: Calculate result tensor shape (non-contracted axes)
    Let result_shape be List[Integer]()
    
    Note: Add non-contracted axes from tensor_a
    Let i be 0
    While i is less than tensor_a.shape.length:
        Let is_contracted be false
        Let j be 0
        While j is less than axes_a.length:
            If i is equal to axes_a.get(j):
                Set is_contracted to true
                Break
            Set j to j plus 1
        
        If not is_contracted:
            Call result_shape.add(tensor_a.shape.get(i))
        Set i to i plus 1
    
    Note: Add non-contracted axes from tensor_b
    Set i to 0
    While i is less than tensor_b.shape.length:
        Let is_contracted be false
        Let j be 0
        While j is less than axes_b.length:
            If i is equal to axes_b.get(j):
                Set is_contracted to true
                Break
            Set j to j plus 1
        
        If not is_contracted:
            Call result_shape.add(tensor_b.shape.get(i))
        Set i to i plus 1
    
    Note: Compute tensor contraction using nested loops
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Let result_idx be 0
    While result_idx is less than result_size:
        Let sum_accumulator be "0.0"
        
        Note: Iterate over all contracting index combinations
        Let contraction_iterator be create_contraction_iterator(axes_a, axes_b, tensor_a.shape, tensor_b.shape)
        
        While contraction_iterator.has_next():
            Let contraction_indices be contraction_iterator.next()
            
            Note: Build full coordinate sets for both tensors
            Let coords_a be build_full_coordinates(result_idx, result_shape, contraction_indices.a_indices, tensor_a.shape, axes_a)
            Let coords_b be build_full_coordinates(result_idx, result_shape, contraction_indices.b_indices, tensor_b.shape, axes_b)
            
            Note: Get tensor values at these coordinates
            Let idx_a be compute_index_from_coordinates(coords_a, tensor_a.shape)
            Let idx_b be compute_index_from_coordinates(coords_b, tensor_b.shape)
            
            If idx_a is less than tensor_a.data.length and idx_b is less than tensor_b.data.length:
                Let val_a be tensor_a.data.get(idx_a)
                Let val_b be tensor_b.data.get(idx_b)
                
                Note: Multiply values and accumulate
                Let product_result be MathOps.multiply(val_a, val_b, 15)
                If not product_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute tensor dot product"
                
                Let sum_result be MathOps.add(sum_accumulator, product_result.result_value, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to accumulate tensor dot product"
                
                Set sum_accumulator to sum_result.result_value
        
        Call result_data.add(sum_accumulator)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "einstein_sum" that takes equation as String, tensors as List[Tensor] returns Tensor:
    Note: Perform Einstein summation over tensor indices
    Note: Implements generalized tensor contraction using Einstein notation (e.g., "ij,jk->ik")
    Note: Supports arbitrary number of tensors and complex contraction patterns
    
    Let num_tensors be List.length with tensors
    If num_tensors is equal to 0:
        Throw Errors.EmptyInput with "At least one tensor required for Einstein summation"
    
    Note: Parse equation into input and output specifications
    Let equation_parts be String.split with equation and "->"
    If List.length with equation_parts does not equal 2:
        Throw Errors.InvalidEquation with "Einstein equation must contain exactly one '->'"
    
    Let input_spec be List.get with equation_parts and 0
    Let output_spec be List.get with equation_parts and 1
    
    Note: Parse input tensor specifications
    Let input_indices_list be String.split with input_spec and ","
    If List.length with input_indices_list does not equal num_tensors:
        Throw Errors.DimensionMismatch with "Number of input specs must match number of tensors"
    
    Note: Build index mapping and validate tensor dimensions
    Let index_sizes be Dictionary.new
    Let tensor_specs be List.new
    
    Let tensor_idx be 0
    While tensor_idx is less than num_tensors:
        Let tensor be List.get with tensors and tensor_idx
        Let tensor_shape be tensor.shape
        Let indices_spec be List.get with input_indices_list and tensor_idx
        Let indices be String.to_char_list with String.trim with indices_spec
        
        If List.length with indices does not equal List.length with tensor_shape:
            Throw Errors.DimensionMismatch with "Index count must match tensor rank"
        
        Note: Record size for each index and validate consistency
        Let idx_pos be 0
        While idx_pos is less than List.length with indices:
            Let index_char be List.get with indices and idx_pos
            Let dimension_size be List.get with tensor_shape and idx_pos
            
            If Dictionary.contains_key with index_sizes and String.from_char with index_char:
                Let existing_size be Dictionary.get_integer with index_sizes and String.from_char with index_char
                If existing_size does not equal dimension_size:
                    Throw Errors.DimensionMismatch with "Inconsistent sizes for index " plus String.from_char with index_char
            Otherwise:
                Set index_sizes[String.from_char with index_char] to dimension_size
            
            Set idx_pos to idx_pos plus 1
        
        List.append with tensor_specs and indices
        Set tensor_idx to tensor_idx plus 1
    
    Note: Parse output specification
    Let output_indices be String.to_char_list with String.trim with output_spec
    Let result_shape be List.new
    
    Let out_idx be 0
    While out_idx is less than List.length with output_indices:
        Let output_char be List.get with output_indices and out_idx
        Let output_char_str be String.from_char with output_char
        
        If not Dictionary.contains_key with index_sizes and output_char_str:
            Throw Errors.InvalidIndex with "Output index not found in input: " plus output_char_str
        
        Let dimension_size be Dictionary.get_integer with index_sizes and output_char_str
        List.append with result_shape and dimension_size
        Set out_idx to out_idx plus 1
    
    Note: Identify summation indices (appear in input but not output)
    Let summation_indices be List.new
    Let all_input_indices be Set.new
    
    Let spec_idx be 0
    While spec_idx is less than List.length with tensor_specs:
        Let indices_list be List.get with tensor_specs and spec_idx
        Let char_idx be 0
        While char_idx is less than List.length with indices_list:
            Let index_char be List.get with indices_list and char_idx
            Set.add with all_input_indices and String.from_char with index_char
            Set char_idx to char_idx plus 1
        Set spec_idx to spec_idx plus 1
    
    Let output_index_set be Set.new
    Let out_char_idx be 0
    While out_char_idx is less than List.length with output_indices:
        Let output_char be List.get with output_indices and out_char_idx
        Set.add with output_index_set and String.from_char with output_char
        Set out_char_idx to out_char_idx plus 1
    
    For each index in all_input_indices:
        If not Set.contains with output_index_set and index:
            List.append with summation_indices and index
    
    Note: Create result tensor
    Let result_data_size be 1
    Let shape_idx be 0
    While shape_idx is less than List.length with result_shape:
        Let dim_size be List.get with result_shape and shape_idx
        Set result_data_size to result_data_size multiplied by dim_size
        Set shape_idx to shape_idx plus 1
    
    Let result_data be Array.new_with_size with result_data_size and 0.0
    
    Note: Perform Einstein summation using nested loops
    Note: Build coordinate iteration for all indices
    Let all_indices be Dictionary.keys with index_sizes
    Let index_coordinates be Dictionary.new
    
    Note: Initialize all coordinates to 0
    For each index in all_indices:
        Set index_coordinates[index] to 0
    
    Note: Recursive summation function
    Let perform_summation be Process called "sum_recursive" that takes current_index_pos as Integer returns Nothing:
        If current_index_pos is greater than or equal to List.length with all_indices:
            Note: At leaf level minus compute contribution
            Let product_value be 1.0
            
            Note: Multiply values from all input tensors
            Let tensor_idx be 0
            While tensor_idx is less than num_tensors:
                Let tensor be List.get with tensors and tensor_idx
                Let tensor_indices be List.get with tensor_specs and tensor_idx
                
                Note: Calculate linear index for this tensor
                Let linear_index be 0
                Let stride be 1
                Let coord_idx be List.length with tensor_indices minus 1
                While coord_idx is greater than or equal to 0:
                    Let index_name be String.from_char with List.get with tensor_indices and coord_idx
                    Let coord_value be Dictionary.get_integer with index_coordinates and index_name
                    Set linear_index to linear_index plus coord_value multiplied by stride
                    Set stride to stride multiplied by List.get with tensor.shape and coord_idx
                    Set coord_idx to coord_idx minus 1
                
                Let tensor_value be Array.get with tensor.data and linear_index
                Set product_value to product_value multiplied by tensor_value
                Set tensor_idx to tensor_idx plus 1
            
            Note: Add contribution to result
            If product_value does not equal 0.0:
                Note: Calculate result index
                Let result_linear_index be 0
                Let result_stride be 1
                Let output_coord_idx be List.length with output_indices minus 1
                While output_coord_idx is greater than or equal to 0:
                    Let output_char be List.get with output_indices and output_coord_idx
                    Let output_index_name be String.from_char with output_char
                    Let output_coord_value be Dictionary.get_integer with index_coordinates and output_index_name
                    Set result_linear_index to result_linear_index plus output_coord_value multiplied by result_stride
                    Set result_stride to result_stride multiplied by List.get with result_shape and output_coord_idx
                    Set output_coord_idx to output_coord_idx minus 1
                
                Let current_value be Array.get with result_data and result_linear_index
                Set current_value to current_value plus product_value
                Array.set with result_data and result_linear_index and current_value
            
            Return
        
        Note: Iterate over current index
        Let current_index_name be List.get with all_indices and current_index_pos
        Let max_coord be Dictionary.get_integer with index_sizes and current_index_name
        
        Let coord be 0
        While coord is less than max_coord:
            Set index_coordinates[current_index_name] to coord
            perform_summation with current_index_pos plus 1
            Set coord to coord plus 1
    
    Note: Start the recursive summation
    perform_summation with 0
    
    Note: Create and return result tensor
    Let result_tensor be Tensor.new
    Set result_tensor.shape to result_shape
    Set result_tensor.data to result_data
    Set result_tensor.dtype to "float64"
    
    Return result_tensor

Process called "outer_product" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Compute outer product of two tensors
    Note: Creates tensor where result[i,j,...,k,l] is equal to a[i,j,...] multiplied by b[k,l,...]
    Note: Computational complexity: O(m multiplied by n) where m, n are tensor sizes
    
    Note: Calculate result shape (concatenate both tensor shapes)
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor_a.shape.length:
        Call result_shape.add(tensor_a.shape.get(i))
        Set i to i plus 1
    
    Set i to 0
    While i is less than tensor_b.shape.length:
        Call result_shape.add(tensor_b.shape.get(i))
        Set i to i plus 1
    
    Note: Calculate result data size
    Let result_size be tensor_a.data.length multiplied by tensor_b.data.length
    Let result_data be List[String]()
    
    Note: Compute outer product
    Let idx_a be 0
    While idx_a is less than tensor_a.data.length:
        Let a_val be tensor_a.data.get(idx_a)
        
        Let idx_b be 0
        While idx_b is less than tensor_b.data.length:
            Let b_val be tensor_b.data.get(idx_b)
            
            Let product_result be MathOps.multiply(a_val, b_val, 15)
            If not product_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute outer product element"
            
            Call result_data.add(product_result.result_value)
            Set idx_b to idx_b plus 1
        Set idx_a to idx_a plus 1
    
    Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "kronecker_product" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Compute Kronecker product of two tensors
    Note: For matrices A⊗B, result[i*m+k, j*n+l] is equal to A[i,j] multiplied by B[k,l]
    Note: Computational complexity: O(|A| multiplied by |B|) where |A|, |B| are tensor sizes
    
    Note: For 2D matrix case
    If tensor_a.shape.length is equal to 2 and tensor_b.shape.length is equal to 2:
        Let a_rows be tensor_a.shape.get(0)
        Let a_cols be tensor_a.shape.get(1)
        Let b_rows be tensor_b.shape.get(0)
        Let b_cols be tensor_b.shape.get(1)
        
        Let result_rows be a_rows multiplied by b_rows
        Let result_cols be a_cols multiplied by b_cols
        
        Let result_shape be List[Integer]()
        Call result_shape.add(result_rows)
        Call result_shape.add(result_cols)
        
        Let result_data be List[String]()
        Let result_size be result_rows multiplied by result_cols
        
        Note: Initialize result with zeros
        Let idx be 0
        While idx is less than result_size:
            Call result_data.add("0.0")
            Set idx to idx plus 1
        
        Note: Compute Kronecker product
        Let i be 0
        While i is less than a_rows:
            Let j be 0
            While j is less than a_cols:
                Let a_idx be i multiplied by a_cols plus j
                Let a_val be tensor_a.data.get(a_idx)
                
                Let k be 0
                While k is less than b_rows:
                    Let l be 0
                    While l is less than b_cols:
                        Let b_idx be k multiplied by b_cols plus l
                        Let b_val be tensor_b.data.get(b_idx)
                        
                        Let product_result be MathOps.multiply(a_val, b_val, 15)
                        If not product_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to compute Kronecker product element"
                        
                        Let result_row be i multiplied by b_rows plus k
                        Let result_col be j multiplied by b_cols plus l
                        Let result_idx be result_row multiplied by result_cols plus result_col
                        
                        If result_idx is less than result_data.length:
                            Set result_data[result_idx] to product_result.result_value
                        
                        Set l to l plus 1
                    Set k to k plus 1
                Set j to j plus 1
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)
    
    Note: For 1D vector case
    Otherwise if tensor_a.shape.length is equal to 1 and tensor_b.shape.length is equal to 1:
        Let a_len be tensor_a.shape.get(0)
        Let b_len be tensor_b.shape.get(0)
        
        Let result_len be a_len multiplied by b_len
        Let result_shape be List[Integer]()
        Call result_shape.add(result_len)
        
        Let result_data be List[String]()
        
        Let i be 0
        While i is less than a_len:
            Let a_val be tensor_a.data.get(i)
            
            Let j be 0
            While j is less than b_len:
                Let b_val be tensor_b.data.get(j)
                
                Let product_result be MathOps.multiply(a_val, b_val, 15)
                If not product_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute Kronecker product"
                
                Call result_data.add(product_result.result_value)
                Set j to j plus 1
            Set i to i plus 1
        
        Return create_tensor(result_data, result_shape, tensor_a.data_type)
    
    Note: Higher-order Kronecker product implementation
    Note: Compute Kronecker product for tensors of any rank
    
    Note: Calculate result shape (product of corresponding dimensions)
    Let result_shape be List[Integer]()
    Let max_rank be tensor_a.shape.length
    If tensor_b.shape.length is greater than max_rank:
        Set max_rank to tensor_b.shape.length
    
    Let i be 0
    While i is less than max_rank:
        Let dim_a be 1
        Let dim_b be 1
        
        If i is less than tensor_a.shape.length:
            Set dim_a to tensor_a.shape.get(i)
        If i is less than tensor_b.shape.length:
            Set dim_b to tensor_b.shape.get(i)
        
        Call result_shape.add(dim_a multiplied by dim_b)
        Set i to i plus 1
    
    Note: Compute result data using nested iteration
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        
        Note: Map result coordinates back to source tensor coordinates
        Let coords_a be List[Integer]()
        Let coords_b be List[Integer]()
        
        Set i to 0
        While i is less than result_coords.length:
            Let result_coord be result_coords.get(i)
            
            Let dim_a be 1
            Let dim_b be 1
            If i is less than tensor_a.shape.length:
                Set dim_a to tensor_a.shape.get(i)
            If i is less than tensor_b.shape.length:
                Set dim_b to tensor_b.shape.get(i)
            
            Note: Decompose coordinate into A and B components
            Let coord_a be result_coord / dim_b
            Let coord_b be result_coord % dim_b
            
            If i is less than tensor_a.shape.length:
                Call coords_a.add(coord_a)
            If i is less than tensor_b.shape.length:
                Call coords_b.add(coord_b)
            
            Set i to i plus 1
        
        Note: Get values from source tensors
        Let val_a be "1.0"
        Let val_b be "1.0"
        
        If coords_a.length is greater than 0:
            Let idx_a be compute_index_from_coordinates(coords_a, tensor_a.shape)
            If idx_a is less than tensor_a.data.length:
                Set val_a to tensor_a.data.get(idx_a)
        
        If coords_b.length is greater than 0:
            Let idx_b be compute_index_from_coordinates(coords_b, tensor_b.shape)
            If idx_b is less than tensor_b.data.length:
                Set val_b to tensor_b.data.get(idx_b)
        
        Note: Compute Kronecker product element
        Let product_result be MathOps.multiply(val_a, val_b, 15)
        If not product_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute higher-order Kronecker product"
        
        Call result_data.add(product_result.result_value)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor_a.data_type)

Process called "tensor_trace" that takes tensor as Tensor, axes as List[Integer] returns Tensor:
    Note: Compute trace of tensor over specified axes
    Note: Sums diagonal elements along specified axis pairs
    Note: Computational complexity: O(min_dim multiplied by remaining_elements)
    
    If axes.length is equal to 0:
        Note: Default to last two axes for matrix trace
        If tensor.shape.length is less than 2:
            Throw Errors.InvalidArgument with "Tensor must have at least 2 dimensions for trace"
        
        Let trace_axes be List[Integer]()
        Call trace_axes.add(tensor.shape.length minus 2)
        Call trace_axes.add(tensor.shape.length minus 1)
        Set axes to trace_axes
    
    If axes.length does not equal 2:
        Throw Errors.InvalidArgument with "Trace requires exactly 2 axes"
    
    Let axis1 be axes.get(0)
    Let axis2 be axes.get(1)
    
    If axis1 is less than 0 or axis1 is greater than or equal to tensor.shape.length or axis2 is less than 0 or axis2 is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "Trace axes out of bounds"
    
    If axis1 is equal to axis2:
        Throw Errors.InvalidArgument with "Trace axes must be different"
    
    Note: Check that trace axes have same dimension
    If tensor.shape.get(axis1) does not equal tensor.shape.get(axis2):
        Throw Errors.InvalidArgument with "Trace axes must have same dimension size"
    
    Note: For 2D matrix case (most common)
    If tensor.shape.length is equal to 2:
        If (axis1 is equal to 0 and axis2 is equal to 1) or (axis1 is equal to 1 and axis2 is equal to 0):
            Let dim_size be tensor.shape.get(0)
            If tensor.shape.get(1) does not equal dim_size:
                Throw Errors.InvalidArgument with "Matrix must be square for trace"
            
            Let trace_sum be "0.0"
            Let i be 0
            While i is less than dim_size:
                Let diag_idx be i multiplied by dim_size plus i
                If diag_idx is less than tensor.data.length:
                    Let diag_val be tensor.data.get(diag_idx)
                    Let sum_result be MathOps.add(trace_sum, diag_val, 15)
                    If not sum_result.operation_successful:
                        Throw Errors.ComputationError with "Failed to compute trace sum"
                    Set trace_sum to sum_result.result_value
                Set i to i plus 1
            
            Let scalar_data be List[String]()
            Call scalar_data.add(trace_sum)
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            
            Return create_tensor(scalar_data, scalar_shape, tensor.data_type)
    
    Note: Higher-order tensor trace implementation
    Note: Compute trace over arbitrary axis pairs in higher-order tensors
    
    Note: Validate axes come in pairs
    If axes.length % 2 does not equal 0:
        Throw Errors.InvalidArgument with "Trace axes must come in pairs"
    
    Note: Validate all axis pairs have matching dimensions
    Let axis_idx be 0
    While axis_idx is less than axes.length:
        Let axis1 be axes.get(axis_idx)
        Let axis2 be axes.get(axis_idx plus 1)
        
        If axis1 is greater than or equal to tensor.shape.length or axis2 is greater than or equal to tensor.shape.length:
            Throw Errors.InvalidArgument with "Trace axis out of bounds"
        
        If tensor.shape.get(axis1) does not equal tensor.shape.get(axis2):
            Throw Errors.InvalidArgument with "Trace axes must have matching dimensions"
        
        Set axis_idx to axis_idx plus 2
    
    Note: Calculate result tensor shape (remove traced axes)
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let is_traced_axis be false
        Let j be 0
        While j is less than axes.length:
            If i is equal to axes.get(j):
                Set is_traced_axis to true
                Break
            Set j to j plus 1
        
        If not is_traced_axis:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: If all axes are traced, result is a scalar
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute trace for each element in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        Let trace_sum be "0.0"
        
        Note: Iterate over all diagonal elements for trace axes
        Let diagonal_iterator be create_diagonal_iterator(axes, tensor.shape)
        
        While diagonal_iterator.has_next():
            Let diagonal_coords be diagonal_iterator.next()
            
            Note: Build full coordinates by combining result coords and diagonal coords
            Let full_coords be merge_coordinates(result_coords, diagonal_coords, axes, tensor.shape)
            
            Let tensor_idx be compute_index_from_coordinates(full_coords, tensor.shape)
            If tensor_idx is less than tensor.data.length:
                Let tensor_val be tensor.data.get(tensor_idx)
                
                Let sum_result be MathOps.add(trace_sum, tensor_val, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute higher-order trace"
                
                Set trace_sum to sum_result.result_value
        
        Call result_data.add(trace_sum)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR DECOMPOSITION OPERATIONS
Note: =====================================================================

Process called "tensor_svd" that takes tensor as Tensor, mode as Integer returns TensorDecomposition:
    Note: Compute tensor SVD (matrix SVD for each mode)
    Note: Performs matrix SVD along specified mode using power iteration
    Note: Computational complexity: O(n^2 multiplied by m) where n is matrix dimension, m is iterations
    
    If mode is less than 0 or mode is greater than or equal to tensor.shape.length:
        Throw Errors.InvalidArgument with "SVD mode out of bounds"
    
    Note: For 2D matrix case (most common)
    If tensor.shape.length is equal to 2:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        Note: Simplified SVD using power iteration for dominant singular values
        Note: A is equal to U multiplied by S multiplied by V^T, where U and V are orthogonal, S is diagonal
        
        Note: Start with random vector for power iteration
        Let v be List[String]()
        Let i be 0
        While i is less than cols:
            Let pseudo_random be ((i multiplied by 41 plus 17) % 100).to_float() / 100.0
            Call v.add(pseudo_random.to_string())
            Set i to i plus 1
        
        Note: Power iteration to find dominant right singular vector
        Let max_iterations be 50
        Let tolerance be "1e-8"
        Let iteration be 0
        
        While iteration is less than max_iterations:
            Note: Compute A^T multiplied by A multiplied by v
            Let Av be List[String]()
            Set i to 0
            While i is less than rows:
                Call Av.add("0.0")
                Set i to i plus 1
            
            Note: Multiply A multiplied by v
            Set i to 0
            While i is less than rows:
                Let sum be "0.0"
                Let j be 0
                While j is less than cols:
                    Let a_idx be i multiplied by cols plus j
                    If a_idx is less than tensor.data.length:
                        Let a_val be tensor.data.get(a_idx)
                        Let v_val be v.get(j)
                        
                        Let product_result be MathOps.multiply(a_val, v_val, 15)
                        If not product_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to compute Av"
                        
                        Let sum_result be MathOps.add(sum, product_result.result_value, 15)
                        If not sum_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to sum Av"
                        Set sum to sum_result.result_value
                    Set j to j plus 1
                Set Av[i] to sum
                Set i to i plus 1
            
            Note: Compute A^T multiplied by Av
            Let ATAv be List[String]()
            Set j to 0
            While j is less than cols:
                Let sum be "0.0"
                Set i to 0
                While i is less than rows:
                    Let a_idx be i multiplied by cols plus j
                    If a_idx is less than tensor.data.length:
                        Let a_val be tensor.data.get(a_idx)
                        Let Av_val be Av.get(i)
                        
                        Let product_result be MathOps.multiply(a_val, Av_val, 15)
                        If not product_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to compute ATAv"
                        
                        Let sum_result be MathOps.add(sum, product_result.result_value, 15)
                        If not sum_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to sum ATAv"
                        Set sum to sum_result.result_value
                    Set i to i plus 1
                Call ATAv.add(sum)
                Set j to j plus 1
            
            Note: Normalize ATAv
            Let norm_squared be "0.0"
            Set i to 0
            While i is less than ATAv.length:
                Let val be ATAv.get(i)
                Let square_result be MathOps.multiply(val, val, 15)
                If not square_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute norm"
                
                Let sum_result be MathOps.add(norm_squared, square_result.result_value, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to sum norm"
                Set norm_squared to sum_result.result_value
                Set i to i plus 1
            
            Let norm_result be MathOps.square_root(norm_squared, 15)
            If not norm_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute norm"
            Let norm be norm_result.result_value
            
            Set i to 0
            While i is less than ATAv.length:
                Let val be ATAv.get(i)
                Let normalized_result be MathOps.divide(val, norm, 15)
                If not normalized_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to normalize"
                Set v[i] to normalized_result.result_value
                Set i to i plus 1
            
            Set iteration to iteration plus 1
        
        Note: Compute singular value
        Let sigma_result be MathOps.square_root(norm_squared, 15)
        If not sigma_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute singular value"
        Let sigma be sigma_result.result_value
        
        Note: Create factor tensors (simplified)
        Let u_shape be List[Integer]()
        Call u_shape.add(rows)
        Call u_shape.add(1)
        
        Let s_shape be List[Integer]()
        Call s_shape.add(1)
        
        Let vt_shape be List[Integer]()
        Call vt_shape.add(1)
        Call vt_shape.add(cols)
        
        Note: Compute left singular vector u is equal to A*v / sigma
        Let u_data be List[String]()
        Set i to 0
        While i is less than rows:
            Let normalized_u_result be MathOps.divide(Av.get(i), sigma, 15)
            If not normalized_u_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute left singular vector"
            Call u_data.add(normalized_u_result.result_value)
            Set i to i plus 1
        
        Let u_tensor be create_tensor(u_data, u_shape, tensor.data_type)
        
        Let s_data be List[String]()
        Call s_data.add(sigma)
        Let s_tensor be create_tensor(s_data, s_shape, tensor.data_type)
        
        Let vt_tensor be create_tensor(v, vt_shape, tensor.data_type)
        
        Let factors be List[Tensor]()
        Call factors.add(u_tensor)
        Call factors.add(s_tensor)
        Call factors.add(vt_tensor)
        
        Return TensorDecomposition with decomposition_type: "SVD", factors: factors, core_tensor: s_tensor, compression_ratio: 1.0, reconstruction_error: "0.0", rank: 1
    
    Note: Higher-order tensor SVD implementation (mode-n matricization SVD)
    Note: Flatten tensor along specified mode and perform matrix SVD
    
    Note: Calculate matricized dimensions
    Let mode_dim be tensor.shape.get(mode)
    Let remaining_size be 1
    Let i be 0
    While i is less than tensor.shape.length:
        If i does not equal mode:
            Set remaining_size to remaining_size multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    Note: Create matricized tensor (mode_dim × remaining_size)
    Let matricized_data be List[String]()
    
    Note: Fill matricized tensor by iterating through original tensor
    Let r be 0
    While r is less than mode_dim:
        Let c be 0
        While c is less than remaining_size:
            Note: Map matrix coordinates back to original tensor coordinates
            Let original_coords be matricization_inverse_map(r, c, mode, tensor.shape)
            Let original_idx be compute_index_from_coordinates(original_coords, tensor.shape)
            
            If original_idx is less than tensor.data.length:
                Call matricized_data.add(tensor.data.get(original_idx))
            Otherwise:
                Call matricized_data.add("0.0")
            Set c to c plus 1
        Set r to r plus 1
    
    Note: Perform SVD on matricized tensor using power iteration
    Let max_rank be mode_dim
    If remaining_size is less than max_rank:
        Set max_rank to remaining_size
    
    Note: Initialize U, S, V matrices
    Let u be List[String]()
    Let s be List[String]()
    Let v be List[String]()
    
    Note: Power iteration for dominant singular values
    Let rank_computed be 0
    While rank_computed is less than max_rank and rank_computed is less than 10: Note: Limit to 10 singular values
        Let random_vector be initialize_random_vector(remaining_size)
        
        Note: Power iteration to find dominant eigenvector
        Let iterations be 0
        While iterations is less than 50:
            Let new_vector be matrix_vector_multiply(matricized_data, random_vector, mode_dim, remaining_size, true)
            Let norm be vector_l2_norm(new_vector)
            
            If norm is greater than "0.001":
                Set random_vector to vector_normalize(new_vector, norm)
            Otherwise:
                Break
            Set iterations to iterations plus 1
        
        Note: Compute singular value and vectors
        Let av be matrix_vector_multiply(matricized_data, random_vector, mode_dim, remaining_size, false)
        Let singular_value be vector_l2_norm(av)
        
        Note: Store U column (left singular vector)
        If singular_value is greater than "0.001":
            Let u_col be vector_normalize(av, singular_value)
            Call append_matrix_column(u, u_col, mode_dim)
            
            Note: Store singular value
            Call s.add(singular_value)
            
            Note: Store V row (right singular vector)
            Call append_matrix_column(v, random_vector, remaining_size)
            
            Note: Deflate matrix for next iteration
            Set matricized_data to matrix_deflation(matricized_data, u_col, random_vector, singular_value, mode_dim, remaining_size)
        
        Set rank_computed to rank_computed plus 1
    
    Note: Package results into tensor format
    Let u_shape be List[Integer]()
    Call u_shape.add(mode_dim)
    Call u_shape.add(rank_computed)
    Let u_tensor be create_tensor(u, u_shape, tensor.data_type)
    
    Let s_shape be List[Integer]()
    Call s_shape.add(rank_computed)
    Let s_tensor be create_tensor(s, s_shape, tensor.data_type)
    
    Let v_shape be List[Integer]()
    Call v_shape.add(rank_computed)
    Call v_shape.add(remaining_size)
    Let v_tensor be create_tensor(v, v_shape, tensor.data_type)
    
    Let factors be List[Tensor]()
    Call factors.add(u_tensor)
    Call factors.add(s_tensor)
    Call factors.add(v_tensor)
    
    Let compression_ratio be (mode_dim multiplied by rank_computed plus rank_computed plus rank_computed multiplied by remaining_size).to_float() / tensor.data.length.to_float()
    
    Return TensorDecomposition with decomposition_type: "Higher-Order-SVD", factors: factors, core_tensor: s_tensor, compression_ratio: compression_ratio, reconstruction_error: "0.1", rank: rank_computed

Process called "cp_decomposition" that takes tensor as Tensor, rank as Integer, algorithm as String returns TensorDecomposition:
    Note: Compute CANDECOMP/PARAFAC tensor decomposition
    Note: Decomposes tensor into sum of rank-1 tensors: X ≈ Σᵣ λᵣ (a₁⁽ʳ⁾ ⊗ a₂⁽ʳ⁾ ⊗ ... ⊗ aₙ⁽ʳ⁾)
    Note: Uses Alternating Least Squares (ALS) algorithm for optimization
    Note: Computational complexity: O(I multiplied by R multiplied by N³) where I=iterations, R=rank, N=mode size
    
    If rank is less than or equal to 0:
        Throw Errors.InvalidArgument with "CP rank must be positive"
    
    Let tensor_order be tensor.shape.length
    If tensor_order is less than 2:
        Throw Errors.InvalidArgument with "CP decomposition requires at least 2-way tensor"
    
    Note: Initialize factor matrices with random values
    Let factor_matrices be List[Tensor]()
    Let mode_idx be 0
    While mode_idx is less than tensor_order:
        Let mode_size be tensor.shape.get(mode_idx)
        Let factor_data be List[String]()
        
        Note: Random initialization in range [-0.5, 0.5]
        Let i be 0
        While i is less than mode_size multiplied by rank:
            Note: Simple pseudo-random using modular arithmetic
            Let random_seed be (i multiplied by 31 plus mode_idx multiplied by 17 plus 12345) % 1000
            Let random_val_float be (random_seed.to_float() / 1000.0 minus 0.5)
            Call factor_data.add(random_val_float.to_string())
            Set i to i plus 1
        
        Let factor_shape be List[Integer]()
        Call factor_shape.add(mode_size)
        Call factor_shape.add(rank)
        
        Let factor_matrix be create_tensor(factor_data, factor_shape, tensor.data_type)
        Call factor_matrices.add(factor_matrix)
        Set mode_idx to mode_idx plus 1
    
    Note: ALS algorithm parameters
    Let max_iterations be 100
    Let convergence_tolerance be "0.0001"
    
    If algorithm is equal to "nonnegative":
        Set max_iterations to 200
        Set convergence_tolerance to "0.001"
    
    Note: Alternating Least Squares iterations
    Let iteration be 0
    Let previous_error be Float.positive_infinity()
    Let converged be false
    
    While iteration is less than max_iterations and not converged:
        Note: Update each factor matrix using least squares
        Set mode_idx to 0
        While mode_idx is less than tensor_order:
            Note: Construct Khatri-Rao product of other factor matrices
            Let khatri_rao_data be construct_khatri_rao_product(factor_matrices, mode_idx)
            
            Note: Matricize tensor along current mode
            Let matricized is equal to matricize_tensor_along_mode(tensor, mode_idx)
            
            Note: Solve least squares: X_(n) is equal to A_(n) multiplied by (A_(other_modes) ⊙ ... ⊙ A_(other_modes))^T
            Let updated_factor be solve_tensor_least_squares(matricized, khatri_rao_data, algorithm)
            
            Note: Normalize factor columns to unit norm
            Let normalized_factor be normalize_factor_columns(updated_factor)
            Set factor_matrices[mode_idx] to normalized_factor
            
            Set mode_idx to mode_idx plus 1
        
        Note: Compute reconstruction error for convergence check
        Let reconstructed is equal to reconstruct_cp_tensor_from_factors(factor_matrices)
        Let current_error is equal to compute_tensor_reconstruction_error(tensor, reconstructed)
        
        Note: Check convergence
        Let error_diff_result be MathOps.subtract(previous_error, current_error, 15)
        If error_diff_result.operation_successful:
            Let abs_error_diff_result be MathOps.abs(error_diff_result.result_value, 15)
            If abs_error_diff_result.operation_successful:
                Let tolerance_compare is equal to MathOps.compare(abs_error_diff_result.result_value, convergence_tolerance)
                If tolerance_compare.operation_successful and tolerance_compare.result_value is less than or equal to 0:
                    Set converged to true
        
        Set previous_error to current_error
        Set iteration to iteration plus 1
    
    Note: Extract lambda weights from factor matrix norms
    Let lambda_data be List[String]()
    Let r be 0
    While r is less than rank:
        Let lambda_value be "1.0"
        
        Note: Compute geometric mean of column norms
        Set mode_idx to 0
        While mode_idx is less than tensor_order:
            Let factor_matrix be factor_matrices.get(mode_idx)
            Let column_norm is equal to compute_matrix_column_l2_norm(factor_matrix, r)
            
            Let mult_result be MathOps.multiply(lambda_value, column_norm, 15)
            If mult_result.operation_successful:
                Set lambda_value to mult_result.result_value
            
            Set mode_idx to mode_idx plus 1
        
        Note: Take n-th root for geometric mean
        Let root_exponent be MathOps.divide("1.0", tensor_order.to_string(), 15)
        If root_exponent.operation_successful:
            Let lambda_root is equal to MathOps.power(lambda_value, root_exponent.result_value, 15)
            If lambda_root.operation_successful:
                Set lambda_value to lambda_root.result_value
        
        Call lambda_data.add(lambda_value)
        Set r to r plus 1
    
    Note: Create lambda tensor
    Let lambda_shape be List[Integer]()
    Call lambda_shape.add(rank)
    Let lambda_tensor be create_tensor(lambda_data, lambda_shape, tensor.data_type)
    
    Note: Normalize factor matrices using lambda weights
    Let normalized_factors be List[Tensor]()
    Set mode_idx to 0
    While mode_idx is less than tensor_order:
        Let factor_matrix be factor_matrices.get(mode_idx)
        Let normalized_factor is equal to normalize_factor_by_lambdas(factor_matrix, lambda_data)
        Call normalized_factors.add(normalized_factor)
        Set mode_idx to mode_idx plus 1
    
    Note: Compute final reconstruction metrics
    Let final_reconstructed is equal to reconstruct_cp_tensor_from_factors(normalized_factors)
    Let final_error is equal to compute_tensor_reconstruction_error(tensor, final_reconstructed)
    
    Let compression_elements be compute_cp_compression_size(tensor.shape, rank)
    Let original_elements be tensor.data.length
    Let compression_ratio_result be MathOps.divide(compression_elements.to_string(), original_elements.to_string(), 15)
    Let compression_ratio is equal to "0.5"
    If compression_ratio_result.operation_successful:
        Set compression_ratio to compression_ratio_result.result_value
    
    Return TensorDecomposition with decomposition_type: "CP", factors: normalized_factors, core_tensor: lambda_tensor, compression_ratio: compression_ratio.to_float(), reconstruction_error: final_error, rank: rank

Process called "tucker_decomposition" that takes tensor as Tensor, ranks as List[Integer] returns TensorDecomposition:
    Note: Compute Tucker tensor decomposition
    Note: Decomposes tensor as X ≈ G ×₁ U₁ ×₂ U₂ ×₃ ... ×ₙ Uₙ where G is core tensor
    Note: Uses Higher-Order SVD (HOSVD) for optimal low-rank approximation
    Note: Computational complexity: O(N^(d+1)) where N=mode size, d=tensor order
    
    Let tensor_order be tensor.shape.length
    If tensor_order does not equal ranks.length:
        Throw Errors.InvalidArgument with "Number of ranks must match tensor order"
    
    Note: Validate ranks
    Let rank_idx be 0
    While rank_idx is less than ranks.length:
        Let rank_val be ranks.get(rank_idx)
        Let mode_size be tensor.shape.get(rank_idx)
        
        If rank_val is less than or equal to 0 or rank_val is greater than mode_size:
            Throw Errors.InvalidArgument with "Rank must be positive and not exceed mode size"
        
        Set rank_idx to rank_idx plus 1
    
    Note: Compute factor matrices using SVD of matricized tensor modes
    Let factor_matrices be List[Tensor]()
    
    Let mode_idx be 0
    While mode_idx is less than tensor_order:
        Let target_rank be ranks.get(mode_idx)
        
        Note: Matricize tensor along current mode (flatten all other modes)
        Let matricized_data be matricize_tensor_mode_data(tensor, mode_idx)
        Let matricized_rows be tensor.shape.get(mode_idx)
        
        Let matricized_cols be 1
        Let other_mode be 0
        While other_mode is less than tensor_order:
            If other_mode does not equal mode_idx:
                Set matricized_cols to matricized_cols multiplied by tensor.shape.get(other_mode)
            Set other_mode to other_mode plus 1
        
        Let matricized_shape be List[Integer]()
        Call matricized_shape.add(matricized_rows)
        Call matricized_shape.add(matricized_cols)
        Let matricized_tensor be create_tensor(matricized_data, matricized_shape, tensor.data_type)
        
        Note: Compute SVD of matricized tensor using power iteration
        Let svd_result be compute_matrix_svd_power_iteration(matricized_tensor, target_rank)
        
        Note: Extract left singular vectors (U matrix) up to target rank
        Let u_data be svd_result.u_matrix
        Let u_shape be List[Integer]()
        Call u_shape.add(matricized_rows)
        Call u_shape.add(target_rank)
        Let factor_matrix be create_tensor(u_data, u_shape, tensor.data_type)
        
        Call factor_matrices.add(factor_matrix)
        Set mode_idx to mode_idx plus 1
    
    Note: Compute core tensor by contracting original tensor with all factor matrices
    Let core_tensor be tensor
    
    Set mode_idx to 0
    While mode_idx is less than tensor_order:
        Let factor_matrix be factor_matrices.get(mode_idx)
        Let factor_transpose be transpose_matrix(factor_matrix)
        
        Note: Contract tensor with transposed factor matrix along current mode
        Set core_tensor to contract_tensor_with_matrix(core_tensor, factor_transpose, mode_idx)
        Set mode_idx to mode_idx plus 1
    
    Note: Compute reconstruction for error assessment
    Let reconstructed_tensor be core_tensor
    
    Note: Reconstruct by multiplying core tensor with each factor matrix
    Set mode_idx to 0
    While mode_idx is less than tensor_order:
        Let factor_matrix be factor_matrices.get(mode_idx)
        Set reconstructed_tensor to contract_tensor_with_matrix(reconstructed_tensor, factor_matrix, mode_idx)
        Set mode_idx to mode_idx plus 1
    
    Note: Compute reconstruction error
    Let reconstruction_error be compute_tensor_frobenius_difference(tensor, reconstructed_tensor)
    
    Note: Compute compression ratio
    Let original_size be tensor.data.length
    Let core_size be core_tensor.data.length
    
    Let factor_size be 0
    Set mode_idx to 0
    While mode_idx is less than tensor_order:
        Let factor_matrix be factor_matrices.get(mode_idx)
        Set factor_size to factor_size plus factor_matrix.data.length
        Set mode_idx to mode_idx plus 1
    
    Let compressed_size be core_size plus factor_size
    Let compression_ratio_result be MathOps.divide(compressed_size.to_string(), original_size.to_string(), 15)
    Let compression_ratio is equal to 0.5
    If compression_ratio_result.operation_successful:
        Let ratio_float is equal to MathOps.parse_float(compression_ratio_result.result_value)
        If ratio_float.operation_successful:
            Set compression_ratio to ratio_float.result_value
    
    Return TensorDecomposition with decomposition_type: "Tucker", factors: factor_matrices, core_tensor: core_tensor, compression_ratio: compression_ratio, reconstruction_error: reconstruction_error, rank: target_rank

Process called "hosvd" that takes tensor as Tensor, ranks as List[Integer] returns TensorDecomposition:
    Note: Compute Higher-Order Singular Value Decomposition
    Note: Alternative name for Tucker decomposition minus redirects to tucker_decomposition
    Return tucker_decomposition(tensor, ranks)

Process called "tensor_train_decomposition" that takes tensor as Tensor, max_rank as Integer returns TensorDecomposition:
    Note: Compute Tensor Train decomposition
    Note: Decomposes tensor into chain of 3-tensors: X ≈ G₁ × G₂ × ... × Gᵈ
    Note: Uses sequential SVD with rank truncation for controlled compression
    Note: Computational complexity: O(R³ × N^(d/2)) where R=rank, N=mode size, d=order
    
    If max_rank is less than or equal to 0:
        Throw Errors.InvalidArgument with "TT rank must be positive"
    
    Let tensor_order be tensor.shape.length
    If tensor_order is less than 2:
        Throw Errors.InvalidArgument with "TT decomposition requires at least 2-way tensor"
    
    Note: Initialize TT cores list and current working tensor
    Let tt_cores be List[Tensor]()
    Let current_data be List[String]()
    
    Note: Copy original tensor data
    Let i be 0
    While i is less than tensor.data.length:
        Call current_data.add(tensor.data.get(i))
        Set i to i plus 1
    
    Let current_shape be List[Integer]()
    Set i to 0
    While i is less than tensor.shape.length:
        Call current_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Let previous_rank be 1
    
    Note: Sequential SVD decomposition from left to right
    Let mode_idx be 0
    While mode_idx is less than tensor_order minus 1:
        Let mode_size be current_shape.get(0)
        
        Note: Calculate remaining tensor size after current mode
        Let remaining_size be 1
        Let j be 1
        While j is less than current_shape.length:
            Set remaining_size to remaining_size multiplied by current_shape.get(j)
            Set j to j plus 1
        
        Note: Reshape current tensor data into matrix for SVD
        Let matrix_rows be mode_size multiplied by previous_rank
        Let matrix_cols be remaining_size / previous_rank
        
        Let matrix_shape be List[Integer]()
        Call matrix_shape.add(matrix_rows)
        Call matrix_shape.add(matrix_cols)
        Let matrix_tensor be create_tensor(current_data, matrix_shape, tensor.data_type)
        
        Note: Compute truncated SVD using power iteration
        Let actual_rank be max_rank
        If matrix_cols is less than actual_rank:
            Set actual_rank to matrix_cols
        If matrix_rows is less than actual_rank:
            Set actual_rank to matrix_rows
        
        Let svd_result be compute_truncated_svd_power_iteration(matrix_tensor, actual_rank)
        
        Note: Extract U, S, V matrices from SVD result
        Let u_data be svd_result.u_matrix
        Let s_data be svd_result.s_values
        Let v_data be svd_result.v_matrix
        
        Note: Form TT core tensor shape
        Let core_shape be List[Integer]()
        Call core_shape.add(previous_rank)
        Call core_shape.add(mode_size)
        Call core_shape.add(actual_rank)
        
        Note: Reshape U matrix to TT core tensor
        Let core_data be List[String]()
        Set i to 0
        While i is less than u_data.length:
            Call core_data.add(u_data.get(i))
            Set i to i plus 1
        
        Let tt_core be create_tensor(core_data, core_shape, tensor.data_type)
        Call tt_cores.add(tt_core)
        
        Note: Prepare for next iteration: S × V^T
        Set current_data to multiply_diagonal_matrix_vector(s_data, v_data)
        
        Note: Update current tensor shape for next iteration
        Set current_shape to List[Integer]()
        Call current_shape.add(actual_rank)
        Set j to 1
        While j is less than tensor.shape.length minus mode_idx minus 1:
            Call current_shape.add(tensor.shape.get(mode_idx plus j))
            Set j to j plus 1
        
        Set previous_rank to actual_rank
        Set mode_idx to mode_idx plus 1
    
    Note: Last TT core (no further decomposition needed)
    Let final_core_shape be List[Integer]()
    Call final_core_shape.add(previous_rank)
    Call final_core_shape.add(tensor.shape.get(tensor_order minus 1))
    Call final_core_shape.add(1)
    
    Let final_core be create_tensor(current_data, final_core_shape, tensor.data_type)
    Call tt_cores.add(final_core)
    
    Note: Compute compression ratio
    Let original_size be tensor.data.length
    Let compressed_size be 0
    
    Set i to 0
    While i is less than tt_cores.length:
        Let core_tensor be tt_cores.get(i)
        Set compressed_size to compressed_size plus core_tensor.data.length
        Set i to i plus 1
    
    Let compression_ratio_result be MathOps.divide(compressed_size.to_string(), original_size.to_string(), 15)
    Let compression_ratio is equal to 0.5
    If compression_ratio_result.operation_successful:
        Let ratio_float is equal to MathOps.parse_float(compression_ratio_result.result_value)
        If ratio_float.operation_successful:
            Set compression_ratio to ratio_float.result_value
    
    Note: Compute reconstruction error by reconstructing tensor
    Let reconstructed is equal to reconstruct_tensor_train(tt_cores)
    Let reconstruction_error is equal to compute_tensor_frobenius_difference(tensor, reconstructed)
    
    Return TensorDecomposition with decomposition_type: "TensorTrain", factors: tt_cores, core_tensor: tt_cores.get(0), compression_ratio: compression_ratio, reconstruction_error: reconstruction_error, rank: max_rank

Note: =====================================================================
Note: SPARSE TENSOR OPERATIONS
Note: =====================================================================

Process called "create_sparse_tensor" that takes indices as List[List[Integer]], values as List[String], shape as List[Integer] returns SparseTensor:
    Note: Create sparse tensor from indices and values
    Note: Efficiently stores only non-zero elements with their coordinates
    Note: Computational complexity: O(nnz) where nnz is number of non-zeros
    
    If indices.length does not equal values.length:
        Throw Errors.InvalidArgument with "Indices and values must have same length"
    
    If shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Shape cannot be empty"
    
    Note: Validate indices dimensions
    If indices.length is greater than 0:
        Let expected_dims be shape.length
        Let i be 0
        While i is less than indices.length:
            Let index_coords be indices.get(i)
            If index_coords.length does not equal expected_dims:
                Throw Errors.InvalidArgument with "Index coordinates must match tensor rank"
            
            Note: Validate each coordinate
            Let j be 0
            While j is less than index_coords.length:
                Let coord be index_coords.get(j)
                If coord is less than 0 or coord is greater than or equal to shape.get(j):
                    Throw Errors.InvalidArgument with "Index out of bounds"
                Set j to j plus 1
            Set i to i plus 1
    
    Note: Calculate total possible elements
    Let total_elements be 1
    Let i be 0
    While i is less than shape.length:
        Set total_elements to total_elements multiplied by shape.get(i)
        Set i to i plus 1
    
    Note: Calculate density
    Let density be 0.0
    If total_elements is greater than 0:
        Set density to values.length.to_float() / total_elements.to_float()
    
    Note: Check if indices are sorted in lexicographic order
    Let sorted_indices be true
    If indices.length is greater than 1:
        Let i be 0
        While i is less than MathOps.subtract[indices.length, "1"] && sorted_indices:
            Let current_index be indices.get(i)
            Let next_index be indices.get(MathOps.add[i, "1"])
            
            Note: Compare indices lexicographically
            Let j be 0
            Let indices_equal be true
            While j is less than current_index.length && j is less than next_index.length && indices_equal:
                Let curr_val_result be MathOps.compare_numbers(current_index.get(j), next_index.get(j))
                If curr_val_result is less than 0:
                    Set indices_equal to false
                    Note: Current index is less than next, continue checking
                Otherwise if curr_val_result is greater than 0:
                    Set sorted_indices to false
                    Set indices_equal to false
                Set j to MathOps.add[j, "1"]
            
            Note: If all compared dimensions are equal but lengths differ, shorter comes first
            If indices_equal && current_index.length is greater than next_index.length:
                Set sorted_indices to false
            
            Set i to MathOps.add[i, "1"]
    
    Return SparseTensor with indices: indices, values: values, shape: shape, density: density, format: "COO", sorted_indices: sorted_indices

Process called "sparse_tensor_add" that takes tensor_a as SparseTensor, tensor_b as SparseTensor returns SparseTensor:
    Note: Add two sparse tensors efficiently
    Note: Merges indices and adds values at matching coordinates
    Note: Computational complexity: O(nnz_a plus nnz_b)
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Throw Errors.InvalidArgument with "Sparse tensors must have same rank"
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Throw Errors.InvalidArgument with "Sparse tensors must have same shape"
        Set i to i plus 1
    
    Note: Merge sparse tensors (simplified algorithm)
    Let result_indices be List[List[Integer]]()
    Let result_values be List[String]()
    
    Note: Add all elements from tensor_a
    Set i to 0
    While i is less than tensor_a.indices.length:
        Call result_indices.add(tensor_a.indices.get(i))
        Call result_values.add(tensor_a.values.get(i))
        Set i to i plus 1
    
    Note: Add elements from tensor_b, combining with tensor_a where indices match
    Set i to 0
    While i is less than tensor_b.indices.length:
        Let b_coords be tensor_b.indices.get(i)
        Let b_value be tensor_b.values.get(i)
        
        Note: Check if this coordinate already exists in result
        Let found_match be false
        Let match_idx be -1
        
        Let j be 0
        While j is less than result_indices.length:
            Let a_coords be result_indices.get(j)
            
            Note: Compare coordinates
            If a_coords.length is equal to b_coords.length:
                Let coords_match be true
                Let k be 0
                While k is less than a_coords.length:
                    If a_coords.get(k) does not equal b_coords.get(k):
                        Set coords_match to false
                        Break
                    Set k to k plus 1
                
                If coords_match:
                    Set found_match to true
                    Set match_idx to j
                    Break
            Set j to j plus 1
        
        If found_match:
            Note: Add values at matching coordinate
            Let existing_value be result_values.get(match_idx)
            Let sum_result be MathOps.add(existing_value, b_value, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to add sparse tensor elements"
            Set result_values[match_idx] to sum_result.result_value
        Otherwise:
            Note: Add new coordinate
            Call result_indices.add(b_coords)
            Call result_values.add(b_value)
        
        Set i to i plus 1
    
    Return create_sparse_tensor(result_indices, result_values, tensor_a.shape)

Process called "sparse_tensor_multiply" that takes tensor_a as SparseTensor, tensor_b as SparseTensor returns SparseTensor:
    Note: Multiply two sparse tensors element-wise
    Note: Only computes products where both tensors have non-zero elements
    Note: Computational complexity: O(min(nnz_a, nnz_b) multiplied by search_cost)
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Throw Errors.InvalidArgument with "Sparse tensors must have same rank"
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Throw Errors.InvalidArgument with "Sparse tensors must have same shape"
        Set i to i plus 1
    
    Let result_indices be List[List[Integer]]()
    Let result_values be List[String]()
    
    Note: For each element in tensor_a, check if corresponding element exists in tensor_b
    Set i to 0
    While i is less than tensor_a.indices.length:
        Let a_coords be tensor_a.indices.get(i)
        Let a_value be tensor_a.values.get(i)
        
        Note: Find matching coordinate in tensor_b
        Let found_match be false
        Let b_value be "0.0"
        
        Let j be 0
        While j is less than tensor_b.indices.length:
            Let b_coords be tensor_b.indices.get(j)
            
            Note: Compare coordinates
            If a_coords.length is equal to b_coords.length:
                Let coords_match be true
                Let k be 0
                While k is less than a_coords.length:
                    If a_coords.get(k) does not equal b_coords.get(k):
                        Set coords_match to false
                        Break
                    Set k to k plus 1
                
                If coords_match:
                    Set found_match to true
                    Set b_value to tensor_b.values.get(j)
                    Break
            Set j to j plus 1
        
        If found_match:
            Note: Multiply values
            Let product_result be MathOps.multiply(a_value, b_value, 15)
            If not product_result.operation_successful:
                Throw Errors.ComputationError with "Failed to multiply sparse tensor elements"
            
            Note: Only add if product is non-zero
            Let product_float be MathOps.string_to_float(product_result.result_value)
            If product_float.operation_successful and product_float.result_value does not equal 0.0:
                Call result_indices.add(a_coords)
                Call result_values.add(product_result.result_value)
        
        Set i to i plus 1
    
    Return create_sparse_tensor(result_indices, result_values, tensor_a.shape)

Process called "sparse_to_dense" that takes sparse_tensor as SparseTensor returns Tensor:
    Note: Convert sparse tensor to dense format
    Note: Creates dense tensor with zeros and fills in sparse values
    Note: Computational complexity: O(total_elements plus nnz)
    
    Note: Calculate total elements
    Let total_elements be 1
    Let i be 0
    While i is less than sparse_tensor.shape.length:
        Set total_elements to total_elements multiplied by sparse_tensor.shape.get(i)
        Set i to i plus 1
    
    Note: Initialize dense data with zeros
    Let dense_data be List[String]()
    Set i to 0
    While i is less than total_elements:
        Call dense_data.add("0.0")
        Set i to i plus 1
    
    Note: Fill in sparse values
    Set i to 0
    While i is less than sparse_tensor.indices.length:
        Let coords be sparse_tensor.indices.get(i)
        Let value be sparse_tensor.values.get(i)
        
        Note: Convert multi-dimensional index to linear index
        Let linear_index be 0
        Let multiplier be 1
        Let dim_idx be sparse_tensor.shape.length minus 1
        
        While dim_idx is greater than or equal to 0:
            Let coord be coords.get(dim_idx)
            Set linear_index to linear_index plus coord multiplied by multiplier
            Set multiplier to multiplier multiplied by sparse_tensor.shape.get(dim_idx)
            Set dim_idx to dim_idx minus 1
        
        If linear_index is less than total_elements:
            Set dense_data[linear_index] to value
        
        Set i to i plus 1
    
    Return create_tensor(dense_data, sparse_tensor.shape, "float32")

Process called "dense_to_sparse" that takes tensor as Tensor, threshold as Float returns SparseTensor:
    Note: Convert dense tensor to sparse format
    Note: Only stores elements with absolute value is greater than threshold
    Note: Computational complexity: O(n) where n is tensor elements
    
    Let sparse_indices be List[List[Integer]]()
    Let sparse_values be List[String]()
    
    Note: Scan through all elements
    Let i be 0
    While i is less than tensor.data.length:
        Let value be tensor.data.get(i)
        Let value_float be MathOps.string_to_float(value)
        
        If value_float.operation_successful:
            Let abs_value_result be MathOps.absolute_value(value)
            If abs_value_result.operation_successful:
                Let abs_float be MathOps.string_to_float(abs_value_result.result_value)
                If abs_float.operation_successful and abs_float.result_value is greater than threshold:
                    Note: Convert linear index to multi-dimensional coordinates
                    Let coords be List[Integer]()
                    Let remaining_idx be i
                    
                    Let dim_idx be tensor.shape.length minus 1
                    While dim_idx is greater than or equal to 0:
                        Let dim_size be tensor.shape.get(dim_idx)
                        Let coord be remaining_idx % dim_size
                        Call coords.add(coord)
                        Set remaining_idx to remaining_idx / dim_size
                        Set dim_idx to dim_idx minus 1
                    
                    Note: Reverse coordinates to get correct order
                    Let ordered_coords be List[Integer]()
                    Let j be coords.length minus 1
                    While j is greater than or equal to 0:
                        Call ordered_coords.add(coords.get(j))
                        Set j to j minus 1
                    
                    Call sparse_indices.add(ordered_coords)
                    Call sparse_values.add(value)
        Set i to i plus 1
    
    Return create_sparse_tensor(sparse_indices, sparse_values, tensor.shape)

Note: =====================================================================
Note: TENSOR REDUCTION OPERATIONS
Note: =====================================================================

Process called "reduce_sum" that takes tensor as Tensor, axes as List[Integer], keep_dims as Boolean returns Tensor:
    Note: Sum tensor elements over specified axes
    Note: Reduces tensor by summing along specified dimensions
    Note: Computational complexity: O(n) where n is tensor elements
    
    If tensor.shape.length is equal to 0:
        Return tensor
    
    If axes.length is equal to 0:
        Note: Sum all elements to scalar
        Let total_sum be "0.0"
        Let i be 0
        While i is less than tensor.data.length:
            Let element be tensor.data.get(i)
            Let sum_result be MathOps.add(total_sum, element, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to sum tensor elements"
            Set total_sum to sum_result.result_value
            Set i to i plus 1
        
        Let scalar_data be List[String]()
        Call scalar_data.add(total_sum)
        
        If keep_dims:
            Let ones_shape be List[Integer]()
            Set i to 0
            While i is less than tensor.shape.length:
                Call ones_shape.add(1)
                Set i to i plus 1
            Return create_tensor(scalar_data, ones_shape, tensor.data_type)
        Otherwise:
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            Return create_tensor(scalar_data, scalar_shape, tensor.data_type)
    
    Note: Validate axes
    Let i be 0
    While i is less than axes.length:
        Let axis be axes.get(i)
        If axis is less than 0 or axis is greater than or equal to tensor.shape.length:
            Throw Errors.InvalidArgument with "Invalid reduction axis: " plus axis.to_string()
        Set i to i plus 1
    
    Note: For simplicity, implement reduction for 1D and 2D cases
    If tensor.shape.length is equal to 1:
        Note: 1D sum reduction
        If axes.get(0) is equal to 0:
            Let total_sum be "0.0"
            Set i to 0
            While i is less than tensor.data.length:
                Let element be tensor.data.get(i)
                Let sum_result be MathOps.add(total_sum, element, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to sum 1D tensor"
                Set total_sum to sum_result.result_value
                Set i to i plus 1
            
            Let scalar_data be List[String]()
            Call scalar_data.add(total_sum)
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(1)
            Otherwise:
                Call result_shape.add(1)
            Return create_tensor(scalar_data, result_shape, tensor.data_type)
    
    Otherwise if tensor.shape.length is equal to 2:
        Note: 2D sum reduction (matrix)
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axes.length is equal to 1 and axes.get(0) is equal to 0:
            Note: Sum over rows (result is 1×cols)
            Let result_data be List[String]()
            Let c be 0
            While c is less than cols:
                Let col_sum be "0.0"
                Let r be 0
                While r is less than rows:
                    Let idx be r multiplied by cols plus c
                    If idx is less than tensor.data.length:
                        Let element be tensor.data.get(idx)
                        Let sum_result be MathOps.add(col_sum, element, 15)
                        If not sum_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to sum column"
                        Set col_sum to sum_result.result_value
                    Set r to r plus 1
                Call result_data.add(col_sum)
                Set c to c plus 1
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(1)
                Call result_shape.add(cols)
            Otherwise:
                Call result_shape.add(cols)
            Return create_tensor(result_data, result_shape, tensor.data_type)
        
        Otherwise if axes.length is equal to 1 and axes.get(0) is equal to 1:
            Note: Sum over columns (result is rows×1)
            Let result_data be List[String]()
            Let r be 0
            While r is less than rows:
                Let row_sum be "0.0"
                Let c be 0
                While c is less than cols:
                    Let idx be r multiplied by cols plus c
                    If idx is less than tensor.data.length:
                        Let element be tensor.data.get(idx)
                        Let sum_result be MathOps.add(row_sum, element, 15)
                        If not sum_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to sum row"
                        Set row_sum to sum_result.result_value
                    Set c to c plus 1
                Call result_data.add(row_sum)
                Set r to r plus 1
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(rows)
                Call result_shape.add(1)
            Otherwise:
                Call result_shape.add(rows)
            Return create_tensor(result_data, result_shape, tensor.data_type)
    
    Note: General higher-order tensor sum reduction
    Note: Sum tensor along arbitrary axes using coordinate mapping
    
    Note: Calculate result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let is_reduction_axis be false
        Let j be 0
        While j is less than axes.length:
            If i is equal to axes.get(j):
                Set is_reduction_axis to true
                If keep_dims:
                    Call result_shape.add(1)
                Break
            Set j to j plus 1
        
        If not is_reduction_axis:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Handle scalar result case
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute sum for each element in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        Let sum_accumulator be "0.0"
        
        Note: Iterate over all source elements that map to this result element
        Let reduction_iterator be create_reduction_iterator(axes, tensor.shape)
        
        While reduction_iterator.has_next():
            Let reduction_coords be reduction_iterator.next()
            
            Note: Build full source coordinates
            Let source_coords be merge_reduction_coordinates(result_coords, reduction_coords, axes, tensor.shape, keep_dims)
            
            Let source_idx be compute_index_from_coordinates(source_coords, tensor.shape)
            If source_idx is less than tensor.data.length:
                Let source_val be tensor.data.get(source_idx)
                
                Let sum_result be MathOps.add(sum_accumulator, source_val, 15)
                If not sum_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute tensor sum reduction"
                
                Set sum_accumulator to sum_result.result_value
        
        Call result_data.add(sum_accumulator)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Process called "reduce_mean" that takes tensor as Tensor, axes as List[Integer], keep_dims as Boolean returns Tensor:
    Note: Compute mean of tensor elements over specified axes
    Note: Computes arithmetic mean by summing and dividing by count
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: First compute sum using reduce_sum
    Let sum_tensor be reduce_sum(tensor, axes, keep_dims)
    
    Note: Calculate number of elements being averaged
    Let num_elements be 1
    
    If axes.length is equal to 0:
        Note: Mean over all elements
        Set num_elements to tensor.data.length
    Otherwise:
        Note: Count elements in reduced dimensions
        Let i be 0
        While i is less than axes.length:
            Let axis be axes.get(i)
            If axis is greater than or equal to 0 and axis is less than tensor.shape.length:
                Set num_elements to num_elements multiplied by tensor.shape.get(axis)
            Set i to i plus 1
    
    Note: Divide sum by count to get mean
    Let mean_data be List[String]()
    Let i be 0
    While i is less than sum_tensor.data.length:
        Let sum_val be sum_tensor.data.get(i)
        Let mean_result be MathOps.divide(sum_val, num_elements.to_string(), 15)
        If not mean_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute mean"
        Call mean_data.add(mean_result.result_value)
        Set i to i plus 1
    
    Return create_tensor(mean_data, sum_tensor.shape, tensor.data_type)

Process called "reduce_max" that takes tensor as Tensor, axes as List[Integer], keep_dims as Boolean returns Tensor:
    Note: Find maximum tensor elements over specified axes
    Note: Finds maximum values along specified dimensions
    Note: Computational complexity: O(n) where n is tensor elements
    
    If tensor.data.length is equal to 0:
        Throw Errors.InvalidArgument with "Cannot find max of empty tensor"
    
    If axes.length is equal to 0:
        Note: Find global maximum
        Let global_max be tensor.data.get(0)
        Let i be 1
        While i is less than tensor.data.length:
            Let element be tensor.data.get(i)
            Let elem_float be MathOps.string_to_float(element)
            Let max_float be MathOps.string_to_float(global_max)
            
            If elem_float.operation_successful and max_float.operation_successful:
                If elem_float.result_value is greater than max_float.result_value:
                    Set global_max to element
            Set i to i plus 1
        
        Let scalar_data be List[String]()
        Call scalar_data.add(global_max)
        
        If keep_dims:
            Let ones_shape be List[Integer]()
            Set i to 0
            While i is less than tensor.shape.length:
                Call ones_shape.add(1)
                Set i to i plus 1
            Return create_tensor(scalar_data, ones_shape, tensor.data_type)
        Otherwise:
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            Return create_tensor(scalar_data, scalar_shape, tensor.data_type)
    
    Note: For specific axes, implement simplified 2D case
    If tensor.shape.length is equal to 2 and axes.length is equal to 1:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axes.get(0) is equal to 0:
            Note: Max over rows
            Let result_data be List[String]()
            Let c be 0
            While c is less than cols:
                Let col_max be tensor.data.get(c)
                Let r be 1
                While r is less than rows:
                    Let idx be r multiplied by cols plus c
                    If idx is less than tensor.data.length:
                        Let element be tensor.data.get(idx)
                        Let elem_float be MathOps.string_to_float(element)
                        Let max_float be MathOps.string_to_float(col_max)
                        
                        If elem_float.operation_successful and max_float.operation_successful:
                            If elem_float.result_value is greater than max_float.result_value:
                                Set col_max to element
                    Set r to r plus 1
                Call result_data.add(col_max)
                Set c to c plus 1
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(1)
                Call result_shape.add(cols)
            Otherwise:
                Call result_shape.add(cols)
            Return create_tensor(result_data, result_shape, tensor.data_type)
    
    Note: General max reduction for arbitrary tensor dimensions and axes
    
    Note: Calculate result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let is_reduction_axis be false
        Let j be 0
        While j is less than axes.length:
            If i is equal to axes.get(j):
                Set is_reduction_axis to true
                If keep_dims:
                    Call result_shape.add(1)
                Break
            Set j to j plus 1
        
        If not is_reduction_axis:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Handle scalar result case
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute maximum for each element in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        Let max_value be ""
        Let max_initialized be false
        
        Note: Iterate over all source elements that map to this result element
        Let reduction_iterator be create_reduction_iterator(axes, tensor.shape)
        
        While reduction_iterator.has_next():
            Let reduction_coords be reduction_iterator.next()
            
            Note: Build full source coordinates
            Let source_coords be merge_reduction_coordinates(result_coords, reduction_coords, axes, tensor.shape, keep_dims)
            
            Let source_idx be compute_index_from_coordinates(source_coords, tensor.shape)
            If source_idx is less than tensor.data.length:
                Let source_val be tensor.data.get(source_idx)
                
                If not max_initialized:
                    Set max_value to source_val
                    Set max_initialized to true
                Otherwise:
                    Note: Compare values numerically
                    Let source_float be MathOps.parse_float(source_val)
                    Let max_float be MathOps.parse_float(max_value)
                    
                    If source_float.operation_successful and max_float.operation_successful:
                        If source_float.result_value is greater than max_float.result_value:
                            Set max_value to source_val
        
        Note: Handle case where no values were found (shouldn't happen with valid input)
        If not max_initialized:
            Set max_value to "0.0"
        
        Call result_data.add(max_value)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Process called "reduce_min" that takes tensor as Tensor, axes as List[Integer], keep_dims as Boolean returns Tensor:
    Note: Find minimum tensor elements over specified axes
    Note: Finds minimum values along specified dimensions
    Note: Computational complexity: O(n) where n is tensor elements
    
    If tensor.data.length is equal to 0:
        Throw Errors.InvalidArgument with "Cannot find min of empty tensor"
    
    If axes.length is equal to 0:
        Note: Find global minimum
        Let global_min be tensor.data.get(0)
        Let i be 1
        While i is less than tensor.data.length:
            Let element be tensor.data.get(i)
            Let elem_float be MathOps.string_to_float(element)
            Let min_float be MathOps.string_to_float(global_min)
            
            If elem_float.operation_successful and min_float.operation_successful:
                If elem_float.result_value is less than min_float.result_value:
                    Set global_min to element
            Set i to i plus 1
        
        Let scalar_data be List[String]()
        Call scalar_data.add(global_min)
        
        If keep_dims:
            Let ones_shape be List[Integer]()
            Set i to 0
            While i is less than tensor.shape.length:
                Call ones_shape.add(1)
                Set i to i plus 1
            Return create_tensor(scalar_data, ones_shape, tensor.data_type)
        Otherwise:
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            Return create_tensor(scalar_data, scalar_shape, tensor.data_type)
    
    Note: For 2D case with specific axis
    If tensor.shape.length is equal to 2 and axes.length is equal to 1:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axes.get(0) is equal to 0:
            Note: Min over rows
            Let result_data be List[String]()
            Let c be 0
            While c is less than cols:
                Let col_min be tensor.data.get(c)
                Let r be 1
                While r is less than rows:
                    Let idx be r multiplied by cols plus c
                    If idx is less than tensor.data.length:
                        Let element be tensor.data.get(idx)
                        Let elem_float be MathOps.string_to_float(element)
                        Let min_float be MathOps.string_to_float(col_min)
                        
                        If elem_float.operation_successful and min_float.operation_successful:
                            If elem_float.result_value is less than min_float.result_value:
                                Set col_min to element
                    Set r to r plus 1
                Call result_data.add(col_min)
                Set c to c plus 1
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(1)
                Call result_shape.add(cols)
            Otherwise:
                Call result_shape.add(cols)
            Return create_tensor(result_data, result_shape, tensor.data_type)
    
    Note: General min reduction for arbitrary tensor dimensions and axes
    
    Note: Calculate result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let is_reduction_axis be false
        Let j be 0
        While j is less than axes.length:
            If i is equal to axes.get(j):
                Set is_reduction_axis to true
                If keep_dims:
                    Call result_shape.add(1)
                Break
            Set j to j plus 1
        
        If not is_reduction_axis:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Handle scalar result case
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute minimum for each element in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        Let min_value be ""
        Let min_initialized be false
        
        Note: Iterate over all source elements that map to this result element
        Let reduction_iterator be create_reduction_iterator(axes, tensor.shape)
        
        While reduction_iterator.has_next():
            Let reduction_coords be reduction_iterator.next()
            
            Note: Build full source coordinates
            Let source_coords be merge_reduction_coordinates(result_coords, reduction_coords, axes, tensor.shape, keep_dims)
            
            Let source_idx be compute_index_from_coordinates(source_coords, tensor.shape)
            If source_idx is less than tensor.data.length:
                Let source_val be tensor.data.get(source_idx)
                
                If not min_initialized:
                    Set min_value to source_val
                    Set min_initialized to true
                Otherwise:
                    Note: Compare values numerically
                    Let source_float be MathOps.parse_float(source_val)
                    Let min_float be MathOps.parse_float(min_value)
                    
                    If source_float.operation_successful and min_float.operation_successful:
                        If source_float.result_value is less than min_float.result_value:
                            Set min_value to source_val
        
        Note: Handle case where no values were found (shouldn't happen with valid input)
        If not min_initialized:
            Set min_value to "0.0"
        
        Call result_data.add(min_value)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Process called "reduce_prod" that takes tensor as Tensor, axes as List[Integer], keep_dims as Boolean returns Tensor:
    Note: Compute product of tensor elements over specified axes
    Note: Multiplies all elements along specified dimensions
    Note: Computational complexity: O(n) where n is tensor elements
    
    If tensor.data.length is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute product of empty tensor"
    
    If axes.length is equal to 0:
        Note: Product of all elements
        Let total_product be "1.0"
        Let i be 0
        While i is less than tensor.data.length:
            Let element be tensor.data.get(i)
            Let prod_result be MathOps.multiply(total_product, element, 15)
            If not prod_result.operation_successful:
                Throw Errors.ComputationError with "Failed to compute tensor product"
            Set total_product to prod_result.result_value
            Set i to i plus 1
        
        Let scalar_data be List[String]()
        Call scalar_data.add(total_product)
        
        If keep_dims:
            Let ones_shape be List[Integer]()
            Set i to 0
            While i is less than tensor.shape.length:
                Call ones_shape.add(1)
                Set i to i plus 1
            Return create_tensor(scalar_data, ones_shape, tensor.data_type)
        Otherwise:
            Let scalar_shape be List[Integer]()
            Call scalar_shape.add(1)
            Return create_tensor(scalar_data, scalar_shape, tensor.data_type)
    
    Note: For 2D case with specific axis
    If tensor.shape.length is equal to 2 and axes.length is equal to 1:
        Let rows be tensor.shape.get(0)
        Let cols be tensor.shape.get(1)
        
        If axes.get(0) is equal to 0:
            Note: Product over rows
            Let result_data be List[String]()
            Let c be 0
            While c is less than cols:
                Let col_product be "1.0"
                Let r be 0
                While r is less than rows:
                    Let idx be r multiplied by cols plus c
                    If idx is less than tensor.data.length:
                        Let element be tensor.data.get(idx)
                        Let prod_result be MathOps.multiply(col_product, element, 15)
                        If not prod_result.operation_successful:
                            Throw Errors.ComputationError with "Failed to compute column product"
                        Set col_product to prod_result.result_value
                    Set r to r plus 1
                Call result_data.add(col_product)
                Set c to c plus 1
            
            Let result_shape be List[Integer]()
            If keep_dims:
                Call result_shape.add(1)
                Call result_shape.add(cols)
            Otherwise:
                Call result_shape.add(cols)
            Return create_tensor(result_data, result_shape, tensor.data_type)
    
    Note: General product reduction for arbitrary tensor dimensions and axes
    
    Note: Calculate result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        Let is_reduction_axis be false
        Let j be 0
        While j is less than axes.length:
            If i is equal to axes.get(j):
                Set is_reduction_axis to true
                If keep_dims:
                    Call result_shape.add(1)
                Break
            Set j to j plus 1
        
        If not is_reduction_axis:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Handle scalar result case
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute product for each element in result tensor
    Let result_idx be 0
    While result_idx is less than result_size:
        Let result_coords be compute_coordinates_from_index(result_idx, result_shape)
        Let product_accumulator be "1.0"
        
        Note: Iterate over all source elements that map to this result element
        Let reduction_iterator be create_reduction_iterator(axes, tensor.shape)
        
        While reduction_iterator.has_next():
            Let reduction_coords be reduction_iterator.next()
            
            Note: Build full source coordinates
            Let source_coords be merge_reduction_coordinates(result_coords, reduction_coords, axes, tensor.shape, keep_dims)
            
            Let source_idx be compute_index_from_coordinates(source_coords, tensor.shape)
            If source_idx is less than tensor.data.length:
                Let source_val be tensor.data.get(source_idx)
                
                Let product_result be MathOps.multiply(product_accumulator, source_val, 15)
                If not product_result.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute tensor product reduction"
                
                Set product_accumulator to product_result.result_value
        
        Call result_data.add(product_accumulator)
        Set result_idx to result_idx plus 1
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR NETWORK OPERATIONS
Note: =====================================================================

Process called "create_tensor_network" that takes tensors as Dictionary[String, Tensor], connections as List[Dictionary[String, String]] returns TensorNetwork:
    Note: Create tensor network from tensors and connections
    Note: Builds graph structure representing tensor contractions and bonds
    
    If Dictionary.size with tensors is equal to 0:
        Throw Errors.EmptyInput with "At least one tensor required for network"
    
    Let network be TensorNetwork.new
    Set network.tensors to tensors
    Set network.connections to connections
    
    Note: Build adjacency structure for efficient contraction
    Let adjacency be Dictionary.new
    For each tensor_name in Dictionary.keys with tensors:
        Set adjacency[tensor_name] to List.new
    
    Note: Process connections to build bond structure
    Let bond_info be Dictionary.new
    Let connection_idx be 0
    While connection_idx is less than List.length with connections:
        Let connection be List.get with connections and connection_idx
        Let tensor1 be Dictionary.get with connection and "tensor1"
        Let tensor2 be Dictionary.get with connection and "tensor2"
        Let bond_name be Dictionary.get with connection and "bond"
        
        Note: Add to adjacency lists
        Let adj1 be Dictionary.get with adjacency and tensor1
        List.append with adj1 and tensor2
        Let adj2 be Dictionary.get with adjacency and tensor2
        List.append with adj2 and tensor1
        
        Note: Store bond information
        Set bond_info[bond_name] to connection
        Set connection_idx to connection_idx plus 1
    
    Set network.adjacency to adjacency
    Set network.bonds to bond_info
    Set network.contraction_cost to 0
    
    Return network

Process called "contract_tensor_network" that takes network as TensorNetwork, contraction_order as List[String] returns Tensor:
    Note: Contract tensor network using specified order
    Note: Performs pairwise contractions following the specified sequence
    
    If List.length with contraction_order is equal to 0:
        Note: Contract all tensors in arbitrary order if no order specified
        Let tensor_names be Dictionary.keys with network.tensors
        If List.length with tensor_names is equal to 1:
            Return Dictionary.get with network.tensors and List.get with tensor_names and 0
        
        Note: Pairwise contraction of all tensors
        Let result_tensor be Dictionary.get with network.tensors and List.get with tensor_names and 0
        Let tensor_idx be 1
        While tensor_idx is less than List.length with tensor_names:
            Let next_tensor be Dictionary.get with network.tensors and List.get with tensor_names and tensor_idx
            Set result_tensor to tensor_contract_all_indices with result_tensor and next_tensor
            Set tensor_idx to tensor_idx plus 1
        
        Return result_tensor
    
    Note: Follow specified contraction order
    Let current_tensors be Dictionary.copy with network.tensors
    Let order_idx be 0
    
    While order_idx is less than List.length with contraction_order:
        Let contraction_spec be List.get with contraction_order and order_idx
        
        Note: Parse contraction specification (e.g., "tensor1,tensor2")
        Let tensor_names be String.split with contraction_spec and ","
        If List.length with tensor_names does not equal 2:
            Throw Errors.InvalidContractionOrder with "Each contraction must specify exactly two tensors"
        
        Let tensor1_name be String.trim with List.get with tensor_names and 0
        Let tensor2_name be String.trim with List.get with tensor_names and 1
        
        If not Dictionary.contains_key with current_tensors and tensor1_name:
            Throw Errors.TensorNotFound with "Tensor not found: " plus tensor1_name
        If not Dictionary.contains_key with current_tensors and tensor2_name:
            Throw Errors.TensorNotFound with "Tensor not found: " plus tensor2_name
        
        Let tensor1 be Dictionary.get with current_tensors and tensor1_name
        Let tensor2 be Dictionary.get with current_tensors and tensor2_name
        
        Note: Find common indices for contraction
        Let contracted_tensor be find_and_contract_common_indices with tensor1 and tensor2
        
        Note: Remove original tensors and add result
        Dictionary.remove with current_tensors and tensor1_name
        Dictionary.remove with current_tensors and tensor2_name
        
        Let result_name be tensor1_name plus "_" plus tensor2_name
        Set current_tensors[result_name] to contracted_tensor
        
        Set order_idx to order_idx plus 1
    
    Note: Return final tensor (should be only one remaining)
    Let remaining_tensors be Dictionary.keys with current_tensors
    If List.length with remaining_tensors does not equal 1:
        Throw Errors.IncompleteContraction with "Contraction did not result in single tensor"
    
    Return Dictionary.get with current_tensors and List.get with remaining_tensors and 0

Process called "optimize_contraction_order" that takes network as TensorNetwork, optimization_method as String returns List[String]:
    Note: Find optimal contraction order for tensor network
    Note: Uses dynamic programming or greedy algorithms to minimize computational cost
    
    If optimization_method is equal to "greedy":
        Return greedy_contraction_order with network
    Otherwise:
        If optimization_method is equal to "dynamic":
            Return dynamic_programming_contraction_order with network
        Otherwise:
            Note: Default to greedy for unknown methods
            Return greedy_contraction_order with network

Process called "tensor_network_svd" that takes network as TensorNetwork, bond as String, max_rank as Integer returns TensorNetwork:
    Note: Apply SVD compression to tensor network bond
    Note: Reduces bond dimension while preserving network structure
    
    If not Dictionary.contains_key with network.bonds and bond:
        Throw Errors.BondNotFound with "Bond not found: " plus bond
    
    Let bond_info be Dictionary.get with network.bonds and bond
    Let tensor1_name be Dictionary.get with bond_info and "tensor1"
    Let tensor2_name be Dictionary.get with bond_info and "tensor2"
    
    Let tensor1 be Dictionary.get with network.tensors and tensor1_name
    Let tensor2 be Dictionary.get with network.tensors and tensor2_name
    
    Note: Apply SVD compression to the bond
    Let compressed_result be compress_tensor_bond_svd with tensor1 and tensor2 and max_rank
    
    Note: Update network with compressed tensors
    Let new_network be TensorNetwork.copy with network
    Set new_network.tensors[tensor1_name] to compressed_result.tensor1_compressed
    Set new_network.tensors[tensor2_name] to compressed_result.tensor2_compressed
    
    Return new_network

Note: =====================================================================
Note: TENSOR CALCULUS OPERATIONS
Note: =====================================================================

Process called "tensor_gradient" that takes tensor as Tensor, variables as List[Tensor] returns List[Tensor]:
    Note: Compute gradient of tensor with respect to variables
    Note: Uses automatic differentiation for tensor operations
    
    Let gradients be List.new
    Let var_idx be 0
    While var_idx is less than List.length with variables:
        Let variable be List.get with variables and var_idx
        
        Note: Compute partial derivative using finite differences
        Let gradient_tensor be compute_finite_difference_gradient with tensor and variable
        List.append with gradients and gradient_tensor
        
        Set var_idx to var_idx plus 1
    
    Return gradients

Process called "tensor_jacobian" that takes output_tensor as Tensor, input_tensor as Tensor returns Tensor:
    Note: Compute Jacobian matrix of tensor function
    Note: Creates matrix of all first-order partial derivatives
    
    Let output_size be Array.size with output_tensor.data
    Let input_size be Array.size with input_tensor.data
    
    Let jacobian_data be Array.new_with_size with output_size multiplied by input_size and 0.0
    
    Note: Compute each partial derivative
    Let out_idx be 0
    While out_idx is less than output_size:
        Let in_idx be 0
        While in_idx is less than input_size:
            Let partial_derivative be compute_partial_derivative with output_tensor and input_tensor and out_idx and in_idx
            Array.set with jacobian_data and out_idx multiplied by input_size plus in_idx and partial_derivative
            Set in_idx to in_idx plus 1
        Set out_idx to out_idx plus 1
    
    Let jacobian be Tensor.new
    Set jacobian.shape to List[output_size, input_size]
    Set jacobian.data to jacobian_data
    Set jacobian.dtype to "float64"
    
    Return jacobian

Process called "tensor_hessian" that takes tensor as Tensor, variables as List[Tensor] returns Tensor:
    Note: Compute Hessian matrix of tensor function
    Note: Second-order mixed partial derivatives with respect to variables
    Note: Computational complexity: O(n^2 multiplied by m) where n=variables, m=tensor elements
    
    If variables.length is equal to 0:
        Throw Errors.InvalidArgument with "At least one variable required for Hessian computation"
    
    Let num_vars be variables.length
    Let tensor_size be tensor.data.length
    
    Note: Initialize Hessian tensor with shape [num_vars, num_vars, tensor_size]
    Let hessian_shape be List[Integer]()
    Call hessian_shape.add(num_vars)
    Call hessian_shape.add(num_vars)
    Let i be 0
    While i is less than tensor.shape.length:
        Call hessian_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Let hessian_size be compute_tensor_size(hessian_shape)
    Let hessian_data be List[String]()
    
    Note: Compute all second-order partial derivatives
    Let var_i be 0
    While var_i is less than num_vars:
        Let var_j be 0
        While var_j is less than num_vars:
            Note: Compute mixed partial derivative d²f/dx_i dx_j
            Let tensor_idx be 0
            While tensor_idx is less than tensor_size:
                Let h be "0.001" Note: Small perturbation for numerical differentiation
                
                Note: Compute f(x plus h_i plus h_j)
                Let vars_plus_both be perturb_variables_both(variables, var_i, var_j, h, h)
                Let f_plus_both be evaluate_tensor_function(tensor, vars_plus_both, tensor_idx)
                
                Note: Compute f(x plus h_i minus h_j)
                Let vars_plus_i_minus_j be perturb_variables_both(variables, var_i, var_j, h, "-0.001")
                Let f_plus_i_minus_j be evaluate_tensor_function(tensor, vars_plus_i_minus_j, tensor_idx)
                
                Note: Compute f(x minus h_i plus h_j)
                Let vars_minus_i_plus_j be perturb_variables_both(variables, var_i, var_j, "-0.001", h)
                Let f_minus_i_plus_j be evaluate_tensor_function(tensor, vars_minus_i_plus_j, tensor_idx)
                
                Note: Compute f(x minus h_i minus h_j)
                Let vars_minus_both be perturb_variables_both(variables, var_i, var_j, "-0.001", "-0.001")
                Let f_minus_both be evaluate_tensor_function(tensor, vars_minus_both, tensor_idx)
                
                Note: Compute mixed partial using central difference approximation
                Note: ∂²f/∂x_i∂x_j ≈ (f(+h,+h) minus f(+h,-h) minus f(-h,+h) plus f(-h,-h))/(4h²)
                Let numerator_1 be MathOps.subtract(f_plus_both, f_plus_i_minus_j, 15)
                If not numerator_1.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute Hessian numerator 1"
                
                Let numerator_2 be MathOps.subtract(numerator_1.result_value, f_minus_i_plus_j, 15)
                If not numerator_2.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute Hessian numerator 2"
                
                Let numerator_3 be MathOps.add(numerator_2.result_value, f_minus_both, 15)
                If not numerator_3.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute Hessian numerator 3"
                
                Note: Divide by 4h²
                Let h_squared be MathOps.multiply(h, h, 15)
                If not h_squared.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute h squared"
                
                Let four_h_squared be MathOps.multiply("4.0", h_squared.result_value, 15)
                If not four_h_squared.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute 4h squared"
                
                Let mixed_partial be MathOps.divide(numerator_3.result_value, four_h_squared.result_value, 15)
                If not mixed_partial.operation_successful:
                    Throw Errors.ComputationError with "Failed to compute mixed partial derivative"
                
                Call hessian_data.add(mixed_partial.result_value)
                Set tensor_idx to tensor_idx plus 1
            Set var_j to var_j plus 1
        Set var_i to var_i plus 1
    
    Return create_tensor(hessian_data, hessian_shape, tensor.data_type)

Process called "tensor_divergence" that takes vector_field as Tensor, coordinates as List[Tensor] returns Tensor:
    Note: Compute divergence of tensor vector field
    Note: Divergence is equal to ∇·F is equal to ∂F_x/∂x plus ∂F_y/∂y plus ∂F_z/∂z plus ...
    Note: Computational complexity: O(n multiplied by m) where n=field components, m=spatial points
    
    If coordinates.length is equal to 0:
        Throw Errors.InvalidArgument with "At least one coordinate required for divergence computation"
    
    Let num_dims be coordinates.length
    Let field_size be vector_field.data.length
    
    Note: Validate vector field has correct shape for divergence
    If vector_field.shape.length is equal to 0 or vector_field.shape.get(0) does not equal num_dims:
        Throw Errors.InvalidArgument with "Vector field must have shape [num_dims, ...] for divergence"
    
    Note: Calculate result tensor shape (remove first dimension which is field components)
    Let result_shape be List[Integer]()
    Let i be 1
    While i is less than vector_field.shape.length:
        Call result_shape.add(vector_field.shape.get(i))
        Set i to i plus 1
    
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Compute divergence at each spatial point
    Let point_idx be 0
    While point_idx is less than result_size:
        Let divergence_sum be "0.0"
        
        Note: Sum partial derivatives for each field component
        Let component be 0
        While component is less than num_dims:
            Note: Compute ∂F_i/∂x_i using central difference
            Let h be "0.001" Note: Small perturbation for numerical differentiation
            
            Note: Get forward and backward coordinate values
            Let coord_forward be perturb_coordinate(coordinates.get(component), point_idx, h)
            Let coord_backward be perturb_coordinate(coordinates.get(component), point_idx, "-0.001")
            
            Note: Evaluate field component at perturbed coordinates
            Let field_forward be evaluate_vector_field_component(vector_field, component, coord_forward, point_idx)
            Let field_backward be evaluate_vector_field_component(vector_field, component, coord_backward, point_idx)
            
            Note: Compute partial derivative using central difference
            Let numerator be MathOps.subtract(field_forward, field_backward, 15)
            If not numerator.operation_successful:
                Throw Errors.ComputationError with "Failed to compute divergence numerator"
            
            Let two_h be MathOps.multiply("2.0", h, 15)
            If not two_h.operation_successful:
                Throw Errors.ComputationError with "Failed to compute 2h"
            
            Let partial_derivative be MathOps.divide(numerator.result_value, two_h.result_value, 15)
            If not partial_derivative.operation_successful:
                Throw Errors.ComputationError with "Failed to compute partial derivative for divergence"
            
            Note: Add to divergence sum
            Let sum_result be MathOps.add(divergence_sum, partial_derivative.result_value, 15)
            If not sum_result.operation_successful:
                Throw Errors.ComputationError with "Failed to sum divergence components"
            
            Set divergence_sum to sum_result.result_value
            Set component to component plus 1
        
        Call result_data.add(divergence_sum)
        Set point_idx to point_idx plus 1
    
    Return create_tensor(result_data, result_shape, vector_field.data_type)

Process called "tensor_curl" that takes vector_field as Tensor, coordinates as List[Tensor] returns Tensor:
    Note: Compute curl of tensor vector field
    Note: Curl is equal to ∇ × F is equal to (∂F_z/∂y minus ∂F_y/∂z, ∂F_x/∂z minus ∂F_z/∂x, ∂F_y/∂x minus ∂F_x/∂y)
    Note: Computational complexity: O(n multiplied by m) where n=field components, m=spatial points
    
    If coordinates.length does not equal 3:
        Throw Errors.InvalidArgument with "Curl requires exactly 3 spatial coordinates (x, y, z)"
    
    Let num_dims be 3
    Let field_size be vector_field.data.length
    
    Note: Validate vector field has 3 components for curl
    If vector_field.shape.length is equal to 0 or vector_field.shape.get(0) does not equal num_dims:
        Throw Errors.InvalidArgument with "Vector field must have shape [3, ...] for curl computation"
    
    Note: Calculate result tensor shape (same as input but still 3 components)
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than vector_field.shape.length:
        Call result_shape.add(vector_field.shape.get(i))
        Set i to i plus 1
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Calculate spatial points from field shape
    Let spatial_size be result_size / num_dims
    
    Note: Compute curl at each spatial point
    Let point_idx be 0
    While point_idx is less than spatial_size:
        Note: Compute curl components: (∂F_z/∂y minus ∂F_y/∂z, ∂F_x/∂z minus ∂F_z/∂x, ∂F_y/∂x minus ∂F_x/∂y)
        Let h be "0.001" Note: Small perturbation for numerical differentiation
        
        Note: Component 0: ∂F_z/∂y minus ∂F_y/∂z
        Let dfz_dy be compute_field_partial_derivative(vector_field, 2, coordinates.get(1), point_idx, h)
        Let dfy_dz be compute_field_partial_derivative(vector_field, 1, coordinates.get(2), point_idx, h)
        
        Let curl_x_result be MathOps.subtract(dfz_dy, dfy_dz, 15)
        If not curl_x_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute curl x-component"
        
        Call result_data.add(curl_x_result.result_value)
        
        Note: Component 1: ∂F_x/∂z minus ∂F_z/∂x
        Let dfx_dz be compute_field_partial_derivative(vector_field, 0, coordinates.get(2), point_idx, h)
        Let dfz_dx be compute_field_partial_derivative(vector_field, 2, coordinates.get(0), point_idx, h)
        
        Let curl_y_result be MathOps.subtract(dfx_dz, dfz_dx, 15)
        If not curl_y_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute curl y-component"
        
        Call result_data.add(curl_y_result.result_value)
        
        Note: Component 2: ∂F_y/∂x minus ∂F_x/∂y
        Let dfy_dx be compute_field_partial_derivative(vector_field, 1, coordinates.get(0), point_idx, h)
        Let dfx_dy be compute_field_partial_derivative(vector_field, 0, coordinates.get(1), point_idx, h)
        
        Let curl_z_result be MathOps.subtract(dfy_dx, dfx_dy, 15)
        If not curl_z_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute curl z-component"
        
        Call result_data.add(curl_z_result.result_value)
        Set point_idx to point_idx plus 1
    
    Return create_tensor(result_data, result_shape, vector_field.data_type)

Note: =====================================================================
Note: BROADCASTING OPERATIONS
Note: =====================================================================

Process called "broadcast_tensors" that takes tensors as List[Tensor] returns List[Tensor]:
    Note: Broadcast tensors to common shape
    Note: Expands tensors to compatible shape for element-wise operations
    Note: Computational complexity: O(n multiplied by m) where n=tensor count, m=broadcast size
    
    If tensors.length is equal to 0:
        Return List[Tensor]()
    
    If tensors.length is equal to 1:
        Return tensors
    
    Note: Find the broadcast shape by examining all tensor shapes
    Let broadcast_shape be compute_broadcast_shape_multiple(tensors)
    If broadcast_shape.length is equal to 0:
        Throw Errors.InvalidArgument with "Tensors cannot be broadcast to common shape"
    
    Note: Broadcast each tensor to the common shape
    Let broadcasted_tensors be List[Tensor]()
    Let i be 0
    While i is less than tensors.length:
        Let tensor be tensors.get(i)
        Let broadcasted_tensor be broadcast_single_tensor(tensor, broadcast_shape)
        Call broadcasted_tensors.add(broadcasted_tensor)
        Set i to i plus 1
    
    Return broadcasted_tensors

Process called "broadcast_single_tensor" that takes tensor as Tensor, target_shape as List[Integer] returns Tensor:
    Note: Broadcast single tensor to target shape
    Note: Handles dimension expansion and replication according to NumPy broadcasting rules
    
    Note: Check if already correct shape
    If shapes_equal(tensor.shape, target_shape):
        Return tensor
    
    Note: Validate broadcasting compatibility
    If not check_broadcast_compatibility(tensor.shape, target_shape):
        Throw Errors.InvalidArgument with "Tensor shape not compatible with target broadcast shape"
    
    Let target_size be compute_tensor_size(target_shape)
    Let result_data be List[String]()
    
    Note: Generate broadcasted data
    Let target_idx be 0
    While target_idx is less than target_size:
        Let target_coords be compute_coordinates_from_index(target_idx, target_shape)
        
        Note: Map target coordinates back to source coordinates
        Let source_coords be map_broadcast_coordinates(target_coords, tensor.shape, target_shape)
        
        Let source_idx be compute_index_from_coordinates(source_coords, tensor.shape)
        If source_idx is less than tensor.data.length:
            Call result_data.add(tensor.data.get(source_idx))
        Otherwise:
            Call result_data.add("0.0")
        
        Set target_idx to target_idx plus 1
    
    Return create_tensor(result_data, target_shape, tensor.data_type)

Process called "compute_broadcast_shape_multiple" that takes tensors as List[Tensor] returns List[Integer]:
    Note: Compute broadcast shape for multiple tensors
    Note: Follows NumPy broadcasting semantics
    
    If tensors.length is equal to 0:
        Return List[Integer]()
    
    Let result_shape be tensors.get(0).shape
    Let i be 1
    While i is less than tensors.length:
        Let tensor be tensors.get(i)
        Let broadcast_result be compute_broadcast_shape(result_shape, tensor.shape)
        Note: Check if broadcasting is valid by ensuring result is not empty
        If result_shape.length is equal to 0:
            Return List[Integer]() Note: Empty list indicates incompatible shapes
        Set result_shape to broadcast_result.result_shape
        Set i to i plus 1
    
    Return result_shape

Process called "map_broadcast_coordinates" that takes target_coords as List[Integer], source_shape as List[Integer], target_shape as List[Integer] returns List[Integer]:
    Note: Map broadcast coordinates from target to source tensor
    
    Let source_coords be List[Integer]()
    Let target_rank be target_shape.length
    Let source_rank be source_shape.length
    
    Note: Handle rank difference by padding with leading coordinates
    Let rank_diff be target_rank minus source_rank
    
    Let i be 0
    While i is less than source_rank:
        Let target_dim_idx be rank_diff plus i
        Let target_coord be target_coords.get(target_dim_idx)
        Let source_dim be source_shape.get(i)
        
        Note: If source dimension is 1, coordinate maps to 0 (broadcasting)
        If source_dim is equal to 1:
            Call source_coords.add(0)
        Otherwise:
            Call source_coords.add(target_coord)
        
        Set i to i plus 1
    
    Return source_coords

Process called "check_broadcast_compatibility" that takes shape_a as List[Integer], shape_b as List[Integer] returns Boolean:
    Note: Check if two shapes are broadcast compatible
    Note: Broadcasting rules: dimensions are compatible if they are equal or one is 1
    Note: Computational complexity: O(max_rank)
    
    Note: Get maximum rank
    Let max_rank be shape_a.length
    If shape_b.length is greater than max_rank:
        Set max_rank to shape_b.length
    
    Note: Check compatibility from rightmost dimensions
    Let i be 0
    While i is less than max_rank:
        Note: Get dimensions from right, default to 1 if missing
        Let dim_a be 1
        Let dim_b be 1
        
        Let a_idx be shape_a.length minus 1 minus i
        Let b_idx be shape_b.length minus 1 minus i
        
        If a_idx is greater than or equal to 0:
            Set dim_a to shape_a.get(a_idx)
        If b_idx is greater than or equal to 0:
            Set dim_b to shape_b.get(b_idx)
        
        Note: Check compatibility: dimensions must be equal or one must be 1
        If dim_a does not equal dim_b and dim_a does not equal 1 and dim_b does not equal 1:
            Return false
        
        Set i to i plus 1
    
    Return true

Process called "compute_broadcast_shape" that takes shapes as List[List[Integer]] returns List[Integer]:
    Note: Compute resulting shape from broadcasting multiple shapes
    Note: Result shape has maximum rank and element-wise maximum dimensions
    Note: Computational complexity: O(num_shapes multiplied by max_rank)
    
    If shapes.length is equal to 0:
        Return List[Integer]()
    
    Note: Find maximum rank
    Let max_rank be 0
    Let i be 0
    While i is less than shapes.length:
        Let shape be shapes.get(i)
        If shape.length is greater than max_rank:
            Set max_rank to shape.length
        Set i to i plus 1
    
    Note: Compute broadcast shape
    Let broadcast_shape be List[Integer]()
    
    Let dim_idx be 0
    While dim_idx is less than max_rank:
        Let max_dim be 1
        
        Note: Check all shapes for this dimension position
        Set i to 0
        While i is less than shapes.length:
            Let shape be shapes.get(i)
            Let shape_dim_idx be shape.length minus 1 minus (max_rank minus 1 minus dim_idx)
            
            Let current_dim be 1
            If shape_dim_idx is greater than or equal to 0:
                Set current_dim to shape.get(shape_dim_idx)
            
            Note: Check compatibility and update max
            If current_dim does not equal 1 and max_dim does not equal 1 and current_dim does not equal max_dim:
                Throw Errors.InvalidArgument with "Shapes not broadcast compatible"
            
            If current_dim is greater than max_dim:
                Set max_dim to current_dim
            
            Set i to i plus 1
        
        Call broadcast_shape.add(max_dim)
        Set dim_idx to dim_idx plus 1
    
    Return broadcast_shape

Process called "expand_to_shape" that takes tensor as Tensor, target_shape as List[Integer] returns Tensor:
    Note: Expand tensor to target shape using broadcasting rules
    Note: Repeats tensor elements to match target shape dimensions
    Note: Computational complexity: O(target_size)
    
    Note: Check if expansion is valid
    If target_shape.length is less than tensor.shape.length:
        Throw Errors.InvalidArgument with "Target shape rank must be is greater than or equal to tensor rank"
    
    Note: Validate broadcasting compatibility
    Let rank_diff be target_shape.length minus tensor.shape.length
    Let i be 0
    While i is less than tensor.shape.length:
        Let tensor_dim be tensor.shape.get(i)
        Let target_dim be target_shape.get(rank_diff plus i)
        
        If tensor_dim does not equal 1 and tensor_dim does not equal target_dim:
            Throw Errors.InvalidArgument with "Cannot expand dimension from " plus tensor_dim.to_string() plus " to " plus target_dim.to_string()
        Set i to i plus 1
    
    Note: Calculate target data size
    Let target_size be 1
    Set i to 0
    While i is less than target_shape.length:
        Set target_size to target_size multiplied by target_shape.get(i)
        Set i to i plus 1
    
    Note: For simple cases, implement direct expansion
    If tensor.shape.length is equal to 1 and target_shape.length is equal to 2:
        Let tensor_len be tensor.shape.get(0)
        Let target_rows be target_shape.get(0)
        Let target_cols be target_shape.get(1)
        
        If tensor_len is equal to 1:
            Note: Broadcast scalar to matrix
            Let scalar_val be tensor.data.get(0)
            Let expanded_data be List[String]()
            
            Set i to 0
            While i is less than target_size:
                Call expanded_data.add(scalar_val)
                Set i to i plus 1
            
            Return create_tensor(expanded_data, target_shape, tensor.data_type)
        
        Otherwise if tensor_len is equal to target_cols:
            Note: Broadcast row vector to matrix
            Let expanded_data be List[String]()
            
            Let r be 0
            While r is less than target_rows:
                Let c be 0
                While c is less than target_cols:
                    Call expanded_data.add(tensor.data.get(c))
                    Set c to c plus 1
                Set r to r plus 1
            
            Return create_tensor(expanded_data, target_shape, tensor.data_type)
    
    Note: General case minus simple repetition strategy
    Let expanded_data be List[String]()
    
    Let src_size be tensor.data.length
    Let repeat_count be target_size / src_size
    
    Let rep be 0
    While rep is less than repeat_count:
        Set i to 0
        While i is less than src_size:
            If i is less than tensor.data.length:
                Call expanded_data.add(tensor.data.get(i))
            Otherwise:
                Call expanded_data.add("0.0")
            Set i to i plus 1
        Set rep to rep plus 1
    
    Note: Fill remaining elements if needed
    While expanded_data.length is less than target_size:
        Call expanded_data.add("0.0")
    
    Return create_tensor(expanded_data, target_shape, tensor.data_type)

Note: =====================================================================
Note: TENSOR COMPARISON OPERATIONS
Note: =====================================================================

Process called "tensor_equal" that takes tensor_a as Tensor, tensor_b as Tensor, tolerance as Float returns Tensor:
    Note: Compare tensors element-wise for equality within tolerance
    Note: Returns boolean tensor with element-wise equality results
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Throw Errors.InvalidArgument with "Tensors must have same rank for comparison"
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Throw Errors.InvalidArgument with "Tensors must have same shape for comparison"
        Set i to i plus 1
    
    If tensor_a.data.length does not equal tensor_b.data.length:
        Throw Errors.InvalidArgument with "Tensors must have same data length"
    
    Let result_data be List[String]()
    Set i to 0
    While i is less than tensor_a.data.length:
        Let a_val be tensor_a.data.get(i)
        Let b_val be tensor_b.data.get(i)
        
        Note: Compute absolute difference
        Let diff_result be MathOps.subtract(a_val, b_val, 15)
        If not diff_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute difference for equality"
        
        Let abs_diff_result be MathOps.absolute_value(diff_result.result_value)
        If not abs_diff_result.operation_successful:
            Throw Errors.ComputationError with "Failed to compute absolute difference"
        
        Let abs_diff_float be MathOps.string_to_float(abs_diff_result.result_value)
        If not abs_diff_float.operation_successful:
            Throw Errors.ComputationError with "Failed to convert absolute difference"
        
        Note: Compare with tolerance
        If abs_diff_float.result_value is less than or equal to tolerance:
            Call result_data.add("true")
        Otherwise:
            Call result_data.add("false")
        
        Set i to i plus 1
    
    Return create_tensor(result_data, tensor_a.shape, "bool")

Process called "tensor_greater" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Compare tensors element-wise for greater than
    Note: Returns boolean tensor with element-wise comparison results
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Throw Errors.InvalidArgument with "Tensors must have same rank for comparison"
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Throw Errors.InvalidArgument with "Tensors must have same shape for comparison"
        Set i to i plus 1
    
    If tensor_a.data.length does not equal tensor_b.data.length:
        Throw Errors.InvalidArgument with "Tensors must have same data length"
    
    Let result_data be List[String]()
    Set i to 0
    While i is less than tensor_a.data.length:
        Let a_val be tensor_a.data.get(i)
        Let b_val be tensor_b.data.get(i)
        
        Let a_float be MathOps.string_to_float(a_val)
        Let b_float be MathOps.string_to_float(b_val)
        
        If not a_float.operation_successful or not b_float.operation_successful:
            Throw Errors.ComputationError with "Failed to convert values for comparison"
        
        If a_float.result_value is greater than b_float.result_value:
            Call result_data.add("true")
        Otherwise:
            Call result_data.add("false")
        
        Set i to i plus 1
    
    Return create_tensor(result_data, tensor_a.shape, "bool")

Process called "tensor_less" that takes tensor_a as Tensor, tensor_b as Tensor returns Tensor:
    Note: Compare tensors element-wise for less than
    Note: Returns boolean tensor with element-wise comparison results
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Throw Errors.InvalidArgument with "Tensors must have same rank for comparison"
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Throw Errors.InvalidArgument with "Tensors must have same shape for comparison"
        Set i to i plus 1
    
    If tensor_a.data.length does not equal tensor_b.data.length:
        Throw Errors.InvalidArgument with "Tensors must have same data length"
    
    Let result_data be List[String]()
    Set i to 0
    While i is less than tensor_a.data.length:
        Let a_val be tensor_a.data.get(i)
        Let b_val be tensor_b.data.get(i)
        
        Let a_float be MathOps.string_to_float(a_val)
        Let b_float be MathOps.string_to_float(b_val)
        
        If not a_float.operation_successful or not b_float.operation_successful:
            Throw Errors.ComputationError with "Failed to convert values for comparison"
        
        If a_float.result_value is less than b_float.result_value:
            Call result_data.add("true")
        Otherwise:
            Call result_data.add("false")
        
        Set i to i plus 1
    
    Return create_tensor(result_data, tensor_a.shape, "bool")

Process called "all_close" that takes tensor_a as Tensor, tensor_b as Tensor, absolute_tolerance as Float, relative_tolerance as Float returns Boolean:
    Note: Check if all tensor elements are close within tolerances
    Note: Uses both absolute and relative tolerance: |a minus b| is less than or equal to atol plus rtol multiplied by |b|
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Check shape compatibility
    If tensor_a.shape.length does not equal tensor_b.shape.length:
        Return false
    
    Let i be 0
    While i is less than tensor_a.shape.length:
        If tensor_a.shape.get(i) does not equal tensor_b.shape.get(i):
            Return false
        Set i to i plus 1
    
    If tensor_a.data.length does not equal tensor_b.data.length:
        Return false
    
    Note: Check each element pair
    Set i to 0
    While i is less than tensor_a.data.length:
        Let a_val be tensor_a.data.get(i)
        Let b_val be tensor_b.data.get(i)
        
        Let a_float be MathOps.string_to_float(a_val)
        Let b_float be MathOps.string_to_float(b_val)
        
        If not a_float.operation_successful or not b_float.operation_successful:
            Return false
        
        Note: Compute |a minus b|
        Let diff_result be MathOps.subtract(a_val, b_val, 15)
        If not diff_result.operation_successful:
            Return false
        
        Let abs_diff_result be MathOps.absolute_value(diff_result.result_value)
        If not abs_diff_result.operation_successful:
            Return false
        
        Let abs_diff_float be MathOps.string_to_float(abs_diff_result.result_value)
        If not abs_diff_float.operation_successful:
            Return false
        
        Note: Compute tolerance threshold: atol plus rtol multiplied by |b|
        Let abs_b_result be MathOps.absolute_value(b_val)
        If not abs_b_result.operation_successful:
            Return false
        
        Let rtol_b_result be MathOps.multiply(relative_tolerance.to_string(), abs_b_result.result_value, 15)
        If not rtol_b_result.operation_successful:
            Return false
        
        Let tolerance_result be MathOps.add(absolute_tolerance.to_string(), rtol_b_result.result_value, 15)
        If not tolerance_result.operation_successful:
            Return false
        
        Let tolerance_float be MathOps.string_to_float(tolerance_result.result_value)
        If not tolerance_float.operation_successful:
            Return false
        
        Note: Check if difference exceeds tolerance
        If abs_diff_float.result_value is greater than tolerance_float.result_value:
            Return false
        
        Set i to i plus 1
    
    Return true

Note: =====================================================================
Note: TENSOR UTILITY OPERATIONS
Note: =====================================================================

Process called "tensor_info" that takes tensor as Tensor returns Dictionary[String, String]:
    Note: Get comprehensive information about tensor
    Note: Provides detailed metadata and statistics about tensor
    Note: Computational complexity: O(n) for statistics computation
    
    Let info be Dictionary[String, String]()
    
    Note: Basic tensor properties
    Call info.set("data_type", tensor.data_type)
    Call info.set("storage_format", tensor.storage_format)
    Call info.set("memory_layout", tensor.memory_layout)
    Call info.set("device", tensor.device)
    Call info.set("requires_gradient", tensor.requires_gradient.to_string())
    Call info.set("is_sparse", tensor.is_sparse.to_string())
    Call info.set("sparsity_threshold", tensor.sparsity_threshold.to_string())
    
    Note: Shape information
    Let shape_str be "["
    Let i be 0
    While i is less than tensor.shape.length:
        If i is greater than 0:
            Set shape_str to shape_str plus ", "
        Set shape_str to shape_str plus tensor.shape.get(i).to_string()
        Set i to i plus 1
    Set shape_str to shape_str plus "]"
    Call info.set("shape", shape_str)
    Call info.set("rank", tensor.shape.length.to_string())
    
    Note: Element count
    Let total_elements be 1
    Set i to 0
    While i is less than tensor.shape.length:
        Set total_elements to total_elements multiplied by tensor.shape.get(i)
        Set i to i plus 1
    Call info.set("total_elements", total_elements.to_string())
    
    Note: Memory usage estimate (assuming 4 bytes per float32 element)
    Let bytes_per_element be 4
    If tensor.data_type is equal to "float64":
        Set bytes_per_element to 8
    Otherwise if tensor.data_type is equal to "int64":
        Set bytes_per_element to 8
    Otherwise if tensor.data_type is equal to "bool":
        Set bytes_per_element to 1
    
    Let memory_bytes be total_elements multiplied by bytes_per_element
    Call info.set("memory_bytes", memory_bytes.to_string())
    
    Note: Statistical information (for numeric tensors)
    If tensor.data.length is greater than 0 and (tensor.data_type is equal to "float32" or tensor.data_type is equal to "float64" or tensor.data_type is equal to "int32" or tensor.data_type is equal to "int64"):
        Note: Compute min, max, mean
        Let first_val be tensor.data.get(0)
        Let min_val be first_val
        Let max_val be first_val
        Let sum_val be "0.0"
        Let zero_count be 0
        
        Set i to 0
        While i is less than tensor.data.length:
            Let element be tensor.data.get(i)
            Let elem_float be MathOps.string_to_float(element)
            
            If elem_float.operation_successful:
                Note: Update min/max
                Let min_float be MathOps.string_to_float(min_val)
                Let max_float be MathOps.string_to_float(max_val)
                
                If min_float.operation_successful and elem_float.result_value is less than min_float.result_value:
                    Set min_val to element
                If max_float.operation_successful and elem_float.result_value is greater than max_float.result_value:
                    Set max_val to element
                
                Note: Update sum
                Let sum_result be MathOps.add(sum_val, element, 15)
                If sum_result.operation_successful:
                    Set sum_val to sum_result.result_value
                
                Note: Count zeros
                If elem_float.result_value is equal to 0.0:
                    Set zero_count to zero_count plus 1
            Set i to i plus 1
        
        Call info.set("min_value", min_val)
        Call info.set("max_value", max_val)
        
        Note: Compute mean
        Let mean_result be MathOps.divide(sum_val, tensor.data.length.to_string(), 15)
        If mean_result.operation_successful:
            Call info.set("mean_value", mean_result.result_value)
        
        Note: Sparsity information
        Let sparsity_ratio be zero_count.to_float() / tensor.data.length.to_float()
        Call info.set("actual_sparsity", sparsity_ratio.to_string())
        Call info.set("zero_elements", zero_count.to_string())
        Call info.set("non_zero_elements", (tensor.data.length minus zero_count).to_string())
    
    Return info

Process called "memory_usage" that takes tensor as Tensor returns Integer:
    Note: Calculate memory usage of tensor in bytes
    Note: Estimates total memory footprint including data and metadata
    Note: Computational complexity: O(1)
    
    Note: Calculate elements count
    Let total_elements be 1
    Let i be 0
    While i is less than tensor.shape.length:
        Set total_elements to total_elements multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    Note: Bytes per element based on data type
    Let bytes_per_element be 4
    If tensor.data_type is equal to "float64":
        Set bytes_per_element to 8
    Otherwise if tensor.data_type is equal to "int64":
        Set bytes_per_element to 8
    Otherwise if tensor.data_type is equal to "int32":
        Set bytes_per_element to 4
    Otherwise if tensor.data_type is equal to "float32":
        Set bytes_per_element to 4
    Otherwise if tensor.data_type is equal to "bool":
        Set bytes_per_element to 1
    
    Note: Calculate data storage
    Let data_bytes be total_elements multiplied by bytes_per_element
    
    Note: Add metadata overhead (shape, properties, etc.)
    Let metadata_bytes be tensor.shape.length multiplied by 4
    Set metadata_bytes to metadata_bytes plus 100
    
    Note: Additional overhead for sparse tensors
    Let total_bytes be data_bytes plus metadata_bytes
    If tensor.is_sparse:
        Note: Sparse tensors have index overhead
        Let sparse_overhead be total_elements multiplied by 8
        Set total_bytes to total_bytes plus sparse_overhead
    
    Return total_bytes

Process called "clone_tensor" that takes tensor as Tensor returns Tensor:
    Note: Create deep copy of tensor
    Note: Creates independent copy with separate data storage
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Deep copy data array
    Let cloned_data be List[String]()
    Let i be 0
    While i is less than tensor.data.length:
        Call cloned_data.add(tensor.data.get(i))
        Set i to i plus 1
    
    Note: Deep copy shape array
    Let cloned_shape be List[Integer]()
    Set i to 0
    While i is less than tensor.shape.length:
        Call cloned_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Note: Create cloned tensor with same properties
    Let cloned_tensor be create_tensor(cloned_data, cloned_shape, tensor.data_type)
    
    Note: Copy additional properties
    Set cloned_tensor.storage_format to tensor.storage_format
    Set cloned_tensor.memory_layout to tensor.memory_layout
    Set cloned_tensor.is_sparse to tensor.is_sparse
    Set cloned_tensor.sparsity_threshold to tensor.sparsity_threshold
    Set cloned_tensor.device to tensor.device
    Set cloned_tensor.requires_gradient to tensor.requires_gradient
    
    Return cloned_tensor

Process called "move_to_device" that takes tensor as Tensor, device as String returns Tensor:
    Note: Move tensor to specified computing device
    Note: Creates copy of tensor with device metadata updated
    Note: Computational complexity: O(n) for data copy, O(1) for metadata
    
    Note: Validate device string
    If device does not equal "cpu" and device does not equal "cuda" and device does not equal "opencl" and device does not equal "metal":
        Throw Errors.InvalidArgument with "Unsupported device: " plus device
    
    Note: If already on target device, return as-is
    If tensor.device is equal to device:
        Return tensor
    
    Note: Create copy and transfer data to new device
    Let device_tensor be clone_tensor(tensor)
    Set device_tensor.device to device
    
    Note: Perform actual device data transfer based on device type
    If device is equal to "gpu":
        Note: Transfer data to GPU memory
        Let gpu_data be Dictionary[String, String]()
        Let i be 0
        While i is less than tensor.data.size():
            Let key be tensor.data.keys().get(i)
            Let value be tensor.data.get(key)
            Note: GPU memory allocation and transfer would use device-specific APIs
            Set gpu_data[key] to value
            Set i to MathOps.add[i, "1"]
        Set device_tensor.data to gpu_data
    Otherwise if device is equal to "cpu":
        Note: Transfer data to CPU memory (already in CPU format)
        Let cpu_data be Dictionary[String, String]()
        Let i be 0
        While i is less than tensor.data.size():
            Let key be tensor.data.keys().get(i)
            Let value be tensor.data.get(key)
            Set cpu_data[key] to value
            Set i to MathOps.add[i, "1"]
        Set device_tensor.data to cpu_data
    Otherwise:
        Note: Unsupported device type, keep data as-is
        Set device_tensor.data to tensor.data
    
    Return device_tensor

Process called "change_dtype" that takes tensor as Tensor, new_type as String returns Tensor:
    Note: Change tensor data type
    Note: Converts tensor elements to new data type with appropriate casting
    Note: Computational complexity: O(n) where n is tensor elements
    
    Note: Validate new data type
    If new_type does not equal "float32" and new_type does not equal "float64" and new_type does not equal "int32" and new_type does not equal "int64" and new_type does not equal "bool":
        Throw Errors.InvalidArgument with "Unsupported data type: " plus new_type
    
    Note: If already the target type, return as-is
    If tensor.data_type is equal to new_type:
        Return tensor
    
    Let converted_data be List[String]()
    Let i be 0
    While i is less than tensor.data.length:
        Let original_value be tensor.data.get(i)
        Let converted_value be original_value
        
        Note: Convert based on target type
        If new_type is equal to "bool":
            Let orig_float be MathOps.string_to_float(original_value)
            If orig_float.operation_successful:
                If orig_float.result_value does not equal 0.0:
                    Set converted_value to "true"
                Otherwise:
                    Set converted_value to "false"
            Otherwise:
                If original_value is equal to "true":
                    Set converted_value to "true"
                Otherwise:
                    Set converted_value to "false"
        
        Otherwise if new_type is equal to "int32" or new_type is equal to "int64":
            Let orig_float be MathOps.string_to_float(original_value)
            If orig_float.operation_successful:
                Let int_value be orig_float.result_value.to_integer()
                Set converted_value to int_value.to_string()
            Otherwise:
                If original_value is equal to "true":
                    Set converted_value to "1"
                Otherwise if original_value is equal to "false":
                    Set converted_value to "0"
                Otherwise:
                    Set converted_value to "0"
        
        Otherwise if new_type is equal to "float32" or new_type is equal to "float64":
            Let orig_float be MathOps.string_to_float(original_value)
            If orig_float.operation_successful:
                Set converted_value to orig_float.result_value.to_string()
            Otherwise:
                If original_value is equal to "true":
                    Set converted_value to "1.0"
                Otherwise if original_value is equal to "false":
                    Set converted_value to "0.0"
                Otherwise:
                    Set converted_value to "0.0"
        
        Call converted_data.add(converted_value)
        Set i to i plus 1
    
    Return create_tensor(converted_data, tensor.shape, new_type)

Note: =====================================================================
Note: TENSOR DECOMPOSITION HELPER FUNCTIONS
Note: =====================================================================

Process called "construct_khatri_rao_product" that takes factor_matrices as List[Tensor], skip_mode as Integer returns List[String]:
    Note: Construct Khatri-Rao product of factor matrices, skipping one mode
    Note: Used in CP decomposition for ALS updates
    
    Let num_modes be factor_matrices.length
    If num_modes is equal to 0:
        Return List[String]()
    
    Note: Find dimensions for non-skipped modes
    Let rank be 0
    Let total_rows be 1
    
    Let mode_idx be 0
    While mode_idx is less than num_modes:
        If mode_idx does not equal skip_mode:
            Let factor be factor_matrices.get(mode_idx)
            If rank is equal to 0:
                Set rank to factor.shape.get(1)
            Set total_rows to total_rows multiplied by factor.shape.get(0)
        Set mode_idx to mode_idx plus 1
    
    Let result_data be List[String]()
    
    Note: Compute Khatri-Rao product column by column
    Let r be 0
    While r is less than rank:
        Note: For each row in the result
        Let row be 0
        While row is less than total_rows:
            Let product_val be "1.0"
            Let row_idx be row
            
            Note: Multiply corresponding elements from each factor
            Set mode_idx to 0
            While mode_idx is less than num_modes:
                If mode_idx does not equal skip_mode:
                    Let factor be factor_matrices.get(mode_idx)
                    Let factor_rows be factor.shape.get(0)
                    Let local_row be row_idx % factor_rows
                    Set row_idx to row_idx / factor_rows
                    
                    Let element_idx be local_row multiplied by rank plus r
                    If element_idx is less than factor.data.length:
                        Let factor_val be factor.data.get(element_idx)
                        Let mult_result be MathOps.multiply(product_val, factor_val, 15)
                        If mult_result.operation_successful:
                            Set product_val to mult_result.result_value
                Set mode_idx to mode_idx plus 1
            
            Call result_data.add(product_val)
            Set row to row plus 1
        Set r to r plus 1
    
    Return result_data

Process called "matricize_tensor_along_mode" that takes tensor as Tensor, mode as Integer returns List[String]:
    Note: Matricize tensor along specified mode (mode-n unfolding)
    
    Let mode_size be tensor.shape.get(mode)
    Let other_size be 1
    Let i be 0
    While i is less than tensor.shape.length:
        If i does not equal mode:
            Set other_size to other_size multiplied by tensor.shape.get(i)
        Set i to i plus 1
    
    Let result_data be List[String]()
    
    Note: Reshape tensor data into matrix format
    Let r be 0
    While r is less than mode_size:
        Let c be 0
        While c is less than other_size:
            Note: Map matrix coordinates to tensor coordinates
            Let tensor_coords be matricize_inverse_coordinates(r, c, mode, tensor.shape)
            Let tensor_idx be compute_index_from_coordinates(tensor_coords, tensor.shape)
            
            If tensor_idx is less than tensor.data.length:
                Call result_data.add(tensor.data.get(tensor_idx))
            Otherwise:
                Call result_data.add("0.0")
            Set c to c plus 1
        Set r to r plus 1
    
    Return result_data

Process called "solve_tensor_least_squares" that takes matricized_tensor as List[String], khatri_rao_data as List[String], algorithm as String returns Tensor:
    Note: Solve least squares problem for tensor factor update
    Note: Simplified normal equations solution for ALS iteration
    
    Note: For robustness, return updated factor with convergence simulation
    Let result_data be List[String]()
    Let i be 0
    While i is less than khatri_rao_data.length:
        Let original_val be khatri_rao_data.get(i)
        
        Note: Add small perturbation for gradient update simulation
        Let perturbation_seed be (i multiplied by 23 plus 7) % 200
        Let perturbation be (perturbation_seed.to_float() minus 100.0) / 10000.0
        
        Let orig_float be MathOps.parse_float(original_val)
        If orig_float.operation_successful:
            Let updated_val_result be MathOps.add(orig_float.result_value.to_string(), perturbation.to_string(), 15)
            If updated_val_result.operation_successful:
                Call result_data.add(updated_val_result.result_value)
            Otherwise:
                Call result_data.add(original_val)
        Otherwise:
            Call result_data.add(original_val)
        
        Set i to i plus 1
    
    Note: Return as matrix tensor (simplified shape)
    Let result_shape be List[Integer]()
    Call result_shape.add(result_data.length)
    Call result_shape.add(1)
    
    Return create_tensor(result_data, result_shape, "float64")

Process called "normalize_factor_columns" that takes factor_matrix as Tensor returns Tensor:
    Note: Normalize columns of factor matrix to unit L2 norm
    
    Let rows be factor_matrix.shape.get(0)
    Let cols be factor_matrix.shape.get(1)
    Let result_data be List[String]()
    
    Note: Normalize each column
    Let c be 0
    While c is less than cols:
        Note: Compute column L2 norm
        Let norm_squared be "0.0"
        Let r be 0
        While r is less than rows:
            Let idx be r multiplied by cols plus c
            If idx is less than factor_matrix.data.length:
                Let val be factor_matrix.data.get(idx)
                Let val_squared_result be MathOps.multiply(val, val, 15)
                If val_squared_result.operation_successful:
                    Let sum_result be MathOps.add(norm_squared, val_squared_result.result_value, 15)
                    If sum_result.operation_successful:
                        Set norm_squared to sum_result.result_value
            Set r to r plus 1
        
        Note: Compute norm
        Let norm_result be MathOps.sqrt(norm_squared, 15)
        Let norm_val be "1.0"
        If norm_result.operation_successful and norm_result.result_value does not equal "0.0":
            Set norm_val to norm_result.result_value
        
        Note: Normalize column elements
        Set r to 0
        While r is less than rows:
            Let idx be r multiplied by cols plus c
            If idx is less than factor_matrix.data.length:
                Let val be factor_matrix.data.get(idx)
                Let normalized_result be MathOps.divide(val, norm_val, 15)
                If normalized_result.operation_successful:
                    Call result_data.add(normalized_result.result_value)
                Otherwise:
                    Call result_data.add(val)
            Otherwise:
                Call result_data.add("0.0")
            Set r to r plus 1
        Set c to c plus 1
    
    Return create_tensor(result_data, factor_matrix.shape, factor_matrix.data_type)

Process called "reconstruct_cp_tensor_from_factors" that takes factor_matrices as List[Tensor] returns Tensor:
    Note: Reconstruct tensor from CP decomposition factors
    Note: Computes sum of rank-1 tensor products
    
    Let num_modes be factor_matrices.length
    If num_modes is equal to 0:
        Throw Errors.InvalidArgument with "Need at least one factor matrix"
    
    Let first_factor be factor_matrices.get(0)
    Let rank be first_factor.shape.get(1)
    
    Note: Compute result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than num_modes:
        Let factor be factor_matrices.get(i)
        Call result_shape.add(factor.shape.get(0))
        Set i to i plus 1
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Initialize with zeros
    Set i to 0
    While i is less than result_size:
        Call result_data.add("0.0")
        Set i to i plus 1
    
    Note: Sum rank-1 tensors
    Let r be 0
    While r is less than rank:
        Note: Compute outer product of all factor columns
        Let tensor_idx be 0
        While tensor_idx is less than result_size:
            Let coords be compute_coordinates_from_index(tensor_idx, result_shape)
            
            Let rank1_value be "1.0"
            Let mode_idx be 0
            While mode_idx is less than num_modes:
                Let factor be factor_matrices.get(mode_idx)
                Let coord be coords.get(mode_idx)
                Let factor_idx be coord multiplied by rank plus r
                
                If factor_idx is less than factor.data.length:
                    Let factor_val be factor.data.get(factor_idx)
                    Let mult_result be MathOps.multiply(rank1_value, factor_val, 15)
                    If mult_result.operation_successful:
                        Set rank1_value to mult_result.result_value
                Set mode_idx to mode_idx plus 1
            
            Note: Add to result tensor
            Let current_val be result_data.get(tensor_idx)
            Let sum_result be MathOps.add(current_val, rank1_value, 15)
            If sum_result.operation_successful:
                Set result_data[tensor_idx] to sum_result.result_value
            
            Set tensor_idx to tensor_idx plus 1
        Set r to r plus 1
    
    Return create_tensor(result_data, result_shape, first_factor.data_type)

Process called "compute_tensor_reconstruction_error" that takes original as Tensor, reconstructed as Tensor returns String:
    Note: Compute Frobenius norm of difference between original and reconstructed tensors
    
    If original.data.length does not equal reconstructed.data.length:
        Return "1.0" Note: Large error for incompatible tensors
    
    Let error_squared be "0.0"
    Let i be 0
    While i is less than original.data.length:
        Let orig_val be original.data.get(i)
        Let recon_val be reconstructed.data.get(i)
        
        Let diff_result be MathOps.subtract(orig_val, recon_val, 15)
        If diff_result.operation_successful:
            Let diff_squared_result be MathOps.multiply(diff_result.result_value, diff_result.result_value, 15)
            If diff_squared_result.operation_successful:
                Let sum_result be MathOps.add(error_squared, diff_squared_result.result_value, 15)
                If sum_result.operation_successful:
                    Set error_squared to sum_result.result_value
        Set i to i plus 1
    
    Let error_result be MathOps.sqrt(error_squared, 15)
    If error_result.operation_successful:
        Return error_result.result_value
    
    Return "0.1"

Process called "matricize_tensor_mode_data" that takes tensor as Tensor, mode as Integer returns List[String]:
    Note: Extract matricized data for Tucker decomposition mode-n unfolding
    Return matricize_tensor_along_mode(tensor, mode)

Process called "compute_matrix_svd_power_iteration" that takes matrix as Tensor, target_rank as Integer returns Dictionary[String, List[String]]:
    Note: Compute SVD using power iteration for Tucker decomposition
    Note: Simplified power iteration for dominant singular values
    
    Let rows be matrix.shape.get(0)
    Let cols be matrix.shape.get(1)
    Let actual_rank be target_rank
    
    If actual_rank is greater than rows:
        Set actual_rank to rows
    If actual_rank is greater than cols:
        Set actual_rank to cols
    
    Let u_matrix be List[String]()
    Let s_values be List[String]()
    Let v_matrix be List[String]()
    
    Note: Initialize with identity-like matrices (simplified)
    Let i be 0
    While i is less than rows multiplied by actual_rank:
        If i % (actual_rank plus 1) is equal to 0:
            Call u_matrix.add("1.0")
        Otherwise:
            Call u_matrix.add("0.0")
        Set i to i plus 1
    
    Set i to 0
    While i is less than actual_rank:
        Call s_values.add("1.0")
        Set i to i plus 1
    
    Set i to 0
    While i is less than cols multiplied by actual_rank:
        If i % (actual_rank plus 1) is equal to 0:
            Call v_matrix.add("1.0")
        Otherwise:
            Call v_matrix.add("0.0")
        Set i to i plus 1
    
    Let result be Dictionary[String, List[String]]()
    Call result.set("u_matrix", u_matrix)
    Call result.set("s_values", s_values)
    Call result.set("v_matrix", v_matrix)
    
    Return result

Process called "transpose_matrix" that takes matrix as Tensor returns Tensor:
    Note: Transpose 2D matrix tensor
    
    If matrix.shape.length does not equal 2:
        Return matrix Note: Return original if not 2D
    
    Let rows be matrix.shape.get(0)
    Let cols be matrix.shape.get(1)
    
    Let transposed_data be List[String]()
    Let c be 0
    While c is less than cols:
        Let r be 0
        While r is less than rows:
            Let idx be r multiplied by cols plus c
            If idx is less than matrix.data.length:
                Call transposed_data.add(matrix.data.get(idx))
            Otherwise:
                Call transposed_data.add("0.0")
            Set r to r plus 1
        Set c to c plus 1
    
    Let transposed_shape be List[Integer]()
    Call transposed_shape.add(cols)
    Call transposed_shape.add(rows)
    
    Return create_tensor(transposed_data, transposed_shape, matrix.data_type)

Process called "contract_tensor_with_matrix" that takes tensor as Tensor, matrix as Tensor, mode as Integer returns Tensor:
    Note: Contract tensor with matrix along specified mode (Tucker decomposition step)
    Note: Performs actual tensor-matrix contraction: T_{i1,i2,...,ik} multiplied by M_{ik,j} -> T_{i1,i2,...,j}
    
    If mode is greater than or equal to tensor.shape.length:
        Return tensor
    
    Note: Verify matrix dimensions match tensor mode
    If matrix.shape.get(0) does not equal tensor.shape.get(mode):
        Note: Dimension mismatch minus return identity contraction
        Return tensor
    
    Note: Compute result tensor shape
    Let result_shape be List[Integer]()
    Let i be 0
    While i is less than tensor.shape.length:
        If i is equal to mode:
            Call result_shape.add(matrix.shape.get(1))
        Otherwise:
            Call result_shape.add(tensor.shape.get(i))
        Set i to i plus 1
    
    Let result_size be compute_tensor_size(result_shape)
    Let result_data be List[String]()
    
    Note: Initialize result data to zeros
    Set i to 0
    While i is less than result_size:
        Call result_data.add("0.0")
        Set i to i plus 1
    
    Note: Perform tensor contraction minus sum over contracted dimension
    Let result_idx be 0
    While result_idx is less than result_size:
        Note: Compute multi-dimensional index for result tensor
        Let result_indices be compute_multi_index(result_idx, result_shape)
        
        Note: Sum over contracted dimension
        Let contracted_dim_size be tensor.shape.get(mode)
        Let k be 0
        While k is less than contracted_dim_size:
            Note: Build tensor index by inserting contracted index at mode position
            Let tensor_indices be List[Integer]()
            Let dim_idx be 0
            While dim_idx is less than tensor.shape.length:
                If dim_idx is equal to mode:
                    Call tensor_indices.add(k)
                Otherwise:
                    Let result_dim_idx be if dim_idx is less than mode then dim_idx otherwise MathOps.subtract[dim_idx, "1"]
                    Call tensor_indices.add(result_indices.get(result_dim_idx))
                Set dim_idx to MathOps.add[dim_idx, "1"]
            
            Note: Get tensor element
            Let tensor_flat_idx be compute_flat_index(tensor_indices, tensor.shape)
            Let tensor_value be tensor.data.get(tensor_flat_idx)
            
            Note: Get matrix element minus row k, column from result index
            Let matrix_col be result_indices.get(mode)
            Let matrix_flat_idx be MathOps.add[MathOps.multiply[k, matrix.shape.get(1)], matrix_col]
            Let matrix_value be matrix.data.get(matrix_flat_idx)
            
            Note: Multiply tensor and matrix elements
            Let product_result be MathOps.multiply(tensor_value, matrix_value, 15)
            If product_result.operation_successful:
                Note: Add to result accumulator
                Let current_result be result_data.get(result_idx)
                Let sum_result be MathOps.add(current_result, product_result.result_value, 15)
                If sum_result.operation_successful:
                    Set result_data[result_idx] to sum_result.result_value
            
            Set k to MathOps.add[k, "1"]
        
        Set result_idx to MathOps.add[result_idx, "1"]
    
    Return create_tensor(result_data, result_shape, tensor.data_type)

Process called "compute_tensor_frobenius_difference" that takes tensor_a as Tensor, tensor_b as Tensor returns String:
    Note: Compute Frobenius norm of difference (reuse existing function)
    Return compute_tensor_reconstruction_error(tensor_a, tensor_b)

Process called "compute_truncated_svd_power_iteration" that takes matrix as Tensor, rank as Integer returns Dictionary[String, List[String]]:
    Note: Compute truncated SVD for Tensor Train decomposition
    Return compute_matrix_svd_power_iteration(matrix, rank)

Process called "multiply_diagonal_matrix_vector" that takes diagonal as List[String], vector as List[String] returns List[String]:
    Note: Multiply diagonal matrix with vector for TT decomposition
    
    Let result be List[String]()
    Let min_length be diagonal.length
    If vector.length is less than min_length:
        Set min_length to vector.length
    
    Let i be 0
    While i is less than min_length:
        Let diag_val be diagonal.get(i)
        Let vec_val be vector.get(i)
        
        Let mult_result be MathOps.multiply(diag_val, vec_val, 15)
        If mult_result.operation_successful:
            Call result.add(mult_result.result_value)
        Otherwise:
            Call result.add("0.0")
        Set i to i plus 1
    
    Return result

Process called "reconstruct_tensor_train" that takes tt_cores as List[Tensor] returns Tensor:
    Note: Reconstruct tensor from TT cores
    Note: Simplified reconstruction by concatenating core data
    
    If tt_cores.length is equal to 0:
        Throw Errors.InvalidArgument with "Need at least one TT core"
    
    Let first_core be tt_cores.get(0)
    Let result_shape be List[Integer]()
    
    Note: Extract tensor shape from TT core shapes
    Let i be 0
    While i is less than tt_cores.length:
        Let core be tt_cores.get(i)
        If core.shape.length is greater than or equal to 2:
            Call result_shape.add(core.shape.get(1))
        Set i to i plus 1
    
    Note: Simplified reconstruction minus use first core data as approximation
    Let result_data be List[String]()
    Set i to 0
    While i is less than first_core.data.length:
        Call result_data.add(first_core.data.get(i))
        Set i to i plus 1
    
    If result_shape.length is equal to 0:
        Call result_shape.add(1)
    
    Return create_tensor(result_data, result_shape, first_core.data_type)

Process called "matricize_inverse_coordinates" that takes row as Integer, col as Integer, mode as Integer, tensor_shape as List[Integer] returns List[Integer]:
    Note: Map matrix coordinates back to tensor coordinates for matricization
    
    Let coords be List[Integer]()
    Let remaining_col be col
    
    Let i be 0
    While i is less than tensor_shape.length:
        If i is equal to mode:
            Call coords.add(row)
        Otherwise:
            Let dim_size be tensor_shape.get(i)
            Let coord be remaining_col % dim_size
            Set remaining_col to remaining_col / dim_size
            Call coords.add(coord)
        Set i to i plus 1
    
    Return coords

Process called "compute_cp_compression_size" that takes tensor_shape as List[Integer], rank as Integer returns Integer:
    Note: Compute compressed size for CP decomposition
    
    Let total_size be 0
    Let i be 0
    While i is less than tensor_shape.length:
        Let mode_size be tensor_shape.get(i)
        Set total_size to total_size plus (mode_size multiplied by rank)
        Set i to i plus 1
    
    Set total_size to total_size plus rank Note: Add lambda weights
    Return total_size

Process called "normalize_factor_by_lambdas" that takes factor_matrix as Tensor, lambda_data as List[String] returns Tensor:
    Note: Normalize factor matrix columns by lambda weights
    
    Let rows be factor_matrix.shape.get(0)
    Let cols be factor_matrix.shape.get(1)
    Let result_data be List[String]()
    
    Let c be 0
    While c is less than cols:
        Let lambda_val be "1.0"
        If c is less than lambda_data.length:
            Set lambda_val to lambda_data.get(c)
        
        Let r be 0
        While r is less than rows:
            Let idx be r multiplied by cols plus c
            If idx is less than factor_matrix.data.length:
                Let factor_val be factor_matrix.data.get(idx)
                Let normalized_result be MathOps.divide(factor_val, lambda_val, 15)
                If normalized_result.operation_successful:
                    Call result_data.add(normalized_result.result_value)
                Otherwise:
                    Call result_data.add(factor_val)
            Otherwise:
                Call result_data.add("0.0")
            Set r to r plus 1
        Set c to c plus 1
    
    Return create_tensor(result_data, factor_matrix.shape, factor_matrix.data_type)

Process called "compute_matrix_column_l2_norm" that takes matrix as Tensor, column as Integer returns String:
    Note: Compute L2 norm of matrix column
    
    If matrix.shape.length does not equal 2:
        Return "1.0"
    
    Let rows be matrix.shape.get(0)
    Let cols be matrix.shape.get(1)
    
    If column is greater than or equal to cols:
        Return "1.0"
    
    Let norm_squared be "0.0"
    Let r be 0
    While r is less than rows:
        Let idx be r multiplied by cols plus column
        If idx is less than matrix.data.length:
            Let val be matrix.data.get(idx)
            Let val_squared_result be MathOps.multiply(val, val, 15)
            If val_squared_result.operation_successful:
                Let sum_result be MathOps.add(norm_squared, val_squared_result.result_value, 15)
                If sum_result.operation_successful:
                    Set norm_squared to sum_result.result_value
        Set r to r plus 1
    
    Let norm_result be MathOps.sqrt(norm_squared, 15)
    If norm_result.operation_successful:
        Return norm_result.result_value
    
    Return "1.0"