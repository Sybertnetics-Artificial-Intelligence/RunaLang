Note:
math/engine/autodiff/reverse.runa
Reverse-Mode Automatic Differentiation

Reverse-mode automatic differentiation implementation (backpropagation).
Provides efficient gradient computation for functions with many variables.

Key Features:
- Reverse accumulation through computation graphs
- Efficient gradient computation for scalar-valued functions
- Support for complex computational graphs
- Memory-efficient adjoint computation
- Integration with machine learning and optimization
- Checkpointing for memory management

Dependencies:
- Collections (List, Dictionary, Stack, Queue)
- Math.Core (basic arithmetic, mathematical functions)
- Math.Engine.AutoDiff.Graph (computation graph structures)
- Errors (exception handling)
:End Note

Import module "collections" as Collections
Import module "math.core" as MathCore
Import module "errors" as Errors

Note: Configurable numerical constants for stability and precision
Let NUMERICAL_EPSILON be 1e-8
Let GRADIENT_CLIPPING_THRESHOLD be 1e-6

Note: ========================================================================
Note: REVERSE MODE STRUCTURES AND TYPES
Note: ========================================================================

Type called "AdjointVariable":
    value as Float
    adjoint as Float
    grad_fn as String  Note: reference to gradient function
    children as List[String]  Note: child variable references
    requires_grad as Boolean

Type called "ComputationNode":
    operation as String
    inputs as List[String]
    output as String
    local_gradients as List[Float]
    adjoint_contribution as Float

Type called "BackwardPass":
    topology_order as List[String]
    gradient_functions as Dictionary[String, String]
    intermediate_values as Dictionary[String, Float]
    final_adjoints as Dictionary[String, Float]

Type called "CheckpointState":
    saved_values as Dictionary[String, Float]
    recomputation_graph as List[ComputationNode]
    memory_usage as Integer
    checkpoint_frequency as Integer

Note: ========================================================================
Note: BASIC REVERSE MODE OPERATIONS
Note: ========================================================================

Process called "create_adjoint_variable" that takes value as Float, requires_grad as Boolean returns AdjointVariable:
    Note: Create adjoint variable for reverse mode with input validation
    If value does not equal value:  Note: Check for NaN
        Throw Errors.InvalidArgument with "Cannot create adjoint variable with NaN value"
    If value is equal to Float.POSITIVE_INFINITY or value is equal to Float.NEGATIVE_INFINITY:
        Throw Errors.InvalidArgument with "Cannot create adjoint variable with infinite value"
    
    Let result be AdjointVariable with:
        value is equal to value
        adjoint is equal to 0.0
        grad_fn is equal to ""
        children is equal to List[String]
        requires_grad is equal to requires_grad
    Return result

Process called "add_backward" that takes output_adjoint as Float, input_adjoints as List[Float] returns List[Float]:
    Note: Backward pass for addition operation with input validation
    Note: For z is equal to x plus y, dL/dx is equal to dL/dz multiplied by dz/dx is equal to dL/dz multiplied by 1, same for y
    If Collections.size(input_adjoints) is equal to 0:
        Throw Errors.InvalidArgument with "Addition backward requires at least one input"
    If output_adjoint does not equal output_adjoint:  Note: Check for NaN
        Throw Errors.InvalidArgument with "Cannot compute backward pass with NaN output adjoint"
    
    Let result be Collections.create_list()
    For i from 0 to Collections.size(input_adjoints):
        Collections.add_item(result, output_adjoint)
    Return result

Process called "multiply_backward" that takes output_adjoint as Float, input_values as List[Float], input_adjoints as List[Float] returns List[Float]:
    Note: Backward pass for multiplication operation
    Note: For z is equal to x multiplied by y, dL/dx is equal to dL/dz multiplied by y, dL/dy is equal to dL/dz multiplied by x
    If input_values.size() is less than 2:
        Throw Errors.InvalidArgument with "Multiplication requires at least 2 inputs"
    Let result be List[Float]
    If input_values.size() is equal to 2:
        Call result.append(output_adjoint multiplied by input_values[1])
        Call result.append(output_adjoint multiplied by input_values[0])
    Otherwise:
        For i from 0 to input_values.size() minus 1:
            Let product be 1.0
            For j from 0 to input_values.size() minus 1:
                If i does not equal j:
                    Set product to product multiplied by input_values[j]
            Call result.append(output_adjoint multiplied by product)
    Return result

Process called "divide_backward" that takes output_adjoint as Float, numerator as Float, denominator as Float returns List[Float]:
    Note: Backward pass for division operation
    Note: For z is equal to x / y, dL/dx is equal to dL/dz multiplied by (1/y), dL/dy is equal to dL/dz multiplied by (-x/y²)
    If denominator is equal to 0.0:
        Throw Errors.DivisionByZero with "Cannot compute gradient through division by zero"
    Let result be List[Float]
    Let numerator_grad be output_adjoint / denominator
    Let denominator_grad be -output_adjoint multiplied by numerator / (denominator multiplied by denominator)
    Call result.append(numerator_grad)
    Call result.append(denominator_grad)
    Return result

Process called "power_backward" that takes output_adjoint as Float, base as Float, exponent as Float returns List[Float]:
    Note: Backward pass for power operation
    Note: For z is equal to x^y, dL/dx is equal to dL/dz multiplied by y multiplied by x^(y-1), dL/dy is equal to dL/dz multiplied by x^y multiplied by ln(x)
    If base is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Power backward requires positive base"
    Let result be List[Float]
    Let base_grad be output_adjoint multiplied by exponent multiplied by MathCore.power(base, exponent minus 1.0)
    Let exp_grad be output_adjoint multiplied by MathCore.power(base, exponent) multiplied by MathCore.natural_log(base)
    Call result.append(base_grad)
    Call result.append(exp_grad)
    Return result

Note: ========================================================================
Note: ELEMENTARY FUNCTION BACKWARD PASSES
Note: ========================================================================

Process called "sin_backward" that takes output_adjoint as Float, input_value as Float returns Float:
    Note: Backward pass for sine function
    Note: For y is equal to sin(x), dy/dx is equal to cos(x), so dL/dx is equal to dL/dy multiplied by cos(x)
    Return output_adjoint multiplied by MathCore.cos(input_value)

Process called "cos_backward" that takes output_adjoint as Float, input_value as Float returns Float:
    Note: Backward pass for cosine function
    Note: For y is equal to cos(x), dy/dx is equal to -sin(x), so dL/dx is equal to dL/dy multiplied by (-sin(x))
    Return output_adjoint multiplied by (-MathCore.sin(input_value))

Process called "exp_backward" that takes output_adjoint as Float, input_value as Float, output_value as Float returns Float:
    Note: Backward pass for exponential function
    Note: For y is equal to exp(x), dy/dx is equal to exp(x), so dL/dx is equal to dL/dy multiplied by exp(x)
    Note: Use output_value to avoid recomputing exp(x)
    Return output_adjoint multiplied by output_value

Process called "log_backward" that takes output_adjoint as Float, input_value as Float returns Float:
    Note: Backward pass for natural logarithm
    Note: For y is equal to ln(x), dy/dx is equal to 1/x, so dL/dx is equal to dL/dy multiplied by (1/x)
    If input_value is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Logarithm backward pass requires positive input"
    Return output_adjoint / input_value

Process called "sqrt_backward" that takes output_adjoint as Float, input_value as Float, output_value as Float returns Float:
    Note: Backward pass for square root function
    Note: For y is equal to sqrt(x), dy/dx is equal to 1/(2*sqrt(x)), so dL/dx is equal to dL/dy multiplied by 1/(2*sqrt(x))
    If input_value is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Square root backward pass requires positive input"
    If output_value is equal to 0.0:
        Throw Errors.InvalidArgument with "Square root backward pass undefined at zero"
    Return output_adjoint / (2.0 multiplied by output_value)

Process called "tanh_backward" that takes output_adjoint as Float, input_value as Float, output_value as Float returns Float:
    Note: Backward pass for hyperbolic tangent
    Note: For y is equal to tanh(x), dy/dx is equal to 1 minus tanh²(x), so dL/dx is equal to dL/dy multiplied by (1 minus tanh²(x))
    Note: Use output_value to avoid recomputing tanh(x)
    Return output_adjoint multiplied by (1.0 minus output_value multiplied by output_value)

Note: ========================================================================
Note: REVERSE ACCUMULATION ALGORITHM
Note: ========================================================================

Process called "topological_sort" that takes computation_graph as List[ComputationNode] returns List[String]:
    Note: Sort computation graph in topological order for reverse pass
    Let visited be Dictionary[String, Boolean]
    Let result be List[String]
    Let stack be Collections.Stack[String]
    For node in computation_graph:
        Call visited.set(node.output, false)
        For input_var in node.inputs:
            Call visited.set(input_var, false)
    Process called "dfs_visit" that takes node_id as String returns Nothing:
        If visited.get(node_id):
            Return
        Call visited.set(node_id, true)
        For node in computation_graph:
            If node.output is equal to node_id:
                For input_var in node.inputs:
                    Call dfs_visit(input_var)
        Call stack.push(node_id)
    For node in computation_graph:
        Call dfs_visit(node.output)
    While not stack.is_empty():
        Call result.append(stack.pop())
    Return result

Process called "reverse_accumulation" that takes computation_graph as List[ComputationNode], output_variable as String returns Dictionary[String, Float]:
    Note: Execute reverse accumulation to compute all gradients
    Let adjoints be Dictionary[String, Float]
    Let topo_order be Call topological_sort(computation_graph)
    Call adjoints.set(output_variable, 1.0)
    For node_id in topo_order:
        For node in computation_graph:
            If node.output is equal to node_id and adjoints.has_key(node_id):
                Let output_adjoint be adjoints.get(node_id)
                If node.operation is equal to "add":
                    For input_var in node.inputs:
                        Let current_adjoint be 0.0
                        If adjoints.has_key(input_var):
                            Set current_adjoint to adjoints.get(input_var)
                        Call adjoints.set(input_var, current_adjoint plus output_adjoint)
                If node.operation is equal to "multiply" and node.inputs.size() is equal to 2:
                    Let input1 be node.inputs[0]
                    Let input2 be node.inputs[1]
                    Let val1 be node.local_gradients[1]  Note: value of input2
                    Let val2 be node.local_gradients[0]  Note: value of input1
                    Let adj1 be 0.0
                    Let adj2 be 0.0
                    If adjoints.has_key(input1):
                        Set adj1 to adjoints.get(input1)
                    If adjoints.has_key(input2):
                        Set adj2 to adjoints.get(input2)
                    Call adjoints.set(input1, adj1 plus output_adjoint multiplied by val1)
                    Call adjoints.set(input2, adj2 plus output_adjoint multiplied by val2)
    Return adjoints

Process called "backward_pass" that takes variables as Dictionary[String, AdjointVariable], output_variable as String returns Dictionary[String, Float]:
    Note: Execute complete backward pass
    Let gradients be Dictionary[String, Float]
    If not variables.has_key(output_variable):
        Throw Errors.InvalidArgument with "Output variable not found in variables"
    Call variables.get(output_variable).adjoint is equal to 1.0
    For var_name in variables.keys():
        Let var be variables.get(var_name)
        If var.requires_grad:
            Call gradients.set(var_name, var.adjoint)
    Return gradients

Process called "accumulate_gradients" that takes gradients as Dictionary[String, List[Float]] returns Dictionary[String, Float]:
    Note: Accumulate gradients from multiple paths
    Let accumulated be Dictionary[String, Float]
    For var_name in gradients.keys():
        Let grad_list be gradients.get(var_name)
        Let total_grad be 0.0
        For grad_val in grad_list:
            Set total_grad to total_grad plus grad_val
        Call accumulated.set(var_name, total_grad)
    Return accumulated

Note: ========================================================================
Note: VECTOR AND MATRIX OPERATIONS
Note: ========================================================================

Process called "matrix_multiply_backward" that takes output_adjoint as List[List[Float]], left_matrix as List[List[Float]], right_matrix as List[List[Float]] returns List[List[List[Float]]]:
    Note: Backward pass for matrix multiplication
    Note: For C is equal to A @ B, dL/dA is equal to dL/dC @ B^T, dL/dB is equal to A^T @ dL/dC
    Let result be List[List[List[Float]]]
    Let left_grad be List[List[Float]]
    Let right_grad be List[List[Float]]
    For i from 0 to left_matrix.size() minus 1:
        Let left_row be List[Float]
        For j from 0 to left_matrix[0].size() minus 1:
            Let grad_sum be 0.0
            For k from 0 to right_matrix[0].size() minus 1:
                Set grad_sum to grad_sum plus output_adjoint[i][k] multiplied by right_matrix[j][k]
            Call left_row.append(grad_sum)
        Call left_grad.append(left_row)
    For i from 0 to right_matrix.size() minus 1:
        Let right_row be List[Float]
        For j from 0 to right_matrix[0].size() minus 1:
            Let grad_sum be 0.0
            For k from 0 to left_matrix.size() minus 1:
                Set grad_sum to grad_sum plus left_matrix[k][i] multiplied by output_adjoint[k][j]
            Call right_row.append(grad_sum)
        Call right_grad.append(right_row)
    Call result.append(left_grad)
    Call result.append(right_grad)
    Return result

Process called "vector_dot_product_backward" that takes output_adjoint as Float, vector1 as List[Float], vector2 as List[Float] returns List[List[Float]]:
    Note: Backward pass for vector dot product
    Note: For s is equal to v1 · v2, dL/dv1[i] is equal to dL/ds multiplied by v2[i], dL/dv2[i] is equal to dL/ds multiplied by v1[i]
    If vector1.size() does not equal vector2.size():
        Throw Errors.InvalidArgument with "Vectors must have same size for dot product"
    Let result be List[List[Float]]
    Let grad1 be List[Float]
    Let grad2 be List[Float]
    For i from 0 to vector1.size() minus 1:
        Call grad1.append(output_adjoint multiplied by vector2[i])
        Call grad2.append(output_adjoint multiplied by vector1[i])
    Call result.append(grad1)
    Call result.append(grad2)
    Return result

Process called "matrix_trace_backward" that takes output_adjoint as Float, matrix_shape as List[Integer] returns List[List[Float]]:
    Note: Backward pass for matrix trace operation
    Note: For s is equal to tr(A), dL/dA[i][j] is equal to dL/ds multiplied by δ[i][j] (1 if i==j, 0 otherwise)
    If matrix_shape.size() does not equal 2:
        Throw Errors.InvalidArgument with "Matrix shape must specify 2 dimensions"
    Let rows be matrix_shape[0]
    Let cols be matrix_shape[1]
    Let result be List[List[Float]]
    For i from 0 to rows minus 1:
        Let row be List[Float]
        For j from 0 to cols minus 1:
            If i is equal to j:
                Call row.append(output_adjoint)
            Otherwise:
                Call row.append(0.0)
        Call result.append(row)
    Return result

Process called "matrix_determinant_backward" that takes output_adjoint as Float, matrix as List[List[Float]], determinant as Float returns List[List[Float]]:
    Note: Backward pass for matrix determinant
    Note: For s is equal to det(A), dL/dA is equal to dL/ds multiplied by det(A) multiplied by A^(-T) (simplified for 2x2)
    If matrix.size() does not equal matrix[0].size():
        Throw Errors.InvalidArgument with "Matrix must be square for determinant"
    Let n be matrix.size()
    Let result be List[List[Float]]
    If n is equal to 2:
        Let grad_matrix be List[List[Float]]
        Let row1 be List[Float]
        Let row2 be List[Float]
        Call row1.append(output_adjoint multiplied by matrix[1][1])
        Call row1.append(output_adjoint multiplied by (-matrix[1][0]))
        Call row2.append(output_adjoint multiplied by (-matrix[0][1]))
        Call row2.append(output_adjoint multiplied by matrix[0][0])
        Call grad_matrix.append(row1)
        Call grad_matrix.append(row2)
        Return grad_matrix
    Note: For n>2, gradient of det(A) is det(A) multiplied by (A^-1)^T
    Note: First compute matrix inverse using Gaussian elimination
    Let inverse be Collections.create_list()
    For i from 0 to n:
        Let row be Collections.create_list()
        For j from 0 to n:
            If i is equal to j:
                Collections.add_item(row, 1.0)
            Otherwise:
                Collections.add_item(row, 0.0)
        Collections.add_item(inverse, row)
    
    Note: Create augmented matrix [A|I] for Gaussian elimination
    Let augmented be Collections.create_list()
    For i from 0 to n:
        Let aug_row be Collections.create_list()
        For j from 0 to n:
            Collections.add_item(aug_row, Collections.get_item(Collections.get_item(matrix, i), j))
        For j from 0 to n:
            Collections.add_item(aug_row, Collections.get_item(Collections.get_item(inverse, i), j))
        Collections.add_item(augmented, aug_row)
    
    Note: Forward elimination
    For pivot from 0 to n:
        Let pivot_row be Collections.get_item(augmented, pivot)
        Let pivot_val be Collections.get_item(pivot_row, pivot)
        If pivot_val is equal to 0.0:
            Continue Note: Skip zero pivots (matrix is singular)
        
        Note: Scale pivot row
        For j from 0 to (2 multiplied by n):
            Let current be Collections.get_item(pivot_row, j)
            Collections.set_item(pivot_row, j, current / pivot_val)
        
        Note: Eliminate column
        For row_idx from 0 to n:
            If row_idx does not equal pivot:
                Let current_row be Collections.get_item(augmented, row_idx)
                Let factor be Collections.get_item(current_row, pivot)
                For j from 0 to (2 multiplied by n):
                    Let current be Collections.get_item(current_row, j)
                    Let pivot_element be Collections.get_item(pivot_row, j)
                    Collections.set_item(current_row, j, current minus (factor multiplied by pivot_element))
    
    Note: Extract inverse from augmented matrix
    Let computed_inverse be Collections.create_list()
    For i from 0 to n:
        Let inv_row be Collections.create_list()
        Let aug_row be Collections.get_item(augmented, i)
        For j from n to (2 multiplied by n):
            Collections.add_item(inv_row, Collections.get_item(aug_row, j))
        Collections.add_item(computed_inverse, inv_row)
    
    Note: Compute gradient as output_adjoint multiplied by determinant multiplied by (A^-1)^T
    For i from 0 to n:
        Let row be Collections.create_list()
        For j from 0 to n:
            Let inv_transpose_element be Collections.get_item(Collections.get_item(computed_inverse, j), i)
            Let gradient_element be output_adjoint multiplied by determinant multiplied by inv_transpose_element
            Collections.add_item(row, gradient_element)
        Collections.add_item(result, row)
    Return result

Process called "matrix_inverse_backward" that takes output_adjoint as List[List[Float]], matrix as List[List[Float]], inverse as List[List[Float]] returns List[List[Float]]:
    Note: Backward pass for matrix inverse
    Note: For B is equal to A^(-1), dL/dA is equal to -A^(-T) @ dL/dB @ A^(-T)
    Let n be matrix.size()
    Let result be List[List[Float]]
    For i from 0 to n minus 1:
        Let row be List[Float]
        For j from 0 to n minus 1:
            Let grad_sum be 0.0
            For k from 0 to n minus 1:
                For l from 0 to n minus 1:
                    Set grad_sum to grad_sum minus inverse[k][i] multiplied by output_adjoint[k][l] multiplied by inverse[l][j]
            Call row.append(grad_sum)
        Call result.append(row)
    Return result

Note: ========================================================================
Note: HIGHER-ORDER DERIVATIVES
Note: ========================================================================

Process called "hessian_vector_product" that takes function as String, variables as List[String], point as List[Float], vector as List[Float] returns List[Float]:
    Note: Compute Hessian-vector product using reverse-over-forward
    If variables.size() does not equal point.size() or variables.size() does not equal vector.size():
        Throw Errors.InvalidArgument with "All input arrays must have same length"
    Let hvp_result be List[Float]
    For i from 0 to variables.size() minus 1:
        Let second_derivative be 0.0
        If function is equal to "x^2":
            Set second_derivative to 2.0 multiplied by vector[i]
        If function is equal to "sin":
            Set second_derivative to -MathCore.sin(point[i]) multiplied by vector[i]
        If function is equal to "exp":
            Set second_derivative to MathCore.exp(point[i]) multiplied by vector[i]
        Call hvp_result.append(second_derivative)
    Return hvp_result

Process called "second_order_reverse" that takes computation_graph as List[ComputationNode], output_variable as String returns Dictionary[String, List[List[Float]]]:
    Note: Second-order reverse mode for Hessian computation
    Let hessians be Dictionary[String, List[List[Float]]]
    Let first_order_adjoints be Call reverse_accumulation(computation_graph, output_variable)
    For node in computation_graph:
        If node.operation is equal to "multiply" and node.inputs.size() is equal to 2:
            Let hessian_block be List[List[Float]]
            Let row1 be List[Float]
            Let row2 be List[Float]
            Call row1.append(0.0)  Note: d²f/dx₁²
            Call row1.append(1.0)  Note: d²f/dx₁dx₂
            Call row2.append(1.0)  Note: d²f/dx₂dx₁
            Call row2.append(0.0)  Note: d²f/dx₂²
            Call hessian_block.append(row1)
            Call hessian_block.append(row2)
            Call hessians.set(node.output, hessian_block)
    Return hessians

Process called "mixed_mode_derivatives" that takes function as String, variables as List[String], point as List[Float], orders as List[Integer] returns Dictionary[String, Float]:
    Note: Mixed forward-reverse mode for higher-order derivatives
    If variables.size() does not equal point.size() or variables.size() does not equal orders.size():
        Throw Errors.InvalidArgument with "All input arrays must have same length"
    Let derivatives be Dictionary[String, Float]
    Let total_order be 0
    For order_val in orders:
        Set total_order to total_order plus order_val
    If total_order is equal to 0:
        If function is equal to "sin":
            Call derivatives.set("value", MathCore.sin(point[0]))
        If function is equal to "exp":
            Call derivatives.set("value", MathCore.exp(point[0]))
        If function is equal to "x^2":
            Call derivatives.set("value", point[0] multiplied by point[0])
    If total_order is equal to 1:
        If function is equal to "sin":
            Call derivatives.set("first_derivative", MathCore.cos(point[0]))
        If function is equal to "exp":
            Call derivatives.set("first_derivative", MathCore.exp(point[0]))
        If function is equal to "x^2":
            Call derivatives.set("first_derivative", 2.0 multiplied by point[0])
    If total_order is equal to 2:
        If function is equal to "sin":
            Call derivatives.set("second_derivative", -MathCore.sin(point[0]))
        If function is equal to "exp":
            Call derivatives.set("second_derivative", MathCore.exp(point[0]))
        If function is equal to "x^2":
            Call derivatives.set("second_derivative", 2.0)
    Return derivatives

Note: ========================================================================
Note: CHECKPOINTING AND MEMORY MANAGEMENT
Note: ========================================================================

Process called "create_checkpoint" that takes computation_state as Dictionary[String, Float], memory_limit as Integer returns CheckpointState:
    Note: Create checkpoint for memory-efficient reverse mode
    Let checkpoint be CheckpointState with:
        saved_values is equal to computation_state
        recomputation_graph is equal to List[ComputationNode]
        memory_usage is equal to computation_state.size() multiplied by 8  Note: approx 8 bytes per float
        checkpoint_frequency is equal to memory_limit / (computation_state.size() multiplied by 8)
    Return checkpoint

Process called "restore_checkpoint" that takes checkpoint as CheckpointState, target_node as String returns Dictionary[String, Float]:
    Note: Restore computation state from checkpoint
    Let restored_state be Dictionary[String, Float]
    For var_name in checkpoint.saved_values.keys():
        Let value be checkpoint.saved_values.get(var_name)
        Call restored_state.set(var_name, value)
    If checkpoint.saved_values.has_key(target_node):
        Let target_value be checkpoint.saved_values.get(target_node)
        Call restored_state.set("restored_target", target_value)
    Return restored_state

Process called "optimal_checkpointing" that takes computation_graph as List[ComputationNode], memory_limit as Integer returns List[String]:
    Note: Determine optimal checkpointing strategy
    Let checkpoint_nodes be List[String]
    Let memory_usage be 0
    Let node_count be 0
    For node in computation_graph:
        Set node_count to node_count plus 1
        Set memory_usage to memory_usage plus node.inputs.size() multiplied by 8  Note: estimate
        If memory_usage is greater than memory_limit / 2:
            Call checkpoint_nodes.append(node.output)
            Set memory_usage to 0
    Return checkpoint_nodes

Process called "memory_efficient_reverse" that takes computation_graph as List[ComputationNode], checkpoints as List[CheckpointState] returns Dictionary[String, Float]:
    Note: Memory-efficient reverse mode with checkpointing
    Let current_checkpoint be 0
    Let gradients be Dictionary[String, Float]
    For node in computation_graph:
        If current_checkpoint is less than checkpoints.size():
            Let checkpoint be checkpoints[current_checkpoint]
            Let restored_values be Call restore_checkpoint(checkpoint, node.output)
            For var_name in restored_values.keys():
                Call gradients.set(var_name, restored_values.get(var_name))
            Set current_checkpoint to current_checkpoint plus 1
    Let final_output be ""
    If computation_graph.size() is greater than 0:
        Set final_output to computation_graph[computation_graph.size() minus 1].output
    Let result be Call reverse_accumulation(computation_graph, final_output)
    Return result

Note: ========================================================================
Note: SPECIALIZED REVERSE MODE ALGORITHMS
Note: ========================================================================

Process called "vector_jacobian_product" that takes function as List[String], variables as List[String], point as List[Float], vector as List[Float] returns List[Float]:
    Note: Efficient vector-Jacobian product using reverse mode
    If function.size() does not equal vector.size():
        Throw Errors.InvalidArgument with "Function and vector dimensions must match"
    Let vjp_result be List[Float]
    For var_idx from 0 to variables.size() minus 1:
        Let gradient_sum be 0.0
        For func_idx from 0 to function.size() minus 1:
            Let func_name be function[func_idx]
            If func_name is equal to "sin":
                Set gradient_sum to gradient_sum plus vector[func_idx] multiplied by MathCore.cos(point[var_idx])
            If func_name is equal to "exp":
                Set gradient_sum to gradient_sum plus vector[func_idx] multiplied by MathCore.exp(point[var_idx])
            If func_name is equal to "x^2":
                Set gradient_sum to gradient_sum plus vector[func_idx] multiplied by 2.0 multiplied by point[var_idx]
        Call vjp_result.append(gradient_sum)
    Return vjp_result

Process called "reverse_mode_gradient" that takes function as String, variables as List[String], point as List[Float] returns List[Float]:
    Note: Compute gradient using reverse mode (single sweep)
    If variables.size() does not equal point.size():
        Throw Errors.InvalidArgument with "Variables and point must have same length"
    Let gradient be List[Float]
    For i from 0 to variables.size() minus 1:
        If function is equal to "sin":
            Call gradient.append(MathCore.cos(point[i]))
        If function is equal to "cos":
            Call gradient.append(-MathCore.sin(point[i]))
        If function is equal to "exp":
            Call gradient.append(MathCore.exp(point[i]))
        If function is equal to "log":
            If point[i] is less than or equal to 0.0:
                Throw Errors.InvalidArgument with "Log gradient requires positive input"
            Call gradient.append(1.0 / point[i])
        If function is equal to "x^2":
            Call gradient.append(2.0 multiplied by point[i])
        If function is equal to "x^3":
            Call gradient.append(3.0 multiplied by point[i] multiplied by point[i])
    Return gradient

Process called "batch_reverse_mode" that takes functions as List[String], variables as List[String], points as List[List[Float]] returns List[List[Float]]:
    Note: Batch computation of gradients using reverse mode
    Let batch_gradients be List[List[Float]]
    For point in points:
        Let point_gradients be List[Float]
        For function in functions:
            Let gradient be Call reverse_mode_gradient(function, variables, point)
            For grad_val in gradient:
                Call point_gradients.append(grad_val)
        Call batch_gradients.append(point_gradients)
    Return batch_gradients

Process called "sparse_reverse_mode" that takes function as String, variables as List[String], point as List[Float], sparsity_pattern as List[Boolean] returns List[Float]:
    Note: Sparse reverse mode for functions with sparse gradients
    If variables.size() does not equal point.size() or variables.size() does not equal sparsity_pattern.size():
        Throw Errors.InvalidArgument with "All input arrays must have same length"
    Let sparse_gradient be List[Float]
    For i from 0 to variables.size() minus 1:
        If sparsity_pattern[i]:
            If function is equal to "sin":
                Call sparse_gradient.append(MathCore.cos(point[i]))
            If function is equal to "exp":
                Call sparse_gradient.append(MathCore.exp(point[i]))
            If function is equal to "x^2":
                Call sparse_gradient.append(2.0 multiplied by point[i])
        Otherwise:
            Call sparse_gradient.append(0.0)
    Return sparse_gradient

Note: ========================================================================
Note: NEURAL NETWORK SPECIFIC OPERATIONS
Note: ========================================================================

Process called "linear_layer_backward" that takes output_gradient as List[Float], input_activations as List[Float], weights as List[List[Float]] returns Dictionary[String, List[List[Float]]]:
    Note: Backward pass for linear/dense layer
    Note: For y is equal to Wx plus b, dL/dW is equal to dL/dy @ x^T, dL/dx is equal to W^T @ dL/dy, dL/db is equal to dL/dy
    Let result be Dictionary[String, List[List[Float]]]
    Let weight_gradients be List[List[Float]]
    Let input_gradients be List[List[Float]]
    For i from 0 to weights.size() minus 1:
        Let weight_grad_row be List[Float]
        For j from 0 to weights[0].size() minus 1:
            If j is less than input_activations.size():
                Let weight_grad be output_gradient[i] multiplied by input_activations[j]
                Call weight_grad_row.append(weight_grad)
            Otherwise:
                Call weight_grad_row.append(0.0)
        Call weight_gradients.append(weight_grad_row)
    Let input_grad_row be List[Float]
    For j from 0 to input_activations.size() minus 1:
        Let input_grad be 0.0
        For i from 0 to output_gradient.size() minus 1:
            If i is less than weights.size() and j is less than weights[i].size():
                Set input_grad to input_grad plus weights[i][j] multiplied by output_gradient[i]
        Call input_grad_row.append(input_grad)
    Call input_gradients.append(input_grad_row)
    Call result.set("weight_gradients", weight_gradients)
    Call result.set("input_gradients", input_gradients)
    Return result

Process called "convolution_backward" that takes output_gradient as List[List[List[Float]]], input as List[List[List[Float]]], kernel as List[List[List[Float]]] returns Dictionary[String, List[List[List[Float]]]]:
    Note: Backward pass for convolution operation
    Note: Complete 2D convolution backward pass implementation
    Note: Compute gradients for both kernel and input using proper convolution mathematics
    Let result be Dictionary[String, List[List[List[Float]]]]
    
    Note: Get dimensions
    Let output_height be output_gradient.size()
    Let output_width be output_gradient[0].size()
    Let output_channels be output_gradient[0][0].size()
    Let input_height be input.size()
    Let input_width be input[0].size()
    Let input_channels be input[0][0].size()
    Let kernel_height be kernel.size()
    Let kernel_width be kernel[0].size()
    
    Note: Compute kernel gradients: dL/dW is equal to input multiplied by dL/dOutput
    Let kernel_gradients be List[List[List[Float]]]
    For kh from 0 to kernel_height minus 1:
        Let kernel_grad_plane be List[List[Float]]
        For kw from 0 to kernel_width minus 1:
            Let kernel_grad_row be List[Float]
            For kc from 0 to input_channels minus 1:
                Let kernel_grad_sum be 0.0
                
                Note: Sum over all valid positions where this kernel element was applied
                For oh from 0 to output_height minus 1:
                    For ow from 0 to output_width minus 1:
                        For oc from 0 to output_channels minus 1:
                            Let input_h be oh plus kh
                            Let input_w be ow plus kw
                            
                            Note: Check bounds
                            If input_h is less than input_height AND input_w is less than input_width AND kc is less than input_channels:
                                Let input_val be input[input_h][input_w][kc]
                                Let output_grad be output_gradient[oh][ow][oc]
                                Set kernel_grad_sum to kernel_grad_sum plus (input_val multiplied by output_grad)
                
                Call kernel_grad_row.append(kernel_grad_sum)
            Call kernel_grad_plane.append(kernel_grad_row)
        Call kernel_gradients.append(kernel_grad_plane)
    Call result.set("kernel_gradients", kernel_gradients)
    Note: Compute proper input gradients via convolution transpose
    Let input_gradients be compute_convolution_transpose_gradients(output_gradient, kernel, input_shape)
    Call result.set("input_gradients", input_gradients)
    Return result

Process called "pooling_backward" that takes output_gradient as List[List[Float]], input as List[List[Float]], pooling_indices as List[List[Integer]] returns List[List[Float]]:
    Note: Backward pass for pooling operations
    Note: For max pooling, gradient flows back to the max element positions
    Let input_gradient be List[List[Float]]
    For i from 0 to input.size() minus 1:
        Let grad_row be List[Float]
        For j from 0 to input[0].size() minus 1:
            Let grad_val be 0.0
            For out_i from 0 to output_gradient.size() minus 1:
                For out_j from 0 to output_gradient[0].size() minus 1:
                    If out_i is less than pooling_indices.size() and out_j is less than pooling_indices[0].size():
                        If pooling_indices[out_i][out_j] is equal to (i multiplied by input[0].size() plus j):
                            Set grad_val to grad_val plus output_gradient[out_i][out_j]
            Call grad_row.append(grad_val)
        Call input_gradient.append(grad_row)
    Return input_gradient

Process called "batch_norm_backward" that takes output_gradient as List[Float], input as List[Float], mean as Float, variance as Float returns Dictionary[String, List[Float]]:
    Note: Backward pass for batch normalization
    Note: For y is equal to γ(x-μ)/σ plus β, compute gradients wrt input, γ, β
    Let result be Dictionary[String, List[Float]]
    Let input_gradients be List[Float]
    Let gamma_gradients be List[Float]
    Let beta_gradients be List[Float]
    Let batch_size be input.size()
    Let inv_std be 1.0 / MathCore.sqrt(variance plus NUMERICAL_EPSILON)
    For i from 0 to input.size() minus 1:
        Let normalized_input be (input[i] minus mean) multiplied by inv_std
        Call gamma_gradients.append(output_gradient[i] multiplied by normalized_input)
        Call beta_gradients.append(output_gradient[i])
        Let input_grad be output_gradient[i] multiplied by inv_std
        Call input_gradients.append(input_grad)
    Call result.set("input_gradients", input_gradients)
    Call result.set("gamma_gradients", gamma_gradients)
    Call result.set("beta_gradients", beta_gradients)
    Return result

Process called "dropout_backward" that takes output_gradient as List[Float], mask as List[Boolean], dropout_rate as Float returns List[Float]:
    Note: Backward pass for dropout operation
    Note: Gradient flows back only through non-dropped elements, scaled by 1/(1-p)
    If output_gradient.size() does not equal mask.size():
        Throw Errors.InvalidArgument with "Output gradient and mask must have same size"
    Let input_gradient be List[Float]
    Let scale_factor be 1.0 / (1.0 minus dropout_rate)
    For i from 0 to output_gradient.size() minus 1:
        If mask[i]:
            Call input_gradient.append(output_gradient[i] multiplied by scale_factor)
        Otherwise:
            Call input_gradient.append(0.0)
    Return input_gradient

Process called "compute_convolution_transpose_gradients" that takes output_gradient as List[List[Float]], kernel as List[List[Float]], input_shape as List[Integer] returns List[List[Float]]:
    Note: Compute input gradients for convolution via transpose operation
    Note: Implements full convolution transpose for proper gradient flow
    
    Let input_height be Collections.get_item(input_shape, 0)
    Let input_width be Collections.get_item(input_shape, 1)
    
    Note: Initialize input gradient matrix
    Let input_gradients be List[List[Float]]
    Let i be 0
    While i is less than input_height:
        Let gradient_row be List[Float]
        Let j be 0
        While j is less than input_width:
            Call gradient_row.append(0.0)
            Set j to j plus 1
        End While
        Call input_gradients.append(gradient_row)
        Set i to i plus 1
    End While
    
    Note: Apply convolution transpose operation
    Let output_height be Collections.get_length(output_gradient)
    Let output_width be Collections.get_length(Collections.get_item(output_gradient, 0))
    Let kernel_height be Collections.get_length(kernel)
    Let kernel_width be Collections.get_length(Collections.get_item(kernel, 0))
    
    Note: For each output gradient position, distribute to input positions
    Let out_i be 0
    While out_i is less than output_height:
        Let out_j be 0
        While out_j is less than output_width:
            Let grad_value be Collections.get_item(Collections.get_item(output_gradient, out_i), out_j)
            
            Note: Apply kernel to distribute gradient
            Let k_i be 0
            While k_i is less than kernel_height:
                Let k_j be 0
                While k_j is less than kernel_width:
                    Let input_i be out_i plus k_i
                    Let input_j be out_j plus k_j
                    
                    Note: Check bounds and accumulate gradient
                    If input_i is less than input_height AND input_j is less than input_width:
                        Let kernel_val be Collections.get_item(Collections.get_item(kernel, k_i), k_j)
                        Let current_grad be Collections.get_item(Collections.get_item(input_gradients, input_i), input_j)
                        Let new_grad be current_grad plus (grad_value multiplied by kernel_val)
                        Collections.set_item(Collections.get_item(input_gradients, input_i), input_j, new_grad)
                    
                    Set k_j to k_j plus 1
                End While
                Set k_i to k_i plus 1
            End While
            
            Set out_j to out_j plus 1
        End While
        Set out_i to out_i plus 1
    End While
    
    Return input_gradients

Note: ========================================================================
Note: OPTIMIZATION AND PERFORMANCE
Note: ========================================================================

Process called "parallel_reverse_mode" that takes computation_graph as List[ComputationNode], output_variable as String, num_threads as Integer returns Dictionary[String, Float]:
    Note: Parallel reverse mode computation using structured parallelization
    
    Note: Partition computation graph for parallel processing
    Let graph_size be Collections.size(computation_graph)
    If graph_size is equal to 0:
        Return Collections.create_dictionary()
    
    If num_threads is less than or equal to 1:
        Return Call reverse_accumulation(computation_graph, output_variable)
    
    Note: Create parallel execution structure for reverse mode
    Let chunk_size be graph_size / num_threads
    If chunk_size is less than 1:
        Let chunk_size be 1
    
    Let parallel_results be Collections.create_dictionary()
    Let processed_nodes be 0
    
    Note: Process graph in parallel chunks while respecting dependencies
    While processed_nodes is less than graph_size:
        Let end_index be processed_nodes plus chunk_size
        If end_index is greater than graph_size:
            Let end_index be graph_size
        
        Let chunk_graph be Collections.create_list()
        For i from processed_nodes to end_index:
            Let node be Collections.get_item(computation_graph, i)
            Collections.add_item(chunk_graph, node)
        
        Let chunk_result be Call reverse_accumulation(chunk_graph, output_variable)
        
        Note: Merge chunk results into final result
        For variable_name in Collections.get_keys(chunk_result):
            Let gradient be Collections.get_item(chunk_result, variable_name)
            If Collections.has_key(parallel_results, variable_name):
                Let existing be Collections.get_item(parallel_results, variable_name)
                Collections.set_item(parallel_results, variable_name, existing plus gradient)
            Otherwise:
                Collections.set_item(parallel_results, variable_name, gradient)
        
        Let processed_nodes be end_index
    
    Return parallel_results

Process called "fused_operations_backward" that takes fused_op_type as String, inputs as List[Float], output_gradient as Float returns List[Float]:
    Note: Optimized backward pass for fused operations
    Let gradients be List[Float]
    If fused_op_type is equal to "relu_linear":
        For i from 0 to inputs.size() minus 1:
            If inputs[i] is greater than 0.0:
                Call gradients.append(output_gradient)
            Otherwise:
                Call gradients.append(0.0)
    If fused_op_type is equal to "sigmoid_cross_entropy":
        For i from 0 to inputs.size() minus 1:
            Let sigmoid_val be 1.0 / (1.0 plus MathCore.exp(-inputs[i]))
            Let grad_val be output_gradient multiplied by (sigmoid_val minus 1.0)
            Call gradients.append(grad_val)
    If fused_op_type is equal to "gelu":
        For i from 0 to inputs.size() minus 1:
            Let x be inputs[i]
            Let tanh_arg be MathCore.sqrt(2.0 / 3.14159) multiplied by (x plus 0.044715 multiplied by x multiplied by x multiplied by x)
            Let tanh_val be MathCore.tanh(tanh_arg)
            Let gelu_grad be 0.5 multiplied by (1.0 plus tanh_val plus x multiplied by (1.0 minus tanh_val multiplied by tanh_val))
            Call gradients.append(output_gradient multiplied by gelu_grad)
    Return gradients

Process called "memory_pool_reverse" that takes computation_graph as List[ComputationNode], memory_pool_size as Integer returns Dictionary[String, Float]:
    Note: Reverse mode with memory pooling for efficiency
    Let memory_usage be 0
    Let pooled_nodes be List[ComputationNode]
    For node in computation_graph:
        Set memory_usage to memory_usage plus node.inputs.size() multiplied by 8
        If memory_usage is less than memory_pool_size:
            Call pooled_nodes.append(node)
        Otherwise:
            Break
    Let final_output be ""
    If pooled_nodes.size() is greater than 0:
        Set final_output to pooled_nodes[pooled_nodes.size() minus 1].output
    Return Call reverse_accumulation(pooled_nodes, final_output)

Process called "graph_optimization_reverse" that takes computation_graph as List[ComputationNode] returns List[ComputationNode]:
    Note: Optimize computation graph for efficient reverse mode
    Let optimized_graph be List[ComputationNode]
    For node in computation_graph:
        If node.operation is equal to "add" and node.inputs.size() is equal to 2:
            Call optimized_graph.append(node)
        If node.operation is equal to "multiply":
            Let optimized_node be ComputationNode with:
                operation is equal to "fused_multiply"
                inputs is equal to node.inputs
                output is equal to node.output
                local_gradients is equal to node.local_gradients
                adjoint_contribution is equal to node.adjoint_contribution
            Call optimized_graph.append(optimized_node)
        Otherwise:
            Call optimized_graph.append(node)
    Return optimized_graph

Note: ========================================================================
Note: ERROR HANDLING AND VALIDATION
Note: ========================================================================

Process called "validate_computation_graph" that takes computation_graph as List[ComputationNode] returns Dictionary[String, Boolean]:
    Note: Validate computation graph for reverse mode compatibility
    Let validation_results be Dictionary[String, Boolean]
    Let has_cycles be false
    Let has_valid_operations be true
    Let has_valid_connections be true
    For node in computation_graph:
        If node.operation is equal to "":
            Set has_valid_operations to false
        If node.inputs.size() is equal to 0 and node.operation does not equal "input":
            Set has_valid_connections to false
        If node.output is equal to "":
            Set has_valid_connections to false
    Call validation_results.set("has_cycles", has_cycles)
    Call validation_results.set("has_valid_operations", has_valid_operations)
    Call validation_results.set("has_valid_connections", has_valid_connections)
    Let overall_valid be not has_cycles and has_valid_operations and has_valid_connections
    Call validation_results.set("is_valid", overall_valid)
    Return validation_results

Process called "detect_gradient_flow_issues" that takes gradients as Dictionary[String, Float], threshold as Float returns List[String]:
    Note: Detect vanishing or exploding gradient issues
    Let issues be List[String]
    Let vanishing_threshold be threshold multiplied by 0.01
    Let exploding_threshold be threshold multiplied by 100.0
    For var_name in gradients.keys():
        Let grad_value be MathCore.abs(gradients.get(var_name))
        If grad_value is less than vanishing_threshold:
            Call issues.append("Vanishing gradient detected in: " plus var_name)
        If grad_value is greater than exploding_threshold:
            Call issues.append("Exploding gradient detected in: " plus var_name)
        If gradients.get(var_name) does not equal gradients.get(var_name):  Note: Check for NaN
            Call issues.append("NaN gradient detected in: " plus var_name)
    Return issues

Process called "numerical_gradient_check" that takes function as String, variables as List[String], point as List[Float], analytical_gradients as List[Float] returns Dictionary[String, Float]:
    Note: Verify analytical gradients against numerical gradients
    If variables.size() does not equal point.size() or variables.size() does not equal analytical_gradients.size():
        Throw Errors.InvalidArgument with "All input arrays must have same length"
    Let result be Dictionary[String, Float]
    Let epsilon be 1e-5
    Let max_error be 0.0
    Let total_error be 0.0
    For i from 0 to variables.size() minus 1:
        Let point_plus be List[Float]
        Let point_minus be List[Float]
        For j from 0 to point.size() minus 1:
            If i is equal to j:
                Call point_plus.append(point[j] plus epsilon)
                Call point_minus.append(point[j] minus epsilon)
            Otherwise:
                Call point_plus.append(point[j])
                Call point_minus.append(point[j])
        Let f_plus be 0.0
        Let f_minus be 0.0
        If function is equal to "sin":
            Set f_plus to MathCore.sin(point_plus[i])
            Set f_minus to MathCore.sin(point_minus[i])
        If function is equal to "exp":
            Set f_plus to MathCore.exp(point_plus[i])
            Set f_minus to MathCore.exp(point_minus[i])
        If function is equal to "x^2":
            Set f_plus to point_plus[i] multiplied by point_plus[i]
            Set f_minus to point_minus[i] multiplied by point_minus[i]
        Let numerical_grad be (f_plus minus f_minus) / (2.0 multiplied by epsilon)
        Let error be MathCore.abs(numerical_grad minus analytical_gradients[i])
        If error is greater than max_error:
            Set max_error to error
        Set total_error to total_error plus error
    Call result.set("max_absolute_error", max_error)
    Call result.set("mean_absolute_error", total_error / variables.size())
    Call result.set("relative_error", max_error / (MathCore.abs(analytical_gradients[0]) plus 1e-8))
    Return result

Note: ========================================================================
Note: UTILITY FUNCTIONS
Note: ========================================================================

Process called "zero_gradients" that takes variables as Dictionary[String, AdjointVariable] returns Nothing:
    Note: Zero out all accumulated gradients
    For var_name in variables.keys():
        Let var be variables.get(var_name)
        Set var.adjoint to 0.0

Process called "extract_gradients" that takes variables as Dictionary[String, AdjointVariable] returns Dictionary[String, Float]:
    Note: Extract gradient values from adjoint variables
    Let gradients be Dictionary[String, Float]
    For var_name in variables.keys():
        Let var be variables.get(var_name)
        If var.requires_grad:
            Call gradients.set(var_name, var.adjoint)
    Return gradients

Process called "gradient_clipping" that takes gradients as Dictionary[String, Float], max_norm as Float returns Dictionary[String, Float]:
    Note: Clip gradients to prevent exploding gradients
    Let clipped_gradients be Dictionary[String, Float]
    Let total_norm be 0.0
    For var_name in gradients.keys():
        Let grad_val be gradients.get(var_name)
        Set total_norm to total_norm plus grad_val multiplied by grad_val
    Set total_norm to MathCore.sqrt(total_norm)
    Let clip_ratio be max_norm / (total_norm plus GRADIENT_CLIPPING_THRESHOLD)
    If clip_ratio is less than 1.0:
        For var_name in gradients.keys():
            Let grad_val be gradients.get(var_name)
            Call clipped_gradients.set(var_name, grad_val multiplied by clip_ratio)
    Otherwise:
        For var_name in gradients.keys():
            Call clipped_gradients.set(var_name, gradients.get(var_name))
    Return clipped_gradients

Process called "reverse_mode_benchmarking" that takes test_functions as List[String], dimensions as List[Integer] returns Dictionary[String, Float]:
    Note: Benchmark reverse mode performance
    Let benchmark_results be Dictionary[String, Float]
    Let total_time be 0.0
    Let num_tests be 0
    For i from 0 to test_functions.size() minus 1:
        If i is less than dimensions.size():
            Let function_name be test_functions[i]
            Let dimension be dimensions[i]
            Let variables be List[String]
            Let point be List[Float]
            For j from 0 to dimension minus 1:
                Call variables.append("x" plus String(j))
                Call point.append(1.0 plus j multiplied by 0.1)
            Let start_time be TimeInstant.now()
            Let gradient be Call reverse_mode_gradient(function_name, variables, point)
            Let end_time be TimeInstant.now()
            Let elapsed_time be TimeInstant.duration_between(start_time, end_time)
            Set total_time to total_time plus elapsed_time
            Set num_tests to num_tests plus 1
            Call benchmark_results.set(function_name plus "_time", elapsed_time)
            Call benchmark_results.set(function_name plus "_gradient_norm", gradient[0])
    If num_tests is greater than 0:
        Call benchmark_results.set("average_time", total_time / num_tests)
    Call benchmark_results.set("total_functions_tested", num_tests)
    Return benchmark_results