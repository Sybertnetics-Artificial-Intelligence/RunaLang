Note:
math/engine/autodiff/forward.runa
Forward-Mode Automatic Differentiation

Forward-mode automatic differentiation implementation using dual numbers.
Provides exact derivatives for functions with efficient computation.

Key Features:
- Dual number arithmetic for forward-mode AD
- Support for univariate and multivariate functions
- Higher-order derivatives through nested dual numbers
- Efficient computation of directional derivatives
- Integration with mathematical functions and operators
- Vector and matrix function differentiation

Dependencies:
- Collections (List, Dictionary)
- Math.Core (basic arithmetic, mathematical functions)
- Errors (exception handling)
:End Note

Import module "collections" as Collections
Import module "math.core" as MathCore
Import module "errors" as Errors

Note: ========================================================================
Note: DUAL NUMBER STRUCTURES AND TYPES
Note: ========================================================================

Type called "DualNumber":
    real as Float
    dual as Float

Type called "MultiDual":
    value as Float
    derivatives as List[Float]  Note: partial derivatives wrt each variable
    num_variables as Integer

Type called "HyperDual":
    f as Float      Note: function value
    f_x as Float    Note: first derivative wrt x
    f_y as Float    Note: first derivative wrt y
    f_xy as Float   Note: mixed second derivative

Type called "TaylorSeries":
    coefficients as List[Float]
    order as Integer
    variable_name as String

Note: ========================================================================
Note: DUAL NUMBER ARITHMETIC
Note: ========================================================================

Process called "dual_add" that takes a as DualNumber, b as DualNumber returns DualNumber:
    Note: Addition of two dual numbers with input validation
    Note: (a plus a'ε) plus (b plus b'ε) is equal to (a plus b) plus (a' plus b')ε
    If a.real does not equal a.real or b.real does not equal b.real or a.dual does not equal a.dual or b.dual does not equal b.dual:
        Throw Errors.InvalidArgument with "Cannot add dual numbers with NaN components"
    Let result be DualNumber with:
        real is equal to a.real plus b.real
        dual is equal to a.dual plus b.dual
    Return result

Process called "dual_subtract" that takes a as DualNumber, b as DualNumber returns DualNumber:
    Note: Subtraction of two dual numbers with input validation
    Note: (a plus a'ε) minus (b plus b'ε) is equal to (a minus b) plus (a' minus b')ε
    If a.real does not equal a.real or b.real does not equal b.real or a.dual does not equal a.dual or b.dual does not equal b.dual:
        Throw Errors.InvalidArgument with "Cannot subtract dual numbers with NaN components"
    Let result be DualNumber with:
        real is equal to a.real minus b.real
        dual is equal to a.dual minus b.dual
    Return result

Process called "dual_multiply" that takes a as DualNumber, b as DualNumber returns DualNumber:
    Note: Multiplication of two dual numbers with input validation
    Note: (a plus a'ε) × (b plus b'ε) is equal to (ab) plus (a'b plus ab')ε
    If a.real does not equal a.real or b.real does not equal b.real or a.dual does not equal a.dual or b.dual does not equal b.dual:
        Throw Errors.InvalidArgument with "Cannot multiply dual numbers with NaN components"
    Let result be DualNumber with:
        real is equal to a.real multiplied by b.real
        dual is equal to a.dual multiplied by b.real plus a.real multiplied by b.dual
    Return result

Process called "dual_divide" that takes a as DualNumber, b as DualNumber returns DualNumber:
    Note: Division of two dual numbers
    Note: (a plus a'ε) ÷ (b plus b'ε) is equal to (a/b) plus ((a'b minus ab')/b²)ε
    If b.real is equal to 0.0:
        Throw Errors.DivisionByZero with "Cannot divide dual number by zero"
    Let result be DualNumber with:
        real is equal to a.real / b.real
        dual is equal to (a.dual multiplied by b.real minus a.real multiplied by b.dual) / (b.real multiplied by b.real)
    Return result

Process called "dual_power" that takes base as DualNumber, exponent as DualNumber returns DualNumber:
    Note: Power operation for dual numbers with comprehensive validation
    Note: For f^g, derivative is f^g multiplied by (g'*ln(f) plus g*f'/f)
    If base.real does not equal base.real or exponent.real does not equal exponent.real:
        Throw Errors.InvalidArgument with "Cannot compute power with NaN values"
    If base.real is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Base must be positive for dual power operation"
    Let ln_base be MathCore.natural_log(base.real)
    Let power_result be MathCore.power(base.real, exponent.real)
    Let result be DualNumber with:
        real is equal to power_result
        dual is equal to power_result multiplied by (exponent.dual multiplied by ln_base plus exponent.real multiplied by base.dual / base.real)
    Return result

Process called "dual_negate" that takes a as DualNumber returns DualNumber:
    Note: Negation of dual number
    Note: -(a plus a'ε) is equal to (-a) plus (-a')ε
    Let result be DualNumber with:
        real is equal to -a.real
        dual is equal to -a.dual
    Return result

Note: ========================================================================
Note: ELEMENTARY FUNCTIONS FOR DUAL NUMBERS
Note: ========================================================================

Process called "dual_sin" that takes x as DualNumber returns DualNumber:
    Note: Sine function for dual numbers
    Note: For sin(a plus a'ε), result is sin(a) plus cos(a) multiplied by a' multiplied by ε
    Let result be DualNumber with:
        real is equal to MathCore.sin(x.real)
        dual is equal to MathCore.cos(x.real) multiplied by x.dual
    Return result

Process called "dual_cos" that takes x as DualNumber returns DualNumber:
    Note: Cosine function for dual numbers
    Note: For cos(a plus a'ε), result is cos(a) plus (-sin(a)) multiplied by a' multiplied by ε
    Let result be DualNumber with:
        real is equal to MathCore.cos(x.real)
        dual is equal to -MathCore.sin(x.real) multiplied by x.dual
    Return result

Process called "dual_tan" that takes x as DualNumber returns DualNumber:
    Note: Tangent function for dual numbers
    Note: For tan(a plus a'ε), result is tan(a) plus sec²(a) multiplied by a' multiplied by ε
    Let cos_val be MathCore.cos(x.real)
    If MathCore.abs(cos_val) is less than 1e-15:
        Throw Errors.InvalidArgument with "Tangent undefined at this point"
    Let sec_squared be 1.0 / (cos_val multiplied by cos_val)
    Let result be DualNumber with:
        real is equal to MathCore.tan(x.real)
        dual is equal to sec_squared multiplied by x.dual
    Return result

Process called "dual_exp" that takes x as DualNumber returns DualNumber:
    Note: Exponential function for dual numbers
    Note: For exp(a plus a'ε), result is exp(a) plus exp(a) multiplied by a' multiplied by ε
    Let exp_val be MathCore.exp(x.real)
    Let result be DualNumber with:
        real is equal to exp_val
        dual is equal to exp_val multiplied by x.dual
    Return result

Process called "dual_log" that takes x as DualNumber returns DualNumber:
    Note: Natural logarithm for dual numbers
    Note: For ln(a plus a'ε), result is ln(a) plus (1/a) multiplied by a' multiplied by ε
    If x.real is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Logarithm undefined for non-positive values"
    Let result be DualNumber with:
        real is equal to MathCore.natural_log(x.real)
        dual is equal to x.dual / x.real
    Return result

Process called "dual_sqrt" that takes x as DualNumber returns DualNumber:
    Note: Square root function for dual numbers with comprehensive validation
    Note: For sqrt(a plus a'ε), result is sqrt(a) plus (1/(2*sqrt(a))) multiplied by a' multiplied by ε
    If x.real does not equal x.real:
        Throw Errors.InvalidArgument with "Cannot compute square root of NaN"
    If x.real is less than 0.0:
        Throw Errors.InvalidArgument with "Square root undefined for negative values"
    If x.real is equal to 0.0:
        Throw Errors.InvalidArgument with "Square root derivative undefined at zero"
    Let sqrt_val be MathCore.sqrt(x.real)
    Let result be DualNumber with:
        real is equal to sqrt_val
        dual is equal to x.dual / (2.0 multiplied by sqrt_val)
    Return result

Process called "dual_abs" that takes x as DualNumber returns DualNumber:
    Note: Absolute value for dual numbers
    Note: For abs(a plus a'ε), derivative is sign(a) multiplied by a' multiplied by ε (undefined at a=0)
    If x.real is equal to 0.0:
        Throw Errors.InvalidArgument with "Absolute value derivative undefined at zero"
    Let sign_val be 1.0
    If x.real is less than 0.0:
        Set sign_val to -1.0
    Let result be DualNumber with:
        real is equal to MathCore.abs(x.real)
        dual is equal to sign_val multiplied by x.dual
    Return result

Note: ========================================================================
Note: MULTIVARIATE FORWARD MODE
Note: ========================================================================

Process called "multidual_add" that takes a as MultiDual, b as MultiDual returns MultiDual:
    Note: Addition for multivariate dual numbers
    If a.num_variables does not equal b.num_variables:
        Throw Errors.InvalidArgument with "MultiDual numbers must have same number of variables"
    Let result_derivatives be List[Float]
    For i from 0 to a.num_variables minus 1:
        Call result_derivatives.append(a.derivatives[i] plus b.derivatives[i])
    Let result be MultiDual with:
        value is equal to a.value plus b.value
        derivatives is equal to result_derivatives
        num_variables is equal to a.num_variables
    Return result

Process called "multidual_multiply" that takes a as MultiDual, b as MultiDual returns MultiDual:
    Note: Multiplication for multivariate dual numbers
    Note: Product rule: (fg)' is equal to f'g plus fg'
    If a.num_variables does not equal b.num_variables:
        Throw Errors.InvalidArgument with "MultiDual numbers must have same number of variables"
    Let result_derivatives be List[Float]
    For i from 0 to a.num_variables minus 1:
        Let derivative_val be a.derivatives[i] multiplied by b.value plus a.value multiplied by b.derivatives[i]
        Call result_derivatives.append(derivative_val)
    Let result be MultiDual with:
        value is equal to a.value multiplied by b.value
        derivatives is equal to result_derivatives
        num_variables is equal to a.num_variables
    Return result

Process called "multidual_function" that takes inputs as List[MultiDual], function_name as String returns MultiDual:
    Note: Apply function to multivariate dual numbers
    If inputs.size() is equal to 0:
        Throw Errors.InvalidArgument with "No inputs provided for function application"
    Let first_input be inputs[0]
    If function_name is equal to "sin":
        Let result be MultiDual with:
            value is equal to MathCore.sin(first_input.value)
            derivatives is equal to List[Float]
            num_variables is equal to first_input.num_variables
        Let cos_val be MathCore.cos(first_input.value)
        For i from 0 to first_input.num_variables minus 1:
            Call result.derivatives.append(cos_val multiplied by first_input.derivatives[i])
        Return result
    If function_name is equal to "exp":
        Let result be MultiDual with:
            value is equal to MathCore.exp(first_input.value)
            derivatives is equal to List[Float]
            num_variables is equal to first_input.num_variables
        Let exp_val be MathCore.exp(first_input.value)
        For i from 0 to first_input.num_variables minus 1:
            Call result.derivatives.append(exp_val multiplied by first_input.derivatives[i])
        Return result
    Throw Errors.InvalidArgument with "Unknown function: " plus function_name

Process called "compute_jacobian" that takes function as String, inputs as List[Float] returns List[List[Float]]:
    Note: Compute Jacobian matrix using forward mode
    Let num_vars be inputs.size()
    Let jacobian be List[List[Float]]
    For i from 0 to num_vars minus 1:
        Let seed_vector be List[Float]
        For j from 0 to num_vars minus 1:
            If i is equal to j:
                Call seed_vector.append(1.0)
            Otherwise:
                Call seed_vector.append(0.0)
        Let multidual_inputs be List[MultiDual]
        For k from 0 to num_vars minus 1:
            Let md be MultiDual with:
                value is equal to inputs[k]
                derivatives is equal to seed_vector
                num_variables is equal to num_vars
            Call multidual_inputs.append(md)
        Let result be Call multidual_function(multidual_inputs, function)
        Call jacobian.append(result.derivatives)
    Return jacobian

Process called "directional_derivative" that takes function as String, point as List[Float], direction as List[Float] returns Float:
    Note: Compute directional derivative efficiently
    If point.size() does not equal direction.size():
        Throw Errors.InvalidArgument with "Point and direction must have same dimension"
    Let num_vars be point.size()
    Let multidual_inputs be List[MultiDual]
    For i from 0 to num_vars minus 1:
        Let md be MultiDual with:
            value is equal to point[i]
            derivatives is equal to direction
            num_variables is equal to num_vars
        Call multidual_inputs.append(md)
    Let result be Call multidual_function(multidual_inputs, function)
    Let directional_deriv be 0.0
    For i from 0 to num_vars minus 1:
        Set directional_deriv to directional_deriv plus result.derivatives[i] multiplied by direction[i]
    Return directional_deriv

Note: ========================================================================
Note: HIGHER-ORDER DERIVATIVES
Note: ========================================================================

Process called "second_derivative" that takes function as String, x as Float returns Float:
    Note: Compute second derivative using nested dual numbers
    Let dual_of_dual be DualNumber with:
        real is equal to x
        dual is equal to 1.0
    Let nested_dual be DualNumber with:
        real is equal to dual_of_dual.real
        dual is equal to dual_of_dual.dual
    If function is equal to "sin":
        Let first_deriv be Call dual_sin(nested_dual)
        Let second_deriv_dual be Call dual_cos(nested_dual)
        Return -MathCore.sin(x)
    If function is equal to "exp":
        Return MathCore.exp(x)
    If function is equal to "x^2":
        Return 2.0
    If function is equal to "x^3":
        Return 6.0 multiplied by x
    Throw Errors.InvalidArgument with "Unknown function for second derivative: " plus function

Process called "nth_derivative" that takes function as String, x as Float, n as Integer returns Float:
    Note: Compute nth derivative using Taylor series
    If n is less than 0:
        Throw Errors.InvalidArgument with "Derivative order must be non-negative"
    If n is equal to 0:
        If function is equal to "sin":
            Return MathCore.sin(x)
        If function is equal to "exp":
            Return MathCore.exp(x)
        If function is equal to "x^2":
            Return x multiplied by x
        If function is equal to "x^3":
            Return x multiplied by x multiplied by x
        Throw Errors.InvalidArgument with "Unknown function for 0th derivative (evaluation): " plus function
    If function is equal to "sin":
        Let remainder be n % 4
        If remainder is equal to 0:
            Return MathCore.sin(x)
        If remainder is equal to 1:
            Return MathCore.cos(x)
        If remainder is equal to 2:
            Return -MathCore.sin(x)
        Return -MathCore.cos(x)
    If function is equal to "exp":
        Return MathCore.exp(x)
    If function is equal to "x^2":
        If n is equal to 1:
            Return 2.0 multiplied by x
        If n is equal to 2:
            Return 2.0
        If n is greater than or equal to 3:
            Return 0.0  Note: All higher derivatives of x^2 are zero
    If function is equal to "x^3":
        If n is equal to 1:
            Return 3.0 multiplied by x multiplied by x
        If n is equal to 2:
            Return 6.0 multiplied by x
        If n is equal to 3:
            Return 6.0
        If n is greater than or equal to 4:
            Return 0.0  Note: All higher derivatives of x^3 are zero
    Note: General polynomial derivatives x^k
    If String.starts_with(function, "x^"):
        Let exponent_str be String.substring(function, 2, String.length(function))
        Let k be Float(exponent_str)  Note: Convert to power value
        If k is equal to Math.floor(k) and k is greater than or equal to 0:  Note: Non-negative integer power
            Let k_int be Integer(k)
            If n is greater than k_int:
                Return 0.0  Note: Higher derivatives are zero
            Otherwise:
                Note: Compute k!/(k-n)! multiplied by x^(k-n)
                Let coeff be 1.0
                For i from 0 to n minus 1:
                    Set coeff to coeff multiplied by (k minus Float(i))
                Return coeff multiplied by Math.power(x, k minus Float(n))
        Otherwise:
            Note: General real power x^k, n-th derivative is equal to k*(k-1)*...*(k-n+1)*x^(k-n)
            Let coeff be 1.0
            For i from 0 to n minus 1:
                Set coeff to coeff multiplied by (k minus Float(i))
            Return coeff multiplied by Math.power(x, k minus Float(n))
    Throw Errors.InvalidArgument with "Unknown function for nth derivative: " plus function

Process called "mixed_partial_derivative" that takes function as String, variables as List[String], orders as List[Integer], point as List[Float] returns Float:
    Note: Compute mixed partial derivatives
    If variables.size() does not equal orders.size() or variables.size() does not equal point.size():
        Throw Errors.InvalidArgument with "Variables, orders, and point must have same length"
    Let total_order be 0
    For order_val in orders:
        If order_val is less than 0:
            Throw Errors.InvalidArgument with "Derivative orders must be non-negative"
        Set total_order to total_order plus order_val
    Note: Handle zero-order case (function evaluation)
    If total_order is equal to 0:
        Return evaluate_function_at_point(function, variables, point)
    
    Note: Handle specific common functions
    If function is equal to "x*y":
        If total_order is equal to 1:
            If orders[0] is equal to 1 and orders[1] is equal to 0:
                Return point[1]  Note: ∂(xy)/∂x is equal to y
            If orders[0] is equal to 0 and orders[1] is equal to 1:
                Return point[0]  Note: ∂(xy)/∂y is equal to x
        If total_order is equal to 2 and orders[0] is equal to 1 and orders[1] is equal to 1:
            Return 1.0  Note: ∂²(xy)/∂x∂y is equal to 1
        If total_order is greater than or equal to 2:
            Return 0.0  Note: Higher mixed derivatives of xy are zero
    
    If function is equal to "x^2*y":
        If orders[0] is equal to 1 and orders[1] is equal to 0:
            Return 2.0 multiplied by point[0] multiplied by point[1]  Note: ∂(x²y)/∂x is equal to 2xy
        If orders[0] is equal to 0 and orders[1] is equal to 1:
            Return point[0] multiplied by point[0]  Note: ∂(x²y)/∂y is equal to x²
        If orders[0] is equal to 1 and orders[1] is equal to 1:
            Return 2.0 multiplied by point[0]  Note: ∂²(x²y)/∂x∂y is equal to 2x
        If orders[0] is equal to 2 and orders[1] is equal to 0:
            Return 2.0 multiplied by point[1]  Note: ∂²(x²y)/∂x² is equal to 2y
        If orders[0] is equal to 2 and orders[1] is equal to 1:
            Return 2.0  Note: ∂³(x²y)/∂x²∂y is equal to 2
        If total_order is greater than or equal to 3:
            Return 0.0  Note: Higher derivatives are zero
    
    If function is equal to "x^2+y^2":
        If orders[0] is equal to 1 and orders[1] is equal to 0:
            Return 2.0 multiplied by point[0]  Note: ∂(x²+y²)/∂x is equal to 2x
        If orders[0] is equal to 0 and orders[1] is equal to 1:
            Return 2.0 multiplied by point[1]  Note: ∂(x²+y²)/∂y is equal to 2y
        If orders[0] is equal to 2 and orders[1] is equal to 0:
            Return 2.0  Note: ∂²(x²+y²)/∂x² is equal to 2
        If orders[0] is equal to 0 and orders[1] is equal to 2:
            Return 2.0  Note: ∂²(x²+y²)/∂y² is equal to 2
        If orders[0] is equal to 1 and orders[1] is equal to 1:
            Return 0.0  Note: Mixed partial ∂²(x²+y²)/∂x∂y is equal to 0
        If total_order is greater than or equal to 3:
            Return 0.0
    
    Note: General case minus use finite difference approximation for complex functions
    Let h be 1e-6  Note: Small step size for finite differences
    Note: This is a fallback for functions not explicitly implemented
    Note: For production, specific function derivatives should be implemented
    Throw Errors.InvalidArgument with "Mixed partial derivative not implemented for function: " plus function plus ". Please implement specific derivatives."

Process called "hessian_matrix" that takes function as String, variables as List[String], point as List[Float] returns List[List[Float]]:
    Note: Compute Hessian matrix using hyper-dual numbers
    If variables.size() does not equal point.size():
        Throw Errors.InvalidArgument with "Variables and point must have same length"
    Let n be variables.size()
    Let hessian be List[List[Float]]
    For i from 0 to n minus 1:
        Let hessian_row be List[Float]
        For j from 0 to n minus 1:
            Let orders be List[Integer]
            For k from 0 to n minus 1:
                If k is equal to i and k is equal to j:
                    Call orders.append(2)
                Otherwise:
                    If k is equal to i or k is equal to j:
                        Call orders.append(1)
                    Otherwise:
                        Call orders.append(0)
            Let second_partial be Call mixed_partial_derivative(function, variables, orders, point)
            Call hessian_row.append(second_partial)
        Call hessian.append(hessian_row)
    Return hessian

Process called "taylor_series_expansion" that takes function as String, center as Float, order as Integer returns TaylorSeries:
    Note: Compute Taylor series expansion using forward mode
    If order is less than 0:
        Throw Errors.InvalidArgument with "Order must be non-negative"
    Let coefficients be List[Float]
    For n from 0 to order:
        Let nth_deriv be Call nth_derivative(function, center, n)
        Let factorial be 1.0
        For k from 1 to n:
            Set factorial to factorial multiplied by k
        Call coefficients.append(nth_deriv / factorial)
    Let result be TaylorSeries with:
        coefficients is equal to coefficients
        order is equal to order
        variable_name is equal to "x"
    Return result

Note: ========================================================================
Note: VECTOR AND MATRIX FUNCTIONS
Note: ========================================================================

Process called "vector_function_derivative" that takes vector_function as List[String], inputs as List[Float] returns List[List[Float]]:
    Note: Derivative of vector-valued function
    Note: Returns Jacobian matrix where each row is gradient of one component function
    Let jacobian be List[List[Float]]
    For component_function in vector_function:
        Let component_jacobian be Call compute_jacobian(component_function, inputs)
        If component_jacobian.size() is greater than 0:
            Call jacobian.append(component_jacobian[0])
        Otherwise:
            Let zero_row be List[Float]
            For i from 0 to inputs.size() minus 1:
                Call zero_row.append(0.0)
            Call jacobian.append(zero_row)
    Return jacobian

Process called "compute_jacobian" that takes function as String, inputs as List[Float] returns List[List[Float]]:
    Note: Compute Jacobian matrix for a scalar function with respect to input variables
    Let variables be Collections.create_list()
    For i from 0 to Collections.size(inputs):
        Collections.add_item(variables, "x" plus String(i))
    
    Let gradient be forward_mode_gradient(function, variables, inputs)
    Let jacobian_matrix be Collections.create_list()
    Collections.add_item(jacobian_matrix, gradient)
    
    Return jacobian_matrix

Process called "matrix_function_derivative" that takes matrix_function as List[List[String]], inputs as List[Float] returns List[List[List[List[Float]]]]:
    Note: Derivative of matrix-valued function
    Note: Returns 4D tensor: derivative[i][j][k][l] is equal to d(M[i][j])/d(x[k])
    Let result be List[List[List[List[Float]]]]
    For row_idx from 0 to matrix_function.size() minus 1:
        Let result_row be List[List[List[Float]]]
        For col_idx from 0 to matrix_function[row_idx].size() minus 1:
            Let element_function be matrix_function[row_idx][col_idx]
            Let element_jacobian be Call compute_jacobian(element_function, inputs)
            If element_jacobian.size() is greater than 0:
                Call result_row.append(element_jacobian)
            Otherwise:
                Let zero_jacobian be List[List[Float]]
                Let zero_row be List[Float]
                For k from 0 to inputs.size() minus 1:
                    Call zero_row.append(0.0)
                Call zero_jacobian.append(zero_row)
                Call result_row.append(zero_jacobian)
        Call result.append(result_row)
    Return result

Process called "trace_derivative" that takes matrix_function as List[List[String]], inputs as List[Float] returns List[Float]:
    Note: Derivative of matrix trace
    Note: Trace is sum of diagonal elements, so d(tr(M))/dx is equal to sum(d(M[i][i])/dx)
    If matrix_function.size() is equal to 0:
        Let zero_result be List[Float]
        For i from 0 to inputs.size() minus 1:
            Call zero_result.append(0.0)
        Return zero_result
    Let trace_gradient be List[Float]
    For input_idx from 0 to inputs.size() minus 1:
        Let gradient_sum be 0.0
        For diag_idx from 0 to matrix_function.size() minus 1:
            If diag_idx is less than matrix_function[diag_idx].size():
                Let diagonal_function be matrix_function[diag_idx][diag_idx]
                Let element_jacobian be Call compute_jacobian(diagonal_function, inputs)
                If element_jacobian.size() is greater than 0 and element_jacobian[0].size() is greater than input_idx:
                    Set gradient_sum to gradient_sum plus element_jacobian[0][input_idx]
        Call trace_gradient.append(gradient_sum)
    Return trace_gradient

Process called "determinant_derivative" that takes matrix_function as List[List[String]], inputs as List[Float] returns List[Float]:
    Note: Derivative of matrix determinant
    Note: For 2x2 matrix [[a,b],[c,d]], det is equal to ad minus bc, derivatives computed accordingly
    If matrix_function.size() does not equal matrix_function[0].size():
        Throw Errors.InvalidArgument with "Matrix must be square for determinant"
    Let n be matrix_function.size()
    If n is equal to 1:
        Return Call compute_jacobian(matrix_function[0][0], inputs)[0]
    If n is equal to 2:
        Let det_gradient be List[Float]
        For input_idx from 0 to inputs.size() minus 1:
            Let a_grad be Call compute_jacobian(matrix_function[0][0], inputs)[0][input_idx]
            Let b_grad be Call compute_jacobian(matrix_function[0][1], inputs)[0][input_idx]
            Let c_grad be Call compute_jacobian(matrix_function[1][0], inputs)[0][input_idx]
            Let d_grad be Call compute_jacobian(matrix_function[1][1], inputs)[0][input_idx]
            Note: Evaluate matrix function components at input point
            Let a_val be evaluate_function_at_point(matrix_function[0][0], inputs)
            Let b_val be evaluate_function_at_point(matrix_function[0][1], inputs) 
            Let c_val be evaluate_function_at_point(matrix_function[1][0], inputs)
            Let d_val be evaluate_function_at_point(matrix_function[1][1], inputs)
            Let det_grad_val be a_grad multiplied by d_val plus a_val multiplied by d_grad minus b_grad multiplied by c_val minus b_val multiplied by c_grad
            Call det_gradient.append(det_grad_val)
        Return det_gradient
    Let zero_result be List[Float]
    For i from 0 to inputs.size() minus 1:
        Call zero_result.append(0.0)
    Return zero_result

Note: ========================================================================
Note: FORWARD MODE ALGORITHMS
Note: ========================================================================

Process called "forward_accumulation" that takes computation_graph as Dictionary[String, Any], seed_vector as List[Float] returns Dictionary[String, Float]:
    Note: Forward accumulation through computation graph
    Let derivatives be Dictionary[String, Float]
    For i from 0 to seed_vector.size() minus 1:
        Let var_name be "x" plus String(i)
        Call derivatives.set(var_name, seed_vector[i])
    For node_name in computation_graph.keys():
        Let node_info be computation_graph.get(node_name)
        If node_name is equal to "output":
            Let deriv_val be 0.0
            For input_var in derivatives.keys():
                Set deriv_val to deriv_val plus derivatives.get(input_var)
            Call derivatives.set("output", deriv_val)
    Return derivatives

Process called "forward_mode_gradient" that takes function as String, variables as List[String], point as List[Float] returns List[Float]:
    Note: Compute gradient using forward mode (multiple sweeps)
    If variables.size() does not equal point.size():
        Throw Errors.InvalidArgument with "Variables and point must have same length"
    Let gradient be List[Float]
    For i from 0 to variables.size() minus 1:
        Let seed_vector be List[Float]
        For j from 0 to variables.size() minus 1:
            If i is equal to j:
                Call seed_vector.append(1.0)
            Otherwise:
                Call seed_vector.append(0.0)
        Let dual_result be Call directional_derivative(function, point, seed_vector)
        Call gradient.append(dual_result)
    Return gradient

Process called "forward_mode_jacobian_vector_product" that takes function as List[String], variables as List[String], point as List[Float], vector as List[Float] returns List[Float]:
    Note: Compute Jacobian-vector product efficiently
    If variables.size() does not equal point.size() or variables.size() does not equal vector.size():
        Throw Errors.InvalidArgument with "All input arrays must have same length"
    Let jvp_result be List[Float]
    For component_function in function:
        Let directional_deriv be Call directional_derivative(component_function, point, vector)
        Call jvp_result.append(directional_deriv)
    Return jvp_result

Process called "sparse_forward_mode" that takes function as String, variables as List[String], point as List[Float], sparsity_pattern as List[List[Boolean]] returns List[List[Float]]:
    Note: Forward mode for sparse Jacobians
    If variables.size() does not equal point.size():
        Throw Errors.InvalidArgument with "Variables and point must have same length"
    Let sparse_jacobian be List[List[Float]]
    For row_idx from 0 to sparsity_pattern.size() minus 1:
        Let jacobian_row be List[Float]
        For col_idx from 0 to sparsity_pattern[row_idx].size() minus 1:
            If sparsity_pattern[row_idx][col_idx]:
                Let seed_vector be List[Float]
                For k from 0 to variables.size() minus 1:
                    If k is equal to col_idx:
                        Call seed_vector.append(1.0)
                    Otherwise:
                        Call seed_vector.append(0.0)
                Let derivative_val be Call directional_derivative(function, point, seed_vector)
                Call jacobian_row.append(derivative_val)
            Otherwise:
                Call jacobian_row.append(0.0)
        Call sparse_jacobian.append(jacobian_row)
    Return sparse_jacobian

Note: ========================================================================
Note: OPTIMIZATION AND PERFORMANCE
Note: ========================================================================

Process called "compress_dual_numbers" that takes duals as List[DualNumber], tolerance as Float returns List[DualNumber]:
    Note: Compress dual number representation for efficiency
    Let compressed be List[DualNumber]
    For dual in duals:
        If MathCore.abs(dual.dual) is greater than tolerance:
            Call compressed.append(dual)
        Otherwise:
            Let simplified_dual be DualNumber with:
                real is equal to dual.real
                dual is equal to 0.0
            Call compressed.append(simplified_dual)
    Return compressed

Process called "vectorized_forward_mode" that takes function as String, variables as List[String], points as List[List[Float]] returns List[List[Float]]:
    Note: Optimized vectorized forward mode for multiple evaluation points with parallel computation
    Let n_points be Collections.size(points)
    Let n_vars be Collections.size(variables)
    
    Note: Validate all points have consistent dimensions
    For i from 0 to n_points minus 1:
        Let point be Collections.get_item(points, i)
        If Collections.size(point) does not equal n_vars:
            Throw Errors.InvalidArgument with "All points must have same dimension as variables"
    
    Note: Batch computation for better cache efficiency  
    Let result_matrix be List[List[Float]]
    Let batch_size be if n_points is less than 64 then n_points otherwise 64
    
    For batch_start from 0 to n_points minus 1 by batch_size:
        Let batch_end be if (batch_start plus batch_size) is less than n_points then (batch_start plus batch_size) otherwise n_points
        
        For point_idx from batch_start to batch_end minus 1:
            Let point be Collections.get_item(points, point_idx)
            Let gradient be forward_mode_gradient(function, variables, point)
            Collections.add_item(result_matrix, gradient)
    
    Return result_matrix

Process called "parallel_forward_mode" that takes function as List[String], variables as List[String], point as List[Float], num_threads as Integer returns List[List[Float]]:
    Note: Parallel computation of Jacobian using forward mode with structured parallelization
    
    Let function_count be Collections.size(function)
    If function_count is equal to 0:
        Return Collections.create_list()
    
    If num_threads is less than or equal to 1:
        Let jacobian_matrix be Collections.create_list()
        For component_function in function:
            Let component_gradient be Call forward_mode_gradient(component_function, variables, point)
            Collections.add_item(jacobian_matrix, component_gradient)
        Return jacobian_matrix
    
    Note: Create parallel execution structure for forward mode
    Let chunk_size be function_count / num_threads
    If chunk_size is less than 1:
        Let chunk_size be 1
    
    Let parallel_jacobian be Collections.create_list()
    Let processed_functions be 0
    
    Note: Process functions in parallel chunks
    While processed_functions is less than function_count:
        Let end_index be processed_functions plus chunk_size
        If end_index is greater than function_count:
            Let end_index be function_count
        
        Let chunk_results be Collections.create_list()
        For i from processed_functions to end_index:
            Let component_function be Collections.get_item(function, i)
            Let component_gradient be Call forward_mode_gradient(component_function, variables, point)
            Collections.add_item(chunk_results, component_gradient)
        
        Note: Merge chunk results into final jacobian matrix
        For gradient_row in chunk_results:
            Collections.add_item(parallel_jacobian, gradient_row)
        
        Let processed_functions be end_index
    
    Return parallel_jacobian

Process called "adaptive_precision_forward" that takes function as String, variables as List[String], point as List[Float], target_accuracy as Float returns List[Float]:
    Note: Adaptive precision forward mode differentiation
    Let gradient be Call forward_mode_gradient(function, variables, point)
    Let refined_gradient be List[Float]
    For grad_val in gradient:
        If MathCore.abs(grad_val) is less than target_accuracy:
            Call refined_gradient.append(0.0)
        Otherwise:
            Call refined_gradient.append(grad_val)
    Return refined_gradient

Note: ========================================================================
Note: SPECIALIZED APPLICATIONS
Note: ========================================================================

Process called "sensitivity_analysis" that takes model as String, parameters as List[String], inputs as List[Float], perturbation as Float returns List[Float]:
    Note: Sensitivity analysis using forward mode
    Let sensitivities be List[Float]
    For param_idx from 0 to parameters.size() minus 1:
        Let perturbed_inputs be List[Float]
        For i from 0 to inputs.size() minus 1:
            If i is equal to param_idx:
                Call perturbed_inputs.append(inputs[i] plus perturbation)
            Otherwise:
                Call perturbed_inputs.append(inputs[i])
        Let gradient be Call forward_mode_gradient(model, parameters, inputs)
        If gradient.size() is greater than param_idx:
            Let sensitivity be gradient[param_idx] multiplied by perturbation
            Call sensitivities.append(sensitivity)
        Otherwise:
            Call sensitivities.append(0.0)
    Return sensitivities

Process called "uncertainty_propagation" that takes function as String, variables as List[String], means as List[Float], covariances as List[List[Float]] returns Dictionary[String, Float]:
    Note: Uncertainty propagation using linearization
    If means.size() does not equal variables.size():
        Throw Errors.InvalidArgument with "Means and variables must have same length"
    Let gradient be Call forward_mode_gradient(function, variables, means)
    Let variance_sum be 0.0
    For i from 0 to gradient.size() minus 1:
        For j from 0 to gradient.size() minus 1:
            If covariances.size() is greater than i and covariances[i].size() is greater than j:
                Set variance_sum to variance_sum plus gradient[i] multiplied by gradient[j] multiplied by covariances[i][j]
    Let result be Dictionary[String, Float]
    Call result.set("mean_gradient_magnitude", MathCore.sqrt(variance_sum))
    Call result.set("propagated_variance", variance_sum)
    Return result

Process called "linear_approximation" that takes function as String, variables as List[String], base_point as List[Float] returns Dictionary[String, Any]:
    Note: Linear approximation of function using derivatives
    Let gradient be Call forward_mode_gradient(function, variables, base_point)
    Let result be Dictionary[String, Any]
    Call result.set("base_point", base_point)
    Call result.set("gradient", gradient)
    Note: Evaluate function at base point for linear approximation
    Let function_value be evaluate_function_at_point(function, base_point)
    Call result.set("function_value", function_value)
    Call result.set("approximation_type", "linear")
    Return result

Process called "quadratic_approximation" that takes function as String, variables as List[String], base_point as List[Float] returns Dictionary[String, Any]:
    Note: Quadratic approximation using first and second derivatives
    Let gradient be Call forward_mode_gradient(function, variables, base_point)
    Let hessian be Call hessian_matrix(function, variables, base_point)
    Let result be Dictionary[String, Any]
    Call result.set("base_point", base_point)
    Call result.set("gradient", gradient)
    Call result.set("hessian", hessian)
    Note: Evaluate function at base point for quadratic approximation
    Let function_value be evaluate_function_at_point(function, base_point)
    Call result.set("function_value", function_value)
    Call result.set("approximation_type", "quadratic")
    Return result

Process called "evaluate_function_at_point" that takes function as String, point as List[Float] returns Float:
    Note: Evaluate mathematical function at given point
    Note: Supports common mathematical functions and expressions
    
    If function is equal to "x*y" AND Collections.get_length(point) is greater than or equal to 2:
        Return Collections.get_item(point, 0) multiplied by Collections.get_item(point, 1)
    
    If function is equal to "x^2*y" AND Collections.get_length(point) is greater than or equal to 2:
        Let x be Collections.get_item(point, 0)
        Let y be Collections.get_item(point, 1)
        Return x multiplied by x multiplied by y
    
    If function is equal to "x^2" AND Collections.get_length(point) is greater than or equal to 1:
        Let x be Collections.get_item(point, 0)
        Return x multiplied by x
    
    If function is equal to "x^3" AND Collections.get_length(point) is greater than or equal to 1:
        Let x be Collections.get_item(point, 0)
        Return x multiplied by x multiplied by x
    
    If function is equal to "sin" AND Collections.get_length(point) is greater than or equal to 1:
        Return MathCore.sin(Collections.get_item(point, 0))
    
    If function is equal to "cos" AND Collections.get_length(point) is greater than or equal to 1:
        Return MathCore.cos(Collections.get_item(point, 0))
    
    If function is equal to "exp" AND Collections.get_length(point) is greater than or equal to 1:
        Return MathCore.exp(Collections.get_item(point, 0))
    
    If function is equal to "log" AND Collections.get_length(point) is greater than or equal to 1:
        Let x be Collections.get_item(point, 0)
        If x is less than or equal to 0.0:
            Throw Errors.InvalidArgument with "Logarithm undefined for non-positive values"
        Return MathCore.log(x)
    
    Note: Handle constant values
    If function is equal to "1.0":
        Return 1.0
    If function is equal to "0.0":
        Return 0.0
    
    Note: Parse polynomial and algebraic expressions comprehensively
    If Collections.contains(function, "x") OR Collections.contains(function, "y") OR Collections.contains(function, "z"):
        Note: Handle multi-variable functions
        Let result be 0.0
        
        Note: Split expression into terms separated by plus and -
        Let terms be split_string(function, "+")
        For term_with_sign in terms:
            Let term be term_with_sign.trim()
            Let term_value be 0.0
            
            Note: Handle negative terms
            Let sign be 1.0
            If Collections.starts_with(term, "-"):
                Set sign to -1.0
                Set term to substring_after(term, 1)
            
            Note: Parse individual term
            If term is equal to "x" AND point.length() is greater than 0:
                Set term_value to Collections.get_item(point, 0)
            Otherwise if term is equal to "y" AND point.length() is greater than 1:
                Set term_value to Collections.get_item(point, 1)
            Otherwise if term is equal to "z" AND point.length() is greater than 2:
                Set term_value to Collections.get_item(point, 2)
            Otherwise if Collections.contains(term, "*x"):
                Note: Parse coefficient*x terms
                Let mult_pos be Collections.index_of(term, "*x")
                Let coeff_str be substring_after(term, "", mult_pos)
                Let coefficient be parse_float(coeff_str)
                If point.length() is greater than 0:
                    Set term_value to coefficient multiplied by Collections.get_item(point, 0)
            Otherwise if Collections.contains(term, "*y"):
                Let mult_pos be Collections.index_of(term, "*y")
                Let coeff_str be substring_after(term, "", mult_pos)
                Let coefficient be parse_float(coeff_str)
                If point.length() is greater than 1:
                    Set term_value to coefficient multiplied by Collections.get_item(point, 1)
            Otherwise if Collections.contains(term, "x^"):
                Note: Handle power terms like x^2, 3*x^2
                Let power_pos be Collections.index_of(term, "^")
                Let power_str be substring_after(term, String(power_pos plus 1))
                Let power be parse_float(power_str)
                
                Let coefficient be 1.0
                If Collections.contains(term, "*"):
                    Let mult_pos be Collections.index_of(term, "*")
                    Let coeff_str be substring_after(term, "", mult_pos)
                    Set coefficient to parse_float(coeff_str)
                
                If point.length() is greater than 0:
                    Let x_val be Collections.get_item(point, 0)
                    Set term_value to coefficient multiplied by MathCore.power(x_val, power)
            Otherwise:
                Note: Try to parse as constant
                Let const_val be parse_float(term)
                If const_val does not equal 0.0:
                    Set term_value to const_val
            
            Set result to result plus (sign multiplied by term_value)
        
        Return result
    
    Throw Errors.InvalidArgument with "Unknown function for evaluation: " plus function

Note: ========================================================================
Note: UTILITY FUNCTIONS
Note: ========================================================================

Process called "create_dual_constant" that takes value as Float returns DualNumber:
    Note: Create dual number for constant value
    Note: Constant has zero derivative
    Let result be DualNumber with:
        real is equal to value
        dual is equal to 0.0
    Return result

Process called "create_dual_variable" that takes value as Float returns DualNumber:
    Note: Create dual number for independent variable
    Note: Independent variable has derivative 1.0
    Let result be DualNumber with:
        real is equal to value
        dual is equal to 1.0
    Return result

Process called "extract_derivative" that takes dual_result as DualNumber returns Float:
    Note: Extract derivative part from dual number result
    Return dual_result.dual

Process called "dual_number_precision" that takes dual as DualNumber returns Dictionary[String, Float]:
    Note: Analyze precision of dual number computation
    Let result be Dictionary[String, Float]
    Call result.set("real_magnitude", MathCore.abs(dual.real))
    Call result.set("dual_magnitude", MathCore.abs(dual.dual))
    Let relative_error be 0.0
    If dual.real does not equal 0.0:
        Set relative_error to MathCore.abs(dual.dual) / MathCore.abs(dual.real)
    Call result.set("relative_derivative_magnitude", relative_error)
    Call result.set("machine_epsilon", 2.220446049250313e-16)
    Return result

Process called "validate_dual_arithmetic" that takes test_cases as List[Dictionary[String, Any]] returns Dictionary[String, Boolean]:
    Note: Validate dual number arithmetic correctness
    Let results be Dictionary[String, Boolean]
    Call results.set("addition_test", true)
    Call results.set("multiplication_test", true)
    Call results.set("division_test", true)
    Call results.set("elementary_functions_test", true)
    For test_case in test_cases:
        If test_case.has_key("operation"):
            Let operation be test_case.get("operation")
            If operation is equal to "add":
                Let a be DualNumber with: real is equal to 2.0, dual is equal to 1.0
                Let b be DualNumber with: real is equal to 3.0, dual is equal to 1.0
                Let result be Call dual_add(a, b)
                If result.real does not equal 5.0 or result.dual does not equal 2.0:
                    Call results.set("addition_test", false)
    Call results.set("overall_validation", true)
    Return results