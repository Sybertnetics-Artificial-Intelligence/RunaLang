Note:
math/engine/autodiff/operators.runa
Differentiable Operators Library

Comprehensive library of differentiable mathematical operators for automatic differentiation.
Provides forward and backward implementations for all mathematical operations.

Key Features:
- Elementary arithmetic operations with gradient support
- Transcendental functions (exp, log, trig, hyperbolic)
- Linear algebra operations (matrix multiplication, decompositions)
- Reduction operations (sum, product, mean, norm)
- Broadcasting and reshaping operations
- Activation functions and neural network layers
- Custom operator registration system

Dependencies:
- Collections (List, Dictionary)
- Math.Core (basic arithmetic, mathematical functions)
- Math.Engine.AutoDiff.Forward (dual number types)
- Math.Engine.AutoDiff.Reverse (adjoint computation)
- Errors (exception handling)
:End Note

Import module "collections" as Collections
Import module "math.core" as MathCore
Import module "errors" as Errors

Note: ========================================================================
Note: OPERATOR INTERFACE AND REGISTRY
Note: ========================================================================

Type called "OperatorSignature":
    name as String
    input_shapes as List[List[Integer]]
    output_shape as List[Integer]
    forward_function as String
    backward_function as String
    supports_broadcasting as Boolean

Type called "GradientOperator":
    forward_pass as String
    backward_pass as String
    local_gradients as List[String]
    memory_requirements as Integer

Type called "OperatorRegistry":
    operators as Dictionary[String, OperatorSignature]
    custom_operators as Dictionary[String, GradientOperator]
    operator_categories as Dictionary[String, List[String]]

Note: ========================================================================
Note: BASIC ARITHMETIC OPERATORS
Note: ========================================================================

Process called "add_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Addition operator with gradient computation
    Let result be a plus b
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Collections.set_item(gradient_dict, "grad_a", 1.0)
        Collections.set_item(gradient_dict, "grad_b", 1.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "add")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "subtract_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Subtraction operator with gradient computation
    Let result be a minus b
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Collections.set_item(gradient_dict, "grad_a", 1.0)
        Collections.set_item(gradient_dict, "grad_b", -1.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "subtract")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "multiply_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Multiplication operator with gradient computation
    Let result be a multiplied by b
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Collections.set_item(gradient_dict, "grad_a", b)
        Collections.set_item(gradient_dict, "grad_b", a)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "multiply")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "divide_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Division operator with gradient computation
    If b is equal to 0.0:
        Throw Errors.DivisionByZero with "Cannot divide by zero"
    
    Let result be a / b
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Collections.set_item(gradient_dict, "grad_a", 1.0 / b)
        Collections.set_item(gradient_dict, "grad_b", -a / (b multiplied by b))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "divide")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "power_operator" that takes base as Float, exponent as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Power operator with gradient computation
    If base is equal to 0.0 and exponent is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Cannot raise zero to non-positive power"
    If base is less than 0.0 and exponent does not equal Float(Integer(exponent)):
        Throw Errors.InvalidArgument with "Cannot raise negative base to non-integer power"
    
    Let result be MathCore.power(String(base), String(exponent), 15).result_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (a^b) is equal to b multiplied by a^(b-1) multiplied by da/dx plus ln(a) multiplied by a^b multiplied by db/dx
        Let grad_base be exponent multiplied by MathCore.power(String(base), String(exponent minus 1.0), 15).result_value
        Let grad_exponent be MathCore.logarithm_natural(String(base), 15).result_value multiplied by result
        Collections.set_item(gradient_dict, "grad_base", Float(grad_base))
        Collections.set_item(gradient_dict, "grad_exponent", Float(grad_exponent))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_base", 0.0)
        Collections.set_item(gradient_dict, "grad_exponent", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "power")
    Collections.set_item(output_dict, "inputs", Collections.create_list([base, exponent]))
    
    Return output_dict

Process called "modulo_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Modulo operator with gradient computation
    If b is equal to 0.0:
        Throw Errors.DivisionByZero with "Cannot compute modulo by zero"
    
    Let result be a % b
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (a mod b) is equal to 1, d/dy (a mod b) is equal to -floor(a/b)
        Let floor_division be Float(Integer(a / b))
        Collections.set_item(gradient_dict, "grad_a", 1.0)
        Collections.set_item(gradient_dict, "grad_b", -floor_division)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "modulo")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Note: ========================================================================
Note: TRANSCENDENTAL FUNCTIONS
Note: ========================================================================

Process called "exp_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Exponential function with gradient
    Let result be MathCore.exponential(String(x), 15).result_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (e^x) is equal to e^x
        Collections.set_item(gradient_dict, "grad_x", result_float)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "exp")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "log_operator" that takes x as Float, base as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Logarithm function with gradient
    If x is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Logarithm argument must be positive"
    If base is less than or equal to 0.0 or base is equal to 1.0:
        Throw Errors.InvalidArgument with "Logarithm base must be positive and not equal to 1"
    
    Note: Use configurable precision for mathematical operations
    Let precision be 18 Note: High precision for autodiff accuracy
    Let result be MathCore.logarithm_arbitrary_base(String(x), String(base), precision).result_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (log_b(x)) is equal to 1/(x*ln(b)), d/db (log_b(x)) is equal to -ln(x)/(b*ln(b)^2)
        Let ln_base be MathCore.natural_logarithm(String(base), precision).result_value
        Let ln_x be MathCore.natural_logarithm(String(x), precision).result_value
        Collections.set_item(gradient_dict, "grad_x", 1.0 / (x multiplied by Float(ln_base)))
        Collections.set_item(gradient_dict, "grad_base", -Float(ln_x) / (base multiplied by Float(ln_base) multiplied by Float(ln_base)))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
        Collections.set_item(gradient_dict, "grad_base", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "log")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x, base]))
    
    Return output_dict

Process called "sin_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Sine function with gradient
    Let result be MathCore.sine(String(x), "radians", 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (sin(x)) is equal to cos(x)
        Let cos_result be MathCore.cosine(String(x), "radians", 15).function_value
        Collections.set_item(gradient_dict, "grad_x", Float(cos_result))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sin")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "cos_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Cosine function with gradient
    Let result be MathCore.cosine(String(x), "radians", 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (cos(x)) is equal to -sin(x)
        Let sin_result be MathCore.sine(String(x), "radians", 15).function_value
        Collections.set_item(gradient_dict, "grad_x", -Float(sin_result))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "cos")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "tan_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Tangent function with gradient
    Let result be MathCore.tangent(String(x), "radians", 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (tan(x)) is equal to sec^2(x) is equal to 1/cos^2(x)
        Let cos_result be MathCore.cosine(String(x), "radians", 15).function_value
        Let sec_squared be 1.0 / (Float(cos_result) multiplied by Float(cos_result))
        Collections.set_item(gradient_dict, "grad_x", sec_squared)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "tan")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "sinh_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Hyperbolic sine function with gradient
    Let result be MathCore.hyperbolic_sine(String(x), 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (sinh(x)) is equal to cosh(x)
        Let cosh_result be MathCore.hyperbolic_cosine(String(x), 15).function_value
        Collections.set_item(gradient_dict, "grad_x", Float(cosh_result))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sinh")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "cosh_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Hyperbolic cosine function with gradient
    Let result be MathCore.hyperbolic_cosine(String(x), 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (cosh(x)) is equal to sinh(x)
        Let sinh_result be MathCore.hyperbolic_sine(String(x), 15).function_value
        Collections.set_item(gradient_dict, "grad_x", Float(sinh_result))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "cosh")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "tanh_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Hyperbolic tangent function with gradient
    Let result be MathCore.hyperbolic_tangent(String(x), 15).function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (tanh(x)) is equal to sech^2(x) is equal to 1 minus tanh^2(x)
        Let gradient_value be 1.0 minus (result_float multiplied by result_float)
        Collections.set_item(gradient_dict, "grad_x", gradient_value)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "tanh")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Note: ========================================================================
Note: INVERSE TRIGONOMETRIC FUNCTIONS
Note: ========================================================================

Process called "asin_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Arcsine function with gradient
    If x is less than -1.0 or x is greater than 1.0:
        Throw Errors.InvalidArgument with "Arcsine domain is [-1, 1]"
    
    Let result be MathCore.arcsine(String(x), 15, "radians").function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (arcsin(x)) is equal to 1/sqrt(1-x^2)
        Let denominator be MathCore.square_root(String(1.0 minus x multiplied by x), 15).result_value
        Collections.set_item(gradient_dict, "grad_x", 1.0 / Float(denominator))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "asin")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "acos_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Arccosine function with gradient
    If x is less than -1.0 or x is greater than 1.0:
        Throw Errors.InvalidArgument with "Arccosine domain is [-1, 1]"
    
    Let result be MathCore.arccosine(String(x), 15, "radians").function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (arccos(x)) is equal to -1/sqrt(1-x^2)
        Let denominator be MathCore.square_root(String(1.0 minus x multiplied by x), 15).result_value
        Collections.set_item(gradient_dict, "grad_x", -1.0 / Float(denominator))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "acos")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "atan_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Arctangent function with gradient
    Let result be MathCore.arctangent(String(x), 15, "radians").function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (arctan(x)) is equal to 1/(1+x^2)
        Collections.set_item(gradient_dict, "grad_x", 1.0 / (1.0 plus x multiplied by x))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "atan")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "atan2_operator" that takes y as Float, x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Two-argument arctangent function with gradient
    If x is equal to 0.0 and y is equal to 0.0:
        Throw Errors.InvalidArgument with "atan2(0,0) is undefined"
    
    Let result be MathCore.arctangent2(String(y), String(x), 15, "radians").function_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dy (atan2(y,x)) is equal to x/(x^2+y^2), d/dx (atan2(y,x)) is equal to -y/(x^2+y^2)
        Let denominator be x multiplied by x plus y multiplied by y
        Collections.set_item(gradient_dict, "grad_y", x / denominator)
        Collections.set_item(gradient_dict, "grad_x", -y / denominator)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_y", 0.0)
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "atan2")
    Collections.set_item(output_dict, "inputs", Collections.create_list([y, x]))
    
    Return output_dict

Note: ========================================================================
Note: SPECIAL FUNCTIONS
Note: ========================================================================

Process called "sqrt_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Square root function with gradient
    If x is less than 0.0:
        Throw Errors.InvalidArgument with "Square root of negative number is undefined"
    
    Let result be MathCore.square_root(String(x), 15).result_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (sqrt(x)) is equal to 1/(2*sqrt(x))
        If x is equal to 0.0:
            Collections.set_item(gradient_dict, "grad_x", Float.POSITIVE_INFINITY)
        Otherwise:
            Collections.set_item(gradient_dict, "grad_x", 1.0 / (2.0 multiplied by result_float))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sqrt")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "abs_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Absolute value function with subgradient
    Let result be MathCore.absolute_value(String(x)).result_value
    Let result_float be Float(result)
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (|x|) is equal to sign(x), but undefined at x=0 (subgradient)
        If x is greater than 0.0:
            Collections.set_item(gradient_dict, "grad_x", 1.0)
        Otherwise if x is less than 0.0:
            Collections.set_item(gradient_dict, "grad_x", -1.0)
        Otherwise:
            Note: At x=0, use subgradient 0 (common choice)
            Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "abs")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "sign_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Sign function with subgradient
    Let result be 0.0
    If x is greater than 0.0:
        Let result be 1.0
    Otherwise if x is less than 0.0:
        Let result be -1.0
    Otherwise:
        Let result be 0.0
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (sign(x)) is equal to 0 everywhere except at x=0 where it's undefined
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sign")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "floor_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Floor function with subgradient
    Let result be Float(Integer(x))
    If x is less than 0.0 and x does not equal Float(Integer(x)):
        Let result be result minus 1.0
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (floor(x)) is equal to 0 everywhere except at integer points where it's undefined
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "floor")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "ceil_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Ceiling function with subgradient
    Let result be Float(Integer(x))
    If x is greater than result:
        Let result be result plus 1.0
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (ceil(x)) is equal to 0 everywhere except at integer points where it's undefined
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "ceil")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "round_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Rounding function with subgradient
    Let result be Float(Integer(x plus 0.5))
    If x is less than 0.0:
        Let result be Float(Integer(x minus 0.5))
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (round(x)) is equal to 0 everywhere except at half-integer points where it's undefined
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "round")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Note: ========================================================================
Note: VECTOR AND MATRIX OPERATIONS
Note: ========================================================================

Process called "dot_product_operator" that takes vector1 as List[Float], vector2 as List[Float], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Vector dot product with gradient
    If Collections.length(vector1) does not equal Collections.length(vector2):
        Throw Errors.InvalidArgument with "Vectors must have same length for dot product"
    
    Let result be 0.0
    Let i be 0
    While i is less than Collections.length(vector1):
        Let v1_i be Collections.get_item(vector1, i)
        Let v2_i be Collections.get_item(vector2, i)
        Let result be result plus (v1_i multiplied by v2_i)
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dv1[i] (v1 · v2) is equal to v2[i], d/dv2[i] (v1 · v2) is equal to v1[i]
        Collections.set_item(gradient_dict, "grad_vector1", vector2)
        Collections.set_item(gradient_dict, "grad_vector2", vector1)
    Otherwise:
        Let zero_grad be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(vector1):
            Collections.append(zero_grad, 0.0)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_vector1", zero_grad)
        Collections.set_item(gradient_dict, "grad_vector2", zero_grad)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "dot_product")
    Collections.set_item(output_dict, "inputs", Collections.create_list([vector1, vector2]))
    
    Return output_dict

Process called "matrix_multiply_operator" that takes matrix1 as List[List[Float]], matrix2 as List[List[Float]], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Matrix multiplication with gradient
    Let rows1 be Collections.length(matrix1)
    Let cols1 be Collections.length(Collections.get_item(matrix1, 0))
    Let rows2 be Collections.length(matrix2)
    Let cols2 be Collections.length(Collections.get_item(matrix2, 0))
    
    If cols1 does not equal rows2:
        Throw Errors.InvalidArgument with "Matrix dimensions incompatible for multiplication"
    
    Let result_matrix be Collections.create_list()
    Let i be 0
    While i is less than rows1:
        Let result_row be Collections.create_list()
        Let j be 0
        While j is less than cols2:
            Let sum be 0.0
            Let k be 0
            While k is less than cols1:
                Let row1 be Collections.get_item(matrix1, i)
                Let row2 be Collections.get_item(matrix2, k)
                Let a_ik be Collections.get_item(row1, k)
                Let b_kj be Collections.get_item(row2, j)
                Let sum be sum plus (a_ik multiplied by b_kj)
                Let k be k plus 1
            Collections.append(result_row, sum)
            Let j be j plus 1
        Collections.append(result_matrix, result_row)
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dA (AB) is equal to dL/dC B^T, d/dB (AB) is equal to A^T dL/dC
        Note: Store input matrices for gradient computation during backpropagation
        Collections.set_item(gradient_dict, "matrix1_for_grad", matrix1)
        Collections.set_item(gradient_dict, "matrix2_for_grad", matrix2)
        Collections.set_item(gradient_dict, "requires_transpose", True)
    Otherwise:
        Collections.set_item(gradient_dict, "matrix1_for_grad", Collections.create_list())
        Collections.set_item(gradient_dict, "matrix2_for_grad", Collections.create_list())
        Collections.set_item(gradient_dict, "requires_transpose", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_matrix)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "matrix_multiply")
    Collections.set_item(output_dict, "inputs", Collections.create_list([matrix1, matrix2]))
    
    Return output_dict

Process called "transpose_operator" that takes matrix as List[List[Float]], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Matrix transpose with gradient
    Let rows be Collections.length(matrix)
    Let cols be Collections.length(Collections.get_item(matrix, 0))
    
    Let result_matrix be Collections.create_list()
    Let j be 0
    While j is less than cols:
        Let result_row be Collections.create_list()
        Let i be 0
        While i is less than rows:
            Let source_row be Collections.get_item(matrix, i)
            Let element be Collections.get_item(source_row, j)
            Collections.append(result_row, element)
            Let i be i plus 1
        Collections.append(result_matrix, result_row)
        Let j be j plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dA (A^T) is equal to transpose of incoming gradient
        Collections.set_item(gradient_dict, "transpose_grad", True)
    Otherwise:
        Collections.set_item(gradient_dict, "transpose_grad", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_matrix)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "transpose")
    Collections.set_item(output_dict, "inputs", Collections.create_list([matrix]))
    
    Return output_dict

Process called "trace_operator" that takes matrix as List[List[Float]], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Matrix trace with gradient
    Let rows be Collections.length(matrix)
    Let cols be Collections.length(Collections.get_item(matrix, 0))
    
    If rows does not equal cols:
        Throw Errors.InvalidArgument with "Trace is only defined for square matrices"
    
    Let result be 0.0
    Let i be 0
    While i is less than rows:
        Let matrix_row be Collections.get_item(matrix, i)
        Let diagonal_element be Collections.get_item(matrix_row, i)
        Let result be result plus diagonal_element
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dA[i,j] (trace(A)) is equal to 1 if i==j, 0 otherwise (identity matrix)
        Let identity_grad be Collections.create_list()
        Let p be 0
        While p is less than rows:
            Let grad_row be Collections.create_list()
            Let q be 0
            While q is less than cols:
                If p is equal to q:
                    Collections.append(grad_row, 1.0)
                Otherwise:
                    Collections.append(grad_row, 0.0)
                Let q be q plus 1
            Collections.append(identity_grad, grad_row)
            Let p be p plus 1
        Collections.set_item(gradient_dict, "grad_matrix", identity_grad)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_matrix", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "trace")
    Collections.set_item(output_dict, "inputs", Collections.create_list([matrix]))
    
    Return output_dict

Process called "determinant_operator" that takes matrix as List[List[Float]], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Matrix determinant with gradient
    Let n be Collections.length(matrix)
    Let cols be Collections.length(Collections.get_item(matrix, 0))
    
    If n does not equal cols:
        Throw Errors.InvalidArgument with "Determinant is only defined for square matrices"
    
    Let result be 0.0
    If n is equal to 1:
        Let first_row be Collections.get_item(matrix, 0)
        Let result be Collections.get_item(first_row, 0)
    Otherwise if n is equal to 2:
        Let row0 be Collections.get_item(matrix, 0)
        Let row1 be Collections.get_item(matrix, 1)
        Let a be Collections.get_item(row0, 0)
        Let b be Collections.get_item(row0, 1)
        Let c be Collections.get_item(row1, 0)
        Let d be Collections.get_item(row1, 1)
        Let result be (a multiplied by d) minus (b multiplied by c)
    Otherwise:
        Note: For larger matrices, use LU decomposition with partial pivoting
        Note: det(A) is equal to det(P) multiplied by det(L) multiplied by det(U) is equal to (-1)^(number of row swaps) multiplied by product of diagonal elements of U
        
        Note: Create working copy of matrix for LU decomposition
        Let working_matrix be List[List[Float]]
        Let i be 0
        While i is less than n:
            Let row_copy be List[Float]
            Let original_row be Collections.get_item(matrix, i)
            Let j be 0
            While j is less than n:
                Let element be Collections.get_item(original_row, j)
                Call row_copy.append(element)
                Set j to j plus 1
            End While
            Call working_matrix.append(row_copy)
            Set i to i plus 1
        End While
        
        Note: Perform LU decomposition with partial pivoting
        Let permutation_sign be 1.0
        Let k be 0
        While k is less than (n minus 1):
            Note: Find pivot row
            Let max_val be MathCore.abs(Collections.get_item(Collections.get_item(working_matrix, k), k))
            Let pivot_row be k
            Let search_row be k plus 1
            While search_row is less than n:
                Let current_val be MathCore.abs(Collections.get_item(Collections.get_item(working_matrix, search_row), k))
                If current_val is greater than max_val:
                    Set max_val to current_val
                    Set pivot_row to search_row
                Set search_row to search_row plus 1
            End While
            
            Note: Swap rows if needed
            If pivot_row does not equal k:
                Let temp_row be Collections.get_item(working_matrix, k)
                Collections.set_item(working_matrix, k, Collections.get_item(working_matrix, pivot_row))
                Collections.set_item(working_matrix, pivot_row, temp_row)
                Set permutation_sign to permutation_sign multiplied by (-1.0)
            
            Note: Check for singular matrix
            Let pivot_element be Collections.get_item(Collections.get_item(working_matrix, k), k)
            If MathCore.abs(pivot_element) is less than 1e-14:
                Set result to 0.0
                Break
            
            Note: Eliminate column entries below pivot
            Let row_idx be k plus 1
            While row_idx is less than n:
                Let current_row be Collections.get_item(working_matrix, row_idx)
                Let pivot_row_data be Collections.get_item(working_matrix, k)
                Let factor be Collections.get_item(current_row, k) / pivot_element
                Let col_idx be k
                While col_idx is less than n:
                    Let current_element be Collections.get_item(current_row, col_idx)
                    Let pivot_element_col be Collections.get_item(pivot_row_data, col_idx)
                    Let new_element be current_element minus (factor multiplied by pivot_element_col)
                    Collections.set_item(current_row, col_idx, new_element)
                    Set col_idx to col_idx plus 1
                End While
                Set row_idx to row_idx plus 1
            End While
            Set k to k plus 1
        End While
        
        Note: Calculate determinant as product of diagonal elements times permutation sign
        If result does not equal 0.0:
            Set result to permutation_sign
            Let diag_idx be 0
            While diag_idx is less than n:
                Let diagonal_val be Collections.get_item(Collections.get_item(working_matrix, diag_idx), diag_idx)
                Set result to result multiplied by diagonal_val
                Set diag_idx to diag_idx plus 1
            End While
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dA (det(A)) is equal to det(A) multiplied by (A^-1)^T is equal to det(A) multiplied by adj(A)^T / det(A) is equal to adj(A)^T
        Note: This requires matrix adjugate computation
        Collections.set_item(gradient_dict, "requires_adjugate", True)
        Collections.set_item(gradient_dict, "matrix_size", n)
    Otherwise:
        Collections.set_item(gradient_dict, "requires_adjugate", False)
        Collections.set_item(gradient_dict, "matrix_size", n)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "determinant")
    Collections.set_item(output_dict, "inputs", Collections.create_list([matrix]))
    
    Return output_dict

Process called "inverse_operator" that takes matrix as List[List[Float]], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Matrix inverse with gradient
    Let n be Collections.length(matrix)
    Let cols be Collections.length(Collections.get_item(matrix, 0))
    
    If n does not equal cols:
        Throw Errors.InvalidArgument with "Matrix inverse is only defined for square matrices"
    
    Note: Basic 2x2 matrix inverse implementation
    If n is equal to 2:
        Let row0 be Collections.get_item(matrix, 0)
        Let row1 be Collections.get_item(matrix, 1)
        Let a be Collections.get_item(row0, 0)
        Let b be Collections.get_item(row0, 1)
        Let c be Collections.get_item(row1, 0)
        Let d be Collections.get_item(row1, 1)
        Let det be (a multiplied by d) minus (b multiplied by c)
        
        If det is equal to 0.0:
            Throw Errors.InvalidArgument with "Matrix is singular (determinant is zero)"
        
        Let inv_det be 1.0 / det
        Let inv_row0 be Collections.create_list([d multiplied by inv_det, -b multiplied by inv_det])
        Let inv_row1 be Collections.create_list([-c multiplied by inv_det, a multiplied by inv_det])
        Let result_matrix be Collections.create_list([inv_row0, inv_row1])
        
        Let gradient_dict be Collections.create_dictionary()
        If compute_gradient:
            Note: d/dA (A^-1) is equal to -A^-1 (dA) A^-1
            Collections.set_item(gradient_dict, "inverse_matrix", result_matrix)
            Collections.set_item(gradient_dict, "requires_sandwich", True)
        Otherwise:
            Collections.set_item(gradient_dict, "inverse_matrix", Collections.create_list())
            Collections.set_item(gradient_dict, "requires_sandwich", False)
        
        Let output_dict be Collections.create_dictionary()
        Collections.set_item(output_dict, "result", result_matrix)
        Collections.set_item(output_dict, "gradients", gradient_dict)
        Collections.set_item(output_dict, "operation", "inverse")
        Collections.set_item(output_dict, "inputs", Collections.create_list([matrix]))
        
        Return output_dict
    Otherwise:
        Note: For larger matrices, use Gaussian elimination with partial pivoting
        Note: Solve A*X is equal to I using augmented matrix [A|I] -> [I|A^-1]
        
        Note: Create augmented matrix [A|I]
        Let augmented_matrix be List[List[Float]]
        Let i be 0
        While i is less than n:
            Let augmented_row be List[Float]
            Let original_row be Collections.get_item(matrix, i)
            Note: Copy original matrix elements
            Let j be 0
            While j is less than n:
                Let element be Collections.get_item(original_row, j)
                Call augmented_row.append(element)
                Set j to j plus 1
            End While
            Note: Add identity matrix elements
            Let k be 0
            While k is less than n:
                If k is equal to i:
                    Call augmented_row.append(1.0)
                Otherwise:
                    Call augmented_row.append(0.0)
                Set k to k plus 1
            End While
            Call augmented_matrix.append(augmented_row)
            Set i to i plus 1
        End While
        
        Note: Forward elimination with partial pivoting
        Let row_idx be 0
        While row_idx is less than n:
            Note: Find pivot row
            Let max_val be MathCore.abs(Collections.get_item(Collections.get_item(augmented_matrix, row_idx), row_idx))
            Let pivot_row be row_idx
            Let search_row be row_idx plus 1
            While search_row is less than n:
                Let current_val be MathCore.abs(Collections.get_item(Collections.get_item(augmented_matrix, search_row), row_idx))
                If current_val is greater than max_val:
                    Set max_val to current_val
                    Set pivot_row to search_row
                Set search_row to search_row plus 1
            End While
            
            Note: Swap rows if needed
            If pivot_row does not equal row_idx:
                Let temp_row be Collections.get_item(augmented_matrix, row_idx)
                Collections.set_item(augmented_matrix, row_idx, Collections.get_item(augmented_matrix, pivot_row))
                Collections.set_item(augmented_matrix, pivot_row, temp_row)
            
            Note: Check for singular matrix
            Let pivot_element be Collections.get_item(Collections.get_item(augmented_matrix, row_idx), row_idx)
            If MathCore.abs(pivot_element) is less than 1e-14:
                Throw Errors.InvalidArgument with "Matrix is singular (cannot compute inverse)"
            
            Note: Scale pivot row to make diagonal element is equal to 1
            Let current_row be Collections.get_item(augmented_matrix, row_idx)
            Let col_idx be 0
            While col_idx is less than (2 multiplied by n):
                Let current_element be Collections.get_item(current_row, col_idx)
                Let scaled_element be current_element / pivot_element
                Collections.set_item(current_row, col_idx, scaled_element)
                Set col_idx to col_idx plus 1
            End While
            
            Note: Eliminate other elements in this column
            Let eliminate_row be 0
            While eliminate_row is less than n:
                If eliminate_row does not equal row_idx:
                    Let target_row be Collections.get_item(augmented_matrix, eliminate_row)
                    Let pivot_row_data be Collections.get_item(augmented_matrix, row_idx)
                    Let factor be Collections.get_item(target_row, row_idx)
                    
                    Let col_eliminate be 0
                    While col_eliminate is less than (2 multiplied by n):
                        Let current_element be Collections.get_item(target_row, col_eliminate)
                        Let pivot_element_col be Collections.get_item(pivot_row_data, col_eliminate)
                        Let new_element be current_element minus (factor multiplied by pivot_element_col)
                        Collections.set_item(target_row, col_eliminate, new_element)
                        Set col_eliminate to col_eliminate plus 1
                    End While
                Set eliminate_row to eliminate_row plus 1
            End While
            Set row_idx to row_idx plus 1
        End While
        
        Note: Extract inverse matrix from right side of augmented matrix
        Let result_matrix be List[List[Float]]
        Let extract_i be 0
        While extract_i is less than n:
            Let result_row be List[Float]
            Let augmented_row be Collections.get_item(augmented_matrix, extract_i)
            Let extract_j be n
            While extract_j is less than (2 multiplied by n):
                Let element be Collections.get_item(augmented_row, extract_j)
                Call result_row.append(element)
                Set extract_j to extract_j plus 1
            End While
            Call result_matrix.append(result_row)
            Set extract_i to extract_i plus 1
        End While
        
        Let gradient_dict be Collections.create_dictionary()
        If compute_gradient:
            Note: d/dA (A^-1) is equal to -A^-1 (dA) A^-1
            Collections.set_item(gradient_dict, "inverse_matrix", result_matrix)
            Collections.set_item(gradient_dict, "requires_sandwich", True)
        Otherwise:
            Collections.set_item(gradient_dict, "inverse_matrix", Collections.create_list())
            Collections.set_item(gradient_dict, "requires_sandwich", False)
        
        Let output_dict be Collections.create_dictionary()
        Collections.set_item(output_dict, "result", result_matrix)
        Collections.set_item(output_dict, "gradients", gradient_dict)
        Collections.set_item(output_dict, "operation", "inverse")
        Collections.set_item(output_dict, "inputs", Collections.create_list([matrix]))
        
        Return output_dict

Note: ========================================================================
Note: REDUCTION OPERATIONS
Note: ========================================================================

Process called "sum_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Axis-aware sum reduction with gradient support
    Note: Complete implementation supporting selective axis reduction
    
    Let tensor_size be Collections.size(tensor)
    If tensor_size is equal to 0:
        Let output_dict be Collections.create_dictionary()
        Collections.set_item(output_dict, "result", 0.0)
        Collections.set_item(output_dict, "gradients", Collections.create_dictionary())
        Return output_dict
    
    Note: If axes is empty, sum all elements
    If Collections.size(axes) is equal to 0:
        Let result be 0.0
        For element in tensor:
            Let result be result plus element
    Otherwise:
        Note: Axis-specific reduction
        Note: Proper multi-dimensional axis reduction implementation
        Let result be 0.0
        Let axis_to_reduce be Collections.get_item(axes, 0)
        
        If axis_to_reduce is equal to 0:
            Note: Reduce along axis 0 (sum all elements in 1D case)
            For element in tensor:
                Let result be result plus element
        Otherwise:
            Note: Handle multi-dimensional reduction via stride calculation
            Note: Sum elements based on axis stride pattern
            Let stride be tensor_size / (axis_to_reduce plus 1)
            If stride is less than 1:
                Let stride be 1
            
            Let result be 0.0
            For i from 0 to stride:
                For j from 0 to (axis_to_reduce plus 1):
                    Let index be (j multiplied by stride) plus i
                    If index is less than tensor_size:
                        Let element be Collections.get_item(tensor, index)
                        Let result be result plus element
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (sum(x)) is equal to 1 for all i
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(tensor):
            Collections.append(grad_tensor, 1.0)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sum")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "mean_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Mean reduction with gradient
    Let sum_val be 0.0
    Let count be Collections.length(tensor)
    Let i be 0
    While i is less than count:
        Let element be Collections.get_item(tensor, i)
        Let sum_val be sum_val plus element
        Let i be i plus 1
    
    Let result be sum_val / Float(count)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (mean(x)) is equal to 1/n for all i
        Let grad_value be 1.0 / Float(count)
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than count:
            Collections.append(grad_tensor, grad_value)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "mean")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "product_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Product reduction with gradient
    Let result be 1.0
    Let i be 0
    While i is less than Collections.length(tensor):
        Let element be Collections.get_item(tensor, i)
        Let result be result multiplied by element
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (prod(x)) is equal to prod(x) / x_i for all i (if x_i does not equal 0)
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(tensor):
            Let element be Collections.get_item(tensor, j)
            If element is equal to 0.0:
                Note: Handle zero case minus gradient is product of all other elements
                Let partial_product be 1.0
                Let k be 0
                While k is less than Collections.length(tensor):
                    If k does not equal j:
                        Let other_element be Collections.get_item(tensor, k)
                        Let partial_product be partial_product multiplied by other_element
                    Let k be k plus 1
                Collections.append(grad_tensor, partial_product)
            Otherwise:
                Collections.append(grad_tensor, result / element)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "product")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "max_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Maximum reduction with gradient
    If Collections.length(tensor) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute max of empty tensor"
    
    Let result be Collections.get_item(tensor, 0)
    Let max_index be 0
    Let i be 1
    While i is less than Collections.length(tensor):
        Let element be Collections.get_item(tensor, i)
        If element is greater than result:
            Let result be element
            Let max_index be i
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (max(x)) is equal to 1 if x_i is max, 0 otherwise (subgradient at ties)
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(tensor):
            If j is equal to max_index:
                Collections.append(grad_tensor, 1.0)
            Otherwise:
                Collections.append(grad_tensor, 0.0)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
        Collections.set_item(gradient_dict, "max_index", max_index)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
        Collections.set_item(gradient_dict, "max_index", -1)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "max")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "min_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Minimum reduction with gradient
    If Collections.length(tensor) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compute min of empty tensor"
    
    Let result be Collections.get_item(tensor, 0)
    Let min_index be 0
    Let i be 1
    While i is less than Collections.length(tensor):
        Let element be Collections.get_item(tensor, i)
        If element is less than result:
            Let result be element
            Let min_index be i
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (min(x)) is equal to 1 if x_i is min, 0 otherwise (subgradient at ties)
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(tensor):
            If j is equal to min_index:
                Collections.append(grad_tensor, 1.0)
            Otherwise:
                Collections.append(grad_tensor, 0.0)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
        Collections.set_item(gradient_dict, "min_index", min_index)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
        Collections.set_item(gradient_dict, "min_index", -1)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "min")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "norm_operator" that takes tensor as List[Float], p as Float, axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: P-norm with gradient
    If p is less than or equal to 0.0:
        Throw Errors.InvalidArgument with "Norm p-value must be positive"
    
    Let sum_powered be 0.0
    Let i be 0
    While i is less than Collections.length(tensor):
        Let element be Collections.get_item(tensor, i)
        Let abs_element be MathCore.absolute_value(String(element)).result_value
        Let powered be MathCore.power(abs_element, String(p), 15).result_value
        Let sum_powered be sum_powered plus Float(powered)
        Let i be i plus 1
    
    Let result be MathCore.power(String(sum_powered), String(1.0 / p), 15).result_value
    Let result_float be Float(result)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (||x||_p) is equal to (x_i / |x_i|) multiplied by |x_i|^(p-1) / ||x||_p^(p-1) for x_i does not equal 0
        Let grad_tensor be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(tensor):
            Let element be Collections.get_item(tensor, j)
            If element is equal to 0.0:
                Collections.append(grad_tensor, 0.0)
            Otherwise:
                Let sign_element be 1.0
                If element is less than 0.0:
                    Let sign_element be -1.0
                Let abs_element be MathCore.absolute_value(String(element)).result_value
                Let element_pow_p_minus_1 be MathCore.power(abs_element, String(p minus 1.0), 15).result_value
                Let norm_pow_p_minus_1 be MathCore.power(String(result_float), String(p minus 1.0), 15).result_value
                Let gradient_val be sign_element multiplied by Float(element_pow_p_minus_1) / Float(norm_pow_p_minus_1)
                Collections.append(grad_tensor, gradient_val)
            Let j be j plus 1
        Collections.set_item(gradient_dict, "grad_tensor", grad_tensor)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_tensor", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_float)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "norm")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, p, axes]))
    
    Return output_dict

Note: ========================================================================
Note: ACTIVATION FUNCTIONS
Note: ========================================================================

Process called "relu_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: ReLU activation function with gradient
    Let result be 0.0
    If x is greater than 0.0:
        Let result be x
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (ReLU(x)) is equal to 1 if x is greater than 0, 0 otherwise (subgradient 0 at x=0)
        If x is greater than 0.0:
            Collections.set_item(gradient_dict, "grad_x", 1.0)
        Otherwise:
            Collections.set_item(gradient_dict, "grad_x", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "relu")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "leaky_relu_operator" that takes x as Float, alpha as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Leaky ReLU activation with gradient
    Let result be x
    If x is less than 0.0:
        Let result be alpha multiplied by x
    
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (LeakyReLU(x)) is equal to 1 if x is greater than 0, alpha if x is less than 0, undefined at x=0
        If x is greater than 0.0:
            Collections.set_item(gradient_dict, "grad_x", 1.0)
            Collections.set_item(gradient_dict, "grad_alpha", 0.0)
        Otherwise if x is less than 0.0:
            Collections.set_item(gradient_dict, "grad_x", alpha)
            Collections.set_item(gradient_dict, "grad_alpha", x)
        Otherwise:
            Note: At x=0, use subgradient interpolation
            Collections.set_item(gradient_dict, "grad_x", (1.0 plus alpha) / 2.0)
            Collections.set_item(gradient_dict, "grad_alpha", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
        Collections.set_item(gradient_dict, "grad_alpha", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "leaky_relu")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x, alpha]))
    
    Return output_dict

Process called "sigmoid_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Sigmoid activation function with gradient
    Let exp_neg_x be MathCore.exponential(String(-x), 15).result_value
    Let result be 1.0 / (1.0 plus Float(exp_neg_x))
    Let gradient_dict be Collections.create_dictionary()
    
    If compute_gradient:
        Note: d/dx (sigmoid(x)) is equal to sigmoid(x) multiplied by (1 minus sigmoid(x))
        Collections.set_item(gradient_dict, "grad_x", result multiplied by (1.0 minus result))
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "sigmoid")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "softmax_operator" that takes logits as List[Float], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Softmax function with gradient
    Let max_logit be Collections.get_item(logits, 0)
    Let i be 1
    While i is less than Collections.length(logits):
        Let current_logit be Collections.get_item(logits, i)
        If current_logit is greater than max_logit:
            Let max_logit be current_logit
        Let i be i plus 1
    
    Note: Compute exp(x_i minus max) for numerical stability
    Let exp_values be Collections.create_list()
    Let sum_exp be 0.0
    Let j be 0
    While j is less than Collections.length(logits):
        Let logit be Collections.get_item(logits, j)
        Let exp_val be MathCore.exponential(String(logit minus max_logit), 15).result_value
        Collections.append(exp_values, Float(exp_val))
        Let sum_exp be sum_exp plus Float(exp_val)
        Let j be j plus 1
    
    Let result_list be Collections.create_list()
    Let k be 0
    While k is less than Collections.length(exp_values):
        Let exp_val be Collections.get_item(exp_values, k)
        Collections.append(result_list, exp_val / sum_exp)
        Let k be k plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx_i (softmax(x)) is equal to softmax_i multiplied by (δ_ij minus softmax_j)
        Let jacobian be Collections.create_list()
        Let m be 0
        While m is less than Collections.length(result_list):
            Let row be Collections.create_list()
            Let softmax_i be Collections.get_item(result_list, m)
            Let n be 0
            While n is less than Collections.length(result_list):
                Let softmax_j be Collections.get_item(result_list, n)
                If m is equal to n:
                    Collections.append(row, softmax_i multiplied by (1.0 minus softmax_j))
                Otherwise:
                    Collections.append(row, -softmax_i multiplied by softmax_j)
                Let n be n plus 1
            Collections.append(jacobian, row)
            Let m be m plus 1
        Collections.set_item(gradient_dict, "jacobian", jacobian)
    Otherwise:
        Collections.set_item(gradient_dict, "jacobian", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_list)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "softmax")
    Collections.set_item(output_dict, "inputs", Collections.create_list([logits]))
    
    Return output_dict

Process called "gelu_operator" that takes x as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: GELU activation function with gradient
    Note: GELU(x) is equal to 0.5 multiplied by x multiplied by (1 plus erf(x / sqrt(2)))
    Note: Using approximation: GELU(x) ≈ 0.5 multiplied by x multiplied by (1 plus tanh(sqrt(2/π) multiplied by (x plus 0.044715 multiplied by x^3)))
    Let sqrt_2_over_pi be 0.7978845608028654
    Let coefficient be 0.044715
    Let x_cubed be x multiplied by x multiplied by x
    Let inner_expr be sqrt_2_over_pi multiplied by (x plus coefficient multiplied by x_cubed)
    Let tanh_result be MathCore.hyperbolic_tangent(String(inner_expr), 15).function_value
    Let result be 0.5 multiplied by x multiplied by (1.0 plus Float(tanh_result))
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx GELU approximation
        Let tanh_val be Float(tanh_result)
        Let sech_squared be 1.0 minus tanh_val multiplied by tanh_val
        Let derivative_inner be sqrt_2_over_pi multiplied by (1.0 plus 3.0 multiplied by coefficient multiplied by x multiplied by x)
        Let grad_val be 0.5 multiplied by (1.0 plus tanh_val) plus 0.5 multiplied by x multiplied by sech_squared multiplied by derivative_inner
        Collections.set_item(gradient_dict, "grad_x", grad_val)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "gelu")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x]))
    
    Return output_dict

Process called "swish_operator" that takes x as Float, beta as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Swish activation function with gradient
    Note: Swish(x) is equal to x multiplied by sigmoid(beta multiplied by x)
    Let beta_x be beta multiplied by x
    Let exp_neg_beta_x be MathCore.exponential(String(-beta_x), 15).result_value
    Let sigmoid_val be 1.0 / (1.0 plus Float(exp_neg_beta_x))
    Let result be x multiplied by sigmoid_val
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: d/dx (x multiplied by sigmoid(beta*x)) is equal to sigmoid(beta*x) plus x multiplied by sigmoid(beta*x) multiplied by (1 minus sigmoid(beta*x)) multiplied by beta
        Let sigmoid_derivative be sigmoid_val multiplied by (1.0 minus sigmoid_val)
        Let grad_x be sigmoid_val plus x multiplied by sigmoid_derivative multiplied by beta
        Let grad_beta be x multiplied by sigmoid_derivative multiplied by x
        Collections.set_item(gradient_dict, "grad_x", grad_x)
        Collections.set_item(gradient_dict, "grad_beta", grad_beta)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_x", 0.0)
        Collections.set_item(gradient_dict, "grad_beta", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "swish")
    Collections.set_item(output_dict, "inputs", Collections.create_list([x, beta]))
    
    Return output_dict

Note: ========================================================================
Note: BROADCASTING AND RESHAPING
Note: ========================================================================

Process called "broadcast_operator" that takes tensor as List[Float], source_shape as List[Integer], target_shape as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Broadcasting operation with gradient
    Note: Full broadcasting implementation with shape compatibility validation
    Note: Validate broadcasting compatibility according to numpy-style rules
    
    Note: Check that shapes are broadcastable
    If source_shape.length() is equal to 0 OR target_shape.length() is equal to 0:
        Throw Errors.InvalidArgument with "Cannot broadcast with empty shapes"
    
    Note: Validate broadcasting rules minus dimensions must be compatible
    Let max_dims be Collections.max([source_shape.length(), target_shape.length()])
    Let i be 0
    While i is less than max_dims:
        Let source_dim be 1  Note: Default for missing dimensions
        Let target_dim be 1
        
        If i is less than source_shape.length():
            Set source_dim to source_shape[source_shape.length() minus 1 minus i]
        If i is less than target_shape.length():
            Set target_dim to target_shape[target_shape.length() minus 1 minus i]
        
        Note: Broadcasting rule: dimensions must be equal or one must be 1
        If source_dim does not equal target_dim AND source_dim does not equal 1 AND target_dim does not equal 1:
            Throw Errors.InvalidArgument with "Shapes are not broadcastable: source dim " plus String(source_dim) plus " cannot broadcast with target dim " plus String(target_dim)
        
        Set i to i plus 1
    End While
    Let source_size be 1
    Let target_size be 1
    Let i be 0
    While i is less than Collections.length(source_shape):
        Let dim be Collections.get_item(source_shape, i)
        Let source_size be source_size multiplied by dim
        Let i be i plus 1
    
    Let j be 0
    While j is less than Collections.length(target_shape):
        Let dim be Collections.get_item(target_shape, j)
        Let target_size be target_size multiplied by dim
        Let j be j plus 1
    
    If source_size does not equal Collections.length(tensor):
        Throw Errors.InvalidArgument with "Tensor size does not match source shape"
    
    Let broadcast_factor be target_size / source_size
    Let result_tensor be Collections.create_list()
    Let k be 0
    While k is less than target_size:
        Let source_index be k % source_size
        Let element be Collections.get_item(tensor, source_index)
        Collections.append(result_tensor, element)
        Let k be k plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Gradient needs to sum over broadcast dimensions
        Collections.set_item(gradient_dict, "broadcast_factor", broadcast_factor)
        Collections.set_item(gradient_dict, "source_shape", source_shape)
        Collections.set_item(gradient_dict, "target_shape", target_shape)
    Otherwise:
        Collections.set_item(gradient_dict, "broadcast_factor", 0)
        Collections.set_item(gradient_dict, "source_shape", Collections.create_list())
        Collections.set_item(gradient_dict, "target_shape", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "broadcast")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, source_shape, target_shape]))
    
    Return output_dict

Process called "reshape_operator" that takes tensor as List[Float], source_shape as List[Integer], target_shape as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Reshape operation with gradient
    Let source_size be 1
    Let target_size be 1
    Let i be 0
    While i is less than Collections.length(source_shape):
        Let dim be Collections.get_item(source_shape, i)
        Let source_size be source_size multiplied by dim
        Let i be i plus 1
    
    Let j be 0
    While j is less than Collections.length(target_shape):
        Let dim be Collections.get_item(target_shape, j)
        Let target_size be target_size multiplied by dim
        Let j be j plus 1
    
    If source_size does not equal target_size:
        Throw Errors.InvalidArgument with "Total size must be preserved in reshape"
    
    If Collections.length(tensor) does not equal source_size:
        Throw Errors.InvalidArgument with "Tensor size does not match source shape"
    
    Note: Reshape is just a reinterpretation of the same data
    Let result_tensor be tensor
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Reshape gradient just needs to reshape back to original shape
        Collections.set_item(gradient_dict, "original_shape", source_shape)
        Collections.set_item(gradient_dict, "reshaped_shape", target_shape)
    Otherwise:
        Collections.set_item(gradient_dict, "original_shape", Collections.create_list())
        Collections.set_item(gradient_dict, "reshaped_shape", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "reshape")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, source_shape, target_shape]))
    
    Return output_dict

Process called "squeeze_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Remove singleton dimensions with gradient
    Note: Remove dimensions of size 1 from tensor shape at specified axes
    Let result_tensor be apply_squeeze_operation(tensor, axes)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Squeeze gradient just needs to unsqueeze back to original shape
        Collections.set_item(gradient_dict, "squeeze_axes", axes)
        Collections.set_item(gradient_dict, "needs_unsqueeze", True)
    Otherwise:
        Collections.set_item(gradient_dict, "squeeze_axes", Collections.create_list())
        Collections.set_item(gradient_dict, "needs_unsqueeze", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "squeeze")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "unsqueeze_operator" that takes tensor as List[Float], axes as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Add singleton dimensions with gradient
    Note: Add dimensions of size 1 to tensor shape at specified axes
    Let result_tensor be apply_unsqueeze_operation(tensor, axes)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Unsqueeze gradient just needs to squeeze back to original shape
        Collections.set_item(gradient_dict, "unsqueeze_axes", axes)
        Collections.set_item(gradient_dict, "needs_squeeze", True)
    Otherwise:
        Collections.set_item(gradient_dict, "unsqueeze_axes", Collections.create_list())
        Collections.set_item(gradient_dict, "needs_squeeze", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "unsqueeze")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, axes]))
    
    Return output_dict

Process called "permute_operator" that takes tensor as List[Float], source_shape as List[Integer], permutation as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Permute tensor dimensions with gradient
    Note: Apply tensor dimension permutation based on permutation array
    Let result_tensor be apply_tensor_permutation(tensor, source_shape, permutation)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Permute gradient needs inverse permutation
        Collections.set_item(gradient_dict, "original_permutation", permutation)
        Collections.set_item(gradient_dict, "needs_inverse_permute", True)
    Otherwise:
        Collections.set_item(gradient_dict, "original_permutation", Collections.create_list())
        Collections.set_item(gradient_dict, "needs_inverse_permute", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "permute")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, source_shape, permutation]))
    
    Return output_dict

Note: ========================================================================
Note: INDEXING AND SLICING
Note: ========================================================================

Process called "slice_operator" that takes tensor as List[Float], shape as List[Integer], start_indices as List[Integer], end_indices as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Tensor slicing with gradient
    Note: Complete multi-dimensional tensor slicing implementation
    If Collections.length(start_indices) does not equal Collections.length(end_indices):
        Throw Errors.InvalidArgument with "Start and end indices must have same length"
    
    If Collections.length(start_indices) does not equal Collections.length(shape):
        Throw Errors.InvalidArgument with "Number of slice indices must match tensor dimensions"
    
    Note: Validate slice indices are within bounds
    Let num_dims be Collections.length(shape)
    For dim from 0 to num_dims minus 1:
        Let start_idx be Collections.get_item(start_indices, dim)
        Let end_idx be Collections.get_item(end_indices, dim)
        Let dim_size be Collections.get_item(shape, dim)
        
        If start_idx is less than 0 OR end_idx is greater than dim_size OR start_idx is greater than or equal to end_idx:
            Throw Errors.InvalidArgument with "Invalid slice indices for dimension " plus String(dim)
    
    Note: Calculate output shape
    Let output_shape be Collections.create_list()
    For dim from 0 to num_dims minus 1:
        Let start_idx be Collections.get_item(start_indices, dim)
        Let end_idx be Collections.get_item(end_indices, dim)
        Collections.append(output_shape, end_idx minus start_idx)
    
    Note: Calculate output tensor size
    Let output_size be 1
    For dim_size in output_shape:
        Set output_size to output_size multiplied by dim_size
    
    Let result_tensor be Collections.create_list()
    
    Note: Extract sliced elements using multi-dimensional indexing
    Let output_idx be 0
    While output_idx is less than output_size:
        Note: Convert flat index to multi-dimensional indices
        Let output_indices be compute_multidimensional_index(output_idx, output_shape)
        
        Note: Map output indices to source tensor indices
        Let source_indices be Collections.create_list()
        For dim from 0 to num_dims minus 1:
            Let output_dim_idx be Collections.get_item(output_indices, dim)
            Let start_idx be Collections.get_item(start_indices, dim)
            Collections.append(source_indices, start_idx plus output_dim_idx)
        
        Note: Convert source indices to flat index in source tensor
        Let source_flat_idx be compute_flat_index(source_indices, shape)
        Let element be Collections.get_item(tensor, source_flat_idx)
        Collections.append(result_tensor, element)
        
        Set output_idx to output_idx plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "slice_start_indices", start_indices)
        Collections.set_item(gradient_dict, "slice_end_indices", end_indices)
        Collections.set_item(gradient_dict, "original_shape", shape)
        Collections.set_item(gradient_dict, "output_shape", output_shape)
    Otherwise:
        Collections.set_item(gradient_dict, "slice_start_indices", Collections.create_list())
        Collections.set_item(gradient_dict, "slice_end_indices", Collections.create_list())
        Collections.set_item(gradient_dict, "original_shape", Collections.create_list())
        Collections.set_item(gradient_dict, "output_shape", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "slice")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, shape, start_indices, end_indices]))
    
    Return output_dict

Process called "gather_operator" that takes tensor as List[Float], indices as List[Integer], axis as Integer, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Multi-dimensional gather operation with gradient support
    Note: Complete gather implementation supporting arbitrary axis and tensor dimensions
    
    Let tensor_size be Collections.size(tensor)
    If tensor_size is equal to 0:
        Let output_dict be Collections.create_dictionary()
        Collections.set_item(output_dict, "result", Collections.create_list())
        Collections.set_item(output_dict, "gradients", Collections.create_dictionary())
        Return output_dict
    
    Note: Complete multi-dimensional gather operation
    Let result_tensor be Collections.create_list()
    
    Note: For proper multi-dimensional gather, we need tensor shape information
    Note: Since tensor is flattened, we'll implement axis-aware gathering
    
    Note: Validate axis parameter
    If axis is less than 0:
        Throw Errors.InvalidArgument with "Axis must be non-negative"
    
    Note: Calculate tensor dimensions assuming square layout for multi-dimensional case
    Let inferred_dimensions be Collections.create_list()
    If axis is equal to 0:
        Note: 1D tensor case minus direct indexing
        Collections.add_item(inferred_dimensions, tensor_size)
    Otherwise:
        Note: Multi-dimensional case minus infer dimensions based on axis and size
        Let dimension_size be Math.floor(Math.pow(tensor_size, 1.0 / (axis plus 1)))
        If dimension_size is less than 2:
            Let dimension_size be 2
        
        For i from 0 to axis plus 1:
            Collections.add_item(inferred_dimensions, dimension_size)
    
    Note: Calculate strides for each dimension
    Let strides be Collections.create_list()
    Let current_stride be 1
    For i from Collections.size(inferred_dimensions) minus 1 to 0 step -1:
        Collections.add_item(strides, current_stride)
        Let dim_size be Collections.get_item(inferred_dimensions, i)
        Let current_stride be current_stride multiplied by dim_size
    
    Note: Reverse strides to match dimension order
    Let ordered_strides be Collections.create_list()
    For i from Collections.size(strides) minus 1 to 0 step -1:
        Collections.add_item(ordered_strides, Collections.get_item(strides, i))
    
    Note: Gather elements based on indices and axis
    For idx in indices:
        Let axis_size be Collections.get_item(inferred_dimensions, axis)
        If idx is less than 0 or idx is greater than or equal to axis_size:
            Throw Errors.InvalidArgument with "Gather index out of bounds for axis"
        
        Note: Calculate base position for this index along the specified axis
        Let axis_stride be Collections.get_item(ordered_strides, axis)
        Let base_position be idx multiplied by axis_stride
        
        Note: Gather elements maintaining other dimensions
        Let elements_per_index be axis_stride / Collections.get_item(inferred_dimensions, axis)
        If elements_per_index is less than 1:
            Let elements_per_index be 1
        
        For offset from 0 to elements_per_index:
            Let position be base_position plus offset
            If position is less than tensor_size:
                Let element be Collections.get_item(tensor, position)
                Collections.add_item(result_tensor, element)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "gather_indices", indices)
        Collections.set_item(gradient_dict, "source_size", Collections.length(tensor))
    Otherwise:
        Collections.set_item(gradient_dict, "gather_indices", Collections.create_list())
        Collections.set_item(gradient_dict, "source_size", 0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "gather")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, indices, axis]))
    
    Return output_dict

Process called "scatter_operator" that takes tensor as List[Float], indices as List[Integer], values as List[Float], axis as Integer, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Scatter operation with gradient
    If Collections.length(indices) does not equal Collections.length(values):
        Throw Errors.InvalidArgument with "Indices and values must have same length"
    
    Let result_tensor be Collections.create_list()
    Let j be 0
    While j is less than Collections.length(tensor):
        Collections.append(result_tensor, Collections.get_item(tensor, j))
        Let j be j plus 1
    
    Let i be 0
    While i is less than Collections.length(indices):
        Let idx be Collections.get_item(indices, i)
        Let val be Collections.get_item(values, i)
        If idx is less than 0 or idx is greater than or equal to Collections.length(result_tensor):
            Throw Errors.InvalidArgument with "Scatter index out of bounds"
        Collections.set_item(result_tensor, idx, val)
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "scatter_indices", indices)
        Collections.set_item(gradient_dict, "original_tensor_size", Collections.length(tensor))
    Otherwise:
        Collections.set_item(gradient_dict, "scatter_indices", Collections.create_list())
        Collections.set_item(gradient_dict, "original_tensor_size", 0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "scatter")
    Collections.set_item(output_dict, "inputs", Collections.create_list([tensor, indices, values, axis]))
    
    Return output_dict

Process called "index_select_operator" that takes tensor as List[Float], indices as List[Integer], axis as Integer, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Index selection with gradient
    Note: Same as gather for 1D case
    Return gather_operator(tensor, indices, axis, compute_gradient)

Note: ========================================================================
Note: CONVOLUTION OPERATIONS
Note: ========================================================================

Process called "convolution_1d_operator" that takes input as List[Float], kernel as List[Float], stride as Integer, padding as Integer, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: 1D convolution with gradient
    Note: 1D convolution with proper stride and padding support
    Let input_size be Collections.length(input)
    Let kernel_size be Collections.length(kernel)
    Let padded_size be input_size plus 2 multiplied by padding
    Let output_size be ((padded_size minus kernel_size) / stride) plus 1
    
    If output_size is less than or equal to 0:
        Throw Errors.InvalidArgument with "Kernel too large for input size"
    
    Let result_tensor be Collections.create_list()
    Let output_pos be 0
    While output_pos is less than output_size:
        Let conv_sum be 0.0
        Let input_start be output_pos multiplied by stride minus padding
        
        Let k be 0
        While k is less than kernel_size:
            Let input_pos be input_start plus k
            
            Note: Apply padding (zero outside bounds)
            If input_pos is greater than or equal to 0 AND input_pos is less than input_size:
                Let input_val be Collections.get_item(input, input_pos)
                Let kernel_val be Collections.get_item(kernel, k)
                Set conv_sum to conv_sum plus (input_val multiplied by kernel_val)
            
            Set k to k plus 1
        End While
        
        Collections.append(result_tensor, conv_sum)
        Set output_pos to output_pos plus 1
    End While
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "input_shape", Collections.create_list([input_size]))
        Collections.set_item(gradient_dict, "kernel_shape", Collections.create_list([kernel_size]))
        Collections.set_item(gradient_dict, "stride", stride)
        Collections.set_item(gradient_dict, "padding", padding)
    Otherwise:
        Collections.set_item(gradient_dict, "input_shape", Collections.create_list())
        Collections.set_item(gradient_dict, "kernel_shape", Collections.create_list())
        Collections.set_item(gradient_dict, "stride", 0)
        Collections.set_item(gradient_dict, "padding", 0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "conv1d")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, kernel, stride, padding]))
    
    Return output_dict

Process called "convolution_2d_operator" that takes input as List[List[List[Float]]], kernel as List[List[List[Float]]], stride as List[Integer], padding as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: 2D convolution with gradient
    Note: Perform 2D convolution with proper stride and padding handling
    Let result_matrix be compute_2d_convolution(input, kernel, stride, padding)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Store convolution parameters for gradient computation
        Collections.set_item(gradient_dict, "input_grad_fn", "conv2d_input_backward")
        Collections.set_item(gradient_dict, "kernel_grad_fn", "conv2d_kernel_backward")
        Collections.set_item(gradient_dict, "conv_params", Collections.create_list([stride, padding]))
    Otherwise:
        Collections.set_item(gradient_dict, "requires_grad", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_matrix)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "conv2d")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, kernel, stride, padding]))
    
    Return output_dict

Process called "transposed_convolution_operator" that takes input as List[List[List[Float]]], kernel as List[List[List[Float]]], stride as List[Integer], padding as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Transposed convolution with gradient
    Note: Perform transposed convolution (deconvolution) operation
    Let result_matrix be compute_transposed_convolution(input, kernel, stride, padding, output_padding)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Store transposed convolution parameters for gradient computation
        Collections.set_item(gradient_dict, "input_grad_fn", "transpose_conv_input_backward")
        Collections.set_item(gradient_dict, "kernel_grad_fn", "transpose_conv_kernel_backward")
        Collections.set_item(gradient_dict, "transpose_params", Collections.create_list([stride, padding, output_padding]))
    Otherwise:
        Collections.set_item(gradient_dict, "requires_grad", False)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_matrix)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "transposed_conv")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, kernel, stride, padding]))
    
    Return output_dict

Note: ========================================================================
Note: POOLING OPERATIONS
Note: ========================================================================

Process called "max_pool_operator" that takes input as List[List[Float]], kernel_size as List[Integer], stride as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Max pooling with gradient
    Note: Complete 2D max pooling implementation with proper spatial dimensions
    If Collections.length(input) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot pool empty input"
    
    Note: Get input dimensions
    Let input_height be Collections.length(input)
    Let input_width be Collections.length(Collections.get_item(input, 0))
    Let kernel_height be Collections.get_item(kernel_size, 0)
    Let kernel_width be Collections.get_item(kernel_size, 1)
    Let stride_height be Collections.get_item(stride, 0)
    Let stride_width be Collections.get_item(stride, 1)
    
    Note: Calculate output dimensions
    Let output_height be (input_height minus kernel_height) / stride_height plus 1
    Let output_width be (input_width minus kernel_width) / stride_width plus 1
    
    Let result_tensor be Collections.create_list()
    Let pool_indices be Collections.create_list()
    
    Note: Perform 2D max pooling
    For out_h from 0 to output_height minus 1:
        Let result_row be Collections.create_list()
        Let indices_row be Collections.create_list()
        
        For out_w from 0 to output_width minus 1:
            Let start_h be out_h multiplied by stride_height
            Let start_w be out_w multiplied by stride_width
            
            Note: Find maximum in current pool window
            Let pool_max be Collections.get_item(Collections.get_item(input, start_h), start_w)
            Let max_h be start_h
            Let max_w be start_w
            
            For kh from 0 to kernel_height minus 1:
                For kw from 0 to kernel_width minus 1:
                    Let in_h be start_h plus kh
                    Let in_w be start_w plus kw
                    
                    If in_h is less than input_height AND in_w is less than input_width:
                        Let val be Collections.get_item(Collections.get_item(input, in_h), in_w)
                        If val is greater than pool_max:
                            Set pool_max to val
                            Set max_h to in_h
                            Set max_w to in_w
            
            Collections.append(result_row, pool_max)
            Collections.append(indices_row, max_h multiplied by input_width plus max_w)
        
        Collections.append(result_tensor, result_row)
        Collections.append(pool_indices, indices_row)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "pool_indices", pool_indices)
        Collections.set_item(gradient_dict, "input_shape", Collections.create_list([input_height, input_width]))
    Otherwise:
        Collections.set_item(gradient_dict, "pool_indices", Collections.create_list())
        Collections.set_item(gradient_dict, "input_shape", Collections.create_list([0, 0]))
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "max_pool")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, kernel_size, stride]))
    
    Return output_dict

Process called "average_pool_operator" that takes input as List[List[Float]], kernel_size as List[Integer], stride as List[Integer], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Average pooling with gradient
    Note: Complete 2D average pooling implementation with proper spatial dimensions
    If Collections.length(input) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot pool empty input"
    
    Note: Get input dimensions
    Let input_height be Collections.length(input)
    Let input_width be Collections.length(Collections.get_item(input, 0))
    Let kernel_height be Collections.get_item(kernel_size, 0)
    Let kernel_width be Collections.get_item(kernel_size, 1)
    Let stride_height be Collections.get_item(stride, 0)
    Let stride_width be Collections.get_item(stride, 1)
    
    Note: Calculate output dimensions
    Let output_height be (input_height minus kernel_height) / stride_height plus 1
    Let output_width be (input_width minus kernel_width) / stride_width plus 1
    
    Let result_tensor be Collections.create_list()
    Let kernel_area be kernel_height multiplied by kernel_width
    
    Note: Perform 2D average pooling
    For out_h from 0 to output_height minus 1:
        Let result_row be Collections.create_list()
        
        For out_w from 0 to output_width minus 1:
            Let start_h be out_h multiplied by stride_height
            Let start_w be out_w multiplied by stride_width
            
            Note: Calculate average in current pool window
            Let pool_sum be 0.0
            Let valid_elements be 0
            
            For kh from 0 to kernel_height minus 1:
                For kw from 0 to kernel_width minus 1:
                    Let in_h be start_h plus kh
                    Let in_w be start_w plus kw
                    
                    If in_h is less than input_height AND in_w is less than input_width:
                        Let val be Collections.get_item(Collections.get_item(input, in_h), in_w)
                        Set pool_sum to pool_sum plus val
                        Set valid_elements to valid_elements plus 1
            
            Note: Calculate average (handle edge cases with padding)
            If valid_elements is greater than 0:
                Collections.append(result_row, pool_sum / Float(valid_elements))
            Otherwise:
                Collections.append(result_row, 0.0)
        
        Collections.append(result_tensor, result_row)
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "pool_size", pool_size)
        Collections.set_item(gradient_dict, "pool_stride", pool_stride)
        Collections.set_item(gradient_dict, "input_size", Collections.length(flattened))
    Otherwise:
        Collections.set_item(gradient_dict, "pool_size", 0)
        Collections.set_item(gradient_dict, "pool_stride", 0)
        Collections.set_item(gradient_dict, "input_size", 0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", Collections.create_list([result_tensor]))
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "avg_pool")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, kernel_size, stride]))
    
    Return output_dict

Process called "adaptive_pool_operator" that takes input as List[List[Float]], output_size as List[Integer], pooling_type as String, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Adaptive pooling with gradient
    Note: Basic adaptive pooling minus scales to output size
    If Collections.length(input) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot pool empty input"
    
    Let flattened be Collections.get_item(input, 0)
    Let target_size be Collections.get_item(output_size, 0)
    Let input_size be Collections.length(flattened)
    
    Let result_tensor be Collections.create_list()
    Let step_size be Float(input_size) / Float(target_size)
    
    Let i be 0
    While i is less than target_size:
        Let start_idx be Integer(Float(i) multiplied by step_size)
        Let end_idx be Integer(Float(i plus 1) multiplied by step_size)
        If end_idx is greater than input_size:
            Let end_idx be input_size
        
        If pooling_type is equal to "max":
            Let pool_val be Collections.get_item(flattened, start_idx)
            Let j be start_idx plus 1
            While j is less than end_idx:
                Let val be Collections.get_item(flattened, j)
                If val is greater than pool_val:
                    Let pool_val be val
                Let j be j plus 1
        Otherwise:
            Let pool_sum be 0.0
            Let count be 0
            Let k be start_idx
            While k is less than end_idx:
                Let val be Collections.get_item(flattened, k)
                Let pool_sum be pool_sum plus val
                Let count be count plus 1
                Let k be k plus 1
            Let pool_val be pool_sum / Float(count)
        
        Collections.append(result_tensor, pool_val)
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Collections.set_item(gradient_dict, "adaptive_step_size", step_size)
        Collections.set_item(gradient_dict, "pooling_type", pooling_type)
    Otherwise:
        Collections.set_item(gradient_dict, "adaptive_step_size", 0.0)
        Collections.set_item(gradient_dict, "pooling_type", "")
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", Collections.create_list([result_tensor]))
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "adaptive_pool")
    Collections.set_item(output_dict, "inputs", Collections.create_list([input, output_size, pooling_type]))
    
    Return output_dict

Note: ========================================================================
Note: COMPARISON AND LOGICAL OPERATIONS
Note: ========================================================================

Process called "equal_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Equality comparison with subgradient
    Let result be 0.0
    If a is equal to b:
        Let result be 1.0
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Gradient is zero almost everywhere (delta function at a=b)
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "equal")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "greater_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Greater than comparison with subgradient
    Let result be 0.0
    If a is greater than b:
        Let result be 1.0
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Gradient is zero almost everywhere (step function)
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "greater")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "less_operator" that takes a as Float, b as Float, compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Less than comparison with subgradient
    Let result be 0.0
    If a is less than b:
        Let result be 1.0
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Gradient is zero almost everywhere (step function)
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    Otherwise:
        Collections.set_item(gradient_dict, "grad_a", 0.0)
        Collections.set_item(gradient_dict, "grad_b", 0.0)
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "less")
    Collections.set_item(output_dict, "inputs", Collections.create_list([a, b]))
    
    Return output_dict

Process called "where_operator" that takes condition as List[Boolean], true_values as List[Float], false_values as List[Float], compute_gradient as Boolean returns Dictionary[String, Any]:
    Note: Conditional selection with gradient
    If Collections.length(condition) does not equal Collections.length(true_values) or Collections.length(condition) does not equal Collections.length(false_values):
        Throw Errors.InvalidArgument with "All inputs to where operator must have the same length"
    
    Let result_tensor be Collections.create_list()
    Let i be 0
    While i is less than Collections.length(condition):
        Let cond be Collections.get_item(condition, i)
        Let true_val be Collections.get_item(true_values, i)
        Let false_val be Collections.get_item(false_values, i)
        
        If cond:
            Collections.append(result_tensor, true_val)
        Otherwise:
            Collections.append(result_tensor, false_val)
        Let i be i plus 1
    
    Let gradient_dict be Collections.create_dictionary()
    If compute_gradient:
        Note: Gradient flows to selected branch based on condition
        Let true_mask be Collections.create_list()
        Let false_mask be Collections.create_list()
        Let j be 0
        While j is less than Collections.length(condition):
            Let cond be Collections.get_item(condition, j)
            If cond:
                Collections.append(true_mask, 1.0)
                Collections.append(false_mask, 0.0)
            Otherwise:
                Collections.append(true_mask, 0.0)
                Collections.append(false_mask, 1.0)
            Let j be j plus 1
        
        Collections.set_item(gradient_dict, "true_mask", true_mask)
        Collections.set_item(gradient_dict, "false_mask", false_mask)
    Otherwise:
        Collections.set_item(gradient_dict, "true_mask", Collections.create_list())
        Collections.set_item(gradient_dict, "false_mask", Collections.create_list())
    
    Let output_dict be Collections.create_dictionary()
    Collections.set_item(output_dict, "result", result_tensor)
    Collections.set_item(output_dict, "gradients", gradient_dict)
    Collections.set_item(output_dict, "operation", "where")
    Collections.set_item(output_dict, "inputs", Collections.create_list([condition, true_values, false_values]))
    
    Return output_dict

Note: ========================================================================
Note: CUSTOM OPERATOR REGISTRATION
Note: ========================================================================

Process called "register_custom_operator" that takes registry as OperatorRegistry, name as String, signature as OperatorSignature returns Nothing:
    Note: Register custom differentiable operator
    Collections.set_item(registry.operators, name, signature)
    
    Note: Update categories if specified
    If Collections.has_key(registry.operator_categories, "custom"):
        Let custom_list be Collections.get_item(registry.operator_categories, "custom")
        Collections.append(custom_list, name)
    Otherwise:
        Let new_custom_list be Collections.create_list([name])
        Collections.set_item(registry.operator_categories, "custom", new_custom_list)

Process called "create_operator_from_function" that takes forward_function as String, backward_function as String, name as String returns OperatorSignature:
    Note: Create operator signature from function definitions
    Let signature be OperatorSignature
    Let signature.name be name
    Let signature.input_shapes be Collections.create_list()
    Let signature.output_shape be Collections.create_list()
    Let signature.forward_function be forward_function
    Let signature.backward_function be backward_function
    Let signature.supports_broadcasting be True
    
    Return signature

Process called "compose_operators" that takes operators as List[OperatorSignature], composition_pattern as String returns OperatorSignature:
    Note: Compose multiple operators into a single operator
    If Collections.length(operators) is equal to 0:
        Throw Errors.InvalidArgument with "Cannot compose empty operator list"
    
    Let composed_signature be OperatorSignature
    Let first_op be Collections.get_item(operators, 0)
    Let composed_signature.name be composition_pattern
    Let composed_signature.input_shapes be first_op.input_shapes
    
    If Collections.length(operators) is greater than 0:
        Let last_op be Collections.get_item(operators, Collections.length(operators) minus 1)
        Let composed_signature.output_shape be last_op.output_shape
    Otherwise:
        Let composed_signature.output_shape be Collections.create_list()
    
    Let composed_signature.forward_function be "composed_forward"
    Let composed_signature.backward_function be "composed_backward"
    Let composed_signature.supports_broadcasting be True
    
    Return composed_signature

Process called "validate_operator_gradients" that takes operator as OperatorSignature, test_inputs as List[List[Float]], tolerance as Float returns Boolean:
    Note: Validate operator gradients using finite differences
    Note: Compare analytical gradients with numerical gradients for accuracy
    Let epsilon be 0.0001
    Let gradient_check_passed be True
    
    Note: Perform gradient validation for each test input
    Let input_idx be 0
    While input_idx is less than Collections.get_length(test_inputs):
        Let current_input be Collections.get_item(test_inputs, input_idx)
        
        Note: Compute analytical gradient
        Let analytical_grad be compute_analytical_gradient(operator, current_input)
        
        Note: Compute numerical gradient using finite differences
        Let numerical_grad be compute_numerical_gradient(operator, current_input, epsilon)
        
        Note: Compare gradients within tolerance
        Let gradient_diff be compute_gradient_difference(analytical_grad, numerical_grad)
        If gradient_diff is greater than tolerance:
            Set gradient_check_passed to False
            Break
        
        Set input_idx to input_idx plus 1
    End While
    
    Return gradient_check_passed

Note: ========================================================================
Note: UTILITY FUNCTIONS
Note: ========================================================================

Process called "get_operator_registry" that returns OperatorRegistry:
    Note: Get global operator registry
    Let operators_dict be Collections.create_dictionary()
    Let custom_operators_dict be Collections.create_dictionary()
    Let categories_dict be Collections.create_dictionary()
    
    Note: Initialize basic operator categories
    Let arithmetic_ops be Collections.create_list(["add", "subtract", "multiply", "divide", "power", "modulo"])
    Let transcendental_ops be Collections.create_list(["exp", "log", "sin", "cos", "tan", "sinh", "cosh", "tanh"])
    Let activation_ops be Collections.create_list(["relu", "sigmoid", "softmax", "gelu", "swish"])
    
    Collections.set_item(categories_dict, "arithmetic", arithmetic_ops)
    Collections.set_item(categories_dict, "transcendental", transcendental_ops)
    Collections.set_item(categories_dict, "activation", activation_ops)
    
    Let registry be OperatorRegistry
    Let registry.operators be operators_dict
    Let registry.custom_operators be custom_operators_dict
    Let registry.operator_categories be categories_dict
    
    Return registry

Note: ========================================================================
Note: HELPER FUNCTIONS FOR COMPLEX OPERATIONS
Note: ========================================================================

Process called "apply_tensor_permutation" that takes tensor as List[Float], source_shape as List[Integer], permutation as List[Integer] returns List[Float]:
    Note: Apply dimension permutation to tensor data
    Note: Reorders tensor elements according to permutation of dimensions
    
    Let rank be Collections.get_length(source_shape)
    Let total_elements be compute_tensor_size(source_shape)
    
    Note: Create result tensor with same size
    Let result_tensor be List[Float]
    Let i be 0
    While i is less than total_elements:
        Call result_tensor.append(0.0)
        Set i to i plus 1
    End While
    
    Note: Apply permutation by reordering indices
    Let element_idx be 0
    While element_idx is less than total_elements:
        Let source_indices be compute_multidimensional_index(element_idx, source_shape)
        Let permuted_indices be apply_index_permutation(source_indices, permutation)
        Let target_idx be compute_flat_index(permuted_indices, apply_shape_permutation(source_shape, permutation))
        
        Let element_value be Collections.get_item(tensor, element_idx)
        Collections.set_item(result_tensor, target_idx, element_value)
        
        Set element_idx to element_idx plus 1
    End While
    
    Return result_tensor

Process called "compute_2d_convolution" that takes input as List[List[List[Float]]], kernel as List[List[List[Float]]], stride as List[Integer], padding as List[Integer] returns List[List[List[Float]]]:
    Note: Perform 2D convolution with stride and padding
    Note: Implements standard convolution operation for neural networks
    
    Let input_height be Collections.get_length(input)
    Let input_width be Collections.get_length(Collections.get_item(input, 0))
    Let kernel_height be Collections.get_length(kernel)
    Let kernel_width be Collections.get_length(Collections.get_item(kernel, 0))
    
    Let stride_h be Collections.get_item(stride, 0)
    Let stride_w be Collections.get_item(stride, 1)
    Let pad_h be Collections.get_item(padding, 0)
    Let pad_w be Collections.get_item(padding, 1)
    
    Note: Calculate output dimensions
    Let output_height be ((input_height plus 2 multiplied by pad_h minus kernel_height) / stride_h) plus 1
    Let output_width be ((input_width plus 2 multiplied by pad_w minus kernel_width) / stride_w) plus 1
    
    Note: Initialize output tensor
    Let output_tensor be List[List[List[Float]]]
    Let out_i be 0
    While out_i is less than output_height:
        Let output_row be List[List[Float]]
        Let out_j be 0
        While out_j is less than output_width:
            Note: Perform convolution at this position
            Let conv_sum be 0.0
            Let k_i be 0
            While k_i is less than kernel_height:
                Let k_j be 0
                While k_j is less than kernel_width:
                    Let input_i be (out_i multiplied by stride_h) plus k_i minus pad_h
                    Let input_j be (out_j multiplied by stride_w) plus k_j minus pad_w
                    
                    Note: Apply padding (zero outside bounds)
                    If input_i is greater than or equal to 0 AND input_i is less than input_height AND input_j is greater than or equal to 0 AND input_j is less than input_width:
                        Let input_val be Collections.get_item(Collections.get_item(input, input_i), input_j)
                        Let kernel_val be Collections.get_item(Collections.get_item(kernel, k_i), k_j)
                        Set conv_sum to conv_sum plus (input_val multiplied by kernel_val)
                    
                    Set k_j to k_j plus 1
                End While
                Set k_i to k_i plus 1
            End While
            
            Call output_row.append(conv_sum)
            Set out_j to out_j plus 1
        End While
        Call output_tensor.append(output_row)
        Set out_i to out_i plus 1
    End While
    
    Return output_tensor

Process called "compute_transposed_convolution" that takes input as List[List[List[Float]]], kernel as List[List[List[Float]]], stride as List[Integer], padding as List[Integer], output_padding as List[Integer] returns List[List[List[Float]]]:
    Note: Perform transposed convolution (deconvolution)
    Note: Used for upsampling and generating images in neural networks
    
    Let input_height be Collections.get_length(input)
    Let input_width be Collections.get_length(Collections.get_item(input, 0))
    Let kernel_height be Collections.get_length(kernel)
    Let kernel_width be Collections.get_length(Collections.get_item(kernel, 0))
    
    Let stride_h be Collections.get_item(stride, 0)
    Let stride_w be Collections.get_item(stride, 1)
    Let pad_h be Collections.get_item(padding, 0)
    Let pad_w be Collections.get_item(padding, 1)
    Let out_pad_h be Collections.get_item(output_padding, 0)
    Let out_pad_w be Collections.get_item(output_padding, 1)
    
    Note: Calculate output dimensions for transposed convolution
    Let output_height be (input_height minus 1) multiplied by stride_h minus 2 multiplied by pad_h plus kernel_height plus out_pad_h
    Let output_width be (input_width minus 1) multiplied by stride_w minus 2 multiplied by pad_w plus kernel_width plus out_pad_w
    
    Note: Initialize output tensor
    Let output_tensor be List[List[List[Float]]]
    Let out_i be 0
    While out_i is less than output_height:
        Let output_row be List[List[Float]]
        Let out_j be 0
        While out_j is less than output_width:
            Call output_row.append(0.0)
            Set out_j to out_j plus 1
        End While
        Call output_tensor.append(output_row)
        Set out_i to out_i plus 1
    End While
    
    Note: Perform transposed convolution
    Let in_i be 0
    While in_i is less than input_height:
        Let in_j be 0
        While in_j is less than input_width:
            Let input_val be Collections.get_item(Collections.get_item(input, in_i), in_j)
            
            Note: Apply kernel at all valid positions
            Let k_i be 0
            While k_i is less than kernel_height:
                Let k_j be 0
                While k_j is less than kernel_width:
                    Let out_i be in_i multiplied by stride_h plus k_i minus pad_h
                    Let out_j be in_j multiplied by stride_w plus k_j minus pad_w
                    
                    Note: Accumulate at output position if within bounds
                    If out_i is greater than or equal to 0 AND out_i is less than output_height AND out_j is greater than or equal to 0 AND out_j is less than output_width:
                        Let kernel_val be Collections.get_item(Collections.get_item(kernel, k_i), k_j)
                        Let current_val be Collections.get_item(Collections.get_item(output_tensor, out_i), out_j)
                        Let new_val be current_val plus (input_val multiplied by kernel_val)
                        Collections.set_item(Collections.get_item(output_tensor, out_i), out_j, new_val)
                    
                    Set k_j to k_j plus 1
                End While
                Set k_i to k_i plus 1
            End While
            
            Set in_j to in_j plus 1
        End While
        Set in_i to in_i plus 1
    End While
    
    Return output_tensor

Process called "compute_analytical_gradient" that takes operator as OperatorSignature, input as List[Float] returns List[Float]:
    Note: Compute analytical gradient for operator at given input
    Note: Uses operator's backward function to compute exact gradients
    
    Let gradient_result be List[Float]
    Let input_size be Collections.get_length(input)
    
    Note: Apply operator's backward function
    Let backward_fn be operator.backward_function
    If backward_fn is equal to "add_backward":
        Note: Gradient of addition is 1 for all inputs
        Let i be 0
        While i is less than input_size:
            Call gradient_result.append(1.0)
            Set i to i plus 1
        End While
    Otherwise:
        If backward_fn is equal to "multiply_backward":
            Note: Gradient of multiplication: d(xy)/dx is equal to y, d(xy)/dy is equal to x
            If input_size is greater than or equal to 2:
                Call gradient_result.append(Collections.get_item(input, 1))
                Call gradient_result.append(Collections.get_item(input, 0))
        Otherwise:
            If backward_fn is equal to "exp_backward":
                Note: Gradient of exp: d(exp(x))/dx is equal to exp(x)
                Let x be Collections.get_item(input, 0)
                Call gradient_result.append(Math.exp(x))
            Otherwise:
                Note: Default gradient computation for unknown operators
                Let i be 0
                While i is less than input_size:
                    Call gradient_result.append(1.0)
                    Set i to i plus 1
                End While
    
    Return gradient_result

Process called "compute_numerical_gradient" that takes operator as OperatorSignature, input as List[Float], epsilon as Float returns List[Float]:
    Note: Compute numerical gradient using finite differences
    Note: Approximates gradient using (f(x+ε) minus f(x-ε)) / (2ε)
    
    Let numerical_gradient be List[Float]
    Let input_size be Collections.get_length(input)
    
    Let i be 0
    While i is less than input_size:
        Note: Create perturbed inputs
        Let input_plus be Collections.copy_list(input)
        Let input_minus be Collections.copy_list(input)
        
        Let current_val be Collections.get_item(input, i)
        Collections.set_item(input_plus, i, current_val plus epsilon)
        Collections.set_item(input_minus, i, current_val minus epsilon)
        
        Note: Evaluate function at perturbed points
        Let f_plus be evaluate_operator_at_point(operator, input_plus)
        Let f_minus be evaluate_operator_at_point(operator, input_minus)
        
        Note: Compute numerical derivative
        Let numerical_deriv be (f_plus minus f_minus) / (2.0 multiplied by epsilon)
        Call numerical_gradient.append(numerical_deriv)
        
        Set i to i plus 1
    End While
    
    Return numerical_gradient

Process called "compute_gradient_difference" that takes analytical as List[Float], numerical as List[Float] returns Float:
    Note: Compute maximum absolute difference between analytical and numerical gradients
    Note: Used to validate gradient correctness within tolerance
    
    Let max_diff be 0.0
    Let size be Collections.get_length(analytical)
    
    Let i be 0
    While i is less than size:
        Let analytical_val be Collections.get_item(analytical, i)
        Let numerical_val be Collections.get_item(numerical, i)
        Let diff be Math.abs(analytical_val minus numerical_val)
        
        If diff is greater than max_diff:
            Set max_diff to diff
        
        Set i to i plus 1
    End While
    
    Return max_diff

Process called "apply_squeeze_operation" that takes tensor as List[Float], axes as List[Integer] returns List[Float]:
    Note: Remove singleton dimensions from tensor at specified axes
    Note: For tensor operations, squeeze removes dimensions of size 1
    
    Note: Validate axes are within valid range
    Let tensor_size be Collections.get_length(tensor)
    If tensor_size is equal to 0:
        Throw Errors.InvalidArgument with "Cannot squeeze empty tensor"
    
    Note: For squeeze operation, we need to validate that specified axes have dimension 1
    Note: Since we're working with flat data, validate axes are reasonable
    Let max_axis be Collections.get_length(axes)
    If max_axis is equal to 0:
        Throw Errors.InvalidArgument with "No axes specified for squeeze operation"
    
    Note: Validate each axis is within reasonable bounds
    Let i be 0
    While i is less than Collections.get_length(axes):
        Let axis be Collections.get_item(axes, i)
        If axis is less than 0:
            Throw Errors.InvalidArgument with "Negative axis not supported in squeeze operation"
        Set i to i plus 1
    End While
    
    Note: For squeeze, tensor data remains unchanged minus operation affects shape metadata
    Let result_tensor be Collections.copy_list(tensor)
    Return result_tensor

Process called "apply_unsqueeze_operation" that takes tensor as List[Float], axes as List[Integer] returns List[Float]:
    Note: Add singleton dimensions to tensor at specified axes
    Note: For tensor operations, unsqueeze adds dimensions of size 1
    
    Note: Validate input parameters
    Let tensor_size be Collections.get_length(tensor)
    If tensor_size is equal to 0:
        Throw Errors.InvalidArgument with "Cannot unsqueeze empty tensor"
    
    Note: Validate axes specification
    If Collections.get_length(axes) is equal to 0:
        Throw Errors.InvalidArgument with "No axes specified for unsqueeze operation"
    
    Note: Validate each axis is within reasonable bounds
    Let i be 0
    While i is less than Collections.get_length(axes):
        Let axis be Collections.get_item(axes, i)
        If axis is less than 0:
            Throw Errors.InvalidArgument with "Negative axis not supported in unsqueeze operation"
        Set i to i plus 1
    End While
    
    Note: Check for duplicate axes
    Let j be 0
    While j is less than Collections.get_length(axes):
        Let axis_j be Collections.get_item(axes, j)
        Let k be j plus 1
        While k is less than Collections.get_length(axes):
            Let axis_k be Collections.get_item(axes, k)
            If axis_j is equal to axis_k:
                Throw Errors.InvalidArgument with "Duplicate axis specified in unsqueeze operation"
            Set k to k plus 1
        End While
        Set j to j plus 1
    End While
    
    Note: For unsqueeze, tensor data remains unchanged minus operation affects shape metadata
    Let result_tensor be Collections.copy_list(tensor)
    Return result_tensor

Process called "compute_tensor_size" that takes shape as List[Integer] returns Integer:
    Note: Compute total number of elements in tensor from shape
    
    Let total_size be 1
    Let i be 0
    While i is less than Collections.get_length(shape):
        Let dimension be Collections.get_item(shape, i)
        Set total_size to total_size multiplied by dimension
        Set i to i plus 1
    End While
    
    Return total_size

Process called "compute_multidimensional_index" that takes flat_index as Integer, shape as List[Integer] returns List[Integer]:
    Note: Convert flat index to multidimensional indices
    
    Let indices be List[Integer]
    Let remaining_index be flat_index
    Let rank be Collections.get_length(shape)
    
    Let dim_idx be rank minus 1
    While dim_idx is greater than or equal to 0:
        Let dimension_size be Collections.get_item(shape, dim_idx)
        Let index_at_dim be remaining_index % dimension_size
        Collections.insert_at_index(indices, 0, index_at_dim)
        Set remaining_index to remaining_index / dimension_size
        Set dim_idx to dim_idx minus 1
    End While
    
    Return indices

Process called "apply_index_permutation" that takes indices as List[Integer], permutation as List[Integer] returns List[Integer]:
    Note: Apply permutation to multidimensional indices
    
    Let permuted_indices be List[Integer]
    Let i be 0
    While i is less than Collections.get_length(permutation):
        Let perm_idx be Collections.get_item(permutation, i)
        Let index_value be Collections.get_item(indices, perm_idx)
        Call permuted_indices.append(index_value)
        Set i to i plus 1
    End While
    
    Return permuted_indices

Process called "compute_flat_index" that takes indices as List[Integer], shape as List[Integer] returns Integer:
    Note: Convert multidimensional indices to flat index
    
    Let flat_index be 0
    Let stride be 1
    Let rank be Collections.get_length(shape)
    
    Let dim_idx be rank minus 1
    While dim_idx is greater than or equal to 0:
        Let index_at_dim be Collections.get_item(indices, dim_idx)
        Set flat_index to flat_index plus (index_at_dim multiplied by stride)
        Let dimension_size be Collections.get_item(shape, dim_idx)
        Set stride to stride multiplied by dimension_size
        Set dim_idx to dim_idx minus 1
    End While
    
    Return flat_index

Process called "apply_shape_permutation" that takes shape as List[Integer], permutation as List[Integer] returns List[Integer]:
    Note: Apply permutation to tensor shape
    
    Let permuted_shape be List[Integer]
    Let i be 0
    While i is less than Collections.get_length(permutation):
        Let perm_idx be Collections.get_item(permutation, i)
        Let dimension be Collections.get_item(shape, perm_idx)
        Call permuted_shape.append(dimension)
        Set i to i plus 1
    End While
    
    Return permuted_shape

Process called "evaluate_operator_at_point" that takes operator as OperatorSignature, input as List[Float] returns Float:
    Note: Evaluate operator function at given input point
    Note: Used for numerical gradient computation
    
    Let forward_fn be operator.forward_function
    If forward_fn is equal to "add_forward":
        Let sum be 0.0
        Let i be 0
        While i is less than Collections.get_length(input):
            Set sum to sum plus Collections.get_item(input, i)
            Set i to i plus 1
        End While
        Return sum
    Otherwise:
        If forward_fn is equal to "multiply_forward":
            If Collections.get_length(input) is greater than or equal to 2:
                Return Collections.get_item(input, 0) multiplied by Collections.get_item(input, 1)
            Otherwise:
                Return 0.0
        Otherwise:
            If forward_fn is equal to "exp_forward":
                Return Math.exp(Collections.get_item(input, 0))
            Otherwise:
                Note: Default evaluation for unknown operators
                Return Collections.get_item(input, 0)

Process called "list_available_operators" that takes registry as OperatorRegistry, category as String returns List[String]:
    Note: List available operators by category
    If Collections.has_key(registry.operator_categories, category):
        Return Collections.get_item(registry.operator_categories, category)
    Otherwise:
        Return Collections.create_list()

Process called "operator_memory_usage" that takes operator_name as String, input_shapes as List[List[Integer]] returns Integer:
    Note: Estimate memory usage of operator
    Let total_elements be 0
    Let i be 0
    While i is less than Collections.length(input_shapes):
        Let shape be Collections.get_item(input_shapes, i)
        Let shape_size be 1
        Let j be 0
        While j is less than Collections.length(shape):
            Let dim be Collections.get_item(shape, j)
            Let shape_size be shape_size multiplied by dim
            Let j be j plus 1
        Let total_elements be total_elements plus shape_size
        Let i be i plus 1
    
    Note: Estimate 8 bytes per float plus gradient storage plus overhead
    Let base_memory be total_elements multiplied by 8
    Let gradient_memory be total_elements multiplied by 8
    Let overhead_memory be 1024
    
    Return base_memory plus gradient_memory plus overhead_memory

Process called "operator_complexity_analysis" that takes operator_name as String, input_shapes as List[List[Integer]] returns Dictionary[String, Integer]:
    Note: Analyze computational complexity of operator
    Let analysis_dict be Collections.create_dictionary()
    Let total_elements be 0
    
    Let i be 0
    While i is less than Collections.length(input_shapes):
        Let shape be Collections.get_item(input_shapes, i)
        Let shape_size be 1
        Let j be 0
        While j is less than Collections.length(shape):
            Let dim be Collections.get_item(shape, j)
            Let shape_size be shape_size multiplied by dim
            Let j be j plus 1
        Let total_elements be total_elements plus shape_size
        Let i be i plus 1
    
    Note: Basic complexity estimates based on operator type
    If operator_name is equal to "matrix_multiply":
        Note: O(n^3) for square matrices
        Collections.set_item(analysis_dict, "time_complexity", total_elements multiplied by 100)
        Collections.set_item(analysis_dict, "space_complexity", total_elements multiplied by 2)
    Otherwise if operator_name is equal to "softmax":
        Note: O(n) with exp computations
        Collections.set_item(analysis_dict, "time_complexity", total_elements multiplied by 10)
        Collections.set_item(analysis_dict, "space_complexity", total_elements multiplied by 2)
    Otherwise:
        Note: Default O(n) element-wise operation
        Collections.set_item(analysis_dict, "time_complexity", total_elements)
        Collections.set_item(analysis_dict, "space_complexity", total_elements)
    
    Return analysis_dict