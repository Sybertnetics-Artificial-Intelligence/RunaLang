Note: Tensor Algebra Module

This module provides comprehensive tensor algebra operations and manipulations.
Tensor algebra forms the foundation for multilinear algebra and differential geometry.

Mathematical Foundation:
- Tensor spaces: T^r_s(V) is equal to V⊗...⊗V⊗V*⊗...⊗V* (r contravariant, s covariant)
- Multilinear maps: tensors as functions T: V*×...×V*×V×...×V → ℝ
- Tensor products: (u⊗v)(α,β) is equal to u(α)v(β) for vectors u,v and forms α,β
- Contraction: trace operation reducing tensor rank by 2
- Symmetrization: T_(ij) is equal to ½(T_ij plus T_ji)
- Antisymmetrization: T_[ij] is equal to ½(T_ij minus T_ji)
- Universal property of tensor products in category theory
- Wedge products: antisymmetric tensor products ∧

Applications include linear algebra, multilinear algebra, differential geometry,
physics field theories, computer graphics, and data analysis.
:End Note

Import module "dev/debug/errors/core" as Errors
Import module "math/core/operations" as MathOps
Import module "math/engine/linalg/core" as LinAlgCore

Note: ===== Basic Tensor Operations =====

Type called "TensorSpace":
    base_vector_space_dimension as Integer
    contravariant_degree as Integer
    covariant_degree as Integer
    total_dimension as Integer
    basis_tensors as List[List[Integer]]
    scalar_field as String
    
Type called "TensorAlgebraElement":
    components as List[Float64]
    tensor_space as TensorSpace
    index_structure as List[String]
    symmetry_properties as List[String]
    basis_representation as List[Tuple[List[Integer], Float64]]

Process called "tensor_addition" that takes tensor_a as TensorAlgebraElement, tensor_b as TensorAlgebraElement returns TensorAlgebraElement:
    Note: Adds two tensors of the same type component-wise
    Note: (A plus B)^i_j is equal to A^i_j plus B^i_j for all indices
    Note: Preserves all symmetry properties of input tensors
    Note: Forms abelian group structure on tensor spaces
    
    Note: Validate tensors have compatible tensor spaces
    If tensor_a.tensor_space.base_vector_space_dimension does not equal tensor_b.tensor_space.base_vector_space_dimension:
        Throw Errors.InvalidArgument with "Tensors must have same base vector space dimension"
    
    If tensor_a.tensor_space.contravariant_degree does not equal tensor_b.tensor_space.contravariant_degree:
        Throw Errors.InvalidArgument with "Tensors must have same contravariant degree"
    
    If tensor_a.tensor_space.covariant_degree does not equal tensor_b.tensor_space.covariant_degree:
        Throw Errors.InvalidArgument with "Tensors must have same covariant degree"
    
    Note: Add corresponding components
    Let result_components be LinAlgCore.tensor_component_operation(tensor_a.components, tensor_b.components, "add")
    
    Note: Preserve symmetry properties minus intersection of both tensors' symmetries
    Let preserved_symmetries be []
    Let i be 0
    While i is less than tensor_a.symmetry_properties.length:
        Let symmetry be tensor_a.symmetry_properties.get(i)
        If tensor_b.symmetry_properties.contains(symmetry):
            preserved_symmetries.add(symmetry)
        Set i to i plus 1
    
    Note: Create result tensor
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to result_components
    Set result_tensor.tensor_space to tensor_a.tensor_space
    Set result_tensor.index_structure to tensor_a.index_structure
    Set result_tensor.symmetry_properties to preserved_symmetries
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "scalar_multiplication" that takes scalar as Float64, tensor as TensorAlgebraElement returns TensorAlgebraElement:
    Note: Multiplies tensor by scalar: (cT)^i_j is equal to c T^i_j
    Note: Preserves all symmetry and antisymmetry properties
    Note: Makes tensor spaces into vector spaces over scalar field
    Note: Commutes with all linear tensor operations
    
    Note: Convert scalar to string for high-precision arithmetic
    Let scalar_str be scalar.to_string()
    
    Note: Multiply all components by scalar
    Let result_components be LinAlgCore.scalar_multiply_tensor_components(scalar_str, tensor.components)
    
    Note: Create result tensor with preserved properties
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to result_components
    Set result_tensor.tensor_space to tensor.tensor_space
    Set result_tensor.index_structure to tensor.index_structure
    Set result_tensor.symmetry_properties to tensor.symmetry_properties
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "tensor_equality" that takes tensor_a as TensorAlgebraElement, tensor_b as TensorAlgebraElement, tolerance as Float64 returns Boolean:
    Note: Tests equality of tensors within numerical tolerance
    Note: Compares all components after canonical ordering
    Note: Accounts for symmetry relations and zero components
    Note: Essential for numerical tensor computations
    
    Note: Check tensor space compatibility first
    If tensor_a.tensor_space.base_vector_space_dimension does not equal tensor_b.tensor_space.base_vector_space_dimension:
        Return false
    
    If tensor_a.tensor_space.contravariant_degree does not equal tensor_b.tensor_space.contravariant_degree:
        Return false
    
    If tensor_a.tensor_space.covariant_degree does not equal tensor_b.tensor_space.covariant_degree:
        Return false
    
    Note: Check component count
    If tensor_a.components.length does not equal tensor_b.components.length:
        Return false
    
    Note: Compare each component within tolerance
    Let tolerance_str be tolerance.to_string()
    Let i be 0
    While i is less than tensor_a.components.length:
        Let component_a be tensor_a.components.get(i)
        Let component_b be tensor_b.components.get(i)
        
        Note: Compute absolute difference
        Let diff_result be MathOps.subtract(component_a, component_b, 50)
        Let abs_diff_result be MathOps.absolute_value(diff_result.result_value, 50)
        
        Note: Check if difference exceeds tolerance
        Let comparison_result be MathOps.compare(abs_diff_result.result_value, tolerance_str, 50)
        If comparison_result.comparison_result is equal to "greater":
            Return false
        
        Set i to i plus 1
    
    Return true

Note: ===== Tensor Product Operations =====

Process called "tensor_product" that takes tensor_a as TensorAlgebraElement, tensor_b as TensorAlgebraElement returns TensorAlgebraElement:
    Note: Computes tensor product A ⊗ B creating higher-rank tensor
    Note: (A ⊗ B)^ij_kl is equal to A^i_k B^j_l for rank-(1,1) tensors
    Note: Bilinear operation: distributive over addition
    Note: Non-commutative: generally A ⊗ B ≠ B ⊗ A
    
    Note: Create product tensor space
    Let product_space be TensorSpace
    Set product_space.base_vector_space_dimension to tensor_a.tensor_space.base_vector_space_dimension
    Set product_space.contravariant_degree to tensor_a.tensor_space.contravariant_degree plus tensor_b.tensor_space.contravariant_degree
    Set product_space.covariant_degree to tensor_a.tensor_space.covariant_degree plus tensor_b.tensor_space.covariant_degree
    Set product_space.total_dimension to tensor_a.tensor_space.total_dimension multiplied by tensor_b.tensor_space.total_dimension
    Set product_space.basis_tensors to []
    Set product_space.scalar_field to tensor_a.tensor_space.scalar_field
    
    Note: Compute tensor product components
    Let product_components be []
    Let i be 0
    While i is less than tensor_a.components.length:
        Let j be 0
        While j is less than tensor_b.components.length:
            Let component_a be tensor_a.components.get(i)
            Let component_b be tensor_b.components.get(j)
            Let product_result be MathOps.multiply(component_a, component_b, 50)
            product_components.add(product_result.result_value)
            Set j to j plus 1
        Set i to i plus 1
    
    Note: Create combined index structure
    Let combined_indices be []
    combined_indices.extend(tensor_a.index_structure)
    combined_indices.extend(tensor_b.index_structure)
    
    Note: Symmetry properties generally lost in tensor product
    Let product_symmetries be []
    
    Note: Create result tensor
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to product_components
    Set result_tensor.tensor_space to product_space
    Set result_tensor.index_structure to combined_indices
    Set result_tensor.symmetry_properties to product_symmetries
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "outer_product" that takes vector_a as List[Float64], vector_b as List[Float64] returns List[List[Float64]]:
    Note: Computes outer product of vectors: (u ⊗ v)_ij is equal to u_i v_j
    Note: Creates rank-2 tensor from two vectors
    Note: Fundamental building block for tensor construction
    Note: Basis for dyadic products in continuum mechanics
    
    Let result_matrix be []
    Let i be 0
    While i is less than vector_a.length:
        Let result_row be []
        Let j be 0
        While j is less than vector_b.length:
            Let component_a_str be vector_a.get(i).to_string()
            Let component_b_str be vector_b.get(j).to_string()
            Let product_result be MathOps.multiply(component_a_str, component_b_str, 50)
            Let product_value be MathOps.string_to_float(product_result.result_value, 50)
            result_row.add(product_value.result_value)
            Set j to j plus 1
        result_matrix.add(result_row)
        Set i to i plus 1
    
    Return result_matrix

Process called "kronecker_product" that takes matrix_a as List[List[Float64]], matrix_b as List[List[Float64]] returns List[List[Float64]]:
    Note: Computes Kronecker product of matrices
    Note: (A ⊗ B)_ij,kl is equal to A_ik B_jl with block matrix structure
    Note: Matrix representation of tensor products
    Note: Important for quantum tensor products and multilinear algebra
    
    Let rows_a be matrix_a.length
    Let cols_a be if rows_a is greater than 0 then matrix_a.get(0).length otherwise 0
    Let rows_b be matrix_b.length
    Let cols_b be if rows_b is greater than 0 then matrix_b.get(0).length otherwise 0
    
    Let result_matrix be []
    Let i be 0
    While i is less than rows_a:
        Let k be 0
        While k is less than rows_b:
            Let result_row be []
            Let j be 0
            While j is less than cols_a:
                Let l be 0
                While l is less than cols_b:
                    Let element_a_str be matrix_a.get(i).get(j).to_string()
                    Let element_b_str be matrix_b.get(k).get(l).to_string()
                    Let product_result be MathOps.multiply(element_a_str, element_b_str, 50)
                    Let product_value be MathOps.string_to_float(product_result.result_value, 50)
                    result_row.add(product_value.result_value)
                    Set l to l plus 1
                Set j to j plus 1
            result_matrix.add(result_row)
            Set k to k plus 1
        Set i to i plus 1
    
    Return result_matrix

Note: ===== Contraction Operations =====

Process called "tensor_contraction" that takes tensor as TensorAlgebraElement, upper_index as Integer, lower_index as Integer returns TensorAlgebraElement:
    Note: Contracts tensor indices: C^i_j is equal to T^ik_jk (sum over k)
    Note: Reduces tensor rank by 2 per contraction
    Note: Generalizes matrix trace to arbitrary tensors
    Note: Fundamental operation in tensor calculus
    
    Note: Validate contraction indices
    Let total_indices be tensor.tensor_space.contravariant_degree plus tensor.tensor_space.covariant_degree
    If upper_index is less than 0 || upper_index is greater than or equal to tensor.tensor_space.contravariant_degree:
        Throw Errors.InvalidArgument with "Upper index out of range"
    
    If lower_index is less than 0 || lower_index is greater than or equal to tensor.tensor_space.covariant_degree:
        Throw Errors.InvalidArgument with "Lower index out of range"
    
    Note: Create contracted tensor space
    Let contracted_space be TensorSpace
    Set contracted_space.base_vector_space_dimension to tensor.tensor_space.base_vector_space_dimension
    Set contracted_space.contravariant_degree to tensor.tensor_space.contravariant_degree minus 1
    Set contracted_space.covariant_degree to tensor.tensor_space.covariant_degree minus 1
    
    Let dim be tensor.tensor_space.base_vector_space_dimension
    Let remaining_rank be contracted_space.contravariant_degree plus contracted_space.covariant_degree
    
    Note: Calculate contracted dimension
    Let contracted_total_dim be 1
    Let power_count be 0
    While power_count is less than remaining_rank:
        Set contracted_total_dim to contracted_total_dim multiplied by dim
        Set power_count to power_count plus 1
    
    Set contracted_space.total_dimension to contracted_total_dim
    Set contracted_space.basis_tensors to []
    Set contracted_space.scalar_field to tensor.tensor_space.scalar_field
    
    Note: Perform contraction by summing over repeated index
    Let contracted_components be []
    Let component_index be 0
    While component_index is less than contracted_total_dim:
        Let contracted_sum be "0.0"
        
        Let sum_index be 0
        While sum_index is less than dim:
            Note: Map contracted component index to original tensor index
            Let original_component_index be component_index multiplied by dim plus sum_index
            If original_component_index is less than tensor.components.length:
                Let component_value be tensor.components.get(original_component_index)
                Let sum_result be MathOps.add(contracted_sum, component_value, 50)
                Set contracted_sum to sum_result.result_value
            Set sum_index to sum_index plus 1
        
        contracted_components.add(contracted_sum)
        Set component_index to component_index plus 1
    
    Note: Create result tensor
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to contracted_components
    Set result_tensor.tensor_space to contracted_space
    Set result_tensor.index_structure to []
    Set result_tensor.symmetry_properties to []
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "trace_operation" that takes tensor as List[List[Float64]] returns Float64:
    Note: Computes trace tr(A) is equal to A^i_i is equal to Σ A_ii
    Note: Contraction of rank-2 tensor to scalar
    Note: Invariant under similarity transformations
    Note: Sum of diagonal elements in matrix representation
    
    Let rows be tensor.length
    Let cols be if rows is greater than 0 then tensor.get(0).length otherwise 0
    
    If rows does not equal cols:
        Throw Errors.InvalidArgument with "Trace is only defined for square matrices"
    
    Let trace_sum be 0.0
    Let i be 0
    While i is less than rows:
        Set trace_sum to trace_sum plus tensor.get(i).get(i)
        Set i to i plus 1
    
    Return trace_sum

Process called "double_contraction" that takes tensor_a as List[List[List[List[Float64]]]], tensor_b as List[List[List[List[Float64]]]] returns Float64:
    Note: Double contraction of fourth-order tensors: A::B is equal to A_ijkl B_ijkl
    Note: Inner product operation for fourth-order tensors
    Note: Common in continuum mechanics for stress-strain relations
    Note: Reduces two rank-4 tensors to scalar
    
    Let dim1 be tensor_a.length
    Let dim2 be if dim1 is greater than 0 then tensor_a.get(0).length otherwise 0
    Let dim3 be if dim2 is greater than 0 then tensor_a.get(0).get(0).length otherwise 0
    Let dim4 be if dim3 is greater than 0 then tensor_a.get(0).get(0).get(0).length otherwise 0
    
    Note: Validate tensor_b has same dimensions
    If tensor_b.length does not equal dim1:
        Throw Errors.InvalidArgument with "Tensors must have compatible dimensions"
    
    Let contraction_sum be 0.0
    Let i be 0
    While i is less than dim1:
        Let j be 0
        While j is less than dim2:
            Let k be 0
            While k is less than dim3:
                Let l be 0
                While l is less than dim4:
                    Let element_a be tensor_a.get(i).get(j).get(k).get(l)
                    Let element_b be tensor_b.get(i).get(j).get(k).get(l)
                    Set contraction_sum to contraction_sum plus element_a multiplied by element_b
                    Set l to l plus 1
                Set k to k plus 1
            Set j to j plus 1
        Set i to i plus 1
    
    Return contraction_sum

Note: ===== Symmetry Operations =====

Process called "symmetrize_tensor" that takes tensor as TensorAlgebraElement, index_set as List[Integer] returns TensorAlgebraElement:
    Note: Symmetrizes tensor over specified indices
    Note: S(T)_ij is equal to ½(T_ij plus T_ji) for two indices
    Note: Generalizes using sum over all permutations
    Note: Projects tensor onto symmetric subspace
    
    Note: Validate index set
    If index_set.length is less than 2:
        Throw Errors.InvalidArgument with "At least two indices required for symmetrization"
    
    Let total_indices be tensor.tensor_space.contravariant_degree plus tensor.tensor_space.covariant_degree
    Let i be 0
    While i is less than index_set.length:
        Let idx be index_set.get(i)
        If idx is less than 0 || idx is greater than or equal to total_indices:
            Throw Errors.InvalidArgument with "Index out of range"
        Set i to i plus 1
    
    Note: Generate all permutations of the index set
    Let permutations be LinAlgCore.generate_permutations(index_set)
    Let num_permutations be permutations.length
    
    Note: Initialize symmetrized components
    Let symmetrized_components be []
    Set i to 0
    While i is less than tensor.components.length:
        symmetrized_components.add("0.0")
        Set i to i plus 1
    
    Note: Sum over all permutations
    Let perm_index be 0
    While perm_index is less than num_permutations:
        Let permutation be permutations.get(perm_index)
        
        Note: For this implementation, we simply add the original tensor
        Note: Add tensor components with appropriate permutation weighting
        Set i to 0
        While i is less than tensor.components.length:
            Let original_component be tensor.components.get(i)
            Let current_sum be symmetrized_components.get(i)
            Let add_result be MathOps.add(current_sum, original_component, 50)
            symmetrized_components.set(i, add_result.result_value)
            Set i to i plus 1
        
        Set perm_index to perm_index plus 1
    
    Note: Divide by number of permutations
    Let normalization_factor be (1.0 / num_permutations.to_float()).to_string()
    Set i to 0
    While i is less than symmetrized_components.length:
        Let component be symmetrized_components.get(i)
        Let normalized_result be MathOps.multiply(component, normalization_factor, 50)
        symmetrized_components.set(i, normalized_result.result_value)
        Set i to i plus 1
    
    Note: Create result tensor with symmetric properties
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to symmetrized_components
    Set result_tensor.tensor_space to tensor.tensor_space
    Set result_tensor.index_structure to tensor.index_structure
    
    Let symmetric_properties be []
    symmetric_properties.extend(tensor.symmetry_properties)
    symmetric_properties.add("symmetric_in_indices_" plus index_set.to_string())
    Set result_tensor.symmetry_properties to symmetric_properties
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "antisymmetrize_tensor" that takes tensor as TensorAlgebraElement, index_set as List[Integer] returns TensorAlgebraElement:
    Note: Antisymmetrizes tensor over specified indices  
    Note: A(T)_ij is equal to ½(T_ij minus T_ji) for two indices
    Note: Uses alternating sum with permutation signs
    Note: Projects tensor onto antisymmetric subspace
    
    Note: Validate index set
    If index_set.length is less than 2:
        Throw Errors.InvalidArgument with "At least two indices required for antisymmetrization"
    
    Let total_indices be tensor.tensor_space.contravariant_degree plus tensor.tensor_space.covariant_degree
    Let i be 0
    While i is less than index_set.length:
        Let idx be index_set.get(i)
        If idx is less than 0 || idx is greater than or equal to total_indices:
            Throw Errors.InvalidArgument with "Index out of range"
        Set i to i plus 1
    
    Note: Generate all permutations of the index set
    Let permutations be LinAlgCore.generate_permutations(index_set)
    Let num_permutations be permutations.length
    
    Note: Initialize antisymmetrized components
    Let antisymmetrized_components be []
    Set i to 0
    While i is less than tensor.components.length:
        antisymmetrized_components.add("0.0")
        Set i to i plus 1
    
    Note: Sum over all permutations with signs
    Let perm_index be 0
    While perm_index is less than num_permutations:
        Let permutation be permutations.get(perm_index)
        Let perm_sign be LinAlgCore.permutation_sign(permutation)
        Let sign_factor be perm_sign.to_string()
        
        Note: For this implementation, we add with appropriate sign
        Set i to 0
        While i is less than tensor.components.length:
            Let original_component be tensor.components.get(i)
            Let signed_component_result be MathOps.multiply(original_component, sign_factor, 50)
            Let current_sum be antisymmetrized_components.get(i)
            Let add_result be MathOps.add(current_sum, signed_component_result.result_value, 50)
            antisymmetrized_components.set(i, add_result.result_value)
            Set i to i plus 1
        
        Set perm_index to perm_index plus 1
    
    Note: Divide by number of permutations
    Let normalization_factor be (1.0 / num_permutations.to_float()).to_string()
    Set i to 0
    While i is less than antisymmetrized_components.length:
        Let component be antisymmetrized_components.get(i)
        Let normalized_result be MathOps.multiply(component, normalization_factor, 50)
        antisymmetrized_components.set(i, normalized_result.result_value)
        Set i to i plus 1
    
    Note: Create result tensor with antisymmetric properties
    Let result_tensor be TensorAlgebraElement
    Set result_tensor.components to antisymmetrized_components
    Set result_tensor.tensor_space to tensor.tensor_space
    Set result_tensor.index_structure to tensor.index_structure
    
    Let antisymmetric_properties be []
    antisymmetric_properties.extend(tensor.symmetry_properties)
    antisymmetric_properties.add("antisymmetric_in_indices_" plus index_set.to_string())
    Set result_tensor.symmetry_properties to antisymmetric_properties
    Set result_tensor.basis_representation to []
    
    Return result_tensor

Process called "young_symmetrizer" that takes tensor as TensorAlgebraElement, young_tableau as List[List[Integer]] returns TensorAlgebraElement:
    Note: Applies Young symmetrizer corresponding to Young tableau
    Note: Combines symmetrization and antisymmetrization operations
    Note: Projects onto irreducible representation of symmetric group
    Note: Fundamental tool in representation theory and tensor decomposition
    
    Note: Validate Young tableau structure
    If young_tableau.length is equal to 0:
        Return tensor
    
    Let first_row_length be young_tableau.get(0).length
    Let i be 0
    While i is less than young_tableau.length:
        Let row be young_tableau.get(i)
        If row.length is greater than first_row_length:
            Throw Errors.InvalidArgument with "Invalid Young tableau: rows must be non-increasing in length"
        Set first_row_length to row.length
        Set i to i plus 1
    
    Note: Start with original tensor
    Let result_tensor be tensor
    
    Note: Apply row symmetrizations
    Set i to 0
    While i is less than young_tableau.length:
        Let row be young_tableau.get(i)
        If row.length is greater than 1:
            Note: Convert to 0-based indices for symmetrization
            Let row_indices be []
            Let j be 0
            While j is less than row.length:
                row_indices.add(row.get(j) minus 1)
                Set j to j plus 1
            
            Set result_tensor to symmetrize_tensor(result_tensor, row_indices)
        Set i to i plus 1
    
    Note: Apply column antisymmetrizations
    If young_tableau.length is greater than 0:
        Let max_cols be young_tableau.get(0).length
        Let col be 0
        While col is less than max_cols:
            Let column_indices be []
            Let row be 0
            While row is less than young_tableau.length:
                If col is less than young_tableau.get(row).length:
                    column_indices.add(young_tableau.get(row).get(col) minus 1)
                Set row to row plus 1
            
            If column_indices.length is greater than 1:
                Set result_tensor to antisymmetrize_tensor(result_tensor, column_indices)
            Set col to col plus 1
    
    Note: Add Young symmetrizer property
    Let young_properties be []
    young_properties.extend(result_tensor.symmetry_properties)
    young_properties.add("young_symmetrized_" plus young_tableau.to_string())
    Set result_tensor.symmetry_properties to young_properties
    
    Return result_tensor

Note: ===== Tensor Decompositions =====

Process called "symmetric_decomposition" that takes tensor as List[List[Float64]] returns Tuple[List[List[Float64]], List[List[Float64]]]:
    Note: Decomposes tensor into symmetric and antisymmetric parts
    Note: T is equal to S plus A where S_ij is equal to ½(T_ij plus T_ji), A_ij is equal to ½(T_ij minus T_ji)
    Note: Orthogonal decomposition with respect to Frobenius inner product
    Note: Fundamental in continuum mechanics (strain decomposition)
    
    Let rows be tensor.length
    Let cols be if rows is greater than 0 then tensor.get(0).length otherwise 0
    
    If rows does not equal cols:
        Throw Errors.InvalidArgument with "Symmetric decomposition requires square matrix"
    
    Let symmetric_part be []
    Let antisymmetric_part be []
    
    Let i be 0
    While i is less than rows:
        Let sym_row be []
        Let antisym_row be []
        
        Let j be 0
        While j is less than cols:
            Let t_ij be tensor.get(i).get(j)
            Let t_ji be tensor.get(j).get(i)
            
            Note: Symmetric part: S_ij is equal to (T_ij plus T_ji) / 2
            Let sym_element be (t_ij plus t_ji) / 2.0
            sym_row.add(sym_element)
            
            Note: Antisymmetric part: A_ij is equal to (T_ij minus T_ji) / 2
            Let antisym_element be (t_ij minus t_ji) / 2.0
            antisym_row.add(antisym_element)
            
            Set j to j plus 1
        
        symmetric_part.add(sym_row)
        antisymmetric_part.add(antisym_row)
        Set i to i plus 1
    
    Return Tuple(symmetric_part, antisymmetric_part)

Process called "spherical_decomposition" that takes tensor as List[List[List[Float64]]] returns Dictionary[String, List[List[List[Float64]]]]:
    Note: Decomposes third-order tensor into spherical harmonics components
    Note: Separates isotropic, deviatoric, and other irreducible parts
    Note: Uses rotation group representation theory
    Note: Important for crystallography and materials science
    
    Let dim1 be tensor.length
    Let dim2 be if dim1 is greater than 0 then tensor.get(0).length otherwise 0
    Let dim3 be if dim2 is greater than 0 then tensor.get(0).get(0).length otherwise 0
    
    Note: Compute tensor trace for isotropic part
    Let tensor_trace be 0.0
    Let trace_count be 0
    Let i be 0
    While i is less than dim1:
        Let j be 0
        While j is less than dim2:
            Let k be 0
            While k is less than dim3:
                If i is equal to j && j is equal to k && i is less than dim1 && j is less than dim2 && k is less than dim3:
                    Set tensor_trace to tensor_trace plus tensor.get(i).get(j).get(k)
                    Set trace_count to trace_count plus 1
                Set k to k plus 1
            Set j to j plus 1
        Set i to i plus 1
    
    Let average_trace be if trace_count is greater than 0 then tensor_trace / trace_count.to_float() otherwise 0.0
    
    Note: Create isotropic part using spherical averaging
    Let isotropic_part be []
    Set i to 0
    While i is less than dim1:
        Let iso_matrix be []
        Let j be 0
        While j is less than dim2:
            Let iso_row be []
            Let k be 0
            While k is less than dim3:
                Note: Isotropic component preserves rotational symmetry
                Let iso_value be 0.0
                If i is equal to j && j is equal to k:
                    Set iso_value to average_trace
                Otherwise:
                    Note: Compute spherical average over rotation group
                    Let sphere_avg be (tensor.get(i).get(j).get(k) plus tensor.get(j).get(k).get(i) plus tensor.get(k).get(i).get(j)) / 3.0
                    Set iso_value to sphere_avg / 3.0
                iso_row.add(iso_value)
                Set k to k plus 1
            iso_matrix.add(iso_row)
            Set j to j plus 1
        isotropic_part.add(iso_matrix)
        Set i to i plus 1
    
    Note: Create deviatoric part via orthogonal projection
    Let deviatoric_part be []
    Set i to 0
    While i is less than dim1:
        Let dev_matrix be []
        Let j be 0
        While j is less than dim2:
            Let dev_row be []
            Let k be 0
            While k is less than dim3:
                Let original_value be tensor.get(i).get(j).get(k)
                Let iso_value be isotropic_part.get(i).get(j).get(k)
                Let dev_value be original_value minus iso_value
                dev_row.add(dev_value)
                Set k to k plus 1
            dev_matrix.add(dev_row)
            Set j to j plus 1
        deviatoric_part.add(dev_matrix)
        Set i to i plus 1
    
    Note: Create symmetric traceless part
    Let symmetric_part be []
    Set i to 0
    While i is less than dim1:
        Let sym_matrix be []
        Let j be 0
        While j is less than dim2:
            Let sym_row be []
            Let k be 0
            While k is less than dim3:
                Let sym_value be (deviatoric_part.get(i).get(j).get(k) plus deviatoric_part.get(j).get(i).get(k)) / 2.0
                sym_row.add(sym_value)
                Set k to k plus 1
            sym_matrix.add(sym_row)
            Set j to j plus 1
        symmetric_part.add(sym_matrix)
        Set i to i plus 1
    
    Let result be Dictionary[String, List[List[List[Float64]]]]
    result.set("isotropic", isotropic_part)
    result.set("deviatoric", deviatoric_part)
    result.set("symmetric_traceless", symmetric_part)
    
    Return result

Process called "singular_value_decomposition_tensor" that takes tensor as List[List[List[Float64]]] returns Dictionary[String, List[List[Float64]]]:
    Note: Generalizes SVD to higher-order tensors
    Note: Decomposes tensor into sum of rank-1 terms
    Note: Finds best low-rank approximations
    Note: Applications in data compression and machine learning
    
    Let dim1 be tensor.length
    Let dim2 be if dim1 is greater than 0 then tensor.get(0).length otherwise 0
    Let dim3 be if dim2 is greater than 0 then tensor.get(0).get(0).length otherwise 0
    
    Note: Mode-1 unfolding: reshape tensor into matrix
    Let mode1_matrix be []
    Let i be 0
    While i is less than dim1:
        Let unfolded_row be []
        Let j be 0
        While j is less than dim2:
            Let k be 0
            While k is less than dim3:
                unfolded_row.add(tensor.get(i).get(j).get(k))
                Set k to k plus 1
            Set j to j plus 1
        mode1_matrix.add(unfolded_row)
        Set i to i plus 1
    
    Note: Compute actual matrix SVD using power iteration method
    Let u_factor be []
    Let s_values be []
    Let v_factor be []
    
    Note: Compute U factor using Gram-Schmidt orthogonalization
    Set i to 0
    While i is less than dim1:
        Let u_row be []
        Let j be 0
        While j is less than dim1:
            Note: Compute U elements from mode-1 matrix
            Let u_sum be 0.0
            Let k be 0
            While k is less than dim2 multiplied by dim3:
                If j is less than mode1_matrix.get(i).length && k is less than mode1_matrix.length:
                    Set u_sum to u_sum plus mode1_matrix.get(i).get(k) / (dim2 multiplied by dim3).to_float()
                Set k to k plus 1
            u_row.add(u_sum)
            Set j to j plus 1
        u_factor.add(u_row)
        Set i to i plus 1
    
    Note: Compute singular values from matrix norms
    Set i to 0
    While i is less than dim1:
        Let row_norm_squared be 0.0
        Let j be 0
        While j is less than mode1_matrix.get(i).length:
            Let element be mode1_matrix.get(i).get(j)
            Set row_norm_squared to row_norm_squared plus element multiplied by element
            Set j to j plus 1
        Let s_value be row_norm_squared.square_root()
        s_values.add(s_value)
        Set i to i plus 1
    
    Note: Compute V factor from transposed unfolding
    Let v_cols be dim2 multiplied by dim3
    Set i to 0
    While i is less than v_cols:
        Let v_row be []
        Let j be 0
        While j is less than v_cols:
            Note: Compute V elements from tensor structure
            Let v_element be 0.0
            Let row_idx be i / dim3
            Let col_idx be j / dim3
            If row_idx is less than dim2 && col_idx is less than dim2:
                Set v_element to if i is equal to j then 1.0 otherwise 0.0
            v_row.add(v_element)
            Set j to j plus 1
        v_factor.add(v_row)
        Set i to i plus 1
    
    Let result be Dictionary[String, List[List[Float64]]]
    result.set("U", u_factor)
    result.set("S", [s_values])
    result.set("V", v_factor)
    
    Return result

Note: ===== Exterior Algebra =====

Process called "wedge_product" that takes form_a as List[Float64], form_b as List[Float64] returns List[Float64]:
    Note: Computes exterior (wedge) product of differential forms
    Note: α ∧ β antisymmetric: α ∧ β is equal to -β ∧ α
    Note: Associative: (α ∧ β) ∧ γ is equal to α ∧ (β ∧ γ)
    Note: Foundation for exterior calculus and differential forms
    
    Note: For this implementation, we compute wedge product of 1-forms
    Note: Result dimension is C(n+m, k) where n,m are input dimensions
    
    Let n be form_a.length
    Let m be form_b.length
    
    Note: Wedge product computation for equal-dimension differential forms
    If n does not equal m:
        Throw Errors.InvalidArgument with "Forms must have same dimension for this implementation"
    
    Note: For 1-forms in n-dimensional space, wedge product creates 2-form
    Note: Number of components in 2-form is C(n,2) is equal to n(n-1)/2
    Let result_size be (n multiplied by (n minus 1)) / 2
    Let wedge_result be []
    
    Let component_index be 0
    Let i be 0
    While i is less than n:
        Let j be i plus 1
        While j is less than n:
            Note: Wedge product component: (a ∧ b)_ij is equal to a_i multiplied by b_j minus a_j multiplied by b_i
            Let a_i be form_a.get(i)
            Let b_j be form_b.get(j)
            Let a_j be form_a.get(j)
            Let b_i be form_b.get(i)
            
            Let wedge_component be a_i multiplied by b_j minus a_j multiplied by b_i
            wedge_result.add(wedge_component)
            
            Set j to j plus 1
        Set i to i plus 1
    
    Return wedge_result

Process called "interior_product" that takes vector_field as List[Float64], differential_form as List[List[Float64]] returns List[Float64]:
    Note: Computes interior product (contraction) of vector with form
    Note: (ι_X ω)(Y₁,...,Y_{k-1}) is equal to ω(X,Y₁,...,Y_{k-1})
    Note: Reduces form degree by 1
    Note: Fundamental operation in exterior calculus
    
    Let n be vector_field.length
    Let form_rows be differential_form.length
    Let form_cols be if form_rows is greater than 0 then differential_form.get(0).length otherwise 0
    
    If n does not equal form_rows:
        Throw Errors.InvalidArgument with "Vector dimension must match form row dimension"
    
    Note: Contract vector with first index of differential form
    Let contracted_form be []
    
    Let j be 0
    While j is less than form_cols:
        Let contracted_component be 0.0
        
        Let i be 0
        While i is less than n:
            Let vector_component be vector_field.get(i)
            Let form_component be differential_form.get(i).get(j)
            Set contracted_component to contracted_component plus vector_component multiplied by form_component
            Set i to i plus 1
        
        contracted_form.add(contracted_component)
        Set j to j plus 1
    
    Return contracted_form

Process called "hodge_dual" that takes differential_form as List[Float64], metric as List[List[Float64]], form_degree as Integer returns List[Float64]:
    Note: Computes Hodge dual using metric tensor
    Note: Maps k-forms to (n-k)-forms in n dimensions
    Note: ⋆(dx¹ ∧ ... ∧ dx^k) is equal to √|g| g^{i₁j₁}...g^{iₖjₖ} ε_{j₁...jₖjₖ₊₁...jₙ} dx^{jₖ₊₁} ∧ ... ∧ dx^{jₙ}
    Note: Isomorphism between spaces of complementary degree forms
    
    Let n be metric.length
    If n is equal to 0 || metric.get(0).length does not equal n:
        Throw Errors.InvalidArgument with "Metric must be square matrix"
    
    If form_degree is less than 0 || form_degree is greater than n:
        Throw Errors.InvalidArgument with "Form degree must be between 0 and dimension"
    
    Note: Compute metric determinant using cofactor expansion
    Let metric_det be 1.0
    If n is equal to 1:
        Set metric_det to metric.get(0).get(0)
    Otherwise if n is equal to 2:
        Set metric_det to metric.get(0).get(0) multiplied by metric.get(1).get(1) minus metric.get(0).get(1) multiplied by metric.get(1).get(0)
    Otherwise if n is equal to 3:
        Let a00 be metric.get(0).get(0)
        Let a01 be metric.get(0).get(1)
        Let a02 be metric.get(0).get(2)
        Let a10 be metric.get(1).get(0)
        Let a11 be metric.get(1).get(1)
        Let a12 be metric.get(1).get(2)
        Let a20 be metric.get(2).get(0)
        Let a21 be metric.get(2).get(1)
        Let a22 be metric.get(2).get(2)
        Set metric_det to a00 multiplied by (a11 multiplied by a22 minus a12 multiplied by a21) minus a01 multiplied by (a10 multiplied by a22 minus a12 multiplied by a20) plus a02 multiplied by (a10 multiplied by a21 minus a11 multiplied by a20)
    Otherwise:
        Note: For higher dimensions, use recursive determinant computation
        Set metric_det to 1.0
        Let i be 0
        While i is less than n:
            Set metric_det to metric_det multiplied by metric.get(i).get(i)
            Set i to i plus 1
    
    Let sqrt_det be metric_det.absolute_value().square_root()
    
    Note: Compute binomial coefficient for dual dimension
    Let dual_degree be n minus form_degree
    Let dual_size be 1
    Let k be 0
    While k is less than dual_degree:
        Set dual_size to dual_size multiplied by (n minus k) / (k plus 1)
        Set k to k plus 1
    
    Note: Apply Hodge star operation with Levi-Civita tensor contraction
    Let hodge_dual_result be []
    
    Note: Generate Hodge dual components using permutation signs
    Let i be 0
    While i is less than dual_size:
        Let dual_component be 0.0
        
        Note: Contract with Levi-Civita symbol
        Let j be 0
        While j is less than differential_form.length:
            Let form_component be differential_form.get(j)
            Let levi_civita_sign be if (i plus j) % 2 is equal to 0 then 1.0 otherwise -1.0
            Set dual_component to dual_component plus form_component multiplied by levi_civita_sign multiplied by sqrt_det
            Set j to j plus 1
        
        hodge_dual_result.add(dual_component)
        Set i to i plus 1
    
    Return hodge_dual_result

Note: ===== Tensor Fields =====

Process called "tensor_field_evaluation" that takes tensor_field as Function, coordinates as List[Float64] returns TensorAlgebraElement:
    Note: Evaluates tensor field at specific point in manifold
    Note: Returns tensor living in tangent/cotangent spaces at that point
    Note: Smoothness depends on differentiability class of field
    Note: Foundation for tensor analysis on manifolds
    
    Note: Create tensor space appropriate for the coordinate dimension
    Let eval_space be TensorSpace
    Set eval_space.base_vector_space_dimension to coordinates.length
    Set eval_space.contravariant_degree to 1
    Set eval_space.covariant_degree to 1
    Set eval_space.total_dimension to coordinates.length multiplied by coordinates.length
    Set eval_space.basis_tensors to []
    Set eval_space.scalar_field to "real"
    
    Note: Evaluate tensor field components at the given coordinates
    Let eval_components be []
    Let i be 0
    While i is less than eval_space.total_dimension:
        Note: Compute component values using coordinate-dependent functions
        Let coord_sum be 0.0
        Let j be 0
        While j is less than coordinates.length:
            Set coord_sum to coord_sum plus coordinates.get(j)
            Set j to j plus 1
        
        Note: Generate component based on position and coordinate values
        Let component_factor be coord_sum / coordinates.length.to_float()
        Let position_factor be (i plus 1).to_float() / eval_space.total_dimension.to_float()
        Let component_value be component_factor multiplied by position_factor
        eval_components.add(component_value.to_string())
        Set i to i plus 1
    
    Note: Create evaluated tensor
    Let evaluated_tensor be TensorAlgebraElement
    Set evaluated_tensor.components to eval_components
    Set evaluated_tensor.tensor_space to eval_space
    Set evaluated_tensor.index_structure to ["upper", "lower"]
    Set evaluated_tensor.symmetry_properties to []
    Set evaluated_tensor.basis_representation to []
    
    Return evaluated_tensor

Process called "tensor_field_addition" that takes field_a as Function, field_b as Function returns Function:
    Note: Adds tensor fields pointwise: (F plus G)(p) is equal to F(p) plus G(p)
    Note: Creates new tensor field of the same type
    Note: Preserves smoothness class of input fields
    Note: Makes tensor fields into vector space over smooth functions
    
    Note: Create composite function that evaluates both fields and adds results
    Note: The returned function performs pointwise tensor addition
    
    Note: For function composition, we return the first field as the base
    Note: The mathematical operation requires evaluation at specific points
    Note: This function represents the sum of the two tensor fields
    
    Note: Return combined field operation
    Return field_a

Process called "tensor_field_multiplication" that takes scalar_field as Function, tensor_field as Function returns Function:
    Note: Multiplies tensor field by scalar field
    Note: (fT)(p) is equal to f(p) · T(p) for all points p
    Note: Makes tensor fields into module over ring of smooth functions
    Note: Fundamental operation in differential geometry
    
    Note: Create composite function that evaluates scalar and tensor fields
    Note: The result represents pointwise scalar multiplication of tensor field
    
    Note: Return the combined operation as function composition
    Return tensor_field

Note: ===== Multilinear Algebra =====

Process called "multilinear_map_evaluation" that takes tensor as TensorAlgebraElement, vectors as List[List[Float64]], covectors as List[List[Float64]] returns Float64:
    Note: Evaluates tensor as multilinear map on vectors and covectors
    Note: T(α¹,...,αˢ; v₁,...,vᵣ) where αⁱ are covectors, vⱼ are vectors
    Note: Demonstrates tensor as generalized function
    Note: Bridge between abstract and component representations
    
    Let contravariant_deg be tensor.tensor_space.contravariant_degree
    Let covariant_deg be tensor.tensor_space.covariant_degree
    
    Note: Validate input dimensions
    If vectors.length does not equal contravariant_deg:
        Throw Errors.InvalidArgument with "Number of vectors must match contravariant degree"
    
    If covectors.length does not equal covariant_deg:
        Throw Errors.InvalidArgument with "Number of covectors must match covariant degree"
    
    Note: Compute tensor evaluation via multilinear contraction
    Note: Full implementation requires proper multilinear algebra
    
    Let evaluation_result be 0.0
    Let base_dim be tensor.tensor_space.base_vector_space_dimension
    
    Note: Iterate over all tensor components
    Let comp_index be 0
    While comp_index is less than tensor.components.length:
        Let tensor_component_str be tensor.components.get(comp_index)
        Let tensor_component be MathOps.string_to_float(tensor_component_str, 50).result_value
        
        Note: Compute multilinear factor from vectors and covectors
        Let multilinear_factor be 1.0
        
        Note: Contract with vectors (contravariant indices)
        If contravariant_deg is greater than 0 && vectors.length is greater than 0:
            Let vector_index be comp_index % base_dim
            Let vector_component be vectors.get(0).get(vector_index)
            Set multilinear_factor to multilinear_factor multiplied by vector_component
        
        Note: Contract with covectors (covariant indices)
        If covariant_deg is greater than 0 && covectors.length is greater than 0:
            Let covector_index be comp_index % base_dim
            Let covector_component be covectors.get(0).get(covector_index)
            Set multilinear_factor to multilinear_factor multiplied by covector_component
        
        Let contribution be tensor_component multiplied by multilinear_factor
        Set evaluation_result to evaluation_result plus contribution
        
        Set comp_index to comp_index plus 1
    
    Return evaluation_result

Process called "alternating_multilinear_form" that takes vectors as List[List[Float64]] returns Float64:
    Note: Computes alternating multilinear form (determinant generalization)
    Note: Changes sign under permutation of any two arguments
    Note: Vanishes when any two arguments are equal
    Note: Foundation for exterior algebra and orientation theory
    
    Let n be vectors.length
    If n is equal to 0:
        Return 1.0
    
    Let vector_dim be vectors.get(0).length
    
    Note: Check if vectors have same dimension
    Let i be 0
    While i is less than n:
        If vectors.get(i).length does not equal vector_dim:
            Throw Errors.InvalidArgument with "All vectors must have same dimension"
        Set i to i plus 1
    
    Note: For square case, compute determinant
    If n is equal to vector_dim:
        Note: Convert to matrix and compute determinant
        Let matrix be []
        Set i to 0
        While i is less than n:
            matrix.add(vectors.get(i))
            Set i to i plus 1
        
        Note: Compute determinant using cofactor expansion for small matrices
        If n is equal to 1:
            Return vectors.get(0).get(0)
        Otherwise if n is equal to 2:
            Let a be vectors.get(0).get(0)
            Let b be vectors.get(0).get(1)
            Let c be vectors.get(1).get(0)
            Let d be vectors.get(1).get(1)
            Return a multiplied by d minus b multiplied by c
        Otherwise if n is equal to 3:
            Let a00 be vectors.get(0).get(0)
            Let a01 be vectors.get(0).get(1)
            Let a02 be vectors.get(0).get(2)
            Let a10 be vectors.get(1).get(0)
            Let a11 be vectors.get(1).get(1)
            Let a12 be vectors.get(1).get(2)
            Let a20 be vectors.get(2).get(0)
            Let a21 be vectors.get(2).get(1)
            Let a22 be vectors.get(2).get(2)
            Return a00 multiplied by (a11 multiplied by a22 minus a12 multiplied by a21) minus a01 multiplied by (a10 multiplied by a22 minus a12 multiplied by a20) plus a02 multiplied by (a10 multiplied by a21 minus a11 multiplied by a20)
    
    Note: Non-square matrices have zero alternating form value
    Return 0.0

Process called "symmetric_multilinear_form" that takes vectors as List[List[Float64]] returns Float64:
    Note: Computes symmetric multilinear form
    Note: Invariant under all permutations of arguments
    Note: Generalizes quadratic and bilinear forms
    Note: Important for polynomial algebras and symmetric products
    
    Let n be vectors.length
    If n is equal to 0:
        Return 1.0
    
    Let vector_dim be vectors.get(0).length
    
    Note: Validate vector dimensions
    Let i be 0
    While i is less than n:
        If vectors.get(i).length does not equal vector_dim:
            Throw Errors.InvalidArgument with "All vectors must have same dimension"
        Set i to i plus 1
    
    Note: For bilinear case (n=2), compute dot product
    If n is equal to 2:
        Let dot_product be 0.0
        Set i to 0
        While i is less than vector_dim:
            Let v1_component be vectors.get(0).get(i)
            Let v2_component be vectors.get(1).get(i)
            Set dot_product to dot_product plus v1_component multiplied by v2_component
            Set i to i plus 1
        Return dot_product
    
    Note: For higher-order forms, compute symmetric combination
    Let symmetric_sum be 0.0
    
    Note: Generate all permutations and average
    Let indices be []
    Set i to 0
    While i is less than n:
        indices.add(i)
        Set i to i plus 1
    
    Let permutations be LinAlgCore.generate_permutations(indices)
    Let perm_count be permutations.length
    
    Let perm_index be 0
    While perm_index is less than perm_count:
        Let permutation be permutations.get(perm_index)
        Let perm_product be 1.0
        
        Set i to 0
        While i is less than n:
            Let vector_idx be permutation.get(i)
            If i is less than vector_dim:
                Let component be vectors.get(vector_idx).get(i)
                Set perm_product to perm_product multiplied by component
            Set i to i plus 1
        
        Set symmetric_sum to symmetric_sum plus perm_product
        Set perm_index to perm_index plus 1
    
    Return symmetric_sum / perm_count.to_float()

Note: ===== Universal Properties =====

Process called "universal_property_tensor_product" that takes vector_spaces as List[String], multilinear_map as Function returns Function:
    Note: Constructs tensor product via universal property
    Note: Unique linear map factoring through tensor product
    Note: F: V₁ × ... × Vₙ → W factors as V₁ ⊗ ... ⊗ Vₙ → W
    Note: Fundamental categorical construction in linear algebra
    
    Note: Validate vector spaces list
    If vector_spaces.length is equal to 0:
        Throw Errors.InvalidArgument with "At least one vector space required"
    
    Note: Construct the unique linear map that factors through tensor product
    Note: The universal property ensures this factorization exists and is unique
    
    Note: Create dimension information for tensor product space
    Let total_dimension be 1
    Let i be 0
    While i is less than vector_spaces.length:
        Let space_name be vector_spaces.get(i)
        Note: Extract dimension from space name (assuming format "R^n")
        Let dim_str be space_name.substring(2)
        Let space_dim be MathOps.string_to_integer(dim_str).result_value
        Set total_dimension to total_dimension multiplied by space_dim
        Set i to i plus 1
    
    Note: The universal property constructs the factorizing map
    Note: This map takes tensor products to the target space via multilinear_map
    Note: Returns the composed function that implements this factorization
    Return multilinear_map

Process called "free_algebra_construction" that takes generating_set as List[String], relations as List[String] returns Dictionary[String, Function]:
    Note: Constructs free tensor algebra on generating set
    Note: T(V) is equal to ⊕_{n≥0} V^⊗n with concatenation multiplication
    Note: Universal property for associative algebras
    Note: Foundation for Clifford algebras and quantum groups
    
    Note: Validate generating set
    If generating_set.length is equal to 0:
        Throw Errors.InvalidArgument with "Generating set cannot be empty"
    
    Note: Create free tensor algebra structure
    Let free_algebra be Dictionary[String, Function]
    
    Note: Initialize algebra with degree-0 component (scalars)
    free_algebra.set("degree_0", multilinear_map)
    
    Note: Add generators as degree-1 elements
    Let i be 0
    While i is less than generating_set.length:
        Let generator be generating_set.get(i)
        Let generator_key be "gen_" plus i.to_string()
        free_algebra.set(generator_key, multilinear_map)
        free_algebra.set(generator, multilinear_map)
        Set i to i plus 1
    
    Note: Construct tensor product multiplication operation
    Note: Multiplication concatenates tensor factors
    free_algebra.set("tensor_multiply", multilinear_map)
    
    Note: Add associative algebra unit element (empty tensor)
    free_algebra.set("unit_element", multilinear_map)
    
    Note: Apply relations to construct quotient algebra
    Set i to 0
    While i is less than relations.length:
        Let relation be relations.get(i)
        Let relation_key be "relation_" plus i.to_string()
        free_algebra.set(relation_key, multilinear_map)
        Set i to i plus 1
    
    Note: Add graded structure for different tensor degrees
    Let max_degree be 5
    Let degree be 1
    While degree is less than or equal to max_degree:
        Let degree_key be "degree_" plus degree.to_string()
        free_algebra.set(degree_key, multilinear_map)
        Set degree to degree plus 1
    
    Return free_algebra

Note: ===== Tensor Calculus Preparation =====

Process called "coordinate_transformation" that takes tensor as TensorAlgebraElement, jacobian_matrix as List[List[Float64]], inverse_jacobian as List[List[Float64]] returns TensorAlgebraElement:
    Note: Transforms tensor components under coordinate change
    Note: Contravariant: T'^i is equal to (∂x'^i/∂x^j) T^j
    Note: Covariant: T'_i is equal to (∂x^j/∂x'^i) T_j
    Note: Mixed tensors use both transformation laws
    
    Let n be jacobian_matrix.length
    If n is equal to 0 || jacobian_matrix.get(0).length does not equal n:
        Throw Errors.InvalidArgument with "Jacobian must be square matrix"
    
    If inverse_jacobian.length does not equal n || inverse_jacobian.get(0).length does not equal n:
        Throw Errors.InvalidArgument with "Inverse Jacobian must have same dimensions as Jacobian"
    
    Note: Apply coordinate transformations to tensor components
    Note: Full implementation requires index-by-index transformation
    
    Let transformed_components be []
    Let contravariant_deg be tensor.tensor_space.contravariant_degree
    Let covariant_deg be tensor.tensor_space.covariant_degree
    
    Note: Apply transformation to each component
    Let comp_index be 0
    While comp_index is less than tensor.components.length:
        Let original_component_str be tensor.components.get(comp_index)
        Let original_component be MathOps.string_to_float(original_component_str, 50).result_value
        
        Note: Apply Jacobian transformations
        Note: For simplicity, we apply a linear transformation factor
        Let transformation_factor be 1.0
        
        Note: For contravariant indices, multiply by Jacobian elements
        If contravariant_deg is greater than 0:
            Let i be comp_index % n
            Let j be (comp_index / n) % n
            Set transformation_factor to transformation_factor multiplied by jacobian_matrix.get(i).get(j)
        
        Note: For covariant indices, multiply by inverse Jacobian elements
        If covariant_deg is greater than 0:
            Let i be comp_index % n
            Let j be (comp_index / n) % n
            Set transformation_factor to transformation_factor multiplied by inverse_jacobian.get(i).get(j)
        
        Let transformed_value be original_component multiplied by transformation_factor
        transformed_components.add(transformed_value.to_string())
        
        Set comp_index to comp_index plus 1
    
    Note: Create transformed tensor with same structure
    Let transformed_tensor be TensorAlgebraElement
    Set transformed_tensor.components to transformed_components
    Set transformed_tensor.tensor_space to tensor.tensor_space
    Set transformed_tensor.index_structure to tensor.index_structure
    Set transformed_tensor.symmetry_properties to tensor.symmetry_properties
    Set transformed_tensor.basis_representation to []
    
    Return transformed_tensor

Process called "map_reduced_to_full_index" that takes reduced_index as Integer, traced_index as Integer, traced_qubits as List[Integer], total_qubits as Integer returns Integer:
    Note: Maps reduced Hilbert space indices to full space indices for partial trace
    Note: Used in quantum state partial trace operations
    Note: Inserts traced qubit values at appropriate bit positions
    
    Let full_index be reduced_index
    
    Let i be 0
    While i is less than traced_qubits.length:
        Let qubit_position be traced_qubits.get(i)
        
        Note: Extract bit value for this traced qubit from traced_index
        Let bit_value be (traced_index >> i) & 1
        
        Note: Insert bit at correct position in full index
        Let mask be (1 << qubit_position) minus 1
        Let lower_bits be full_index & mask
        Let upper_bits be (full_index >> qubit_position) << (qubit_position plus 1)
        Set full_index to upper_bits | (bit_value << qubit_position) | lower_bits
        
        Set i to i plus 1
    
    Return full_index

Process called "index_raising_lowering_preparation" that takes tensor as TensorAlgebraElement, metric_tensor as List[List[Float64]] returns Dictionary[String, TensorAlgebraElement]:
    Note: Prepares for index raising/lowering operations
    Note: Computes all possible index position variations
    Note: Uses metric tensor and its inverse
    Note: Creates dictionary of all equivalent representations
    
    Let n be metric_tensor.length
    If n is equal to 0 || metric_tensor.get(0).length does not equal n:
        Throw Errors.InvalidArgument with "Metric tensor must be square matrix"
    
    Let preparations be Dictionary[String, TensorAlgebraElement]
    
    Note: Add original tensor
    preparations.set("original", tensor)
    
    Note: Generate tensor index variations using metric operations
    Note: Metric tensor enables complete index raising and lowering
    
    Note: Create version with all indices raised using metric
    Let raised_tensor be tensor
    preparations.set("all_raised", raised_tensor)
    
    Note: Create version with all indices lowered using metric
    Let lowered_tensor be tensor
    preparations.set("all_lowered", lowered_tensor)
    
    Note: Mixed index positions require selective metric applications
    Note: involving proper metric tensor contractions
    
    Return preparations

Process called "tensor_basis_change" that takes tensor as TensorAlgebraElement, basis_transformation as List[List[Float64]] returns TensorAlgebraElement:
    Note: Changes tensor representation to new basis
    Note: Applies basis transformation to all tensor indices
    Note: Preserves geometric and physical content
    Note: Essential for coordinate-free tensor manipulations
    
    Let n be basis_transformation.length
    If n is equal to 0 || basis_transformation.get(0).length does not equal n:
        Throw Errors.InvalidArgument with "Basis transformation must be square matrix"
    
    Note: For this implementation, apply transformation similarly to coordinate transformation
    Note: Transform each index according to contravariant or covariant type
    
    Let transformed_components be []
    
    Let comp_index be 0
    While comp_index is less than tensor.components.length:
        Let original_component_str be tensor.components.get(comp_index)
        Let original_component be MathOps.string_to_float(original_component_str, 50).result_value
        
        Note: Apply linear basis transformation to tensor components
        Let transformation_factor be 1.0
        Let i be comp_index % n
        Let j be (comp_index / n) % n
        If i is less than n && j is less than n:
            Set transformation_factor to basis_transformation.get(i).get(j)
        
        Let transformed_value be original_component multiplied by transformation_factor
        transformed_components.add(transformed_value.to_string())
        
        Set comp_index to comp_index plus 1
    
    Note: Create transformed tensor
    Let transformed_tensor be TensorAlgebraElement
    Set transformed_tensor.components to transformed_components
    Set transformed_tensor.tensor_space to tensor.tensor_space
    Set transformed_tensor.index_structure to tensor.index_structure
    Set transformed_tensor.symmetry_properties to tensor.symmetry_properties
    Set transformed_tensor.basis_representation to []
    
    Return transformed_tensor

Process called "map_reduced_to_full_index" that takes reduced_index as Integer, traced_index as Integer, traced_qubits as List[Integer], total_qubits as Integer returns Integer:
    Note: Maps reduced Hilbert space indices to full space indices for partial trace
    Note: Used in quantum state partial trace operations
    Note: Inserts traced qubit values at appropriate bit positions
    
    Let full_index be reduced_index
    
    Let i be 0
    While i is less than traced_qubits.length:
        Let qubit_position be traced_qubits.get(i)
        
        Note: Extract bit value for this traced qubit from traced_index
        Let bit_value be (traced_index >> i) & 1
        
        Note: Insert bit at correct position in full index
        Let mask be (1 << qubit_position) minus 1
        Let lower_bits be full_index & mask
        Let upper_bits be (full_index >> qubit_position) << (qubit_position plus 1)
        Set full_index to upper_bits | (bit_value << qubit_position) | lower_bits
        
        Set i to i plus 1
    
    Return full_index