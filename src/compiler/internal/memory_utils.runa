Note: Compiler-only memory management utilities - NOT part of stdlib
Note: These are specialized for compiler memory patterns and allocations

Import "../../compiler/frontend/primitives/memory/layout.runa" as Layout
Import "../../compiler/frontend/primitives/types/construction.runa" as Types

@Reasoning
The compiler has unique memory management needs including arena allocation for
AST nodes, memory pools for temporary objects, and specialized alignment requirements
for code generation buffers. These utilities are separate from the standard library
to optimize for compiler-specific access patterns and lifetimes.
@End Reasoning

Type called "MemoryArena":
    Note: Arena allocator for fast bulk allocation
    
    @Implementation
    Arenas allow allocating many objects quickly without individual free calls.
    Perfect for AST nodes and other compiler structures with similar lifetimes.
    @End Implementation
    
    base as Pointer
    current as Pointer
    capacity as Integer
    used as Integer
    alignment as Integer
End Type

Type called "MemoryPool":
    Note: Fixed-size object pool for frequent allocations
    
    @Implementation
    Pools pre-allocate blocks of same-sized objects to eliminate allocation overhead.
    Used for tokens, small AST nodes, and other frequently created objects.
    @End Implementation
    
    blocks as Pointer
    free_list as Pointer
    block_size as Integer
    block_count as Integer
    total_blocks as Integer
End Type

Type called "MemoryRegion":
    Note: Memory region with protection flags
    
    @Implementation
    Represents a memory region with specific permissions for code generation.
    Used when creating executable code buffers that need special protection.
    @End Implementation
    
    base as Pointer
    size as Integer
    protection as Integer  Note: Read=1, Write=2, Execute=4
    is_mapped as Boolean
End Type

Type called "AlignedBuffer":
    Note: Cache-aligned buffer for performance
    
    @Performance_Hints
    Ensures buffers are aligned to cache line boundaries (64 bytes on x86-64).
    This prevents false sharing and improves memory access performance.
    @End Performance_Hints
    
    data as Pointer
    size as Integer
    alignment as Integer
    allocated_base as Pointer  Note: Original allocation for freeing
End Type

Process called "create_arena" that takes size as Integer, alignment as Integer returns MemoryArena:
    Note: Initialize a memory arena
    
    @Implementation
    Creates an arena with specified size and alignment requirements.
    The arena grows in large chunks to minimize system allocation calls.
    @End Implementation
    
    Let arena be MemoryArena
    
    Note: Allocate aligned memory for arena
    Let alloc_size be size plus alignment
    Let base_ptr as Pointer
    Assembly "
        mov rdi, %[alloc_size]   ; size argument
        call malloc              ; call malloc
        mov %[base_ptr], rax     ; store result
    " with inputs [alloc_size] outputs [base_ptr]
    Set arena.base to base_ptr
    
    Note: Align the base pointer
    Let base_int as Integer
    Let aligned_int as Integer
    Assembly "
        mov rax, %[base]         ; get base address
        mov rcx, %[align]        ; get alignment
        dec rcx                  ; alignment - 1
        add rax, rcx             ; base + (alignment - 1)
        not rcx                  ; ~(alignment - 1)
        and rax, rcx             ; align down
        mov %[aligned_int], rax  ; store aligned address
    " with inputs [arena.base, alignment] outputs [aligned_int]
    Set arena.current to aligned_int as Pointer
    
    Set arena.capacity to size
    Set arena.used to 0
    Set arena.alignment to alignment
    
    Return arena
End Process

Process called "arena_alloc" that takes arena as MemoryArena, size as Integer returns Pointer:
    Note: Allocate from arena
    
    @Performance_Hints
    Allocation is just a pointer bump - extremely fast.
    No individual deallocation needed, entire arena freed at once.
    @End Performance_Hints
    
    Note: Align size to arena alignment
    Let aligned_size be (size plus arena.alignment minus 1) divided by arena.alignment times arena.alignment
    
    Note: Check if enough space remains
    If arena.used plus aligned_size is greater than arena.capacity:
        Return null  Note: Arena exhausted
    End If
    
    Note: Get current allocation pointer
    Let result be arena.current
    
    Note: Bump the pointer
    Set arena.current to arena.current plus aligned_size
    Set arena.used to arena.used plus aligned_size
    
    Return result
End Process

Process called "reset_arena" that takes arena as MemoryArena returns Nothing:
    Note: Reset arena for reuse
    
    @Implementation
    Resets the arena without freeing memory, allowing efficient reuse.
    Common pattern: parse phase uses arena, reset, semantic phase reuses.
    @End Implementation
    
    Let base_int be arena.base as Integer
    Let aligned_base be (base_int plus arena.alignment minus 1) divided by arena.alignment times arena.alignment
    
    Set arena.current to aligned_base as Pointer
    Set arena.used to 0
End Process

Process called "free_arena" that takes arena as MemoryArena returns Nothing:
    Note: Release arena memory
    
    @Implementation
    Frees the entire arena in one operation.
    All pointers into this arena become invalid.
    @End Implementation
    
    Assembly "mov rdi, %[arena.base]; call free" with inputs [arena.base]
    Set arena.base to null
    Set arena.current to null
    Set arena.capacity to 0
    Set arena.used to 0
End Process

Process called "create_pool" that takes block_size as Integer, initial_blocks as Integer returns MemoryPool:
    Note: Create fixed-size object pool
    
    @Implementation
    Pools are ideal for objects of the same size that are frequently allocated/freed.
    The free list allows O(1) allocation and deallocation.
    @End Implementation
    
    Let pool be MemoryPool
    Set pool.block_size to block_size
    Set pool.block_count to initial_blocks
    Set pool.total_blocks to initial_blocks
    
    Note: Allocate memory for all blocks
    Let total_size be block_size times initial_blocks
    Assembly "mov rdi, %[total_size]; call malloc; mov %[pool.blocks], rax" with inputs [total_size] outputs [pool.blocks]
    
    Note: Initialize free list
    Set pool.free_list to pool.blocks
    
    Note: Link all blocks into free list
    Let i be 0
    While i is less than initial_blocks minus 1:
        Let current_block be pool.blocks plus (i times block_size)
        Let next_block be pool.blocks plus ((i plus 1) times block_size)
        
        Note: Store next pointer at beginning of block
        Assembly "mov [%[current_block]], %[next_block]" with inputs [current_block, next_block]
        
        Set i to i plus 1
    End While
    
    Note: Last block points to null
    Let last_block be pool.blocks plus ((initial_blocks minus 1) times block_size)
    Assembly "mov qword [%[last_block]], 0" with inputs [last_block]
    
    Return pool
End Process

Process called "pool_alloc" that takes pool as MemoryPool returns Pointer:
    Note: Allocate from pool
    
    @Performance_Hints
    O(1) allocation by taking from free list head.
    No searching or fragmentation issues.
    @End Performance_Hints
    
    Note: Check if free list is empty
    If pool.free_list equals null:
        Note: Could expand pool here in full implementation
        Return null
    End If
    
    Note: Take first block from free list
    Let block be pool.free_list
    
    Note: Update free list to next block
    Assembly "mov rax, [%[block]]; mov %[pool.free_list], rax" with inputs [block] outputs [pool.free_list]
    
    Note: Zero the block before returning
    Assembly "mov rdi, %[block]; xor eax, eax; mov rcx, %[pool.block_size]; rep stosb" with inputs [block, pool.block_size]
    
    Return block
End Process

Process called "pool_free" that takes pool as MemoryPool, block as Pointer returns Nothing:
    Note: Return block to pool
    
    @Implementation
    Returns block to free list for reuse.
    No coalescing needed since all blocks are same size.
    @End Implementation
    
    Note: Add block to head of free list
    Assembly "mov rax, %[pool.free_list]; mov [%[block]], rax" with inputs [pool.free_list, block]
    Set pool.free_list to block
End Process

Process called "free_pool" that takes pool as MemoryPool returns Nothing:
    Note: Release entire pool
    
    @Implementation
    Frees all pool memory in one operation.
    All allocated blocks become invalid.
    @End Implementation
    
    Assembly "mov rdi, %[pool.blocks]; call free" with inputs [pool.blocks]
    Set pool.blocks to null
    Set pool.free_list to null
    Set pool.block_count to 0
End Process

Process called "create_aligned_buffer" that takes size as Integer, alignment as Integer returns AlignedBuffer:
    Note: Create cache-aligned buffer
    
    @Performance_Hints
    Alignment to cache lines (64 bytes) prevents false sharing.
    Critical for multi-threaded compiler operations.
    @End Performance_Hints
    
    Let buffer be AlignedBuffer
    Set buffer.size to size
    Set buffer.alignment to alignment
    
    Note: Allocate with extra space for alignment
    Let alloc_size be size plus alignment
    Assembly "mov rdi, %[alloc_size]; call malloc; mov %[buffer.allocated_base], rax" with inputs [alloc_size] outputs [buffer.allocated_base]
    
    Note: Calculate aligned address
    Let base_int be buffer.allocated_base as Integer
    Let aligned_addr be (base_int plus alignment minus 1) divided by alignment times alignment
    Set buffer.data to aligned_addr as Pointer
    
    Return buffer
End Process

Process called "free_aligned_buffer" that takes buffer as AlignedBuffer returns Nothing:
    Note: Free aligned buffer
    
    @Implementation
    Frees using the original allocation pointer, not the aligned pointer.
    This is critical for correct memory management.
    @End Implementation
    
    Assembly "mov rdi, %[buffer.allocated_base]; call free" with inputs [buffer.allocated_base]
    Set buffer.data to null
    Set buffer.allocated_base to null
    Set buffer.size to 0
End Process

Process called "create_executable_region" that takes size as Integer returns MemoryRegion:
    Note: Create memory region for JIT code
    
    @Implementation
    Allocates memory with execute permissions for generated machine code.
    Uses mmap with appropriate protection flags for JIT compilation.
    @End Implementation
    
    Let region be MemoryRegion
    Set region.size to size
    
    Note: Use mmap for executable memory
    Let prot be 7  Note: PROT_READ | PROT_WRITE | PROT_EXEC
    Let flags be 34  Note: MAP_PRIVATE | MAP_ANONYMOUS
    
    Inline Assembly:
        "mov rdi, 0\n"           Note: addr = NULL
        "mov rsi, %1\n"          Note: length
        "mov rdx, %2\n"          Note: prot
        "mov r10, %3\n"          Note: flags
        "mov r8, -1\n"           Note: fd
        "mov r9, 0\n"            Note: offset
        "mov rax, 9\n"           Note: sys_mmap
        "syscall\n"
        "mov %0, rax\n"          Note: store result
        : "=r"(region.base)
        : "r"(size), "r"(prot), "r"(flags)
        : "rax", "rdi", "rsi", "rdx", "r8", "r9", "r10", "r11", "memory"
    End Assembly
    
    Set region.protection to 7  Note: RWX
    Set region.is_mapped to true
    
    Return region
End Process

Process called "protect_region" that takes region as MemoryRegion, protection as Integer returns Boolean:
    Note: Change memory protection flags
    
    @Implementation
    Changes protection after code generation is complete.
    Common pattern: RW during generation, then change to RX for execution.
    @End Implementation
    
    If not region.is_mapped:
        Return false
    End If
    
    Let result as Boolean
    Inline Assembly:
        "mov rdi, %1\n"          Note: addr
        "mov rsi, %2\n"          Note: length
        "mov rdx, %3\n"          Note: prot
        "mov rax, 10\n"          Note: sys_mprotect
        "syscall\n"
        "test rax, rax\n"
        "setz al\n"
        "movzx eax, al\n"
        "mov %0, rax\n"
        : "=r"(result)
        : "r"(region.base), "r"(region.size), "r"(protection)
        : "rax", "rdi", "rsi", "rdx", "r11", "flags", "memory"
    End Assembly
    If result:
        Set region.protection to protection
    End If
    
    Return result
End Process

Process called "free_region" that takes region as MemoryRegion returns Nothing:
    Note: Release memory region
    
    @Implementation
    Unmaps the memory region if it was mapped with mmap.
    Otherwise uses standard free for malloc'd regions.
    @End Implementation
    
    If region.is_mapped:
        Inline Assembly:
            "mov rdi, %0\n"      Note: addr
            "mov rsi, %1\n"      Note: length
            "mov rax, 11\n"      Note: sys_munmap
            "syscall\n"
            :
            : "r"(region.base), "r"(region.size)
            : "rax", "rdi", "rsi", "r11", "memory"
        End Assembly
    Otherwise:
        Inline Assembly:
            "mov rdi, %0\n"      Note: base pointer
            "call free\n"        Note: free the region
            :
            : "r"(region.base)
            : "rdi", "memory"
        End Assembly
    End If
    
    Set region.base to null
    Set region.size to 0
    Set region.is_mapped to false
End Process

Process called "copy_memory" that takes dest as Pointer, src as Pointer, size as Integer returns Nothing:
    Note: Fast memory copy
    
    @Performance_Hints
    Uses optimized rep movsq for 8-byte aligned copies.
    Falls back to rep movsb for unaligned or small copies.
    @End Performance_Hints
    
    Note: Check alignment for both pointers
    Let dest_int be dest as Integer
    Let src_int be src as Integer
    
    If (dest_int bitwise_and 7) equals 0 and (src_int bitwise_and 7) equals 0:
        Note: Both 8-byte aligned, use fast path
        Let qword_count be size divided by 8
        Let remainder be size modulo 8
        
        Assembly "
            mov rdi, %[dest]
            mov rsi, %[src]
            mov rcx, %[qword_count]
            rep movsq
            mov rcx, %[remainder]
            rep movsb
        " with inputs [dest, src, qword_count, remainder]
    Otherwise:
        Note: Unaligned, use byte copy
        Assembly "
            mov rdi, %[dest]
            mov rsi, %[src]
            mov rcx, %[size]
            rep movsb
        " with inputs [dest, src, size]
    End If
End Process

Process called "zero_memory" that takes ptr as Pointer, size as Integer returns Nothing:
    Note: Fast memory zeroing
    
    @Performance_Hints
    Uses rep stosq for aligned zeroing which is highly optimized on modern CPUs.
    Critical for initializing large compiler structures.
    @End Performance_Hints
    
    Let ptr_int be ptr as Integer
    
    If (ptr_int bitwise_and 7) equals 0:
        Note: 8-byte aligned, use fast path
        Let qword_count be size divided by 8
        Let remainder be size modulo 8
        
        Assembly "
            mov rdi, %[ptr]
            xor rax, rax
            mov rcx, %[qword_count]
            rep stosq
            mov rcx, %[remainder]
            rep stosb
        " with inputs [ptr, qword_count, remainder]
    Otherwise:
        Note: Unaligned, use byte fill
        Assembly "
            mov rdi, %[ptr]
            xor al, al
            mov rcx, %[size]
            rep stosb
        " with inputs [ptr, size]
    End If
End Process