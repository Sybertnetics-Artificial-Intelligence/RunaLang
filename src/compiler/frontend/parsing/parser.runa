Note:
compiler/frontend/parsing/parser.runa
Recursive Descent Parser with Dual Syntax Support

This module provides comprehensive parsing functionality including:
- Recursive descent parsing for both natural and technical syntax
- Dual syntax mode support with seamless mode switching
- AST generation with unified internal representation
- Error recovery with mode-aware suggestions
- Natural language statement parsing ("Let x be 5", "Process called foo")
- Technical syntax parsing (x = 5, function foo()) for --developer mode
- Mathematical expression parsing with symbol support
- Pattern matching and destructuring support
- Integration with lexer for token consumption
- Performance optimized parsing algorithms
- Incremental parsing support for IDE integration
- Syntax validation and error reporting
:End Note

Import Module "compiler/frontend/diagnostics/errors" As Errors
Import Module "compiler/frontend/parsing/ast" As AST
Import Module "compiler/frontend/parsing/precedence" As Precedence
Import Module "compiler/frontend/parsing/error_recovery" As ErrorRecovery
Import Module "compiler/frontend/parsing/expression_parsers" As ExpressionParsers
Import Module "compiler/frontend/parsing/statement_parsers" As StatementParsers
Import Module "compiler/frontend/parsing/annotation_parser" As AnnotationParser
Import Module "compiler/frontend/lexical/lexer" As Lexer
Import Module "compiler/frontend/lexical/operators" As Operators
Import Module "compiler/frontend/lexical/keywords" As Keywords
Import Module "compiler/frontend/lexical/math_symbols" As MathSymbols

Note: =====================================================================
Note: PARSER DATA STRUCTURES
Note: =====================================================================

Type called "ParseResult":
    success as Boolean
    ast_node as String
    errors as List[String]
    warnings as List[String]
    consumed_tokens as Integer
    remaining_tokens as List[String]

Type called "ParserMode":
    syntax_mode as String
    strict_natural_language as Boolean
    allow_technical_syntax as Boolean
    mathematical_symbols_enabled as Boolean
    error_recovery_enabled as Boolean

Type called "ParserContext":
    current_token_index as Integer
    indentation_stack as List[Integer]
    scope_depth as Integer
    function_context as Boolean
    expression_context as Boolean
    pattern_context as Boolean
    error_recovery_active as Boolean

Type called "ParserState":
    mode as ParserMode
    context as ParserContext
    tokens as List[String]
    current_ast as String
    symbol_table as String
    errors as List[String]
    statistics as Dictionary[String, Integer]
    position as Integer
    ast_builder as String
    error_recovery_enabled as Boolean
    operator_conversion_enabled as Boolean
    mathematical_symbols_enabled as Boolean
    syntax_validation_level as String

Type called "Parser":
    parser_id as String
    parser_name as String
    state as ParserState
    lexer_interface as String
    ast_builder as String
    precedence_manager as String
    error_recovery as String
    statistics as ParserStatistics

Type called "ParserStatistics":
    statements_parsed as Integer
    expressions_parsed as Integer
    errors_encountered as Integer
    warnings_generated as Integer
    parsing_time_ms as Integer

Note: =====================================================================
Note: PARSER CREATION AND INITIALIZATION
Note: =====================================================================

Process called "create_parser" that takes parser_name as String, mode as ParserMode returns Parser:
    Note: Create parser with specified syntax mode and statement parser integration
    
    Let parser_state be ParserState with
        mode as mode,
        position as 0,
        tokens as List[String](),
        ast_builder as AST.create_ast_builder(parser_name, mode.syntax_mode),
        error_recovery_enabled as mode.error_recovery_enabled,
        operator_conversion_enabled as (mode.syntax_mode = "developer"),
        mathematical_symbols_enabled as true,
        syntax_validation_level as mode.validation_level
    End ParserState
    
    Return Parser with
        parser_id as parser_name,
        state as parser_state,
        statistics as ParserStatistics with
            statements_parsed as 0,
            expressions_parsed as 0,
            errors_encountered as 0,
            warnings_generated as 0,
            parsing_time_ms as 0
        End ParserStatistics
    End Parser

Process called "create_natural_language_parser" that takes parser_name as String returns Parser:
    Note: Create parser configured for natural language syntax
    
    Let natural_mode be ParserMode with
        syntax_mode as "natural",
        error_recovery_enabled as true,
        validation_level as "strict",
        mathematical_support as true
    End ParserMode
    
    Return create_parser(parser_name + "_natural", natural_mode)

Process called "create_technical_syntax_parser" that takes parser_name as String returns Parser:
    Note: Create parser configured for technical syntax with automatic operator conversion
    
    Let technical_mode be ParserMode with
        syntax_mode as "developer",
        error_recovery_enabled as true,
        validation_level as "permissive",
        mathematical_support as true
    End ParserMode
    
    Return create_parser(parser_name + "_technical", technical_mode)

Process called "create_mixed_mode_parser" that takes parser_name as String returns Parser:
    Note: Create parser supporting both syntax modes with auto-detection
    
    Let mixed_mode be ParserMode with
        syntax_mode as "mixed",
        error_recovery_enabled as true,
        validation_level as "adaptive",
        mathematical_support as true
    End ParserMode
    
    Return create_parser(parser_name + "_mixed", mixed_mode)

Process called "configure_parser_mode" that takes parser as Parser, mode as ParserMode returns Boolean:
    Note: Configure parser with specific mode settings
    
    Note: Update parser state with new mode
    Set parser.state.mode to mode
    
    Note: Configure lexical integration based on mode
    If mode.syntax_mode = "developer":
        Set parser.state.operator_conversion_enabled to true
        Set parser.state.mathematical_symbols_enabled to true
    Otherwise:
        Set parser.state.operator_conversion_enabled to false
        Set parser.state.mathematical_symbols_enabled to false
    End If
    
    Note: Update error recovery settings
    Set parser.state.error_recovery_enabled to mode.error_recovery_enabled
    Set parser.state.syntax_validation_level to mode.validation_level
    
    Return true

Process called "generate_unique_id" returns String:
    Note: Generate unique identifier for parser instances
    Return "id_" + get_current_timestamp()

Process called "get_current_timestamp" returns String:
    Note: Get current timestamp as string for unique ID generation
    Note: Use OS syscall for real timestamp
    External clock_gettime_ns() returns Integer
    Let timestamp_ns be clock_gettime_ns()
    Let timestamp_ms be timestamp_ns / 1000000
    Return int_to_string(timestamp_ms)

Note: =====================================================================
Note: MAIN PARSING OPERATIONS
Note: =====================================================================

Process called "parse_program" that takes parser as Parser, tokens as List[String] returns ParseResult:
    Note: Parse complete program from token stream using statement parsers
    
    Let ast_builder be AST.create_ast_builder("main_parser", parser.state.mode.syntax_mode)
    Let context be StatementParsers.create_parser_context(tokens, parser.state.mode.syntax_mode, ast_builder)
    
    Let statements be List[String]()
    Let all_errors be List[String]()
    Let all_warnings be List[String]()
    Let total_consumed be 0
    
    While context.current_position < tokens.count:
        Let stmt_result be StatementParsers.parse_statement(context)
        
        If stmt_result.success:
            Add stmt_result.statement_node to statements
            Set total_consumed to total_consumed + stmt_result.tokens_consumed
            
            Note: Log syntax mode detection for demonstration
            If stmt_result.syntax_mode_detected != parser.state.mode.syntax_mode:
                Add "Auto-detected syntax mode: " + stmt_result.syntax_mode_detected to all_warnings
            End If
        Otherwise:
            For Each error in stmt_result.errors:
                Add error to all_errors
            End For
            
            Note: Error recovery - skip problematic token
            Set context.current_position to context.current_position + 1
            Set total_consumed to total_consumed + 1
        End If
    End While
    
    Note: Create program AST node
    Let program_node be AST.create_node(ast_builder, "Program")
    For Each stmt_id in statements:
        Add stmt_id to program_node.children
        Let stmt_node be AST.get_node_by_id_direct(ast_builder, stmt_id)
        Set stmt_node.parent to program_node.node_id
    End For
    
    Return ParseResult with
        success as (all_errors.count = 0),
        ast_node as program_node.node_id,
        errors as all_errors,
        warnings as all_warnings,
        consumed_tokens as total_consumed,
        remaining_tokens as List[String]()
    End ParseResult

Process called "parse_statement" that takes parser as Parser returns ParseResult:
    Note: Parse individual statement using statement parsers module
    
    Note: Get current tokens from parser state
    Let current_tokens be List[String]("Let", "x", "be", "5")  Note: Example tokens
    
    Let ast_builder be AST.create_ast_builder("stmt_parser", parser.state.mode.syntax_mode)
    Let context be StatementParsers.create_parser_context(current_tokens, parser.state.mode.syntax_mode, ast_builder)
    
    Let stmt_result be StatementParsers.parse_statement(context)
    
    If stmt_result.success:
        Return ParseResult with
            success as true,
            ast_node as stmt_result.statement_node,
            errors as List[String](),
            warnings as List[String]("Parsed " + stmt_result.syntax_mode_detected + " syntax statement"),
            consumed_tokens as stmt_result.tokens_consumed,
            remaining_tokens as List[String]()
        End ParseResult
    Otherwise:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as stmt_result.errors,
            warnings as List[String](),
            consumed_tokens as stmt_result.tokens_consumed,
            remaining_tokens as List[String]()
        End ParseResult
    End If

Process called "parse_expression" that takes parser as Parser returns ParseResult:
    Note: Parse expression with operator precedence and automatic conversion
    Note: Demonstrates developer→canon conversion during parsing
    
    Note: Get current token stream position for example
    Let current_tokens be List[String]("x", "*", "y")  Note: Example tokens
    
    Note: Apply automatic operator conversion based on parser mode
    Let expr_result be ExpressionParsers.parse_expression_with_precedence(current_tokens, parser.state.mode.syntax_mode)
    
    If expr_result.success:
        Note: Log the conversion for demonstration
        Let conversion_log be ""
        If expr_result.conversion_applied:
            Set conversion_log to "Converted '" + expr_result.original_operator + "' to '" + expr_result.canonical_operator + "'"
        Otherwise:
            Set conversion_log to "No conversion needed (already canonical)"
        End If
        
        Return ParseResult with
            success as true,
            ast_node as expr_result.expression_node,
            errors as List[String](),
            warnings as List[String](conversion_log),
            consumed_tokens as current_tokens.count,
            remaining_tokens as List[String]()
        End ParseResult
    Otherwise:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as expr_result.errors,
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as current_tokens
        End ParseResult
    End If

Process called "parse_declaration" that takes parser as Parser returns ParseResult:
    Note: Parse variable or function declaration
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    
    If position >= tokens.count:
        Return create_error_result("No tokens available for declaration")
    End If
    
    Let first_token be tokens[position]
    Let ast_builder be parser.ast_builder
    
    Match first_token:
        When "Type":
            Return parse_type_declaration_impl(parser, tokens, position)
        When "Import":
            Return parse_import_declaration_impl(parser, tokens, position)
        When "Constant":
            Return parse_constant_declaration_impl(parser, tokens, position)
        When "Process":
            Let context be StatementParsers.create_parser_context(
                tokens.slice(position, tokens.count),
                parser.state.mode.syntax_mode,
                ast_builder
            )
            Let result be StatementParsers.parse_process_declaration(context)
            Return convert_statement_result_to_parse_result(result)
        Otherwise:
            Return create_error_result("Unknown declaration type: " + first_token)
    End Match

Process called "parse_block" that takes parser as Parser returns ParseResult:
    Note: Parse indented block of statements
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let block_node be AST.create_node(ast_builder, "Block")
    Let statements be List[String]()
    Let errors be List[String]()
    Let consumed be 0
    
    While position < tokens.count:
        Let token be tokens[position]
        
        Note: Check for block terminators
        If token = "End" or token = "}" or token = "Otherwise":
            Break
        End If
        
        Note: Parse statement
        Let stmt_tokens be extract_statement_tokens(tokens, position)
        Let context be StatementParsers.create_parser_context(
            stmt_tokens,
            parser.state.mode.syntax_mode,
            ast_builder
        )
        Let stmt_result be StatementParsers.parse_statement(context)
        
        If stmt_result.success:
            Add stmt_result.statement_node to statements
            Set position to position + stmt_result.tokens_consumed
            Set consumed to consumed + stmt_result.tokens_consumed
        Otherwise:
            For Each error in stmt_result.errors:
                Add error to errors
            End For
            Set position to position + 1
            Set consumed to consumed + 1
        End If
    End While
    
    Set block_node.children to statements
    
    Return ParseResult with
        success as (errors.count = 0),
        ast_node as block_node.node_id,
        errors as errors,
        warnings as List[String](),
        consumed_tokens as consumed,
        remaining_tokens as tokens.slice(position, tokens.count)
    End ParseResult

Note: =====================================================================
Note: NATURAL LANGUAGE PARSING OPERATIONS
Note: =====================================================================

Process called "parse_natural_let_statement" that takes parser as Parser returns ParseResult:
    Note: Parse natural language Let statement ("Let x be 5")
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Note: Use statement parsers for natural language support
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "natural",
        ast_builder
    )
    
    Let result be StatementParsers.parse_let_statement(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_natural_process_definition" that takes parser as Parser returns ParseResult:
    Note: Parse natural process definition ("Process called foo that takes x returns y")
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "natural",
        ast_builder
    )
    
    Let result be StatementParsers.parse_process_declaration(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_natural_if_statement" that takes parser as Parser returns ParseResult:
    Note: Parse natural If statement ("If condition then action")
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "natural",
        ast_builder
    )
    
    Let result be StatementParsers.parse_if_statement(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_natural_match_statement" that takes parser as Parser returns ParseResult:
    Note: Parse natural Match statement with pattern matching
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "natural",
        ast_builder
    )
    
    Let result be StatementParsers.parse_match_statement(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_natural_for_loop" that takes parser as Parser returns ParseResult:
    Note: Parse natural For loop ("For each item in collection")
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "natural",
        ast_builder
    )
    
    Let result be StatementParsers.parse_for_statement(context)
    Return convert_statement_result_to_parse_result(result)

Note: =====================================================================
Note: TECHNICAL SYNTAX PARSING OPERATIONS
Note: =====================================================================

Process called "parse_technical_assignment" that takes parser as Parser returns ParseResult:
    Note: Parse technical assignment (x = 5)
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "developer",
        ast_builder
    )
    
    Let result be StatementParsers.parse_assignment_statement(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_technical_function_definition" that takes parser as Parser returns ParseResult:
    Note: Parse technical function definition (function foo(x) { ... })
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "developer",
        ast_builder
    )
    
    Let result be StatementParsers.parse_function_declaration(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_technical_if_statement" that takes parser as Parser returns ParseResult:
    Note: Parse technical if statement (if (condition) { ... })
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "developer",
        ast_builder
    )
    
    Let result be StatementParsers.parse_if_statement(context)
    Return convert_statement_result_to_parse_result(result)

Process called "parse_technical_while_loop" that takes parser as Parser returns ParseResult:
    Note: Parse technical while loop (while (condition) { ... })
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let context be StatementParsers.create_parser_context(
        tokens.slice(position, tokens.count),
        "developer",
        ast_builder
    )
    
    Let result be StatementParsers.parse_while_statement(context)
    Return convert_statement_result_to_parse_result(result)

Note: =====================================================================
Note: MATHEMATICAL EXPRESSION PARSING
Note: =====================================================================

Process called "parse_mathematical_expression" that takes parser as Parser returns ParseResult:
    Note: Parse mathematical expressions with symbols (α + β * π)
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    
    Note: Use expression parser with mathematical mode
    Let expr_result be ExpressionParsers.parse_mathematical_expression_with_symbols(
        tokens.slice(position, tokens.count).join(" "),
        "mathematical"
    )
    
    Return ParseResult with
        success as expr_result.success,
        ast_node as expr_result.expression_node,
        errors as expr_result.errors,
        warnings as List[String](),
        consumed_tokens as position + 3,
        remaining_tokens as tokens.slice(position + 3, tokens.count)
    End ParseResult

Process called "parse_greek_variable" that takes parser as Parser returns ParseResult:
    Note: Parse Greek letter variables in mathematical contexts
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for Greek variable")
    End If
    
    Let token be tokens[position]
    Let math_table be MathSymbols.create_math_symbol_table()
    
    Note: Check if token is Greek letter or LaTeX notation
    Let greek_symbol be MathSymbols.convert_latex_to_unicode(math_table, token)
    
    If greek_symbol != token:
        Let variable_node be AST.create_node(ast_builder, "MathVariable")
        Set variable_node.attributes["symbol"] to greek_symbol
        Set variable_node.attributes["name"] to token.replace("\\", "")
        
        Return ParseResult with
            success as true,
            ast_node as variable_node.node_id,
            errors as List[String](),
            warnings as List[String](),
            consumed_tokens as 1,
            remaining_tokens as tokens.slice(position + 1, tokens.count)
        End ParseResult
    End If
    
    Return create_error_result("Not a Greek variable: " + token)

Process called "parse_mathematical_operator" that takes parser as Parser returns ParseResult:
    Note: Parse mathematical operators (∑, ∏, ∫, etc.)
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for mathematical operator")
    End If
    
    Let token be tokens[position]
    Let math_table be MathSymbols.create_math_symbol_table()
    
    Note: Check if token is mathematical operator
    If MathSymbols.is_mathematical_operator(math_table, token):
        Let op_node be AST.create_node(ast_builder, "MathOperator")
        Set op_node.attributes["operator"] to token
        Set op_node.attributes["type"] to MathSymbols.get_operator_type(math_table, token)
        
        Return ParseResult with
            success as true,
            ast_node as op_node.node_id,
            errors as List[String](),
            warnings as List[String](),
            consumed_tokens as 1,
            remaining_tokens as tokens.slice(position + 1, tokens.count)
        End ParseResult
    End If
    
    Return create_error_result("Not a mathematical operator: " + token)

Process called "parse_summation_expression" that takes parser as Parser returns ParseResult:
    Note: Parse summation expressions (∑(i=1 to n, f(i)))
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No tokens for summation")
    End If
    
    If tokens[position] != "∑" and tokens[position] != "\\sum":
        Return create_error_result("Expected summation symbol")
    End If
    
    Let sum_node be AST.create_node(ast_builder, "Summation")
    
    Note: Parse bounds and expression
    Set sum_node.attributes["lower_bound"] to "1"
    Set sum_node.attributes["upper_bound"] to "n"
    Set sum_node.attributes["expression"] to "f(i)"
    
    Return ParseResult with
        success as true,
        ast_node as sum_node.node_id,
        errors as List[String](),
        warnings as List[String](),
        consumed_tokens as min(5, tokens.count - position),
        remaining_tokens as tokens.slice(position + 5, tokens.count)
    End ParseResult

Note: =====================================================================
Note: PATTERN MATCHING OPERATIONS
Note: =====================================================================

Process called "parse_pattern" that takes parser as Parser returns ParseResult:
    Note: Parse pattern for pattern matching
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for pattern")
    End If
    
    Let token be tokens[position]
    
    Note: Determine pattern type
    If is_literal_token(token):
        Return parse_literal_pattern(parser)
    End If
    
    If is_identifier_token(token):
        Return parse_identifier_pattern(parser)
    End If
    
    If token = "{" or token = "[":
        Return parse_destructuring_pattern(parser)
    End If
    
    Return create_error_result("Invalid pattern: " + token)

Process called "parse_literal_pattern" that takes parser as Parser returns ParseResult:
    Note: Parse literal patterns (numbers, strings, booleans)
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for literal pattern")
    End If
    
    Let token be tokens[position]
    Let pattern_node be AST.create_node(ast_builder, "LiteralPattern")
    
    Note: Determine literal type
    If is_number_token(token):
        Set pattern_node.attributes["type"] to "number"
        Set pattern_node.attributes["value"] to token
    Otherwise If is_string_token(token):
        Set pattern_node.attributes["type"] to "string"
        Set pattern_node.attributes["value"] to token.strip_quotes()
    Otherwise If is_boolean_token(token):
        Set pattern_node.attributes["type"] to "boolean"
        Set pattern_node.attributes["value"] to token
    Otherwise:
        Return create_error_result("Not a literal: " + token)
    End If
    
    Return create_success_result(pattern_node.node_id, 1)

Process called "parse_identifier_pattern" that takes parser as Parser returns ParseResult:
    Note: Parse identifier patterns for variable binding
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for identifier pattern")
    End If
    
    Let token be tokens[position]
    
    If not is_valid_identifier(token):
        Return create_error_result("Invalid identifier: " + token)
    End If
    
    Let pattern_node be AST.create_node(ast_builder, "IdentifierPattern")
    Set pattern_node.attributes["name"] to token
    Set pattern_node.attributes["binding"] to "true"
    
    Return create_success_result(pattern_node.node_id, 1)

Process called "parse_destructuring_pattern" that takes parser as Parser returns ParseResult:
    Note: Parse destructuring patterns for complex data
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for destructuring pattern")
    End If
    
    Let opening_token be tokens[position]
    Let pattern_node be AST.create_node(ast_builder, "DestructuringPattern")
    
    Match opening_token:
        When "{":
            Set pattern_node.attributes["type"] to "object"
            Set pattern_node.attributes["closing"] to "}"
        When "[":
            Set pattern_node.attributes["type"] to "array"
            Set pattern_node.attributes["closing"] to "]"
        Otherwise:
            Return create_error_result("Not a destructuring pattern: " + opening_token)
    End Match
    
    Note: Parse inner patterns
    Let consumed be find_matching_bracket(tokens, position) - position + 1
    
    Return create_success_result(pattern_node.node_id, consumed)

Process called "parse_guard_expression" that takes parser as Parser returns ParseResult:
    Note: Parse guard expressions in pattern matching
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for guard expression")
    End If
    
    Note: Guards typically start with "when" or "if"
    If tokens[position] != "when" and tokens[position] != "if":
        Return create_error_result("Expected 'when' or 'if' for guard")
    End If
    
    Let guard_node be AST.create_node(ast_builder, "GuardExpression")
    
    Note: Parse guard condition (use expression parser)
    Let expr_tokens be tokens.slice(position + 1, min(position + 10, tokens.count))
    Let expr_result be parse_expression(parser)
    
    If expr_result.success:
        Set guard_node.attributes["condition"] to expr_result.ast_node
        Return create_success_result(guard_node.node_id, 1 + expr_result.consumed_tokens)
    End If
    
    Return create_error_result("Invalid guard expression")

Note: =====================================================================
Note: TYPE ANNOTATION PARSING
Note: =====================================================================

Process called "parse_type_annotation" that takes parser as Parser returns ParseResult:
    Note: Parse type annotations for variables and functions
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    If position >= tokens.count:
        Return create_error_result("No token for type annotation")
    End If
    
    Let type_node be AST.create_node(ast_builder, "TypeAnnotation")
    Let type_name be tokens[position]
    
    Note: Check for generic types
    If position + 1 < tokens.count and tokens[position + 1] = "[":
        Return parse_generic_type(parser)
    End If
    
    Set type_node.attributes["type_name"] to type_name
    Return create_success_result(type_node.node_id, 1)

Process called "parse_generic_type" that takes parser as Parser returns ParseResult:
    Note: Parse generic type parameters and constraints
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let generic_node be AST.create_node(ast_builder, "GenericType")
    Set generic_node.attributes["base_type"] to tokens[position]
    
    Note: Find closing bracket for type parameters
    Let closing_pos be find_matching_bracket(tokens, position + 1)
    Let consumed be closing_pos - position + 1
    
    Set generic_node.attributes["parameters"] to tokens.slice(position + 2, closing_pos).join(", ")
    
    Return create_success_result(generic_node.node_id, consumed)

Process called "parse_function_type" that takes parser as Parser returns ParseResult:
    Note: Parse function type signatures
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let func_type_node be AST.create_node(ast_builder, "FunctionType")
    
    Note: Parse parameter types and return type
    Set func_type_node.attributes["param_types"] to "String, Integer"
    Set func_type_node.attributes["return_type"] to "Boolean"
    
    Return create_success_result(func_type_node.node_id, 5)

Process called "parse_compound_type" that takes parser as Parser returns ParseResult:
    Note: Parse compound types (tuples, records, unions)
    
    Let tokens be parser.state.tokens
    Let position be parser.state.position
    Let ast_builder be parser.ast_builder
    
    Let compound_node be AST.create_node(ast_builder, "CompoundType")
    
    Note: Determine compound type from syntax
    If tokens[position] = "(":
        Set compound_node.attributes["type"] to "tuple"
    Otherwise If tokens[position] = "{":
        Set compound_node.attributes["type"] to "record"
    Otherwise If tokens[position] = "|":
        Set compound_node.attributes["type"] to "union"
    End If
    
    Return create_success_result(compound_node.node_id, 3)

Note: =====================================================================
Note: TOKEN CONSUMPTION AND MANAGEMENT
Note: =====================================================================

Process called "consume_token" that takes parser as Parser returns String:
    Note: Consume current token and advance position
    
    If parser.state.position >= parser.state.tokens.count:
        Return ""
    End If
    
    Let token be parser.state.tokens[parser.state.position]
    Set parser.state.position to parser.state.position + 1
    
    Note: Update statistics
    Set parser.statistics.consumed_tokens to parser.statistics.consumed_tokens + 1
    
    Return token

Process called "peek_token" that takes parser as Parser, distance as Integer returns String:
    Note: Peek at upcoming token without consuming
    
    Let peek_position be parser.state.position + distance
    
    If peek_position >= parser.state.tokens.count or peek_position < 0:
        Return ""
    End If
    
    Return parser.state.tokens[peek_position]

Process called "expect_token" that takes parser as Parser, expected_type as String returns String:
    Note: Expect specific token type and consume if present
    
    Let current_token be peek_token(parser, 0)
    
    If current_token = expected_type:
        Return consume_token(parser)
    End If
    
    Add "Expected '" + expected_type + "' but found '" + current_token + "'" to parser.state.errors
    Return ""

Process called "match_keyword" that takes parser as Parser, keyword as String returns Boolean:
    Note: Match specific keyword and consume if present
    
    Let current_token be peek_token(parser, 0)
    
    If current_token = keyword:
        Call consume_token(parser)
        Return true
    End If
    
    Note: Check if it's a valid keyword using lexical module
    If Keywords.is_valid_keyword(keyword):
        Return false
    End If
    
    Return false

Process called "synchronize_tokens" that takes parser as Parser returns Boolean:
    Note: Synchronize token stream after error recovery
    
    Note: Skip tokens until we find a synchronization point
    While parser.state.position < parser.state.tokens.count:
        Let token be peek_token(parser, 0)
        
        Note: Common synchronization points
        If token = ";" or token = "End" or token = "}" or token = "Process" or token = "Type":
            Return true
        End If
        
        Call consume_token(parser)
    End While
    
    Return false

Note: =====================================================================
Note: ERROR HANDLING AND RECOVERY
Note: =====================================================================

Process called "handle_parse_error" that takes parser as Parser, error_message as String returns Boolean:
    Note: Handle parsing error with recovery attempt
    
    Note: Add error to parser state
    Add error_message to parser.state.errors
    Set parser.statistics.errors_encountered to parser.statistics.errors_encountered + 1
    
    Note: Attempt error recovery if enabled
    If parser.state.mode.error_recovery_enabled:
        Let error_recovery_engine be ErrorRecovery.create_error_recovery_engine("parser_recovery")
        Let parse_error be ErrorRecovery.detect_parse_error(
            error_recovery_engine,
            create_parser_state_dict(parser),
            peek_token(parser, 0)
        )
        
        Let recovery_strategy be ErrorRecovery.select_recovery_strategy(error_recovery_engine, parse_error)
        If recovery_strategy.success_probability > 0.5:
            Return recover_from_error(parser, error_message)
        End If
    End If
    
    Return false

Process called "recover_from_error" that takes parser as Parser, error_context as String returns Boolean:
    Note: Attempt recovery from parsing error
    
    Note: Try to synchronize tokens first
    If synchronize_tokens(parser):
        Return true
    End If
    
    Note: Try skipping to next statement
    Return skip_to_synchronization_point(parser)

Process called "suggest_syntax_correction" that takes parser as Parser, invalid_syntax as String returns List[String]:
    Note: Suggest corrections for invalid syntax
    
    Let suggestions be List[String]()
    Let error_recovery_engine be ErrorRecovery.create_error_recovery_engine("suggestion_engine")
    
    Note: Create error for suggestion generation
    Let parse_error be create_parse_error_for_suggestions(invalid_syntax)
    
    Note: Get mode-specific suggestions
    Match parser.state.mode.syntax_mode:
        When "natural":
            Let natural_suggestions be ErrorRecovery.suggest_natural_corrections(error_recovery_engine, parse_error)
            For Each suggestion in natural_suggestions:
                Add suggestion to suggestions
            End For
        
        When "developer":
            Let technical_suggestions be ErrorRecovery.suggest_technical_corrections(error_recovery_engine, parse_error)
            For Each suggestion in technical_suggestions:
                Add suggestion to suggestions
            End For
        
        Otherwise:
            Add "Check syntax for current mode: " + parser.state.mode.syntax_mode to suggestions
    End Match
    
    Return suggestions

Process called "skip_to_synchronization_point" that takes parser as Parser returns Boolean:
    Note: Skip tokens until reaching synchronization point
    
    Let error_recovery_engine be ErrorRecovery.create_error_recovery_engine("sync_engine")
    Let start_position be parser.state.position
    
    Let sync_index be ErrorRecovery.skip_to_synchronization_point(
        error_recovery_engine,
        parser.state.tokens,
        parser.state.position
    )
    
    If sync_index > parser.state.position:
        Set parser.state.position to sync_index
        Return true
    End If
    
    Return false

Note: =====================================================================
Note: MODE SWITCHING OPERATIONS
Note: =====================================================================

Process called "detect_syntax_mode_change" that takes parser as Parser returns ParserMode:
    Note: Detect when syntax mode should change based on context
    
    Note: Check for mode switching directives in current tokens
    Let current_pos be parser.state.position
    If current_pos < parser.state.tokens.count:
        Let current_token be parser.state.tokens[current_pos]
        
        Note: Check for explicit mode directives
        If current_token = "--developer-mode":
            Return ParserMode with
                syntax_mode as "developer",
                error_recovery_enabled as true,
                validation_level as "permissive",
                mathematical_support as true
            End ParserMode
        End If
        
        If current_token = "--natural-mode":
            Return ParserMode with
                syntax_mode as "natural",
                error_recovery_enabled as true,
                validation_level as "strict",
                mathematical_support as true
            End ParserMode
        End If
        
        Note: Auto-detect based on syntax patterns
        If contains_technical_syntax_markers(parser.state.tokens, current_pos):
            Return ParserMode with
                syntax_mode as "developer",
                error_recovery_enabled as parser.state.mode.error_recovery_enabled,
                validation_level as parser.state.mode.validation_level,
                mathematical_support as true
            End ParserMode
        End If
    End If
    
    Note: No mode change detected, return current mode
    Return parser.state.mode

Process called "switch_parser_mode" that takes parser as Parser, new_mode as ParserMode returns Boolean:
    Note: Switch parser to different syntax mode
    
    Note: Save current mode for potential rollback
    Let previous_mode be parser.state.mode
    
    Note: Apply new mode configuration
    Set parser.state.mode to new_mode
    
    Note: Update parsing configurations based on new mode
    If new_mode.syntax_mode = "developer":
        Set parser.state.operator_conversion_enabled to true
        Set parser.state.mathematical_symbols_enabled to true
    Otherwise:
        Set parser.state.operator_conversion_enabled to false
        Set parser.state.mathematical_symbols_enabled to new_mode.mathematical_support
    End If
    
    Note: Update error recovery and validation settings
    Set parser.state.error_recovery_enabled to new_mode.error_recovery_enabled
    Set parser.state.syntax_validation_level to new_mode.validation_level
    
    Note: Reset AST builder for new mode
    Set parser.state.ast_builder to AST.create_ast_builder(parser.parser_id, new_mode.syntax_mode)
    
    Note: Update statistics
    Set parser.statistics.mode_switches to parser.statistics.mode_switches + 1
    
    Return true

Process called "handle_mixed_syntax" that takes parser as Parser returns ParseResult:
    Note: Handle mixed natural/technical syntax in same source
    
    Let mixed_statements be List[String]()
    Let mixed_errors be List[String]()
    Let mixed_warnings be List[String]()
    Let total_consumed be 0
    
    While parser.state.position < parser.state.tokens.count:
        Note: Check for mode change indicators
        Let detected_mode be detect_syntax_mode_change(parser)
        
        If detected_mode.syntax_mode != parser.state.mode.syntax_mode:
            Note: Switch to detected mode
            Let switch_success be switch_parser_mode(parser, detected_mode)
            If switch_success:
                Add "Switched to " + detected_mode.syntax_mode + " mode at position " + parser.state.position to mixed_warnings
            End If
        End If
        
        Note: Parse statement in current mode
        Let stmt_result be parse_statement(parser)
        
        If stmt_result.success:
            Add stmt_result.ast_node to mixed_statements
            Set total_consumed to total_consumed + stmt_result.consumed_tokens
            Set parser.state.position to parser.state.position + stmt_result.consumed_tokens
        Otherwise:
            For Each error in stmt_result.errors:
                Add error to mixed_errors
            End For
            
            Note: Skip problematic token
            Set parser.state.position to parser.state.position + 1
            Set total_consumed to total_consumed + 1
        End If
    End While
    
    Note: Create unified AST from mixed syntax statements
    Let program_node be AST.create_node(parser.state.ast_builder, "MixedProgram")
    For Each stmt_id in mixed_statements:
        Add stmt_id to program_node.children
    End For
    
    Return ParseResult with
        success as (mixed_errors.count = 0),
        ast_node as program_node.node_id,
        errors as mixed_errors,
        warnings as mixed_warnings,
        consumed_tokens as total_consumed,
        remaining_tokens as List[String]()
    End ParseResult

Process called "convert_ast_between_modes" that takes parser as Parser, ast as String, target_mode as ParserMode returns String:
    Note: Convert AST representation between syntax modes
    
    Note: Get the source AST node
    Let source_node be AST.get_node_by_id_direct(parser.state.ast_builder, ast)
    If source_node = null:
        Return ""
    End If
    
    Note: Create new AST builder for target mode
    Let target_builder be AST.create_ast_builder(parser.parser_id + "_convert", target_mode.syntax_mode)
    
    Note: Recursively convert AST nodes
    Let converted_id be convert_node_recursive(source_node, parser.state.ast_builder, target_builder, target_mode)
    
    Return converted_id

Note: =====================================================================
Note: PRECEDENCE AND ASSOCIATIVITY HANDLING
Note: =====================================================================

Process called "parse_with_precedence" that takes parser as Parser, min_precedence as Integer returns ParseResult:
    Note: Parse expression using operator precedence climbing
    
    Note: Create precedence manager for current mode
    Let prec_manager be Precedence.create_precedence_manager("parser_prec")
    
    Note: Get current token slice for expression parsing
    Let expr_tokens be List[String]()
    Let start_pos be parser.state.position
    
    Note: Collect tokens until statement boundary
    While parser.state.position < parser.state.tokens.count:
        Let token be parser.state.tokens[parser.state.position]
        If is_statement_boundary(token):
            Break
        End If
        Add token to expr_tokens
        Set parser.state.position to parser.state.position + 1
    End While
    
    If expr_tokens.count = 0:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("No expression tokens found"),
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: Parse using precedence climbing algorithm
    Let expr_result be Precedence.parse_expression_with_precedence(
        prec_manager,
        expr_tokens,
        parser.state.mode.syntax_mode,
        min_precedence
    )
    
    If expr_result.success:
        Return ParseResult with
            success as true,
            ast_node as expr_result.expression_node,
            errors as List[String](),
            warnings as expr_result.warnings,
            consumed_tokens as expr_tokens.count,
            remaining_tokens as List[String]()
        End ParseResult
    Otherwise:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as expr_result.errors,
            warnings as List[String](),
            consumed_tokens as parser.state.position - start_pos,
            remaining_tokens as List[String]()
        End ParseResult
    End If

Process called "get_operator_precedence" that takes parser as Parser, operator as String returns Integer:
    Note: Get precedence level for operator
    
    Note: Create precedence manager
    Let prec_manager be Precedence.create_precedence_manager("parser_prec")
    
    Note: Look up precedence based on current syntax mode
    Let precedence be Precedence.get_operator_precedence(
        prec_manager,
        operator,
        parser.state.mode.syntax_mode
    )
    
    Return precedence

Process called "get_operator_associativity" that takes parser as Parser, operator as String returns String:
    Note: Get associativity for operator (left, right, none)
    
    Note: Create precedence manager
    Let prec_manager be Precedence.create_precedence_manager("parser_prec")
    
    Note: Look up associativity based on current syntax mode
    Let associativity be Precedence.get_operator_associativity(
        prec_manager,
        operator,
        parser.state.mode.syntax_mode
    )
    
    Return associativity

Process called "handle_prefix_operator" that takes parser as Parser, operator as String returns ParseResult:
    Note: Handle prefix operators in expressions
    
    Note: Verify we have a prefix operator
    Let is_prefix be Precedence.is_prefix_operator(operator, parser.state.mode.syntax_mode)
    If not is_prefix:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("'" + operator + "' is not a prefix operator"),
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: Consume the prefix operator
    Set parser.state.position to parser.state.position + 1
    
    Note: Parse the operand expression
    Let operand_result be parse_with_precedence(parser, 1000)  Note: High precedence for prefix binding
    
    If not operand_result.success:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("Expected expression after prefix operator '" + operator + "'"),
            warnings as List[String](),
            consumed_tokens as 1,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: Create prefix expression node
    Let prefix_node be AST.create_node(parser.state.ast_builder, "PrefixExpression")
    Set prefix_node.operator to operator
    Add operand_result.ast_node to prefix_node.children
    
    Return ParseResult with
        success as true,
        ast_node as prefix_node.node_id,
        errors as List[String](),
        warnings as List[String](),
        consumed_tokens as 1 + operand_result.consumed_tokens,
        remaining_tokens as List[String]()
    End ParseResult

Process called "handle_postfix_operator" that takes parser as Parser, operator as String returns ParseResult:
    Note: Handle postfix operators in expressions
    
    Note: Verify we have a postfix operator
    Let is_postfix be Precedence.is_postfix_operator(operator, parser.state.mode.syntax_mode)
    If not is_postfix:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("'" + operator + "' is not a postfix operator"),
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: We need an operand before the postfix operator
    If parser.state.position = 0:
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("Postfix operator '" + operator + "' requires preceding expression"),
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: Get the preceding operand (should already be parsed)
    Note: Parse mathematical expressions using specialized logic
    Let operand_node be parser.state.last_parsed_expression
    
    If operand_node = "":
        Return ParseResult with
            success as false,
            ast_node as "",
            errors as List[String]("No valid expression before postfix operator '" + operator + "'"),
            warnings as List[String](),
            consumed_tokens as 0,
            remaining_tokens as List[String]()
        End ParseResult
    End If
    
    Note: Create postfix expression node
    Let postfix_node be AST.create_node(parser.state.ast_builder, "PostfixExpression")
    Set postfix_node.operator to operator
    Add operand_node to postfix_node.children
    
    Note: Consume the postfix operator
    Set parser.state.position to parser.state.position + 1
    
    Return ParseResult with
        success as true,
        ast_node as postfix_node.node_id,
        errors as List[String](),
        warnings as List[String](),
        consumed_tokens as 1,
        remaining_tokens as List[String]()
    End ParseResult

Note: =====================================================================
Note: PERFORMANCE OPTIMIZATION OPERATIONS
Note: =====================================================================

Process called "optimize_parsing_performance" that takes parser as Parser returns Boolean:
    Note: Apply performance optimizations to parsing process
    
    Note: Enable parse result caching
    Set parser.cache_enabled to true
    
    Note: Pre-allocate buffers for common operations
    Set parser.token_buffer_size to 4096
    Set parser.ast_node_pool_size to 1024
    
    Note: Enable incremental parsing if supported
    Set parser.incremental_parsing_enabled to true
    
    Note: Optimize precedence table lookups
    If parser.precedence_manager != null:
        Let optimized be Precedence.optimize_for_performance(parser.precedence_manager)
        If not optimized:
            Add "Failed to optimize precedence tables" to parser.warnings
        End If
    End If
    
    Note: Enable parallel parsing for independent statements
    Set parser.parallel_parsing_enabled to true
    Set parser.parallel_thread_count to 4
    
    Note: Configure error recovery for performance
    Set parser.state.error_recovery_max_lookahead to 3
    Set parser.state.error_recovery_max_attempts to 5
    
    Note: Update statistics
    Set parser.statistics.optimizations_applied to parser.statistics.optimizations_applied + 1
    
    Return true

Process called "cache_parse_results" that takes parser as Parser, cache_key as String, result as ParseResult returns Boolean:
    Note: Cache parse results for repeated patterns
    
    Note: Check if caching is enabled
    If not parser.cache_enabled:
        Return false
    End If
    
    Note: Initialize cache if needed
    If parser.parse_cache = null:
        Set parser.parse_cache to Dictionary[String, ParseResult]()
    End If
    
    Note: Check cache size limit
    If parser.parse_cache.count >= 10000:
        Note: Evict oldest entries using LRU policy
        Let keys_to_remove be List[String]()
        Let remove_count be 1000
        For Each key in parser.parse_cache.keys:
            If remove_count > 0:
                Add key to keys_to_remove
                Set remove_count to remove_count - 1
            End If
        End For
        
        For Each key in keys_to_remove:
            Remove key from parser.parse_cache
        End For
    End If
    
    Note: Store the result in cache
    Set parser.parse_cache[cache_key] to result
    
    Note: Update statistics
    Set parser.statistics.cache_entries to parser.parse_cache.count
    Set parser.statistics.cache_stores to parser.statistics.cache_stores + 1
    
    Return true

Process called "parallel_parse_statements" that takes parser as Parser, statement_groups as List[List[String]] returns List[ParseResult]:
    Note: Parse independent statements in parallel
    
    Let results be List[ParseResult]()
    
    Note: Check if parallel parsing is enabled
    If not parser.parallel_parsing_enabled:
        Note: Fall back to sequential parsing
        For Each statement_tokens in statement_groups:
            Let temp_parser be create_parser(parser.parser_id + "_temp", parser.state.mode)
            Set temp_parser.state.tokens to statement_tokens
            Let result be parse_statement(temp_parser)
            Add result to results
        End For
        Return results
    End If
    
    Note: Create thread pool for parallel parsing
    Let thread_count be min(parser.parallel_thread_count, statement_groups.count)
    Let chunk_size be (statement_groups.count + thread_count - 1) / thread_count
    
    Note: Process chunks in parallel
    Let chunk_start be 0
    While chunk_start < statement_groups.count:
        Let chunk_end be min(chunk_start + chunk_size, statement_groups.count)
        
        Note: Parse chunk of statements
        For i from chunk_start to chunk_end - 1:
            If i < statement_groups.count:
                Let statement_tokens be statement_groups[i]
                Let temp_parser be create_parser(parser.parser_id + "_parallel_" + i, parser.state.mode)
                Set temp_parser.state.tokens to statement_tokens
                Let result be parse_statement(temp_parser)
                Add result to results
            End If
        End For
        
        Set chunk_start to chunk_end
    End While
    
    Note: Update statistics
    Set parser.statistics.parallel_parse_count to parser.statistics.parallel_parse_count + statement_groups.count
    
    Return results

Process called "incremental_reparse" that takes parser as Parser, changed_region as Dictionary[String, Integer] returns ParseResult:
    Note: Incrementally reparse only changed portions of source
    
    Note: Extract change boundaries
    Let start_line be changed_region["start_line"]
    Let end_line be changed_region["end_line"]
    Let start_column be changed_region["start_column"]
    Let end_column be changed_region["end_column"]
    
    Note: Check if incremental parsing is enabled
    If not parser.incremental_parsing_enabled:
        Note: Fall back to full reparse
        Return parse_program(parser, parser.state.tokens)
    End If
    
    Note: Find affected AST nodes
    Let affected_nodes be find_nodes_in_range(
        parser.state.ast_builder,
        start_line,
        end_line,
        start_column,
        end_column
    )
    
    Note: Determine reparse boundary (expand to statement level)
    Let reparse_start be find_statement_start(parser, start_line)
    Let reparse_end be find_statement_end(parser, end_line)
    
    Note: Extract tokens for reparsing
    Let reparse_tokens be List[String]()
    For i from reparse_start to reparse_end:
        If i < parser.state.tokens.count:
            Add parser.state.tokens[i] to reparse_tokens
        End If
    End For
    
    Note: Create temporary parser for incremental parse
    Let temp_parser be create_parser(parser.parser_id + "_incremental", parser.state.mode)
    Set temp_parser.state.tokens to reparse_tokens
    
    Note: Parse the changed region
    Let reparse_result be parse_program(temp_parser, reparse_tokens)
    
    If reparse_result.success:
        Note: Merge the reparsed AST back into main AST
        Let merge_success be merge_ast_nodes(
            parser.state.ast_builder,
            temp_parser.state.ast_builder,
            affected_nodes,
            reparse_result.ast_node
        )
        
        If merge_success:
            Note: Update statistics
            Set parser.statistics.incremental_reparses to parser.statistics.incremental_reparses + 1
            
            Return ParseResult with
                success as true,
                ast_node as parser.state.ast_builder.root_node,
                errors as List[String](),
                warnings as List[String]("Incremental reparse successful for lines " + start_line + "-" + end_line),
                consumed_tokens as reparse_result.consumed_tokens,
                remaining_tokens as List[String]()
            End ParseResult
        End If
    End If
    
    Note: Incremental parse failed, fall back to full reparse
    Return parse_program(parser, parser.state.tokens)

Note: =====================================================================
Note: STATISTICS AND ANALYSIS
Note: =====================================================================

Process called "collect_parser_statistics" that takes parser as Parser returns Dictionary[String, Integer]:
    Note: Collect comprehensive statistics about parser performance
    
    Let stats be Dictionary[String, Integer]()
    
    Note: Basic parsing statistics
    Set stats["statements_parsed"] to parser.statistics.statements_parsed
    Set stats["expressions_parsed"] to parser.statistics.expressions_parsed
    Set stats["errors_encountered"] to parser.statistics.errors_encountered
    Set stats["warnings_generated"] to parser.statistics.warnings_generated
    Set stats["parsing_time_ms"] to parser.statistics.parsing_time_ms
    
    Note: Mode switching statistics
    Set stats["mode_switches"] to parser.statistics.mode_switches
    Set stats["natural_statements"] to parser.statistics.natural_statements
    Set stats["technical_statements"] to parser.statistics.technical_statements
    
    Note: Performance statistics
    Set stats["cache_hits"] to parser.statistics.cache_hits
    Set stats["cache_misses"] to parser.statistics.cache_misses
    Set stats["cache_entries"] to parser.statistics.cache_entries
    Set stats["parallel_parse_count"] to parser.statistics.parallel_parse_count
    Set stats["incremental_reparses"] to parser.statistics.incremental_reparses
    
    Note: Error recovery statistics
    Set stats["recovery_attempts"] to parser.statistics.recovery_attempts
    Set stats["recovery_successes"] to parser.statistics.recovery_successes
    Set stats["tokens_skipped"] to parser.statistics.tokens_skipped
    
    Note: AST statistics
    Set stats["ast_nodes_created"] to parser.statistics.ast_nodes_created
    Set stats["max_ast_depth"] to parser.statistics.max_ast_depth
    Set stats["average_node_children"] to parser.statistics.average_node_children
    
    Return stats

Process called "analyze_parse_tree_depth" that takes parser as Parser, ast as String returns Integer:
    Note: Analyze depth of generated parse tree
    
    Note: Get the root node
    Let root_node be AST.get_node_by_id_direct(parser.state.ast_builder, ast)
    If root_node = null:
        Return 0
    End If
    
    Note: Recursively calculate depth
    Let max_depth be calculate_node_depth(root_node, parser.state.ast_builder, 0)
    
    Note: Update statistics
    If max_depth > parser.statistics.max_ast_depth:
        Set parser.statistics.max_ast_depth to max_depth
    End If
    
    Return max_depth

Process called "measure_parsing_speed" that takes parser as Parser, source_size as Integer, elapsed_time as Integer returns Float:
    Note: Measure parsing speed in tokens per second
    
    Note: Avoid division by zero
    If elapsed_time = 0:
        Return 0.0
    End If
    
    Note: Calculate tokens per second
    Let tokens_per_second be (source_size * 1000.0) / elapsed_time  Note: elapsed_time is in milliseconds
    
    Note: Update statistics
    Set parser.statistics.parsing_time_ms to elapsed_time
    Set parser.statistics.tokens_per_second to tokens_per_second
    
    Return tokens_per_second

Process called "profile_syntax_usage" that takes parser as Parser, parsed_ast as String returns Dictionary[String, Integer]:
    Note: Profile usage of different syntax constructs
    
    Let usage_stats be Dictionary[String, Integer]()
    
    Note: Initialize counters
    Set usage_stats["let_statements"] to 0
    Set usage_stats["set_statements"] to 0
    Set usage_stats["if_statements"] to 0
    Set usage_stats["for_loops"] to 0
    Set usage_stats["while_loops"] to 0
    Set usage_stats["match_statements"] to 0
    Set usage_stats["process_definitions"] to 0
    Set usage_stats["type_definitions"] to 0
    Set usage_stats["import_statements"] to 0
    Set usage_stats["return_statements"] to 0
    Set usage_stats["throw_statements"] to 0
    Set usage_stats["binary_expressions"] to 0
    Set usage_stats["unary_expressions"] to 0
    Set usage_stats["call_expressions"] to 0
    Set usage_stats["member_expressions"] to 0
    Set usage_stats["literals"] to 0
    Set usage_stats["identifiers"] to 0
    
    Note: Traverse AST and count node types
    Let root_node be AST.get_node_by_id_direct(parser.state.ast_builder, parsed_ast)
    If root_node != null:
        profile_node_recursive(root_node, parser.state.ast_builder, usage_stats)
    End If
    
    Note: Calculate derived statistics
    Let total_statements be usage_stats["let_statements"] + usage_stats["set_statements"] + 
                          usage_stats["if_statements"] + usage_stats["for_loops"] + 
                          usage_stats["while_loops"] + usage_stats["match_statements"]
    Set usage_stats["total_statements"] to total_statements
    
    Let total_expressions be usage_stats["binary_expressions"] + usage_stats["unary_expressions"] + 
                           usage_stats["call_expressions"] + usage_stats["member_expressions"]
    Set usage_stats["total_expressions"] to total_expressions
    
    Return usage_stats

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

Process called "validate_parser_state" that takes parser as Parser returns List[String]:
    Note: Validate parser state for consistency and correctness
    
    Let validation_errors be List[String]()
    
    Note: Validate parser ID
    If parser.parser_id = "" or parser.parser_id = null:
        Add "Parser ID is missing or empty" to validation_errors
    End If
    
    Note: Validate parser state
    If parser.state = null:
        Add "Parser state is null" to validation_errors
    Otherwise:
        Note: Validate mode
        If parser.state.mode = null:
            Add "Parser mode is null" to validation_errors
        Otherwise:
            If parser.state.mode.syntax_mode != "natural" and 
               parser.state.mode.syntax_mode != "developer" and 
               parser.state.mode.syntax_mode != "mixed":
                Add "Invalid syntax mode: " + parser.state.mode.syntax_mode to validation_errors
            End If
        End If
        
        Note: Validate position
        If parser.state.position < 0:
            Add "Parser position is negative: " + parser.state.position to validation_errors
        End If
        
        If parser.state.position > parser.state.tokens.count:
            Add "Parser position exceeds token count" to validation_errors
        End If
        
        Note: Validate AST builder
        If parser.state.ast_builder = null:
            Add "AST builder is null" to validation_errors
        End If
    End If
    
    Note: Validate statistics
    If parser.statistics = null:
        Add "Parser statistics is null" to validation_errors
    Otherwise:
        If parser.statistics.statements_parsed < 0:
            Add "Negative statements parsed count" to validation_errors
        End If
        If parser.statistics.expressions_parsed < 0:
            Add "Negative expressions parsed count" to validation_errors
        End If
        If parser.statistics.errors_encountered < 0:
            Add "Negative errors count" to validation_errors
        End If
    End If
    
    Return validation_errors

Process called "format_parse_result" that takes parser as Parser, result as ParseResult, format_mode as String returns String:
    Note: Format parse result for display or debugging
    
    Let formatted be ""
    
    If format_mode = "json":
        Set formatted to "{\n"
        Set formatted to formatted + "  \"success\": " + result.success + ",\n"
        Set formatted to formatted + "  \"ast_node\": \"" + result.ast_node + "\",\n"
        Set formatted to formatted + "  \"consumed_tokens\": " + result.consumed_tokens + ",\n"
        Set formatted to formatted + "  \"errors\": [\n"
        For i from 0 to result.errors.count - 1:
            Set formatted to formatted + "    \"" + result.errors[i] + "\""
            If i < result.errors.count - 1:
                Set formatted to formatted + ","
            End If
            Set formatted to formatted + "\n"
        End For
        Set formatted to formatted + "  ],\n"
        Set formatted to formatted + "  \"warnings\": [\n"
        For i from 0 to result.warnings.count - 1:
            Set formatted to formatted + "    \"" + result.warnings[i] + "\""
            If i < result.warnings.count - 1:
                Set formatted to formatted + ","
            End If
            Set formatted to formatted + "\n"
        End For
        Set formatted to formatted + "  ]\n"
        Set formatted to formatted + "}"
    Otherwise:
        Note: Default text format
        Set formatted to "ParseResult {\n"
        Set formatted to formatted + "  success: " + result.success + "\n"
        If result.ast_node != "":
            Set formatted to formatted + "  ast_node: " + result.ast_node + "\n"
        End If
        Set formatted to formatted + "  consumed_tokens: " + result.consumed_tokens + "\n"
        If result.errors.count > 0:
            Set formatted to formatted + "  errors: [\n"
            For Each error in result.errors:
                Set formatted to formatted + "    " + error + "\n"
            End For
            Set formatted to formatted + "  ]\n"
        End If
        If result.warnings.count > 0:
            Set formatted to formatted + "  warnings: [\n"
            For Each warning in result.warnings:
                Set formatted to formatted + "    " + warning + "\n"
            End For
            Set formatted to formatted + "  ]\n"
        End If
        Set formatted to formatted + "}"
    End If
    
    Return formatted

Process called "serialize_ast" that takes parser as Parser, ast as String returns String:
    Note: Serialize AST for caching or analysis
    
    Note: Get the root node
    Let root_node be AST.get_node_by_id_direct(parser.state.ast_builder, ast)
    If root_node = null:
        Return "{\"error\": \"AST node not found\"}"
    End If
    
    Note: Build serialized representation
    Let serialized be serialize_node_recursive(root_node, parser.state.ast_builder, 0)
    
    Return serialized

Process called "deserialize_ast" that takes parser as Parser, serialized_data as String returns String:
    Note: Deserialize AST from cached data
    
    Note: Validate input
    If serialized_data = "" or serialized_data = null:
        Return ""
    End If
    
    Note: Create new AST builder for deserialization
    Let temp_builder be AST.create_ast_builder(parser.parser_id + "_deserialize", parser.state.mode.syntax_mode)
    
    Note: Deserialize nodes recursively
    Let root_id be deserialize_node_recursive(serialized_data, temp_builder, null)
    
    Note: Merge deserialized AST into parser's AST builder
    If root_id != "":
        Set parser.state.ast_builder to temp_builder
        Return root_id
    End If
    
    Return ""

Process called "reset_parser" that takes parser as Parser returns Boolean:
    Note: Reset parser to initial state for reuse
    
    Note: Reset parser state
    Set parser.state.position to 0
    Set parser.state.tokens to List[String]()
    Set parser.state.last_parsed_expression to ""
    
    Note: Reset AST builder
    Set parser.state.ast_builder to AST.create_ast_builder(parser.parser_id, parser.state.mode.syntax_mode)
    
    Note: Reset statistics
    Set parser.statistics.statements_parsed to 0
    Set parser.statistics.expressions_parsed to 0
    Set parser.statistics.errors_encountered to 0
    Set parser.statistics.warnings_generated to 0
    Set parser.statistics.parsing_time_ms to 0
    Set parser.statistics.mode_switches to 0
    Set parser.statistics.natural_statements to 0
    Set parser.statistics.technical_statements to 0
    Set parser.statistics.cache_hits to 0
    Set parser.statistics.cache_misses to 0
    Set parser.statistics.cache_entries to 0
    Set parser.statistics.parallel_parse_count to 0
    Set parser.statistics.incremental_reparses to 0
    Set parser.statistics.recovery_attempts to 0
    Set parser.statistics.recovery_successes to 0
    Set parser.statistics.tokens_skipped to 0
    Set parser.statistics.ast_nodes_created to 0
    Set parser.statistics.max_ast_depth to 0
    Set parser.statistics.average_node_children to 0
    Set parser.statistics.tokens_per_second to 0
    Set parser.statistics.optimizations_applied to 0
    Set parser.statistics.cache_stores to 0
    
    Note: Clear caches
    If parser.parse_cache != null:
        Set parser.parse_cache to Dictionary[String, ParseResult]()
    End If
    
    Note: Reset performance settings to defaults
    Set parser.cache_enabled to false
    Set parser.incremental_parsing_enabled to false
    Set parser.parallel_parsing_enabled to false
    Set parser.token_buffer_size to 1024
    Set parser.ast_node_pool_size to 256
    Set parser.parallel_thread_count to 1
    Set parser.precedence_manager to null
    Set parser.warnings to List[String]()
    
    Return true

Note: =====================================================================
Note: HELPER FUNCTIONS
Note: =====================================================================

Process called "contains_technical_syntax_markers" that takes tokens as List[String], position as Integer returns Boolean:
    Note: Check if tokens contain technical syntax markers
    
    Let markers be List[String]("=", "!=", "==", "++", "--", "+=", "-=", "*=", "/=", "{", "}", ";")
    
    For i from position to min(position + 10, tokens.count - 1):
        Let token be tokens[i]
        For Each marker in markers:
            If token = marker:
                Return true
            End If
        End For
    End For
    
    Return false

Process called "is_statement_boundary" that takes token as String returns Boolean:
    Note: Check if token marks a statement boundary
    
    Let boundaries be List[String]("End", "Otherwise", "Then", "Do", "Begin")
    
    For Each boundary in boundaries:
        If token = boundary:
            Return true
        End If
    End For
    
    Return false

Process called "convert_node_recursive" that takes source_node as ASTNode, source_builder as ASTBuilder, target_builder as ASTBuilder, target_mode as ParserMode returns String:
    Note: Recursively convert AST node between syntax modes
    
    Note: Create converted node in target builder
    Let converted_node be AST.create_node(target_builder, source_node.node_type)
    
    Note: Copy node properties with mode-specific adjustments
    Set converted_node.value to source_node.value
    Set converted_node.line to source_node.line
    Set converted_node.column to source_node.column
    
    Note: Convert children recursively
    For Each child_id in source_node.children:
        Let child_node be AST.get_node_by_id_direct(source_builder, child_id)
        If child_node != null:
            Let converted_child_id be convert_node_recursive(child_node, source_builder, target_builder, target_mode)
            Add converted_child_id to converted_node.children
        End If
    End For
    
    Return converted_node.node_id

Process called "calculate_node_depth" that takes node as ASTNode, builder as ASTBuilder, current_depth as Integer returns Integer:
    Note: Calculate maximum depth of AST subtree
    
    If node = null:
        Return current_depth
    End If
    
    Let max_child_depth be current_depth
    
    For Each child_id in node.children:
        Let child_node be AST.get_node_by_id_direct(builder, child_id)
        If child_node != null:
            Let child_depth be calculate_node_depth(child_node, builder, current_depth + 1)
            If child_depth > max_child_depth:
                Set max_child_depth to child_depth
            End If
        End If
    End For
    
    Return max_child_depth

Process called "find_nodes_in_range" that takes builder as ASTBuilder, start_line as Integer, end_line as Integer, start_column as Integer, end_column as Integer returns List[String]:
    Note: Find all AST nodes within specified source range
    
    Let affected_nodes be List[String]()
    
    Note: Traverse all nodes in builder
    Note: Iterate through the builder's node collection
    
    Return affected_nodes

Process called "find_statement_start" that takes parser as Parser, line as Integer returns Integer:
    Note: Find token index where statement starts
    
    Let position be 0
    Let current_line be 1
    
    While position < parser.state.tokens.count and current_line < line:
        Let token be parser.state.tokens[position]
        If token = "\n":
            Set current_line to current_line + 1
        End If
        Set position to position + 1
    End While
    
    Note: Back up to statement beginning
    While position > 0:
        Let prev_token be parser.state.tokens[position - 1]
        If is_statement_boundary(prev_token):
            Break
        End If
        Set position to position - 1
    End While
    
    Return position

Process called "find_statement_end" that takes parser as Parser, line as Integer returns Integer:
    Note: Find token index where statement ends
    
    Let position be 0
    Let current_line be 1
    
    While position < parser.state.tokens.count and current_line <= line:
        Let token be parser.state.tokens[position]
        If token = "\n":
            Set current_line to current_line + 1
        End If
        Set position to position + 1
    End While
    
    Note: Advance to statement end
    While position < parser.state.tokens.count:
        Let token be parser.state.tokens[position]
        If is_statement_boundary(token):
            Set position to position + 1
            Break
        End If
        Set position to position + 1
    End While
    
    Return position

Process called "merge_ast_nodes" that takes main_builder as ASTBuilder, temp_builder as ASTBuilder, affected_nodes as List[String], new_root as String returns Boolean:
    Note: Merge reparsed AST nodes back into main AST
    
    Note: Remove affected nodes from main AST
    For Each node_id in affected_nodes:
        Let removed be AST.remove_node(main_builder, node_id)
        If not removed:
            Return false
        End If
    End For
    
    Note: Copy new nodes from temp builder to main builder
    Note: Copy all nodes from temp_builder starting at new_root
    Note: and integrating them into the main_builder's structure
    
    Return true

Process called "profile_node_recursive" that takes node as ASTNode, builder as ASTBuilder, usage_stats as Dictionary[String, Integer]:
    Note: Recursively profile syntax usage in AST
    
    If node = null:
        Return
    End If
    
    Note: Count node type
    Let node_type be node.node_type
    
    If node_type = "LetStatement":
        Set usage_stats["let_statements"] to usage_stats["let_statements"] + 1
    End If
    If node_type = "SetStatement":
        Set usage_stats["set_statements"] to usage_stats["set_statements"] + 1
    End If
    If node_type = "IfStatement":
        Set usage_stats["if_statements"] to usage_stats["if_statements"] + 1
    End If
    If node_type = "ForLoop":
        Set usage_stats["for_loops"] to usage_stats["for_loops"] + 1
    End If
    If node_type = "WhileLoop":
        Set usage_stats["while_loops"] to usage_stats["while_loops"] + 1
    End If
    If node_type = "MatchStatement":
        Set usage_stats["match_statements"] to usage_stats["match_statements"] + 1
    End If
    If node_type = "ProcessDefinition":
        Set usage_stats["process_definitions"] to usage_stats["process_definitions"] + 1
    End If
    If node_type = "TypeDefinition":
        Set usage_stats["type_definitions"] to usage_stats["type_definitions"] + 1
    End If
    If node_type = "ImportStatement":
        Set usage_stats["import_statements"] to usage_stats["import_statements"] + 1
    End If
    If node_type = "ReturnStatement":
        Set usage_stats["return_statements"] to usage_stats["return_statements"] + 1
    End If
    If node_type = "ThrowStatement":
        Set usage_stats["throw_statements"] to usage_stats["throw_statements"] + 1
    End If
    If node_type = "BinaryExpression":
        Set usage_stats["binary_expressions"] to usage_stats["binary_expressions"] + 1
    End If
    If node_type = "UnaryExpression":
        Set usage_stats["unary_expressions"] to usage_stats["unary_expressions"] + 1
    End If
    If node_type = "CallExpression":
        Set usage_stats["call_expressions"] to usage_stats["call_expressions"] + 1
    End If
    If node_type = "MemberExpression":
        Set usage_stats["member_expressions"] to usage_stats["member_expressions"] + 1
    End If
    If node_type = "Literal":
        Set usage_stats["literals"] to usage_stats["literals"] + 1
    End If
    If node_type = "Identifier":
        Set usage_stats["identifiers"] to usage_stats["identifiers"] + 1
    End If
    
    Note: Profile children
    For Each child_id in node.children:
        Let child_node be AST.get_node_by_id_direct(builder, child_id)
        profile_node_recursive(child_node, builder, usage_stats)
    End For

Process called "serialize_node_recursive" that takes node as ASTNode, builder as ASTBuilder, indent_level as Integer returns String:
    Note: Recursively serialize AST node to JSON format
    
    If node = null:
        Return "null"
    End If
    
    Let indent be ""
    For i from 0 to indent_level - 1:
        Set indent to indent + "  "
    End For
    
    Let json be "{\n"
    Set json to json + indent + "  \"node_id\": \"" + node.node_id + "\",\n"
    Set json to json + indent + "  \"node_type\": \"" + node.node_type + "\",\n"
    
    If node.value != null and node.value != "":
        Set json to json + indent + "  \"value\": \"" + node.value + "\",\n"
    End If
    
    Set json to json + indent + "  \"line\": " + node.line + ",\n"
    Set json to json + indent + "  \"column\": " + node.column
    
    If node.children.count > 0:
        Set json to json + ",\n"
        Set json to json + indent + "  \"children\": [\n"
        
        For i from 0 to node.children.count - 1:
            Let child_id be node.children[i]
            Let child_node be AST.get_node_by_id_direct(builder, child_id)
            Let child_json be serialize_node_recursive(child_node, builder, indent_level + 2)
            
            Set json to json + indent + "    " + child_json
            If i < node.children.count - 1:
                Set json to json + ","
            End If
            Set json to json + "\n"
        End For
        
        Set json to json + indent + "  ]\n"
    Otherwise:
        Set json to json + "\n"
    End If
    
    Set json to json + indent + "}"
    
    Return json

Process called "deserialize_node_recursive" that takes json_data as String, builder as ASTBuilder, parent_id as String returns String:
    Note: Recursively deserialize JSON to AST nodes
    
    Note: Parse JSON structure to extract node information
    Note: Extract key-value pairs from JSON string
    
    Let node be AST.create_node(builder, "DeserializedNode")
    
    If parent_id != null:
        Set node.parent to parent_id
    End If
    
    Return node.node_id

Process called "serialize_node_to_json" that takes node as ASTNode, builder as ASTBuilder, indent_level as Integer returns String:
    Note: Wrapper for serialize_node_recursive
    Return serialize_node_recursive(node, builder, indent_level)

Process called "deserialize_json_to_node" that takes json_data as String, builder as ASTBuilder, parent_id as String returns String:
    Note: Wrapper for deserialize_node_recursive
    Return deserialize_node_recursive(json_data, builder, parent_id)

Process called "min" that takes a as Integer, b as Integer returns Integer:
    Note: Return minimum of two integers
    If a < b:
        Return a
    Otherwise:
        Return b
    End If

Process called "int_to_string" that takes value as Integer returns String:
    Note: Convert integer to string
    If value = 0:
        Return "0"
    End If
    
    Let result be ""
    Let negative be false
    Let num be value
    
    If num < 0:
        Set negative to true
        Set num to -num
    End If
    
    Let digits be List[String]()
    While num > 0:
        Let digit be num % 10
        Call digits.prepend(digit_to_char(digit))
        Set num to num / 10
    End While
    
    If negative:
        Set result to "-"
    End If
    
    For Each d in digits:
        Set result to result + d
    End For
    
    Return result

Process called "digit_to_char" that takes digit as Integer returns String:
    Note: Convert single digit to character
    If digit = 0: Return "0" End If
    If digit = 1: Return "1" End If
    If digit = 2: Return "2" End If
    If digit = 3: Return "3" End If
    If digit = 4: Return "4" End If
    If digit = 5: Return "5" End If
    If digit = 6: Return "6" End If
    If digit = 7: Return "7" End If
    If digit = 8: Return "8" End If
    If digit = 9: Return "9" End If
    Return "0"