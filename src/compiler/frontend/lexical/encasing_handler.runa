Note:
compiler/frontend/lexical/encasing_handler.runa
Encasing Keyword Pattern Handler

This module handles Runa's revolutionary encasing keyword pairs feature:
- Variable names can contain spaces and natural language
- Keywords create boundaries around identifiers
- "Let my variable name be 42" - identifier is "my variable name"
- "Set complex calculation result to value" - identifier is "complex calculation result"

This enables natural, readable code that matches how humans think.
:End Note

Import "compiler/internal/collections" as Collections
Import "compiler/internal/string_utils" as StringUtils
Import "compiler/frontend/lexical/keywords" as Keywords
Import "compiler/frontend/lexical/token_stream" as TokenStream

Note: =====================================================================
Note: ENCASING STATE TRACKING
Note: =====================================================================

@Reasoning
    The lexer needs to track when it's inside an encasing pattern.
    This state helps determine whether to treat words as part of an
    identifier or as separate tokens.
@End Reasoning

Type called "EncasingState":
    is_active as Boolean
    start_keyword as String
    end_keyword as String
    captured_words as Collections.List
    start_line as Integer
    start_column as Integer
End Type

Process called "create_encasing_state" returns EncasingState:
    Return EncasingState with
        is_active as False,
        start_keyword as "",
        end_keyword as "",
        captured_words as Collections.create_list(),
        start_line as 0,
        start_column as 0
    End EncasingState
End Process

Note: =====================================================================
Note: ENCASING PATTERN DETECTION
Note: =====================================================================

@Reasoning
    When we encounter a potential encasing start keyword, we need to
    determine if it actually starts an encasing pattern or is used
    in a different context.
@End Reasoning

Process called "start_encasing" that takes state as EncasingState, keyword as String, line as Integer, column as Integer returns Nothing:
    Note: Activate encasing mode
    Set state.is_active to True
    Set state.start_keyword to keyword
    Set state.end_keyword to Keywords.get_encasing_end(keyword)
    Set state.captured_words to Collections.create_list()
    Set state.start_line to line
    Set state.start_column to column
End Process

Process called "add_word_to_encasing" that takes state as EncasingState, word as String returns Nothing:
    Note: Add a word to the captured identifier
    Collections.list_append(state.captured_words, word)
End Process

Process called "complete_encasing" that takes state as EncasingState returns String:
    Note: Combine captured words into final identifier
    Let identifier be ""
    
    For i from 0 to Collections.list_length(state.captured_words) minus 1:
        If i is greater than 0:
            Set identifier to StringUtils.concat_strings(identifier, " ")
        End If
        Let word be Collections.list_get(state.captured_words, i)
        Set identifier to StringUtils.concat_strings(identifier, word)
    End For
    
    Note: Reset state
    Set state.is_active to False
    Set state.start_keyword to ""
    Set state.end_keyword to ""
    Set state.captured_words to Collections.create_list()
    
    Return identifier
End Process

Note: =====================================================================
Note: ENCASING VALIDATION
Note: =====================================================================

@Reasoning
    We need to validate that encasing patterns are well-formed.
    This includes checking for nested encasing (not allowed) and
    ensuring end keywords match their start keywords.
@End Reasoning

Process called "is_valid_encasing_context" that takes previous_token as TokenStream.Token, next_char as Character returns Boolean:
    Note: Check if context allows starting an encasing pattern
    
    Note: After Let/Set/Define, we expect a space before the identifier
    If next_char is not equal to ' ' And next_char is not equal to '\t':
        Return False
    End If
    
    Note: Check previous token context
    Match previous_token.token_type:
        When TokenStream.Newline:
            Return True  Note: Start of line is valid
        When TokenStream.Delimiter(";"):
            Return True  Note: After statement separator
        When TokenStream.Keyword("End"):
            Return True  Note: After block end
        When TokenStream.Indent:
            Return True  Note: After indentation
        Otherwise:
            Return False  Note: Other contexts invalid
    End Match
End Process

Process called "should_end_encasing" that takes state as EncasingState, word as String returns Boolean:
    Note: Check if this word ends the current encasing pattern
    
    If Not state.is_active:
        Return False
    End If
    
    Note: Check for the expected end keyword
    If StringUtils.string_equals(word, state.end_keyword):
        Return True
    End If
    
    Note: Check for keywords that force encasing to end
    If Keywords.is_keyword(word) And Not Keywords.can_be_in_identifier(word):
        Return True
    End If
    
    Return False
End Process

Note: =====================================================================
Note: AMBIGUITY RESOLUTION
Note: =====================================================================

@Reasoning
    In cases like "Let x be y be 5", we use the "last occurrence wins" rule.
    The identifier is "x be y" and the value starts after the last "be".
@End Reasoning

Process called "find_last_end_keyword" that takes words as Collections.List, end_keyword as String returns Integer:
    Note: Find the last occurrence of the end keyword
    Let last_index be negative 1
    
    For i from 0 to Collections.list_length(words) minus 1:
        Let word be Collections.list_get(words, i)
        If StringUtils.string_equals(word, end_keyword):
            Set last_index to i
        End If
    End For
    
    Return last_index
End Process

Process called "resolve_ambiguous_encasing" that takes state as EncasingState returns String:
    Note: Handle cases with multiple potential end keywords
    
    Let last_end_index be find_last_end_keyword(state.captured_words, state.end_keyword)
    
    If last_end_index is equal to negative 1:
        Note: No end keyword found in captured words
        Return complete_encasing(state)
    End If
    
    Note: Keep only words before the last end keyword
    Let final_words be Collections.create_list()
    For i from 0 to last_end_index minus 1:
        Let word be Collections.list_get(state.captured_words, i)
        Collections.list_append(final_words, word)
    End For
    
    Note: Update state with resolved words
    Set state.captured_words to final_words
    Return complete_encasing(state)
End Process

Note: =====================================================================
Note: LEXER INTEGRATION
Note: =====================================================================

@Reasoning
    These functions integrate with the main lexer to handle encasing
    patterns during tokenization.
@End Reasoning

Process called "process_potential_encasing_start" that takes char_stream as TokenStream.CharacterStream, keyword as String, state as EncasingState returns Boolean:
    Note: Check if this keyword starts an encasing pattern
    
    If Not Keywords.is_encasing_start(keyword):
        Return False
    End If
    
    If state.is_active:
        Note: Already in encasing mode - this is an error
        Return False
    End If
    
    Note: Peek ahead to check context
    Let next_char be TokenStream.peek_char(char_stream, 0)
    
    Note: For now, accept if followed by whitespace
    If next_char is equal to ' ' Or next_char is equal to '\t':
        start_encasing(state, keyword, char_stream.line, char_stream.column)
        Return True
    End If
    
    Return False
End Process

Process called "handle_word_in_encasing" that takes state as EncasingState, word as String returns String:
    Note: Process a word while in encasing mode
    
    If should_end_encasing(state, word):
        Note: This word ends the encasing
        Return complete_encasing(state)
    Otherwise:
        Note: This word is part of the identifier
        add_word_to_encasing(state, word)
        Return ""  Note: No identifier completed yet
    End If
End Process

Note: =====================================================================
Note: ERROR RECOVERY
Note: =====================================================================

@Reasoning
    If an encasing pattern is malformed, we need graceful recovery.
    This helps the lexer continue processing despite errors.
@End Reasoning

Process called "abort_encasing" that takes state as EncasingState returns String:
    Note: Abort current encasing and return partial identifier
    
    If Collections.list_length(state.captured_words) is greater than 0:
        Let partial be complete_encasing(state)
        Return partial
    End If
    
    Note: Reset state
    Set state.is_active to False
    Set state.start_keyword to ""
    Set state.end_keyword to ""
    Set state.captured_words to Collections.create_list()
    
    Return ""
End Process

Process called "is_encasing_active" that takes state as EncasingState returns Boolean:
    Return state.is_active
End Process

Process called "get_encasing_start_position" that takes state as EncasingState returns TokenStream.StreamPosition:
    Return TokenStream.StreamPosition with
        absolute_position as 0,
        relative_position as 0,
        line_number as state.start_line,
        column_number as state.start_column,
        bookmark_name as ""
    End TokenStream.StreamPosition
End Process