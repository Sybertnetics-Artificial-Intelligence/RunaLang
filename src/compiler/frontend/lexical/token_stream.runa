Note:
compiler/frontend/lexical/token_stream.runa
Token Stream Processing and Management

This module provides comprehensive token stream functionality including:
- Token buffering and lookahead support
- Stream position tracking and navigation
- Dual syntax mode token formatting
- Token filtering and transformation
- Stream synchronization and recovery
- Performance optimized token access
- Integration with lexer and parser
- Token metadata preservation
:End Note

Import "compiler/frontend/diagnostics/errors" as Errors

Note: =====================================================================
Note: TOKEN STREAM DATA STRUCTURES
Note: =====================================================================

Type called "Token":
    token_type as TokenType
    value as String
    line as Integer
    column as Integer
    length as Integer
    file_path as String
    annotation_data as Optional[Annotation]
    annotations as List[Annotation]
    original_form as Optional[String]      Note: Preserve pre-conversion form for viewer mode
    source_mode as String                   Note: Mode this token was lexed in ("canon" or "developer")
End Type

Type TokenType is:
    | Keyword as String
    | Identifier as String
    | Operator as String
    | MathSymbol as String
    | Literal as LiteralType
    | Delimiter as String
    | Annotation as AnnotationType
    | Comment as String
    | Whitespace
    | Newline
    | Indent
    | Dedent
    | EndOfFile
    | Error as String
End Type

Type LiteralType is:
    | Integer as Integer
    | Float as Float
    | String as String
    | Character as Character
    | Boolean as Boolean
End Type

Type AnnotationType is:
    | Reasoning as String
    | Implementation as String
    | Uncertainty as String
    | Request_Clarification as String
    | KnowledgeReference as String
    | TestCases as String
    | Resource_Constraints as String
    | Security_Scope as String
    | Execution_Model as String
    | Performance_Hints as String
    | Progress as String
    | Feedback as String
    | Translation_Note as String
    | Error_Handling as String
    | Task as String
    | Requirements as String
    | Verify as String
    | Request as String
    | Context as String
    | Collaboration as String
    | Iteration as String
    | Clarification as String
    | Custom as (String, String)
End Type

Type Annotation:
    annotation_type as String
    content as String
    scope_start as Integer
    scope_end as Integer
End Type

Type called "TokenStream":
    stream_id as String
    tokens as List[Token]
    current_position as Integer
    buffer_size as Integer
    lookahead_buffer as List[Token]
    mode as String
    statistics as Dictionary[String, Integer]
    bookmarks as Dictionary[String, Integer]
    character_stream as CharacterStream
End Type

Type called "CharacterStream":
    source as String
    position as Integer
    line as Integer
    column as Integer
    file_path as String
    tab_width as Integer

Type called "StreamPosition":
    absolute_position as Integer
    relative_position as Integer
    line_number as Integer
    column_number as Integer
    bookmark_name as String
End Type

Note: =====================================================================
Note: TOKEN STREAM OPERATIONS
Note: =====================================================================

@Reasoning
    Token streams manage the flow of tokens from lexer to parser.
    We initialize all tracking structures and prepare for efficient lookahead.
@End Reasoning

Process called "create_token_stream" that takes stream_name as String, source_text as String, file_path as String returns TokenStream:
    Note: Create character stream for reading
    Let char_stream be CharacterStream with
        source as source_text,
        position as 0,
        line as 1,
        column as 1,
        file_path as file_path,
        tab_width as 4
    End CharacterStream
    
    Note: Initialize token stream
    Let stream be TokenStream with
        stream_id as stream_name,
        tokens as [],
        current_position as 0,
        buffer_size as 16,
        lookahead_buffer as [],
        mode as "canon",
        statistics as {},
        bookmarks as {},
        character_stream as char_stream
    End TokenStream
    
    Note: Initialize statistics
    Set stream.statistics["total_tokens"] to 0
    Set stream.statistics["keywords"] to 0
    Set stream.statistics["identifiers"] to 0
    Set stream.statistics["operators"] to 0
    Set stream.statistics["literals"] to 0
    
    Return stream

@Reasoning
    Retrieves the next token from the stream, advancing position.
    If we've consumed all tokens, returns EndOfFile token.
@End Reasoning

Process called "get_next_token" that takes stream as TokenStream returns Token:
    Note: Check if we have tokens available
    If stream.current_position is greater than or equal to length(stream.tokens):
        Note: Return EOF token when stream exhausted
        Return Token with
            token_type as EndOfFile,
            value as "",
            line as stream.character_stream.line,
            column as stream.character_stream.column,
            length as 0,
            file_path as stream.character_stream.file_path,
            annotations as []
        End Token
    End If
    
    Note: Get token at current position
    Let token be stream.tokens[stream.current_position]
    
    Note: Advance position
    Set stream.current_position to stream.current_position plus 1
    
    Note: Update statistics
    Set stream.statistics["total_tokens"] to stream.statistics["total_tokens"] plus 1
    
    Return token

@Reasoning
    Lookahead functionality for parser - peek without consuming.
    Returns token at specified distance or EOF if beyond stream.
@End Reasoning

Process called "peek_token" that takes stream as TokenStream, distance as Integer returns Token:
    Note: Calculate target position
    Let target_position be stream.current_position plus distance
    
    Note: Check bounds
    If target_position is greater than or equal to length(stream.tokens) Or target_position is less than 0:
        Note: Return EOF for out of bounds
        Return Token with
            token_type as EndOfFile,
            value as "",
            line as stream.character_stream.line,
            column as stream.character_stream.column,
            length as 0,
            file_path as stream.character_stream.file_path,
            annotations as []
        End Token
    End If
    
    Note: Return token at target position without advancing
    Return stream.tokens[target_position]

@Reasoning
    Explicitly consume and return current token.
    Same as get_next_token but more explicit name for parser clarity.
@End Reasoning

Process called "consume_token" that takes stream as TokenStream returns Token:
    Note: Delegate to get_next_token for consistency
    Return get_next_token(stream)

Note: =====================================================================
Note: STREAM NAVIGATION OPERATIONS
Note: =====================================================================

@Reasoning
    Allows jumping to specific positions in the token stream.
    Used for error recovery and bookmark restoration.
@End Reasoning

Process called "seek_to_position" that takes stream as TokenStream, position as Integer returns Boolean:
    Note: Validate position
    If position is less than 0 Or position is greater than length(stream.tokens):
        Return False
    End If
    
    Note: Update stream position
    Set stream.current_position to position
    
    Note: Clear lookahead buffer when seeking
    Set stream.lookahead_buffer to []
    
    Return True

@Reasoning
    Bookmarks allow saving and restoring positions for backtracking.
    Essential for tentative parsing and error recovery.
@End Reasoning

Process called "create_bookmark" that takes stream as TokenStream, bookmark_name as String returns Boolean:
    Note: Save current position with bookmark name
    Set stream.bookmarks[bookmark_name] to stream.current_position
    Return True

@Reasoning
    Restore a previously saved bookmark position.
    Returns false if bookmark doesn't exist.
@End Reasoning

Process called "restore_bookmark" that takes stream as TokenStream, bookmark_name as String returns Boolean:
    Note: Check if bookmark exists
    If Not contains_key(stream.bookmarks, bookmark_name):
        Return False
    End If
    
    Note: Restore position
    Let saved_position be stream.bookmarks[bookmark_name]
    Return seek_to_position(stream, saved_position)

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

Note: =====================================================================
Note: CHARACTER STREAM OPERATIONS
Note: =====================================================================

@Reasoning
    Character stream provides low-level text reading with lookahead.
    Tracks line and column for accurate error reporting.
@End Reasoning

Process called "peek_char" that takes char_stream as CharacterStream, offset as Integer returns Character:
    Note: Look ahead at character without consuming
    Let target_position be char_stream.position plus offset
    
    If target_position is greater than or equal to length(char_stream.source) Or target_position is less than 0:
        Return '\0'  Note: Return null character for EOF
    End If
    
    Return char_stream.source[target_position]
End Process

Process called "advance_char" that takes char_stream as CharacterStream returns Character:
    Note: Get current character and advance position
    If char_stream.position is greater than or equal to length(char_stream.source):
        Return '\0'
    End If
    
    Let current_char be char_stream.source[char_stream.position]
    Set char_stream.position to char_stream.position plus 1
    
    Note: Update line and column tracking
    If current_char is equal to '\n':
        Set char_stream.line to char_stream.line plus 1
        Set char_stream.column to 1
    Otherwise If current_char is equal to '\t':
        Note: Handle tab width for column tracking
        Let spaces_to_next_tab be char_stream.tab_width minus ((char_stream.column minus 1) modulo char_stream.tab_width)
        Set char_stream.column to char_stream.column plus spaces_to_next_tab
    Otherwise:
        Set char_stream.column to char_stream.column plus 1
    End If
    
    Return current_char
End Process

Process called "match_string" that takes char_stream as CharacterStream, expected as String returns Boolean:
    Note: Check if upcoming characters match expected string
    Let match_length be length(expected)
    
    For i from 0 to match_length minus 1:
        If peek_char(char_stream, i) is not equal to expected[i]:
            Return False
        End If
    End For
    
    Note: Consume matched characters
    For i from 0 to match_length minus 1:
        advance_char(char_stream)
    End For
    
    Return True
End Process

Process called "skip_whitespace" that takes char_stream as CharacterStream returns Integer:
    Note: Skip whitespace and return count of spaces skipped
    Let count be 0
    
    While peek_char(char_stream, 0) is equal to ' ' Or peek_char(char_stream, 0) is equal to '\t':
        advance_char(char_stream)
        Set count to count plus 1
    End While
    
    Return count
End Process

Process called "read_until" that takes char_stream as CharacterStream, delimiter as Character returns String:
    Note: Read characters until delimiter is found
    Let result be ""
    
    While peek_char(char_stream, 0) is not equal to delimiter And peek_char(char_stream, 0) is not equal to '\0':
        Let char be advance_char(char_stream)
        Set result to result plus char
    End While
    
    Return result
End Process

Process called "read_while" that takes char_stream as CharacterStream, predicate as Process returns String:
    Note: Read characters while predicate is true
    Let result be ""
    
    While predicate(peek_char(char_stream, 0)):
        Let char be advance_char(char_stream)
        Set result to result plus char
    End While
    
    Return result
End Process

Note: =====================================================================
Note: TOKEN MANIPULATION OPERATIONS
Note: =====================================================================

Process called "add_token" that takes stream as TokenStream, token as Token returns Nothing:
    Note: Add token to stream and update statistics
    stream.tokens.append(token)
    
    Note: Update type-specific statistics
    Match token.token_type:
        When Keyword(_):
            Set stream.statistics["keywords"] to stream.statistics["keywords"] plus 1
        When Identifier(_):
            Set stream.statistics["identifiers"] to stream.statistics["identifiers"] plus 1
        When Operator(_):
            Set stream.statistics["operators"] to stream.statistics["operators"] plus 1
        When Literal(_):
            Set stream.statistics["literals"] to stream.statistics["literals"] plus 1
    End Match
End Process

Process called "current_token" that takes stream as TokenStream returns Token:
    Note: Get current token without advancing
    Return peek_token(stream, 0)
End Process

Process called "has_more_tokens" that takes stream as TokenStream returns Boolean:
    Note: Check if more tokens available
    Return stream.current_position is less than length(stream.tokens)
End Process

Process called "remaining_tokens" that takes stream as TokenStream returns Integer:
    Note: Count remaining tokens in stream
    Return length(stream.tokens) minus stream.current_position
End Process

Note: =====================================================================
Note: UTILITY OPERATIONS
Note: =====================================================================

@Reasoning
    Provides usage statistics for performance monitoring and debugging.
    Helps identify tokenization patterns and potential optimizations.
@End Reasoning

Process called "get_stream_statistics" that takes stream as TokenStream returns Dictionary[String, Integer]:
    Note: Add current position to statistics
    Set stream.statistics["current_position"] to stream.current_position
    Set stream.statistics["total_tokens_in_stream"] to length(stream.tokens)
    Set stream.statistics["bookmark_count"] to length(stream.bookmarks)
    
    Return stream.statistics