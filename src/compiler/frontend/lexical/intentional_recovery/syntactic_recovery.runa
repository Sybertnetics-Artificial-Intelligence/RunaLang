Note:
compiler/frontend/lexical/intentional_recovery/syntactic_recovery.runa  
Syntactic Recovery - Stage 1 Traditional Methods

This module implements traditional error recovery techniques including:
- Synchronization point detection
- Typo correction using edit distance
- Phrase-level recovery
- Panic mode recovery
- Token insertion/deletion heuristics
:End Note

Import Module "compiler/internal/collections" as Collections
Import Module "compiler/internal/string_utils" as StringUtils
Import Module "compiler/frontend/lexical/keywords" as Keywords

Note: =====================================================================
Note: SYNCHRONIZATION POINTS
Note: =====================================================================

Type called "SynchronizationPoint":
    keyword as String
    strength as Float
    context_required as Boolean
    recovery_action as String
End Type

Type called "PhraseContext":
    tokens_before as Collections.List
    error_token as String
    tokens_after as Collections.List
    line_content as String
    indentation_level as Integer
End Type

Note: =====================================================================
Note: SYNCHRONIZATION POINT MANAGEMENT
Note: =====================================================================

Process called "initialize_synchronization_points" returns Collections.List:
    @Reasoning
        Define synchronization points where the parser can safely resume
        after encountering an error. These are typically statement boundaries
        and block delimiters.
    @End Reasoning
    
    Let sync_points be Collections.create_list()
    
    Note: Strong synchronization points (statement boundaries)
    Collections.list_append(sync_points, create_sync_point("End", 1.0, False, "close_block"))
    Collections.list_append(sync_points, create_sync_point("Process", 0.9, True, "start_function"))
    Collections.list_append(sync_points, create_sync_point("Type", 0.9, True, "start_type"))
    Collections.list_append(sync_points, create_sync_point("If", 0.85, True, "start_conditional"))
    Collections.list_append(sync_points, create_sync_point("For", 0.85, True, "start_loop"))
    Collections.list_append(sync_points, create_sync_point("While", 0.85, True, "start_loop"))
    
    Note: Medium synchronization points (statement starters)
    Collections.list_append(sync_points, create_sync_point("Let", 0.7, True, "start_declaration"))
    Collections.list_append(sync_points, create_sync_point("Set", 0.7, True, "start_assignment"))
    Collections.list_append(sync_points, create_sync_point("Return", 0.7, True, "start_return"))
    Collections.list_append(sync_points, create_sync_point("Import", 0.7, True, "start_import"))
    
    Note: Weak synchronization points (expression boundaries)
    Collections.list_append(sync_points, create_sync_point("Otherwise", 0.5, False, "else_branch"))
    Collections.list_append(sync_points, create_sync_point("When", 0.5, True, "match_case"))
    Collections.list_append(sync_points, create_sync_point("Catch", 0.5, False, "error_handler"))
    
    Return sync_points
End Process

Process called "find_recovery_point" that takes context as PhraseContext returns Optional[SynchronizationPoint]:
    @Implementation
        Find the nearest synchronization point where recovery can begin.
        Look both forward and backward from the error location.
    @End Implementation
    
    Let sync_points be initialize_synchronization_points()
    Let best_point be None
    Let best_distance be 999999
    
    Note: Search forward for sync points
    Let forward_distance be 0
    For Each token in context.tokens_after:
        Set forward_distance to forward_distance plus 1
        
        For Each sync_point in sync_points:
            If string_equals(token, sync_point.keyword):
                If forward_distance is less than best_distance:
                    Set best_distance to forward_distance
                    Set best_point to sync_point
                End If
            End If
        End For
        
        Note: Don't search too far
        If forward_distance is greater than 10:
            Break
        End If
    End For
    
    Note: Search backward for strong sync points only
    Let backward_distance be 0
    For Each token in reverse_list(context.tokens_before):
        Set backward_distance to backward_distance plus 1
        
        For Each sync_point in sync_points:
            If sync_point.strength is greater than 0.8:
                If string_equals(token, sync_point.keyword):
                    If backward_distance is less than best_distance:
                        Set best_distance to backward_distance
                        Set best_point to sync_point
                    End If
                End If
            End If
        End For
        
        If backward_distance is greater than 5:
            Break
        End If
    End For
    
    Return best_point
End Process

Note: =====================================================================
Note: TYPO DETECTION AND CORRECTION
Note: =====================================================================

Process called "find_typo_candidates" that takes error_token as String, context as Any returns Collections.List:
    @Reasoning
        Find potential corrections for typos using edit distance.
        Check against keywords, known identifiers, and common patterns.
    @End Reasoning
    
    Let candidates be Collections.create_list()
    Let max_distance be calculate_max_edit_distance(string_length(error_token))
    
    Note: Check against keywords
    Let all_keywords be Keywords.get_all_keywords()
    For Each keyword in all_keywords:
        Let distance be StringUtils.levenshtein_distance(error_token, keyword)
        If distance is less than or equal to max_distance:
            Collections.list_append(candidates, keyword)
        End If
    End For
    
    Note: Check for common typo patterns
    Let common_corrections be detect_common_typo_patterns(error_token)
    For Each correction in common_corrections:
        If not list_contains(candidates, correction):
            Collections.list_append(candidates, correction)
        End If
    End For
    
    Note: Sort candidates by edit distance
    Let sorted_candidates be sort_by_edit_distance(candidates, error_token)
    
    Return sorted_candidates
End Process

Process called "detect_common_typo_patterns" that takes token as String returns Collections.List:
    @Implementation
        Detect and correct common typo patterns like:
        - Transposed characters (teh -> the)
        - Missing characters (Proess -> Process)
        - Extra characters (Processs -> Process)
        - Wrong case (process -> Process)
    @End Implementation
    
    Let corrections be Collections.create_list()
    
    Note: Check for transposed characters
    Let transposed be detect_transposition(token)
    If transposed is not None:
        Collections.list_append(corrections, transposed)
    End If
    
    Note: Check for doubled characters
    Let dedoubled be remove_doubled_characters(token)
    If not string_equals(dedoubled, token):
        Collections.list_append(corrections, dedoubled)
    End If
    
    Note: Check for case variations
    Let case_corrected be correct_keyword_case(token)
    If case_corrected is not None:
        Collections.list_append(corrections, case_corrected)
    End If
    
    Note: Check for common substitutions
    Let substituted be apply_common_substitutions(token)
    If substituted is not None:
        Collections.list_append(corrections, substituted)
    End If
    
    Return corrections
End Process

Note: =====================================================================
Note: PHRASE-LEVEL RECOVERY
Note: =====================================================================

Process called "extract_phrase_context" that takes error as Any, context as Any returns PhraseContext:
    @Implementation
        Extract the phrase context around an error for analysis.
        This includes tokens before and after the error.
    @End Implementation
    
    Let phrase be PhraseContext with
        tokens_before as Collections.create_list(),
        error_token as error.token_text,
        tokens_after as Collections.create_list(),
        line_content as error.line_content,
        indentation_level as calculate_indentation(error.line_content)
    End PhraseContext
    
    Note: Extract tokens before error (up to 5)
    Let before_count be 0
    Let current_pos be error.position minus 1
    While current_pos is greater than or equal to 0 and before_count is less than 5:
        Let token be extract_token_at_position(context, current_pos)
        If token is not None:
            Collections.list_prepend(phrase.tokens_before, token)
            Set before_count to before_count plus 1
        End If
        Set current_pos to current_pos minus 1
    End While
    
    Note: Extract tokens after error (up to 5)
    Let after_count be 0
    Let current_pos be error.position plus 1
    Let max_pos be get_context_length(context)
    While current_pos is less than max_pos and after_count is less than 5:
        Let token be extract_token_at_position(context, current_pos)
        If token is not None:
            Collections.list_append(phrase.tokens_after, token)
            Set after_count to after_count plus 1
        End If
        Set current_pos to current_pos plus 1
    End While
    
    Return phrase
End Process

Process called "generate_phrase_correction" that takes context as PhraseContext, recovery_point as SynchronizationPoint returns String:
    @Reasoning
        Generate a correction based on phrase-level patterns.
        This looks at common statement structures and attempts to complete them.
    @End Reasoning
    
    Note: Analyze the phrase pattern
    Let pattern be identify_phrase_pattern(context)
    
    Match pattern:
        When "incomplete_declaration":
            Return complete_declaration(context)
        When "incomplete_assignment":
            Return complete_assignment(context)
        When "incomplete_function_call":
            Return complete_function_call(context)
        When "incomplete_control_flow":
            Return complete_control_flow(context)
        Default:
            Return generate_default_correction(context, recovery_point)
    End Match
End Process

Process called "identify_phrase_pattern" that takes context as PhraseContext returns String:
    @Implementation
        Identify the type of phrase being constructed based on
        surrounding tokens.
    @End Implementation
    
    Note: Check for declaration pattern (Let ... be/as ...)
    If list_contains_token(context.tokens_before, "Let"):
        Return "incomplete_declaration"
    End If
    
    Note: Check for assignment pattern (Set ... to ...)
    If list_contains_token(context.tokens_before, "Set"):
        Return "incomplete_assignment"
    End If
    
    Note: Check for function call pattern (... with ... as ...)
    If list_contains_token(context.tokens_before, "with"):
        Return "incomplete_function_call"
    End If
    
    Note: Check for control flow pattern (If/For/While ...)
    If list_contains_any(context.tokens_before, ["If", "For", "While"]):
        Return "incomplete_control_flow"
    End If
    
    Return "unknown"
End Process

Note: =====================================================================
Note: PANIC MODE RECOVERY
Note: =====================================================================

Process called "attempt_panic_mode_recovery" that takes engine as Any, error as Any, context as Any returns Any:
    @Implementation
        Panic mode recovery: skip tokens until a synchronization point is found.
        This is the most aggressive recovery strategy.
    @End Implementation
    
    Let sync_points be initialize_synchronization_points()
    Let tokens_skipped be 0
    Let current_pos be error.position
    
    Note: Skip tokens until we find a sync point
    While current_pos is less than get_context_length(context):
        Let token be extract_token_at_position(context, current_pos)
        
        For Each sync_point in sync_points:
            If string_equals(token, sync_point.keyword):
                Note: Found a sync point, recovery successful
                Return create_panic_recovery_result(tokens_skipped, sync_point)
            End If
        End For
        
        Set tokens_skipped to tokens_skipped plus 1
        Set current_pos to current_pos plus 1
        
        Note: Don't skip too many tokens
        If tokens_skipped is greater than 20:
            Break
        End If
    End While
    
    Return create_panic_recovery_failure(tokens_skipped)
End Process

Note: =====================================================================
Note: HELPER FUNCTIONS
Note: =====================================================================

Process called "create_sync_point" that takes keyword as String, strength as Float, context_required as Boolean, action as String returns SynchronizationPoint:
    Return SynchronizationPoint with
        keyword as keyword,
        strength as strength,
        context_required as context_required,
        recovery_action as action
    End SynchronizationPoint
End Process

Process called "calculate_max_edit_distance" that takes token_length as Integer returns Integer:
    @Implementation
        Calculate maximum allowed edit distance based on token length.
        Longer tokens can have more errors.
    @End Implementation
    
    If token_length is less than or equal to 3:
        Return 1
    Otherwise If token_length is less than or equal to 6:
        Return 2
    Otherwise:
        Return 3
    End If
End Process

Process called "sort_by_edit_distance" that takes candidates as Collections.List, original as String returns Collections.List:
    @Implementation
        Sort candidates by their edit distance from the original token.
    @End Implementation
    
    Note: Create list of (candidate, distance) pairs
    Let scored_candidates be Collections.create_list()
    
    For Each candidate in candidates:
        Let distance be StringUtils.levenshtein_distance(original, candidate)
        Let pair be Collections.create_pair(candidate, distance)
        Collections.list_append(scored_candidates, pair)
    End For
    
    Note: Sort by distance
    Let sorted be Collections.sort_by_second(scored_candidates)
    
    Note: Extract just the candidates
    Let result be Collections.create_list()
    For Each pair in sorted:
        Collections.list_append(result, Collections.pair_first(pair))
    End For
    
    Return result
End Process

Process called "detect_transposition" that takes token as String returns Optional[String]:
    @Implementation
        Detect if the token has transposed adjacent characters.
    @End Implementation
    
    Let length be string_length(token)
    
    For i from 0 to length minus 2:
        Note: Try swapping adjacent characters
        Let swapped be swap_characters_at(token, i, i plus 1)
        
        Note: Check if swapped version is a valid keyword
        If Keywords.is_keyword(swapped):
            Return swapped
        End If
    End For
    
    Return None
End Process

Process called "remove_doubled_characters" that takes token as String returns String:
    @Implementation
        Remove accidentally doubled characters (e.g., "Processs" -> "Process").
    @End Implementation
    
    Let result be ""
    Let prev_char be ""
    
    For Each char in string_to_chars(token):
        If not string_equals(char, prev_char):
            Set result to string_concat(result, char)
        End If
        Set prev_char to char
    End For
    
    Return result
End Process

Process called "correct_keyword_case" that takes token as String returns Optional[String]:
    @Implementation
        Correct the case of keywords (e.g., "process" -> "Process").
    @End Implementation
    
    Let lower be string_to_lower(token)
    
    Note: Check if lowercase version matches any keyword
    Let all_keywords be Keywords.get_all_keywords()
    For Each keyword in all_keywords:
        If string_equals(string_to_lower(keyword), lower):
            Return keyword
        End If
    End For
    
    Return None
End Process

Process called "apply_common_substitutions" that takes token as String returns Optional[String]:
    @Implementation
        Apply common character substitutions for typos.
    @End Implementation
    
    Note: Common substitutions: 0->O, 1->I, 5->S, etc.
    Let substitutions be Collections.create_dictionary()
    Collections.dict_set(substitutions, "0", "O")
    Collections.dict_set(substitutions, "1", "I")
    Collections.dict_set(substitutions, "5", "S")
    
    For Each key in Collections.dict_keys(substitutions):
        If string_contains(token, key):
            Let substituted be string_replace(token, key, Collections.dict_get(substitutions, key))
            If Keywords.is_keyword(substituted):
                Return substituted
            End If
        End If
    End For
    
    Return None
End Process

Process called "complete_declaration" that takes context as PhraseContext returns String:
    @Implementation
        Complete an incomplete variable declaration.
    @End Implementation
    
    Note: Pattern: Let <identifier> be <value>
    If list_contains_token(context.tokens_after, "as"):
        Return "be"
    End If
    
    Return "be <value>"
End Process

Process called "complete_assignment" that takes context as PhraseContext returns String:
    @Implementation
        Complete an incomplete assignment statement.
    @End Implementation
    
    Note: Pattern: Set <identifier> to <value>
    Return "to"
End Process

Process called "complete_function_call" that takes context as PhraseContext returns String:
    @Implementation
        Complete an incomplete function call.
    @End Implementation
    
    Note: Pattern: <function> with <param> as <value>
    If list_contains_token(context.tokens_before, "with"):
        Return "as"
    End If
    
    Return "with"
End Process

Process called "complete_control_flow" that takes context as PhraseContext returns String:
    @Implementation
        Complete an incomplete control flow statement.
    @End Implementation
    
    If list_contains_token(context.tokens_before, "If"):
        Return ":"
    Otherwise If list_contains_token(context.tokens_before, "For"):
        If list_contains_token(context.tokens_before, "Each"):
            Return "in"
        Otherwise:
            Return "Each"
        End If
    Otherwise If list_contains_token(context.tokens_before, "While"):
        Return ":"
    End If
    
    Return ":"
End Process

Process called "generate_default_correction" that takes context as PhraseContext, recovery_point as SynchronizationPoint returns String:
    @Implementation
        Generate a default correction when no specific pattern is identified.
    @End Implementation
    
    If recovery_point is not None:
        Return recovery_point.keyword
    End If
    
    Return "End"
End Process

Process called "calculate_indentation" that takes line as String returns Integer:
    @Implementation
        Calculate the indentation level of a line.
    @End Implementation
    
    Let spaces be 0
    For Each char in string_to_chars(line):
        If string_equals(char, " "):
            Set spaces to spaces plus 1
        Otherwise If string_equals(char, "\t"):
            Set spaces to spaces plus 4
        Otherwise:
            Break
        End If
    End For
    
    Return integer_divide(spaces, 4)
End Process

Process called "calculate_phrase_confidence" that takes context as PhraseContext, correction as String returns Float:
    @Implementation
        Calculate confidence in a phrase-level correction.
    @End Implementation
    
    Let confidence be 0.5
    
    Note: Higher confidence if correction completes a known pattern
    If string_contains(correction, "be") or string_contains(correction, "to") or string_contains(correction, "as"):
        Set confidence to confidence plus 0.2
    End If
    
    Note: Higher confidence if indentation is consistent
    If context.indentation_level is greater than 0:
        Set confidence to confidence plus 0.1
    End If
    
    Note: Higher confidence if we have more context
    Let context_size be Collections.list_size(context.tokens_before) plus Collections.list_size(context.tokens_after)
    If context_size is greater than 5:
        Set confidence to confidence plus 0.15
    End If
    
    Return min(confidence, 0.95)
End Process

Process called "calculate_panic_confidence" that takes tokens_skipped as Integer returns Float:
    @Implementation
        Calculate confidence in panic mode recovery.
        Less tokens skipped means higher confidence.
    @End Implementation
    
    If tokens_skipped equals 0:
        Return 0.9
    Otherwise If tokens_skipped is less than 3:
        Return 0.7
    Otherwise If tokens_skipped is less than 6:
        Return 0.5
    Otherwise If tokens_skipped is less than 10:
        Return 0.3
    Otherwise:
        Return 0.1
    End If
End Process