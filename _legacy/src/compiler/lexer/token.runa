Note:
Runa Lexer: Token Types and Utilities

This module defines the core token types and utilities for the Runa lexer,
supporting AI-first case-insensitive variables and natural language syntax.

Key Features:
- Comprehensive token type system
- Case-insensitive token comparison
- Source location tracking
- AI-friendly token normalization
:End Note

Note: Import utilities for shared functions
Import "internal/utilities.runa"

Note: Core Token Types for Runa Lexer
Type TokenType is:
    | KEYWORD               # Case-insensitive keywords (Let, let, LET all same)
    | IDENTIFIER            # Case-insensitive variables (MyVariable = myvariable)
    | QUALIFIED_IDENTIFIER  # Multi-word identifiers (My Variable = my variable)
    | MULTI_WORD_CONSTRUCT  # Multi-word constructs (parser determines meaning)
    | DOT                   # "." for field/method access
    | QUALIFIED_NAME        # Multi-part names (object.field)
    | METHOD_CALL           # Method calls with parameters
    | WITH_KEYWORD          # "with" keyword (case-insensitive)
    | AS_KEYWORD            # "as" keyword (case-insensitive)
    | BE_KEYWORD            # "be" keyword (case-insensitive)
    | LET_KEYWORD           # "let" keyword (case-insensitive)
    | COMMA                 # "," for multiple parameters
    | STRING_LITERAL        # String literals
    | INTEGER_LITERAL       # Integer literals
    | FLOAT_LITERAL         # Float literals
    | BOOLEAN_LITERAL       # true/false (case-insensitive)
    | OPERATOR              # Single-character operators
    | INDENT                # Indentation increase
    | DEDENT                # Indentation decrease
    | NEWLINE               # Line breaks
    | COMMENT               # Note: comments
    | ERROR                 # Lexer errors
    | EOF                   # End of file

Note: Token Structure with AI-First Normalization
Type Token is Dictionary with:
    type as TokenType
    value as String          # Original value (preserved for display)
    normalized_value as String  # Normalized value (for case-insensitive comparison)
    line as Integer
    column as Integer
    file as String
    metadata as Dictionary[String, Any]

Note: Source Location for Error Reporting
Type SourceLocation is Dictionary with:
    line as Integer
    column as Integer
    file as String
    length as Integer

Note: Token Creation Utilities
Process called "create_token" that takes type as TokenType and value as String and line as Integer and column as Integer and file as String returns Token:
    Note: Create a token with automatic normalization
    Let normalized be normalize_identifier with text as value
    
    Return Token with:
        type as type
        value as value
        normalized_value as normalized
        line as line
        column as column
        file as file
        metadata as dictionary containing

Process called "create_error_token" that takes error as LexerError and line as Integer and column as Integer and file as String returns Token:
    Note: Create an error token for error recovery
    Return Token with:
        type as ERROR
        value as "ERROR"
        normalized_value as "error"
        line as line
        column as column
        file as file
        metadata as dictionary containing "error" as error

Process called "normalize_identifier" that takes text as String returns String:
    Note: Normalize identifiers for case-insensitive comparison
    Note: Preserve separators (spaces, underscores) but lowercase everything
    Return to_lowercase with text as text

Note: Token Comparison Utilities
Process called "tokens_equal" that takes token1 as Token and token2 as Token returns Boolean:
    Note: Compare tokens using normalized values for case-insensitive comparison
    Return token1.type is equal to token2.type and
           token1.normalized_value is equal to token2.normalized_value

Process called "is_keyword_token" that takes token as Token returns Boolean:
    Note: Check if token is a keyword
    Return token.type is equal to KEYWORD

Process called "is_identifier_token" that takes token as Token returns Boolean:
    Note: Check if token is an identifier
    Return token.type is equal to IDENTIFIER or token.type is equal to QUALIFIED_IDENTIFIER

Process called "is_multi_word_construct_token" that takes token as Token returns Boolean:
    Note: Check if token is a multi-word construct
    Return token.type is equal to MULTI_WORD_CONSTRUCT

Process called "get_token_location" that takes token as Token returns SourceLocation:
    Note: Extract source location from token
    Return SourceLocation with:
        line as token.line
        column as token.column
        file as token.file
        length as length of token.value

Note: Token Analysis Utilities
Process called "get_token_type_name" that takes token_type as TokenType returns String:
    Note: Get human-readable name for token type
    Match token_type:
        When KEYWORD:
            Return "Keyword"
        When IDENTIFIER:
            Return "Identifier"
        When QUALIFIED_IDENTIFIER:
            Return "Qualified Identifier"
        When MULTI_WORD_CONSTRUCT:
            Return "Multi-Word Construct"
        When DOT:
            Return "Dot"
        When QUALIFIED_NAME:
            Return "Qualified Name"
        When METHOD_CALL:
            Return "Method Call"
        When WITH_KEYWORD:
            Return "With Keyword"
        When AS_KEYWORD:
            Return "As Keyword"
        When BE_KEYWORD:
            Return "Be Keyword"
        When LET_KEYWORD:
            Return "Let Keyword"
        When COMMA:
            Return "Comma"
        When STRING_LITERAL:
            Return "String Literal"
        When INTEGER_LITERAL:
            Return "Integer Literal"
        When FLOAT_LITERAL:
            Return "Float Literal"
        When BOOLEAN_LITERAL:
            Return "Boolean Literal"
        When OPERATOR:
            Return "Operator"
        When INDENT:
            Return "Indent"
        When DEDENT:
            Return "Dedent"
        When NEWLINE:
            Return "Newline"
        When COMMENT:
            Return "Comment"
        When ERROR:
            Return "Error"
        When EOF:
            Return "End of File"

Process called "format_token" that takes token as Token returns String:
    Note: Format token for display
    Let type_name be get_token_type_name with token_type as token.type
    Return type_name plus "('" plus token.value plus "') at " plus token.file plus ":" plus token.line plus ":" plus token.column

Process called "is_literal_token" that takes token as Token returns Boolean:
    Note: Check if token is a literal
    Return token.type is equal to STRING_LITERAL or
           token.type is equal to INTEGER_LITERAL or
           token.type is equal to FLOAT_LITERAL or
           token.type is equal to BOOLEAN_LITERAL

Process called "is_structure_token" that takes token as Token returns Boolean:
    Note: Check if token is a structural token
    Return token.type is equal to INDENT or
           token.type is equal to DEDENT or
           token.type is equal to NEWLINE or
           token.type is equal to COMMENT

Process called "is_operator_token" that takes token as Token returns Boolean:
    Note: Check if token is an operator
    Return token.type is equal to OPERATOR or
           token.type is equal to DOT

Note: Token Collection Utilities
Process called "find_tokens_by_type" that takes tokens as List[Token] and token_type as TokenType returns List[Token]:
    Note: Find all tokens of a specific type
    Let result be list containing
    For each token in tokens:
        If token.type is equal to token_type:
            Add token to result
    Return result

Process called "find_tokens_by_value" that takes tokens as List[Token] and value as String returns List[Token]:
    Note: Find all tokens with a specific value (case-insensitive)
    Let result be list containing
    Let normalized_value be normalize_identifier with text as value
    For each token in tokens:
        If token.normalized_value is equal to normalized_value:
            Add token to result
    Return result

Process called "get_token_summary" that takes tokens as List[Token] returns Dictionary[String, Any]:
    Note: Get summary statistics about tokens
    Let type_counts be dictionary containing
    Let total_tokens be length of tokens
    Let error_tokens be 0
    
    For each token in tokens:
        Let type_name be get_token_type_name with token_type as token.type
        If token.type is equal to ERROR:
            Set error_tokens to error_tokens plus 1
        
        If type_name is in type_counts:
            Set type_counts at key type_name to (type_counts at key type_name) plus 1
        Otherwise:
            Set type_counts at key type_name to 1
    
    Return dictionary containing:
        "total_tokens" as total_tokens
        "error_tokens" as error_tokens
        "type_counts" as type_counts
        "success_rate" as ((total_tokens minus error_tokens) multiplied by 100) divided by total_tokens 