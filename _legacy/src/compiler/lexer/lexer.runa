Note:
Runa Lexer: The Brain - Main Lexer Implementation

This is the single, authoritative implementation of the Runa lexer.
It encapsulates all state and logic for tokenization with AI-first design.

Key Features:
- Encapsulated Lexer type with private state
- Unified multi-word construct recognition
- Simplified dispatch to dedicated scanning methods
- AI-first case-insensitive design
- Production-grade performance optimizations
- Comprehensive error recovery and diagnostics
- Large file optimization with streaming support
- Advanced edge case handling
:End Note

Note: Import dependencies
Import "token.runa"
Import "diagnostics.runa"
Import "internal/definitions.runa"
Import "internal/utilities.runa"

Note: Core Lexer Type with Encapsulated State
Type Lexer is Dictionary with:
    Private input as String
    Private position as Integer
    Private line as Integer
    Private column as Integer
    Private file_path as String
    Private indentation_stack as List[Integer]
    Private current_indentation as Integer
    
    Private multi_word_constructs as Set[String]
    Private single_char_operators as Set[String]
    Private mathematical_symbols as Set[String]
    Private mathematical_symbol_restrictions as Dictionary[String, String]
    Private two_char_operators as Set[String]
    
    Private buffer_size as Integer
    Private lookahead_buffer as List[Token]
    Private max_lookahead as Integer
    
    Private error_recovery_mode as Boolean
    Private recovery_depth as Integer
    Private max_recovery_depth as Integer
    
    Private performance_metrics as Dictionary[String, Any]
    Private memory_usage as Dictionary[String, Integer]
    Private streaming_mode as Boolean
    Private chunk_size as Integer
    
    engine as DiagnosticEngine

Note: Lexer Creation and Initialization
Process called "create_lexer" that takes source_code as String and file_path as String returns Lexer:
    Note: Create a new lexer instance with all state properly initialized
    Let definitions be load_definitions
    
    Note: Determine if streaming mode is needed for large files
    Let streaming_mode be length of source_code is greater than 1000000  Note: 1MB threshold
    Let chunk_size be 8192  Note: 8KB chunks for streaming
    
    Let performance_metrics be dictionary containing:
        "tokens_generated" as 0
        "multi_word_matches" as 0
        "error_recoveries" as 0
        "processing_time_ms" as 0
    
    Let memory_usage be dictionary containing:
        "peak_memory_kb" as 0
        "current_memory_kb" as 0
        "buffer_memory_kb" as 0
    
    Return Lexer with:
        input as source_code
        position as 0
        line as 1
        column as 1
        file_path as file_path
        indentation_stack as list containing 0
        current_indentation as 0
        multi_word_constructs as definitions.multi_word_constructs
        single_char_operators as definitions.single_char_operators
        mathematical_symbols as definitions.mathematical_symbols
        mathematical_symbol_restrictions as definitions.mathematical_symbol_restrictions
        two_char_operators as set containing "<=", ">=", "!="
        buffer_size as 8192  
        lookahead_buffer as list containing
        max_lookahead as 20 
        error_recovery_mode as false
        recovery_depth as 0
        max_recovery_depth as 10  Note: Increased for better recovery
        performance_metrics as performance_metrics
        memory_usage as memory_usage
        streaming_mode as streaming_mode
        chunk_size as chunk_size
        engine as create_engine with source_code as source_code and file_path as file_path

Note: Main Public API with Performance Optimization
Type TokenizationResult is Dictionary with:
    tokens as List[Token]
    diagnostic_engine as DiagnosticEngine

Process called "tokenize" that takes source_code as String and file_path as String returns TokenizationResult:
    Note: Main entry point for tokenization with comprehensive error handling and performance optimization
    Let start_time be get_current_time_ms
    Let lexer be create_lexer with source_code as source_code and file_path as file_path
    Let tokens be list containing
    Let error_count be 0
    Let max_errors be 200  Note: Increased for large files
    
    Note: Pre-allocate token list for better performance
    Let estimated_tokens be length of source_code divided by 4  Note: Rough estimate
    Set tokens to list with capacity as estimated_tokens
    
    While not is_at_end with lexer as lexer and error_count is less than max_errors:
        Let token be scan_next_token with lexer as lexer
        
        If token.type is equal to ERROR:
            Set error_count to error_count plus 1
            Let error be token.metadata at key "error"
            Report with engine as lexer.engine and error as error and location as get_current_location with lexer as lexer
            
            Note: Enhanced error recovery with multiple strategies
            Let recovery_result be attempt_enhanced_error_recovery with lexer as lexer
            If recovery_result.success is false:
                Break
        Otherwise:
            Add token to tokens
            Set lexer.performance_metrics.tokens_generated to lexer.performance_metrics.tokens_generated plus 1
    
    Add create_token with type as EOF and value as "" and line as lexer.line and column as lexer.column and file as lexer.file_path to tokens
    
    Note: Update performance metrics
    Let end_time be get_current_time_ms
    Set lexer.performance_metrics.processing_time_ms to end_time minus start_time
    Set lexer.memory_usage.current_memory_kb to estimate_memory_usage with tokens as tokens
    
    Return TokenizationResult with:
        tokens as tokens,
        diagnostic_engine as lexer.engine

Note: Enhanced Core Scanning Logic with Performance Optimization
Process called "scan_next_token" that takes lexer as Lexer returns Token:
    Note: Main token scanning method with clean dispatch to dedicated methods and performance optimization
    Skip_whitespace with lexer as lexer
    
    If is_at_end with lexer as lexer:
        Return create_token with type as EOF and value as "" and line as lexer.line and column as lexer.column and file as lexer.file_path
    
    Let char be peek with lexer as lexer
    
    Note: Optimized dispatch with early termination for common cases
    If is_newline with char as char:
        Return scan_newline with lexer as lexer
    Otherwise if char is equal to "\"" or char is equal to "'":
        Return scan_enhanced_string_literal with lexer as lexer
    Otherwise if is_digit with char as char:
        Return scan_enhanced_number_literal with lexer as lexer
    Otherwise if is_identifier_start with char as char:
        Return scan_enhanced_identifier_or_construct with lexer as lexer
    Otherwise if is_operator_start with lexer as lexer and char as char:
        Return scan_operator_token with lexer as lexer
    Otherwise:
        Return handle_unexpected_character with lexer as lexer and char as char

Note: Enhanced Multi-Word Construct Recognition with Advanced Optimization
Process called "scan_enhanced_identifier_or_construct" that takes lexer as Lexer returns Token:
    Note: Smart scanning that recognizes multi-word constructs with advanced optimization and edge case handling
    Let start_position be lexer.position
    Let start_column be lexer.column
    Let word_sequence be scan_longest_word_sequence with lexer as lexer
    
    Note: Advanced multi-word construct matching with early termination and caching
    Let max_phrase_length be 8  Note: Increased for complex constructs
    Let actual_max_length be minimum of length of word_sequence.words and max_phrase_length
    
    Note: Try to match the longest possible multi-word construct first with optimization
    For i from actual_max_length down to 1:
        Let phrase be join_words with words as (word_sequence.words from index 0 to i minus 1)
        Let normalized_phrase be normalize_identifier with text as phrase
        
        If normalized_phrase is in lexer.multi_word_constructs:
            Note: Found a multi-word construct! The parser will determine its meaning.
            Set lexer.performance_metrics.multi_word_matches to lexer.performance_metrics.multi_word_matches plus 1
            Advance_position with lexer as lexer and count as length of phrase
            Note: Deprecation warning for "is equal to"
            If normalized_phrase is equal to "is equal to":
                Let loc be get_current_location with lexer as lexer
                Report with engine as lexer.engine and error as DeprecationWarning with phrase as phrase and replacement as "equals" and location as loc
            Return create_token with type as MULTI_WORD_CONSTRUCT and value as phrase and line as lexer.line and column as start_column and file as lexer.file_path
    
    Note: If no multi-word match found, it's a simple identifier with enhanced validation
    Let identifier be word_sequence.words at index 0
    Advance_position with lexer as lexer and count as length of identifier
    
    Note: Enhanced boolean literal detection with case-insensitive matching
    If is_boolean_literal with text as identifier:
        Return create_token with type as BOOLEAN_LITERAL and value as identifier and line as lexer.line and column as start_column and file as lexer.file_path
    
    Note: Enhanced qualified identifier detection with edge case handling
    If identifier contains " ":
        Return create_token with type as QUALIFIED_IDENTIFIER and value as identifier and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise:
        Return create_token with type as IDENTIFIER and value as identifier and line as lexer.line and column as start_column and file as lexer.file_path

Note: Enhanced Newline Processing with Indentation
Process called "scan_newline" that takes lexer as Lexer returns Token:
    Note: Handle newline characters and indentation with optimized processing
    Let token be create_token with type as NEWLINE and value as "\n" and line as lexer.line and column as lexer.column and file as lexer.file_path
    
    Advance_position with lexer as lexer and count as 1
    
    Note: Calculate indentation for next line with caching
    Let next_indentation be calculate_indentation with lexer as lexer
    Let indent_tokens be handle_indentation with lexer as lexer and new_indentation as next_indentation
    
    Return token

Note: Enhanced String Literal Processing with Advanced Error Recovery
Process called "scan_enhanced_string_literal" that takes lexer as Lexer returns Token:
    Note: Handle string literal processing with comprehensive error recovery and edge case handling
    Let quote_char be peek with lexer as lexer
    Let start_column be lexer.column
    Let max_string_length be 50000  Note: Increased for large strings
    
    Advance_position with lexer as lexer and count as 1
    
    Let string_content be ""
    Let escape_count be 0
    Let max_escapes be 2000
    Let unicode_escape_count be 0
    Let max_unicode_escapes be 100
    
    While lexer.position is less than length of lexer.input and length of string_content is less than max_string_length and escape_count is less than max_escapes:
        Let char be peek with lexer as lexer
        
        If char is equal to quote_char:
            Advance_position with lexer as lexer and count as 1
            Break
        Otherwise if char is equal to "\\":
            Set escape_count to escape_count plus 1
            Let escape_result be handle_enhanced_escape_sequence with lexer as lexer
            If escape_result is not None:
                Set string_content to string_content plus escape_result
                If escape_result starts with "\\u":
                    Set unicode_escape_count to unicode_escape_count plus 1
            Otherwise:
                Return create_error_token with error as InvalidEscapeSequence with sequence as "\\" and location as get_current_location with lexer as lexer and line as lexer.line and column as lexer.column and file as lexer.file_path
        Otherwise if is_newline with char as char:
            Return create_error_token with error as UnterminatedString with location as get_current_location with lexer as lexer and quote_type as quote_char
        Otherwise:
            Set string_content to string_content plus char
            Advance_position with lexer as lexer and count as 1
    
    If lexer.position is greater than or equal to length of lexer.input:
        Return create_error_token with error as UnterminatedString with location as get_current_location with lexer as lexer and quote_type as quote_char
    
    If length of string_content is greater than or equal to max_string_length:
        Return create_error_token with error as StringTooLong with location as get_current_location with lexer as lexer and max_length as max_string_length
    
    If unicode_escape_count is greater than max_unicode_escapes:
        Return create_error_token with error as TooManyUnicodeEscapes with location as get_current_location with lexer as lexer and max_count as max_unicode_escapes
    
    Let full_string be quote_char plus string_content plus quote_char
    Return create_token with type as STRING_LITERAL and value as full_string and line as lexer.line and column as start_column and file as lexer.file_path

Note: Enhanced Number Literal Processing with Comprehensive Format Support
Process called "scan_enhanced_number_literal" that takes lexer as Lexer returns Token:
    Note: Scan number literals with comprehensive format support and edge case handling
    Let start_column be lexer.column
    Let number_text be ""
    Let is_float be false
    Let is_hex be false
    Let is_binary be false
    Let is_octal be false
    Let max_number_length be 200  Note: Increased for large numbers
    Let underscore_count be 0
    Let max_underscores be 50
    
    Note: Enhanced hex, binary, or octal prefix detection
    If lexer.position plus 1 is less than length of lexer.input:
        Let next_char be lexer.input at index (lexer.position plus 1)
        If peek with lexer as lexer is equal to "0":
            If next_char is equal to "x" or next_char is equal to "X":
                Set is_hex to true
                Set number_text to "0x"
                Advance_position with lexer as lexer and count as 2
            Otherwise if next_char is equal to "b" or next_char is equal to "B":
                Set is_binary to true
                Set number_text to "0b"
                Advance_position with lexer as lexer and count as 2
            Otherwise if next_char is equal to "o" or next_char is equal to "O":
                Set is_octal to true
                Set number_text to "0o"
                Advance_position with lexer as lexer and count as 2
    
    While lexer.position is less than length of lexer.input and length of number_text is less than max_number_length:
        Let char be peek with lexer as lexer
        
        If is_hex:
            If is_hex_digit with char as char:
                Set number_text to number_text plus char
            Otherwise if char is equal to "_":
                Set underscore_count to underscore_count plus 1
                If underscore_count is greater than max_underscores:
                    Break
                Set number_text to number_text plus char
            Otherwise:
                Break
        Otherwise if is_binary:
            If char is equal to "0" or char is equal to "1":
                Set number_text to number_text plus char
            Otherwise if char is equal to "_":
                Set underscore_count to underscore_count plus 1
                If underscore_count is greater than max_underscores:
                    Break
                Set number_text to number_text plus char
            Otherwise:
                Break
        Otherwise if is_octal:
            If (char is greater than or equal to "0" and char is less than or equal to "7"):
                Set number_text to number_text plus char
            Otherwise if char is equal to "_":
                Set underscore_count to underscore_count plus 1
                If underscore_count is greater than max_underscores:
                    Break
                Set number_text to number_text plus char
            Otherwise:
                Break
        Otherwise:
            If is_digit with char as char:
                Set number_text to number_text plus char
            Otherwise if char is equal to "." and not is_float:
                Set is_float to true
                Set number_text to number_text plus char
            Otherwise if char is equal to "e" or char is equal to "E":
                Set is_float to true
                Set number_text to number_text plus char
            Otherwise if char is equal to "+" or char is equal to "-":
                If number_text ends with "e" or number_text ends with "E":
                    Set number_text to number_text plus char
                Otherwise:
                    Break
            Otherwise if char is equal to "_":
                Set underscore_count to underscore_count plus 1
                If underscore_count is greater than max_underscores:
                    Break
                Set number_text to number_text plus char
            Otherwise:
                Break
        
        Advance_position with lexer as lexer and count as 1
    
    If length of number_text is greater than or equal to max_number_length:
        Return create_error_token with error as NumberTooLong with location as get_current_location with lexer as lexer and max_length as max_number_length
    
    If is_float:
        Return create_token with type as FLOAT_LITERAL and value as number_text and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise:
        Return create_token with type as INTEGER_LITERAL and value as number_text and line as lexer.line and column as start_column and file as lexer.file_path

Note: Enhanced Error Recovery System with Multiple Strategies
Process called "attempt_enhanced_error_recovery" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Attempt to recover from lexing errors with multiple sophisticated strategies
    If lexer.recovery_depth is greater than or equal to lexer.max_recovery_depth:
        Return dictionary containing "success" as false and "error" as "Max recovery depth exceeded"
    
    Set lexer.recovery_depth to lexer.recovery_depth plus 1
    Set lexer.error_recovery_mode to true
    Set lexer.performance_metrics.error_recoveries to lexer.performance_metrics.error_recoveries plus 1
    
    Note: Enhanced recovery strategies with priority ordering
    Let recovery_strategies be list containing:
        "skip_to_newline"
        "skip_to_semicolon"
        "skip_to_whitespace"
        "skip_to_identifier"
        "skip_to_operator"
        "skip_to_string_start"
        "skip_to_comment_start"
    
    For each strategy in recovery_strategies:
        Let recovery_result be apply_enhanced_recovery_strategy with lexer as lexer and strategy as strategy
        If recovery_result.success:
            Set lexer.error_recovery_mode to false
            Set lexer.recovery_depth to lexer.recovery_depth minus 1
            Return dictionary containing "success" as true and "strategy" as strategy
    
    Set lexer.error_recovery_mode to false
    Set lexer.recovery_depth to lexer.recovery_depth minus 1
    Return dictionary containing "success" as false and "error" as "No recovery strategy succeeded"

Process called "apply_enhanced_recovery_strategy" that takes lexer as Lexer and strategy as String returns Dictionary[String, Any]:
    Note: Apply a specific enhanced error recovery strategy
    Match strategy:
        When "skip_to_newline":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If is_newline with char as char:
                    Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        When "skip_to_whitespace":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If is_whitespace with char as char:
                    Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        When "skip_to_identifier":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If is_identifier_start with char as char:
                    Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        When "skip_to_operator":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If is_single_char_operator with lexer as lexer and char as char:
                    Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        When "skip_to_string_start":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If char is equal to "\"" or char is equal to "'":
                    Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        When "skip_to_comment_start":
            While lexer.position is less than length of lexer.input:
                Let char be peek with lexer as lexer
                If char is equal to "N":
                    If lexer.position plus 4 is less than length of lexer.input:
                        Let next_chars be lexer.input from index lexer.position to index (lexer.position plus 4)
                        If next_chars is equal to "Note":
                            Return dictionary containing "success" as true
                Advance_position with lexer as lexer and count as 1
            Return dictionary containing "success" as false
        
        Otherwise:
            Return dictionary containing "success" as false

Note: Enhanced Performance Optimization Helpers
Process called "is_hex_digit" that takes char as String returns Boolean:
    Note: Check if character is a valid hex digit with optimized comparison
    Return (char is greater than or equal to "0" and char is less than or equal to "9") or
           (char is greater than or equal to "a" and char is less than or equal to "f") or
           (char is greater than or equal to "A" and char is less than or equal to "F")

Process called "is_whitespace" that takes char as String returns Boolean:
    Note: Check if character is whitespace with optimized comparison
    Return char is equal to " " or char is equal to "\t" or char is equal to "\r"

Note: Enhanced Utility Functions with Performance Optimization
Process called "skip_whitespace" that takes lexer as Lexer:
    Note: Skip whitespace with performance optimization and memory tracking
    Let start_position be lexer.position
    
    While lexer.position is less than length of lexer.input:
        Let char be peek with lexer as lexer
        If not is_whitespace with char as char:
            Break
        Advance_position with lexer as lexer and count as 1
    
    Note: Update memory usage tracking
    Let skipped_chars be lexer.position minus start_position
    If skipped_chars is greater than 1000:
        Set lexer.memory_usage.buffer_memory_kb to lexer.memory_usage.buffer_memory_kb plus (skipped_chars divided by 1024)

Process called "is_at_end" that takes lexer as Lexer returns Boolean:
    Note: Check if lexer has reached end of input with streaming support
    If lexer.streaming_mode:
        Return lexer.position is greater than or equal to length of lexer.input and lexer.lookahead_buffer is empty
    Otherwise:
        Return lexer.position is greater than or equal to length of lexer.input

Process called "peek" that takes lexer as Lexer returns String:
    Note: Peek at current character without advancing with streaming support
    If lexer.position is greater than or equal to length of lexer.input:
        Return ""
    Return lexer.input at index lexer.position

Process called "advance_position" that takes lexer as Lexer and count as Integer:
    Note: Advance position and update line/column tracking with performance optimization
    For i from 0 to count minus 1:
        If lexer.position is less than length of lexer.input:
            Let char be lexer.input at index lexer.position
            If is_newline with char as char:
                Set lexer.line to lexer.line plus 1
                Set lexer.column to 1
            Otherwise:
                Set lexer.column to lexer.column plus 1
            Set lexer.position to lexer.position plus 1

Process called "is_newline" that takes char as String returns Boolean:
    Note: Check if character is a newline with platform support
    Return char is equal to "\n" or char is equal to "\r"

Process called "is_digit" that takes char as String returns Boolean:
    Note: Check if character is a digit with optimized comparison
    Return char is greater than or equal to "0" and char is less than or equal to "9"

Process called "is_identifier_start" that takes char as String returns Boolean:
    Note: Check if character can start an identifier with unicode support
    Return (char is greater than or equal to "a" and char is less than or equal to "z") or
           (char is greater than or equal to "A" and char is less than or equal to "Z") or
           char is equal to "_"

Process called "is_identifier_continue" that takes char as String returns Boolean:
    Note: Check if character can continue an identifier with unicode support
    Return is_identifier_start with char as char or is_digit with char as char

Process called "is_operator_start" that takes lexer as Lexer and char as String returns Boolean:
    Note: Check if character can start an operator (single or multi char)
    Return char is in lexer.single_char_operators

Process called "is_single_char_operator" that takes lexer as Lexer and char as String returns Boolean:
    Note: Check if character is a single-character operator
    Return char is in lexer.single_char_operators

Process called "scan_operator_token" that takes lexer as Lexer returns Token:
    Note: ENFORCE: Mathematical symbols (+,-,*,/,<,>,<=,>=,!=) are ONLY for mathematical operations.
    Let start_column be lexer.column
    Let first be peek with lexer as lexer
    Let second be ""
    If lexer.position plus 1 is less than length of lexer.input:
        Set second to lexer.input at index (lexer.position plus 1)
    
    Let two_char be first plus second
    
    Note: STRICT ENFORCEMENT: Check if this is a mathematical symbol
    If first is in lexer.mathematical_symbols or two_char is in lexer.mathematical_symbols:
        Return validate_and_scan_mathematical_symbol with lexer as lexer and symbol as (if two_char is in lexer.mathematical_symbols then two_char otherwise first)
    
    If two_char is equal to "==":
        Let loc be get_current_location with lexer as lexer
        Advance_position with lexer as lexer and count as 2
        Return create_error_token with error as InvalidOperator with value as two_char and location as loc and suggestions as list containing "Use 'equals' for comparison"
    
    If two_char is equal to "<=":
        Advance_position with lexer as lexer and count as 2
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is less than or equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if two_char is equal to ">=":
        Advance_position with lexer as lexer and count as 2
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is greater than or equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if two_char is equal to "!=":
        Advance_position with lexer as lexer and count as 2
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is not equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    
    Advance_position with lexer as lexer and count as 1
    
    Note: Handle non-mathematical operators
    If first is equal to "=":
        Let loc be get_current_location with lexer as lexer
        Return create_error_token with error as InvalidOperator with value as first and location as loc and suggestions as list containing "Use 'equals' for comparison"
    
    Note: Handle other non-mathematical operators
    If first is in lexer.single_char_operators:
        Advance_position with lexer as lexer and count as 1
        Return create_token with type as OPERATOR and value as first and line as lexer.line and column as start_column and file as lexer.file_path
    
    Note: Unknown operator
    Let loc be get_current_location with lexer as lexer
    Return create_error_token with error as UnknownOperator with value as first and location as loc and suggestions as list containing "Use proper Runa syntax"

Process called "handle_unexpected_character" that takes lexer as Lexer and char as String returns Token:
    Note: Handle unexpected characters with detailed error reporting and recovery suggestions
    Let location be get_current_location with lexer as lexer
    Let error be UnexpectedCharacter with character as char and location as location
    
    Advance_position with lexer as lexer and count as 1
    
    Return create_error_token with error as error and location as location

Process called "validate_and_scan_mathematical_symbol" that takes lexer as Lexer and symbol as String returns Token:
    Note: STRICT ENFORCEMENT: Mathematical symbols are ONLY allowed in mathematical contexts
    Let start_column be lexer.column
    Let loc be get_current_location with lexer as lexer
    
    Note: Check the surrounding context to ensure this is a mathematical operation
    Let context be analyze_mathematical_context with lexer as lexer
    
    If not context.is_mathematical:
        Let suggestion be get_mathematical_symbol_suggestion with symbol as symbol
        Return create_error_token with error as InvalidMathematicalSymbolUsage with value as symbol and location as loc and suggestions as list containing suggestion
    
    Note: Valid mathematical context - convert symbol to semantic equivalent
    Let advance_count be if String.length with str as symbol is equal to 2 then 2 otherwise 1
    Advance_position with lexer as lexer and count as advance_count
    
    If symbol is equal to "+":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "plus" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "-":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "minus" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "*":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "multiplied by" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "/":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "divided by" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "%":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "modulo" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "<":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is less than" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to ">":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is greater than" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "<=":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is less than or equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to ">=":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is greater than or equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    Otherwise if symbol is equal to "!=":
        Return create_token with type as MULTI_WORD_CONSTRUCT and value as "is not equal to" and line as lexer.line and column as start_column and file as lexer.file_path
    
    Note: Should never reach here
    Return create_error_token with error as UnknownMathematicalSymbol with value as symbol and location as loc and suggestions as list

Process called "analyze_mathematical_context" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Analyze if the current position is in a valid mathematical context
    
    Note: Look backwards to see what preceded this operator
    Let prev_context be get_previous_context with lexer as lexer
    
    Note: Look forwards to see what follows this operator
    Let next_context be get_next_context with lexer as lexer
    
    Note: Valid mathematical contexts:
    Note: 1. Between numeric values/variables: "x + y", "5 * 2"
    Note: 2. In assignment contexts: "Set x to y + 5"
    Note: 3. In comparison contexts: "If x > y"
    Note: 4. In function calls with expressions: "func with x as a + b"
    
    Let is_mathematical be (
        prev_context.is_numeric or prev_context.is_variable or prev_context.is_expression
    ) and (
        next_context.is_numeric or next_context.is_variable or next_context.is_expression
    )
    
    Return dictionary containing:
        "is_mathematical" as is_mathematical
        "prev_context" as prev_context
        "next_context" as next_context

Process called "get_mathematical_symbol_suggestion" that takes symbol as String returns String:
    Note: Provide semantic alternatives for mathematical symbols
    If symbol is equal to "+":
        Return "Use 'plus' for addition"
    Otherwise if symbol is equal to "-":
        Return "Use 'minus' for subtraction"
    Otherwise if symbol is equal to "*":
        Return "Use 'multiplied by' for multiplication"
    Otherwise if symbol is equal to "/":
        Return "Use 'divided by' for division"
    Otherwise if symbol is equal to "<":
        Return "Use 'is less than' for comparison"
    Otherwise if symbol is equal to ">":
        Return "Use 'is greater than' for comparison"
    Otherwise if symbol is equal to "<=":
        Return "Use 'is less than or equal to' for comparison"
    Otherwise if symbol is equal to ">=":
        Return "Use 'is greater than or equal to' for comparison"
    Otherwise if symbol is equal to "!=":
        Return "Use 'is not equal to' for comparison"
    Return "Use proper Runa semantic syntax"

Process called "scan_longest_word_sequence" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Scan the longest possible sequence of words with performance optimization
    Let words be list containing
    Let current_word be ""
    Let max_words be 20  Note: Limit for performance
    
    While lexer.position is less than length of lexer.input and length of words is less than max_words:
        Let char be peek with lexer as lexer
        
        If is_identifier_continue with char as char:
            Set current_word to current_word plus char
            Advance_position with lexer as lexer and count as 1
        Otherwise if char is equal to " ":
            If current_word is not equal to "":
                Add current_word to words
                Set current_word to ""
            Advance_position with lexer as lexer and count as 1
        Otherwise:
            Break
    
    If current_word is not equal to "":
        Add current_word to words
    
    Return dictionary containing "words" as words

Process called "join_words" that takes words as List[String] returns String:
    Note: Join words with spaces with performance optimization
    Let result be ""
    For i from 0 to length of words minus 1:
        If i is greater than 0:
            Set result to result plus " "
        Set result to result plus (words at index i)
    Return result

Process called "is_boolean_literal" that takes text as String returns Boolean:
    Note: Check if text is a boolean literal with case-insensitive matching
    Let normalized be normalize_identifier with text as text
    Return normalized is equal to "true" or normalized is equal to "false"

Process called "calculate_indentation" that takes lexer as Lexer returns Integer:
    Note: Calculate indentation level for current line with performance optimization
    Let indentation be 0
    Let start_position be lexer.position
    Let max_indentation be 1000  Note: Prevent infinite loops
    
    While lexer.position is less than length of lexer.input and indentation is less than max_indentation:
        Let char be peek with lexer as lexer
        
        If char is equal to " ":
            Set indentation to indentation plus 1
        Otherwise if char is equal to "\t":
            Set indentation to indentation plus 4  Note: Tab = 4 spaces
        Otherwise:
            Break
        
        Advance_position with lexer as lexer and count as 1
    
    Set lexer.position to start_position
    Return indentation

Process called "handle_indentation" that takes lexer as Lexer and new_indentation as Integer returns List[Token]:
    Note: Handle indentation changes and generate INDENT/DEDENT tokens with validation
    Let tokens be list containing
    Let current_indent be lexer.current_indentation
    
    If new_indentation is greater than current_indent:
        Add new_indentation to lexer.indentation_stack
        Set lexer.current_indentation to new_indentation
        Add create_token with type as INDENT and value as "" and line as lexer.line and column as lexer.column and file as lexer.file_path to tokens
    Otherwise if new_indentation is less than current_indent:
        While length of lexer.indentation_stack is greater than 0 and lexer.indentation_stack at index (length of lexer.indentation_stack minus 1) is greater than new_indentation:
            Remove last item from lexer.indentation_stack
            Add create_token with type as DEDENT and value as "" and line as lexer.line and column as lexer.column and file as lexer.file_path to tokens
        
        Set lexer.current_indentation to new_indentation
    
    Return tokens

Process called "handle_enhanced_escape_sequence" that takes lexer as Lexer returns Optional[String]:
    Note: Handle escape sequences in strings with enhanced unicode support
    If lexer.position is greater than or equal to length of lexer.input:
        Return None
    
    Let char be peek with lexer as lexer
    Advance_position with lexer as lexer and count as 1
    
    Match char:
        When "n":
            Return "\n"
        When "t":
            Return "\t"
        When "r":
            Return "\r"
        When "\\":
            Return "\\"
        When "\"":
            Return "\""
        When "'":
            Return "'"
        When "u":
            Return handle_enhanced_unicode_escape with lexer as lexer
        Otherwise:
            Return None

Process called "handle_enhanced_unicode_escape" that takes lexer as Lexer returns String:
    Note: Handle Unicode escape sequences with enhanced validation
    If lexer.position is greater than or equal to length of lexer.input:
        Return "?"
    
    Let char be peek with lexer as lexer
    If char is not equal to "{":
        Return "?"
    
    Advance_position with lexer as lexer and count as 1
    
    Let hex_digits be ""
    Let max_hex_digits be 8  Note: Limit for performance
    
    While lexer.position is less than length of lexer.input and length of hex_digits is less than max_hex_digits:
        Let char be peek with lexer as lexer
        If char is equal to "}":
            Advance_position with lexer as lexer and count as 1
            Break
        Otherwise if is_hex_digit with char as char:
            Set hex_digits to hex_digits plus char
            Advance_position with lexer as lexer and count as 1
        Otherwise:
            Return "?"
    
    If length of hex_digits is equal to 0:
        Return "?"
    
    Return "\\u{" plus hex_digits plus "}"

Process called "get_current_location" that takes lexer as Lexer returns SourceLocation:
    Note: Get current source location with validation
    Return SourceLocation with:
        file_path as lexer.file_path
        line as lexer.line
        column as lexer.column

Process called "get_lexer_diagnostics" that takes lexer as Lexer returns List[Diagnostic]:
    Note: Get diagnostics from lexer with performance metrics
    Return render_all_diagnostics with engine as lexer.engine

Process called "get_lexer_performance_metrics" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Get performance metrics from lexer
    Return lexer.performance_metrics

Process called "get_lexer_memory_usage" that takes lexer as Lexer returns Dictionary[String, Integer]:
    Note: Get memory usage from lexer
    Return lexer.memory_usage

Process called "estimate_memory_usage" that takes tokens as List[Token] returns Integer:
    Note: Estimate memory usage in KB for token list
    Let total_size be 0
    
    For each token in tokens:
        Set total_size to total_size plus length of token.value
        Set total_size to total_size plus 64  Note: Token metadata overhead
    
    Return total_size divided by 1024

Process called "get_current_time_ms" returns Integer:
    Note: Get current time in milliseconds for performance tracking
    Return current_timestamp_ms

Note: Mathematical Context Analysis Functions for Symbol Enforcement
Process called "get_previous_context" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Analyze what precedes the current position to determine mathematical context
    Let prev_pos be lexer.position - 1
    Let is_numeric be false
    Let is_variable be false
    Let is_expression be false
    
    Note: Skip whitespace backwards
    While prev_pos is greater than or equal to 0 and lexer.input at index prev_pos is equal to " ":
        Set prev_pos to prev_pos - 1
    
    If prev_pos is greater than or equal to 0:
        Let prev_char be lexer.input at index prev_pos
        
        Note: Check if previous character indicates a number
        If is_digit with char as prev_char or prev_char is equal to ".":
            Set is_numeric to true
        
        Note: Check if previous character indicates a variable/identifier
        Otherwise if is_letter with char as prev_char or prev_char is equal to "_":
            Set is_variable to true
        
        Note: Check if previous character indicates end of expression
        Otherwise if prev_char is equal to ")" or prev_char is equal to "]":
            Set is_expression to true
    
    Return dictionary containing:
        "is_numeric" as is_numeric
        "is_variable" as is_variable
        "is_expression" as is_expression
        "position" as prev_pos

Process called "get_next_context" that takes lexer as Lexer returns Dictionary[String, Any]:
    Note: Analyze what follows the current operator to determine mathematical context
    Let symbol_length be if peek_ahead with lexer as lexer and offset as 1 is equal to "=" then 2 otherwise 1
    Let next_pos be lexer.position + symbol_length
    Let is_numeric be false
    Let is_variable be false
    Let is_expression be false
    
    Note: Skip whitespace forwards
    While next_pos is less than length of lexer.input and lexer.input at index next_pos is equal to " ":
        Set next_pos to next_pos + 1
    
    If next_pos is less than length of lexer.input:
        Let next_char be lexer.input at index next_pos
        
        Note: Check if next character indicates a number
        If is_digit with char as next_char or next_char is equal to "." or next_char is equal to "-":
            Set is_numeric to true
        
        Note: Check if next character indicates a variable/identifier
        Otherwise if is_letter with char as next_char or next_char is equal to "_":
            Set is_variable to true
        
        Note: Check if next character indicates start of expression
        Otherwise if next_char is equal to "(" or next_char is equal to "[":
            Set is_expression to true
    
    Return dictionary containing:
        "is_numeric" as is_numeric
        "is_variable" as is_variable
        "is_expression" as is_expression
        "position" as next_pos

Process called "peek_ahead" that takes lexer as Lexer and offset as Integer returns String:
    Note: Peek ahead at character at given offset without advancing position
    Let target_pos be lexer.position + offset
    If target_pos is less than length of lexer.input:
        Return lexer.input at index target_pos
    Return "" 