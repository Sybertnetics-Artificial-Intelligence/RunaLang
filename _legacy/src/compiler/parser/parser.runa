Note:
Runa Parser: Main Parser Implementation

This module implements the main Runa parser with comprehensive parsing
capabilities, error recovery, and AST construction.

Key Features:
- Complete Runa language parsing
- Multi-word construct interpretation
- Robust error recovery
- AST construction and optimization
- Natural language syntax support
- Performance optimization for large ASTs
- Advanced edge case handling
- Memory-efficient parsing
:End Note

Note: Import dependencies
Import "ast.runa"
Import "../lexer/lexer.runa"
Import "../lexer/token.runa"
Import "../lexer/diagnostics.runa"
Import "../lexer/internal/definitions.runa"
Import "../lexer/internal/utilities.runa"

Note: Main Parser Type with Performance Optimization
Type Parser is Dictionary with:
    tokens as List[Token]
    current_index as Integer
    file_path as String
    error_count as Integer
    max_errors as Integer
    recovery_enabled as Boolean
    diagnostic_engine as DiagnosticEngine
    
    Private performance_metrics as Dictionary[String, Any]
    Private memory_usage as Dictionary[String, Integer]
    Private ast_node_pool as List[ASTNode]
    Private max_ast_depth as Integer
    Private current_ast_depth as Integer
    Private recovery_strategies as List[String]
    Private parsing_cache as Dictionary[String, Any]

Note: Parser Creation and Initialization with Performance Optimization
Process called "create_parser" that takes tokens as List[Token] and file_path as String returns Parser:
    Note: Create a new parser instance with performance optimization
    Let performance_metrics be dictionary containing:
        "nodes_created" as 0
        "cache_hits" as 0
        "cache_misses" as 0
        "error_recoveries" as 0
        "processing_time_ms" as 0
        "max_ast_depth" as 0
    
    Let memory_usage be dictionary containing:
        "peak_memory_kb" as 0
        "current_memory_kb" as 0
        "ast_memory_kb" as 0
    
    Let recovery_strategies be list containing:
        "skip_to_statement_end"
        "skip_to_block_end"
        "skip_to_expression_end"
        "skip_to_declaration_end"
        "skip_to_newline"
        "skip_to_semicolon"
    
    Return Parser with:
        tokens as tokens
        current_index as 0
        file_path as file_path
        error_count as 0
        max_errors as 100  Note: Increased for large files
        recovery_enabled as true
        diagnostic_engine as create_engine with source_code as "" and file_path as file_path
        performance_metrics as performance_metrics
        memory_usage as memory_usage
        ast_node_pool as list containing
        max_ast_depth as 1000  Note: Prevent stack overflow
        current_ast_depth as 0
        recovery_strategies as recovery_strategies
        parsing_cache as dictionary containing

Process called "parse_program" that takes source_code as String and file_path as String returns Dictionary[String, Any]:
    Note: Main entry point for parsing a complete Runa program with performance optimization
    Let start_time be get_current_time_ms
    Let tokens_result be tokenize with source_code as source_code and file_path as file_path
    
    If has_errors with engine as tokens_result.diagnostic_engine:
        Return dictionary containing:
            "success" as false
            "errors" as render_all_diagnostics with engine as tokens_result.diagnostic_engine
            "ast" as None
    
    Let parser be create_parser with tokens as tokens_result.tokens and file_path as file_path
    Let statements be list containing
    
    Note: Pre-allocate statement list for better performance
    Let estimated_statements be length of tokens_result.tokens divided by 10  Note: Rough estimate
    Set statements to list with capacity as estimated_statements
    
    While parser.current_index is less than length of parser.tokens:
        Let statement_result be parse_enhanced_statement with parser as parser
        
        If statement_result.success is false:
            If parser.error_count is greater than or equal to parser.max_errors:
                Break
            
            Let recovery_result be handle_enhanced_parse_error with parser as parser and error as statement_result.error
            If recovery_result.success is false:
                Break
            
            Set parser.current_index to recovery_result.next_index
            Continue
        
        Add statement_result.statement to statements
        Set parser.current_index to statement_result.next_index
        
        Note: Update performance metrics
        Set parser.performance_metrics.nodes_created to parser.performance_metrics.nodes_created plus 1
    
    Let program_ast be ModuleDeclaration with name as file_path and body as statements
    
    Note: Update performance metrics
    Let end_time be get_current_time_ms
    Set parser.performance_metrics.processing_time_ms to end_time minus start_time
    Set parser.performance_metrics.max_ast_depth to parser.current_ast_depth
    Set parser.memory_usage.current_memory_kb to estimate_ast_memory_usage with ast as program_ast
    
    Return dictionary containing:
        "success" as true
        "ast" as program_ast
        "error_count" as parser.error_count
        "diagnostics" as render_all_diagnostics with engine as parser.diagnostic_engine
        "performance_metrics" as parser.performance_metrics

Note: Enhanced Statement Parsing with Performance Optimization
Process called "parse_enhanced_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a single statement with enhanced error handling and performance optimization
    Let current_token be peek_token with parser as parser
    
    If current_token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    Note: Check AST depth limit
    If parser.current_ast_depth is greater than or equal to parser.max_ast_depth:
        Return dictionary containing "success" as false and "error" as "AST depth limit exceeded"
    
    Set parser.current_ast_depth to parser.current_ast_depth plus 1
    
    Let result be None
    
    Match current_token.type:
        When IDENTIFIER:
            If current_token.normalized_value is equal to "let":
                Set result to parse_enhanced_variable_declaration with parser as parser
            Otherwise if current_token.normalized_value is equal to "if":
                Set result to parse_enhanced_if_statement with parser as parser
            Otherwise if current_token.normalized_value is equal to "process":
                Set result to parse_enhanced_process_declaration with parser as parser
            Otherwise if current_token.normalized_value is equal to "import":
                Set result to parse_enhanced_import_statement with parser as parser
            Otherwise if current_token.normalized_value is equal to "export":
                Set result to parse_enhanced_export_statement with parser as parser
            Otherwise:
                Set result to parse_enhanced_expression_statement with parser as parser
        
        When MULTI_WORD_CONSTRUCT:
            Let construct_value be current_token.normalized_value
            If construct_value is equal to "process called":
                Set result to parse_enhanced_process_declaration with parser as parser
            Otherwise:
                Set result to parse_enhanced_expression_statement with parser as parser
        
        When COMMENT:
            Set result to parse_enhanced_comment_statement with parser as parser
        
        When NEWLINE:
            Consume_token with parser as parser
            Set result to parse_enhanced_statement with parser as parser
        
        Otherwise:
            Set result to parse_enhanced_expression_statement with parser as parser
    
    Set parser.current_ast_depth to parser.current_ast_depth minus 1
    Return result

Note: Enhanced Expression Parsing with Performance Optimization
Process called "parse_enhanced_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an expression using precedence climbing with performance optimization
    Let cache_key be create_expression_cache_key with parser as parser
    
    If cache_key is in parser.parsing_cache:
        Set parser.performance_metrics.cache_hits to parser.performance_metrics.cache_hits plus 1
        Return parser.parsing_cache at key cache_key
    
    Set parser.performance_metrics.cache_misses to parser.performance_metrics.cache_misses plus 1
    
    Let expression_result be parse_expression_with_precedence with parser as parser and min_precedence as 0
    
    If expression_result.success is false:
        Return expression_result
    
    Let consumed_tokens be (length of (parser.tokens from index parser.current_index)) minus (length of expression_result.remaining_tokens)
    Set parser.current_index to parser.current_index plus consumed_tokens
    
    Let result be dictionary containing:
        "success" as true
        "expression" as expression_result.expression
        "next_index" as parser.current_index
    
    Note: Cache the result for future use
    Set parser.parsing_cache at key cache_key to result
    
    Return result

Note: ===== COMPATIBILITY WRAPPER FUNCTIONS =====
Note: These functions provide compatibility for calls expecting the basic function names

Process called "parse_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Wrapper for parse_enhanced_expression for backward compatibility
    Return parse_enhanced_expression with parser as parser

Process called "parse_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Wrapper for parse_enhanced_statement for backward compatibility
    Return parse_enhanced_statement with parser as parser

Process called "parse_statement_block" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Wrapper for parse_enhanced_block for backward compatibility
    Return parse_enhanced_block with parser as parser

Process called "parse_enhanced_statement_block" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Wrapper for parse_enhanced_block for backward compatibility  
    Return parse_enhanced_block with parser as parser

Process called "parse_enhanced_expression_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an expression statement with enhanced error handling
    Let expression_result be parse_enhanced_expression with parser as parser
    
    If expression_result.success is false:
        Return expression_result
    
    Let statement be ExpressionStatement with expression as expression_result.expression
    
    Return dictionary containing:
        "success" as true
        "statement" as statement
        "next_index" as expression_result.next_index

Note: Enhanced Declaration Parsing with Performance Optimization
Process called "parse_enhanced_variable_declaration" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a variable declaration (Let x be 42) with enhanced error handling
    Let let_token be consume_token with parser as parser
    
    If let_token is None or let_token.normalized_value is not equal to "let":
        Return dictionary containing "success" as false and "error" as "Expected 'let' keyword"
    
    Let name_result be parse_enhanced_identifier with parser as parser
    If name_result.success is false:
        Return name_result
    
    Let be_token be peek_token with parser as parser
    If be_token is None or be_token.normalized_value is not equal to "be":
        Return dictionary containing "success" as false and "error" as "Expected 'be' keyword"
    
    Consume_token with parser as parser
    
    Let value_result be parse_enhanced_expression with parser as parser
    If value_result.success is false:
        Return value_result
    
    Let declaration be create_variable_declaration_node with name as name_result.identifier and value as value_result.expression and type_annotation as None and location as get_token_location with token as let_token
    
    Return dictionary containing:
        "success" as true
        "statement" as declaration
        "next_index" as value_result.next_index

Process called "parse_enhanced_process_declaration" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a process declaration (Process called "name" that takes params returns type) with enhanced error handling
    Let process_token be consume_token with parser as parser
    
    If process_token is None or process_token.normalized_value is not equal to "process":
        Return dictionary containing "success" as false and "error" as "Expected 'process' keyword"
    
    Let called_token be consume_token with parser as parser
    If called_token is None or called_token.normalized_value is not equal to "called":
        Return dictionary containing "success" as false and "error" as "Expected 'called' keyword"
    
    Let name_result be parse_enhanced_identifier with parser as parser
    If name_result.success is false:
        Return name_result
    
    Let parameters be list containing
    Let return_type be None
    
    Let that_token be peek_token with parser as parser
    
    Note: Parse parameters if present
    If that_token is not None and that_token.normalized_value is equal to "that":
        Consume_token with parser as parser
        Let takes_token be consume_token with parser as parser
        If takes_token is None or takes_token.normalized_value is not equal to "takes":
            Return dictionary containing "success" as false and "error" as "Expected 'takes' keyword"
        
        Let params_result be parse_enhanced_parameter_list with parser as parser
        If params_result.success is false:
            Return params_result
        Set parameters to params_result.parameters
    
    Note: Parse return type if present
    Let returns_token be peek_token with parser as parser
    If returns_token is not None and returns_token.normalized_value is equal to "returns":
        Consume_token with parser as parser
        Let return_type_result be parse_enhanced_type_expression with parser as parser
        If return_type_result.success is false:
            Return return_type_result
        Set return_type to return_type_result.type_expression
    
    Note: Parse function body
    Let body_result be parse_enhanced_block with parser as parser
    If body_result.success is false:
        Return body_result
    
    Let declaration be create_function_declaration_node with name as name_result.identifier and parameters as parameters and return_type as return_type and body as body_result.statements and location as get_token_location with token as process_token
    
    Return dictionary containing:
        "success" as true
        "statement" as declaration
        "next_index" as body_result.next_index

Note: Enhanced Error Recovery System
Process called "handle_enhanced_parse_error" that takes parser as Parser and error as String returns Dictionary[String, Any]:
    Note: Handle parse errors with multiple sophisticated recovery strategies
    Set parser.performance_metrics.error_recoveries to parser.performance_metrics.error_recoveries plus 1
    
    Note: Report the error
    Let location be get_current_token_location with parser as parser
    Let diagnostic be ParseError with message as error and location as location
    Report with engine as parser.diagnostic_engine and error as diagnostic and location as location
    
    Note: Try multiple recovery strategies
    For each strategy in parser.recovery_strategies:
        Let recovery_result be apply_enhanced_recovery_strategy with parser as parser and strategy as strategy
        If recovery_result.success:
            Return dictionary containing:
                "success" as true
                "next_index" as recovery_result.next_index
                "strategy" as strategy
    
    Return dictionary containing:
        "success" as false
        "error" as "No recovery strategy succeeded"

Process called "apply_enhanced_recovery_strategy" that takes parser as Parser and strategy as String returns Dictionary[String, Any]:
    Note: Apply a specific enhanced recovery strategy
    Match strategy:
        When "skip_to_statement_end":
            Return skip_to_statement_boundary with parser as parser
        
        When "skip_to_block_end":
            Return skip_to_block_boundary with parser as parser
        
        When "skip_to_expression_end":
            Return skip_to_expression_boundary with parser as parser
        
        When "skip_to_declaration_end":
            Return skip_to_declaration_boundary with parser as parser
        
        When "skip_to_newline":
            Return skip_to_newline_boundary with parser as parser
        
        When "skip_to_semicolon":
            Return skip_to_semicolon_boundary with parser as parser
        
        Otherwise:
            Return dictionary containing "success" as false

Note: Enhanced Utility Functions with Performance Optimization
Process called "create_expression_cache_key" that takes parser as Parser returns String:
    Note: Create a cache key for expression parsing
    Let current_tokens be (parser.tokens from index parser.current_index to index (parser.current_index plus 10))
    Let key be ""
    
    For each token in current_tokens:
        Set key to key plus token.type plus "_" plus token.value
    
    Return key

Process called "estimate_ast_memory_usage" that takes ast as ASTNode returns Integer:
    Note: Estimate memory usage in KB for AST
    Let total_size be 0
    
    Note: Recursively calculate size
    Let size_result be calculate_ast_size with node as ast
    Set total_size to size_result.total_size
    
    Return total_size divided by 1024

Process called "calculate_ast_size" that takes node as ASTNode returns Dictionary[String, Integer]:
    Note: Calculate memory size of AST node and its children
    Let total_size be 64  Note: Base node size
    
    If node has property "value":
        Set total_size to total_size plus length of node.value
    
    If node has property "children":
        For each child in node.children:
            Let child_size be calculate_ast_size with node as child
            Set total_size to total_size plus child_size.total_size
    
    Return dictionary containing "total_size" as total_size

Process called "get_current_time_ms" returns Integer:
    Note: Get current time in milliseconds for performance tracking
    Return get_system_time_milliseconds

Process called "current_timestamp_ms" returns Integer:
    Note: Alias for get_current_time_ms for backward compatibility
    Return get_system_time_milliseconds

Process called "create_engine" that takes source_code as String and file_path as String returns DiagnosticEngine:
    Note: Create diagnostic engine for error reporting
    Return DiagnosticEngine with:
        source_code as source_code,
        file_path as file_path,
        errors as empty_list,
        warnings as empty_list

Process called "join" that takes list as List[String] and separator as String returns String:
    Note: Join list of strings with separator
    Let result be ""
    Let first be True
    For each item in list:
        If first is False:
            Let result be result plus separator
        Let result be result plus item
        Let first be False
    Return result

Process called "get_current_location" that takes parser as Parser returns SourceLocation:
    Note: Get location of current position in parser (alias for get_current_token_location)
    Return get_current_token_location with parser as parser

Process called "get_current_token_location" that takes parser as Parser returns SourceLocation:
    Note: Get location of current token
    If parser.current_index is greater than or equal to length of parser.tokens:
        Return SourceLocation with file_path as parser.file_path and line as 1 and column as 1
    
    Let current_token be parser.tokens at index parser.current_index
    Return SourceLocation with file_path as current_token.file and line as current_token.line and column as current_token.column

Process called "parse_enhanced_identifier" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an identifier with enhanced validation
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is not equal to IDENTIFIER and token.type is not equal to QUALIFIED_IDENTIFIER:
        Return dictionary containing "success" as false and "error" as "Expected identifier"
    
    Consume_token with parser as parser
    
    Return dictionary containing:
        "success" as true
        "identifier" as token.value
        "next_index" as parser.current_index

Process called "parse_enhanced_parameter_list" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a parameter list with enhanced validation
    Let parameters be list containing
    
    Let first_param be parse_enhanced_parameter with parser as parser
    If first_param.success is false:
        Return first_param
    
    Add first_param.parameter to parameters
    
    Note: Parse additional parameters
    While parser.current_index is less than length of parser.tokens:
        Let comma_token be peek_token with parser as parser
        If comma_token is None or comma_token.type is not equal to OPERATOR or comma_token.value is not equal to ",":
            Break
        
        Consume_token with parser as parser
        
        Let param_result be parse_enhanced_parameter with parser as parser
        If param_result.success is false:
            Return param_result
        
        Add param_result.parameter to parameters
    
    Return dictionary containing:
        "success" as true
        "parameters" as parameters
        "next_index" as parser.current_index

Process called "parse_enhanced_parameter" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a single parameter with enhanced validation
    Let name_result be parse_enhanced_identifier with parser as parser
    If name_result.success is false:
        Return name_result
    
    Let type_annotation be None
    
    Note: Parse optional type annotation
    Let colon_token be peek_token with parser as parser
    If colon_token is not None and colon_token.type is equal to OPERATOR and colon_token.value is equal to ":":
        Consume_token with parser as parser
        Let type_result be parse_enhanced_type_expression with parser as parser
        If type_result.success is false:
            Return type_result
        Set type_annotation to type_result.type_expression
    
    Let parameter be Parameter with name as name_result.identifier and type_annotation as type_annotation
    
    Return dictionary containing:
        "success" as true
        "parameter" as parameter
        "next_index" as parser.current_index

Process called "parse_enhanced_type_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a type expression with enhanced validation
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is equal to IDENTIFIER:
        Consume_token with parser as parser
        Let type_expr be TypeExpression with name as token.value
        
        Return dictionary containing:
            "success" as true
            "type_expression" as type_expr
            "next_index" as parser.current_index
    Otherwise:
        Return dictionary containing "success" as false and "error" as "Expected type expression"

Process called "parse_enhanced_block" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a block of statements with enhanced validation
    Let statements be list containing
    
    Note: Parse statements until end of block
    While parser.current_index is less than length of parser.tokens:
        Let token be peek_token with parser as parser
        If token is None:
            Break
        
        If token.type is equal to DEDENT:
            Consume_token with parser as parser
            Break
        
        Let statement_result be parse_enhanced_statement with parser as parser
        If statement_result.success is false:
            Return statement_result
        
        Add statement_result.statement to statements
    
    Return dictionary containing:
        "success" as true
        "statements" as statements
        "next_index" as parser.current_index

Process called "parse_enhanced_comment_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a comment statement
    Let token be consume_token with parser as parser
    
    If token is None or token.type is not equal to COMMENT:
        Return dictionary containing "success" as false and "error" as "Expected comment"
    
    Let comment_stmt be CommentStatement with content as token.value
    
    Return dictionary containing:
        "success" as true
        "statement" as comment_stmt
        "next_index" as parser.current_index

Process called "parse_enhanced_import_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an import statement with enhanced validation
    Let import_token be consume_token with parser as parser
    
    If import_token is None or import_token.normalized_value is not equal to "import":
        Return dictionary containing "success" as false and "error" as "Expected 'import' keyword"
    
    Let module_result be parse_enhanced_string_literal with parser as parser
    If module_result.success is false:
        Return module_result
    
    Let import_stmt be ImportStatement with module as module_result.literal
    
    Return dictionary containing:
        "success" as true
        "statement" as import_stmt
        "next_index" as module_result.next_index

Process called "parse_enhanced_export_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an export statement with enhanced validation
    Let export_token be consume_token with parser as parser
    
    If export_token is None or export_token.normalized_value is not equal to "export":
        Return dictionary containing "success" as false and "error" as "Expected 'export' keyword"
    
    Let declaration_result be parse_enhanced_statement with parser as parser
    If declaration_result.success is false:
        Return declaration_result
    
    Let export_stmt be ExportStatement with declaration as declaration_result.statement
    
    Return dictionary containing:
        "success" as true
        "statement" as export_stmt
        "next_index" as declaration_result.next_index

Process called "parse_enhanced_if_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an if statement with enhanced validation
    Let if_token be consume_token with parser as parser
    
    If if_token is None or if_token.normalized_value is not equal to "if":
        Return dictionary containing "success" as false and "error" as "Expected 'if' keyword"
    
    Let condition_result be parse_enhanced_expression with parser as parser
    If condition_result.success is false:
        Return condition_result
    
    Let then_block_result be parse_enhanced_block with parser as parser
    If then_block_result.success is false:
        Return then_block_result
    
    Let else_block be None
    
    Note: Parse optional else block
    Let else_token be peek_token with parser as parser
    If else_token is not None and else_token.normalized_value is equal to "otherwise":
        Consume_token with parser as parser
        Let else_result be parse_enhanced_block with parser as parser
        If else_result.success is false:
            Return else_result
        Set else_block to else_result.statements
    
    Let if_stmt be IfStatement with condition as condition_result.expression and then_block as then_block_result.statements and else_block as else_block
    
    Return dictionary containing:
        "success" as true
        "statement" as if_stmt
        "next_index" as (else_result.next_index if else_block is not None else then_block_result.next_index)

Process called "parse_enhanced_string_literal" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a string literal with enhanced validation
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is not equal to STRING_LITERAL:
        Return dictionary containing "success" as false and "error" as "Expected string literal"
    
    Consume_token with parser as parser
    
    Return dictionary containing:
        "success" as true
        "literal" as token.value
        "next_index" as parser.current_index

Note: Recovery Strategy Implementations
Process called "skip_to_statement_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the end of the current statement
    Let start_index be parser.current_index
    
    While parser.current_index is less than length of parser.tokens:
        Let token be parser.tokens at index parser.current_index
        
        If token.type is equal to NEWLINE:
            Set parser.current_index to parser.current_index plus 1
            Break
        Otherwise if token.type is equal to OPERATOR and token.value is equal to ";":
            Set parser.current_index to parser.current_index plus 1
            Break
        
        Set parser.current_index to parser.current_index plus 1
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index

Process called "skip_to_block_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the end of the current block
    Let indent_level be 0
    Let start_index be parser.current_index
    
    While parser.current_index is less than length of parser.tokens:
        Let token be parser.tokens at index parser.current_index
        
        If token.type is equal to INDENT:
            Set indent_level to indent_level plus 1
        Otherwise if token.type is equal to DEDENT:
            Set indent_level to indent_level minus 1
            If indent_level is less than 0:
                Set parser.current_index to parser.current_index plus 1
                Break
        
        Set parser.current_index to parser.current_index plus 1
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index

Process called "skip_to_expression_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the end of the current expression
    Let paren_count be 0
    Let bracket_count be 0
    Let brace_count be 0
    
    While parser.current_index is less than length of parser.tokens:
        Let token be parser.tokens at index parser.current_index
        
        If token.type is equal to OPERATOR:
            If token.value is equal to "(":
                Set paren_count to paren_count plus 1
            Otherwise if token.value is equal to ")":
                Set paren_count to paren_count minus 1
                If paren_count is less than 0:
                    Break
            Otherwise if token.value is equal to "[":
                Set bracket_count to bracket_count plus 1
            Otherwise if token.value is equal to "]":
                Set bracket_count to bracket_count minus 1
                If bracket_count is less than 0:
                    Break
            Otherwise if token.value is equal to "{":
                Set brace_count to brace_count plus 1
            Otherwise if token.value is equal to "}":
                Set brace_count to brace_count minus 1
                If brace_count is less than 0:
                    Break
            Otherwise if token.value is equal to "," or token.value is equal to ";":
                If paren_count is equal to 0 and bracket_count is equal to 0 and brace_count is equal to 0:
                    Break
        
        Set parser.current_index to parser.current_index plus 1
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index

Process called "skip_to_declaration_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the end of the current declaration
    Return skip_to_statement_boundary with parser as parser

Process called "skip_to_newline_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the next newline
    While parser.current_index is less than length of parser.tokens:
        Let token be parser.tokens at index parser.current_index
        
        If token.type is equal to NEWLINE:
            Set parser.current_index to parser.current_index plus 1
            Break
        
        Set parser.current_index to parser.current_index plus 1
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index

Process called "skip_to_semicolon_boundary" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip to the next semicolon
    While parser.current_index is less than length of parser.tokens:
        Let token be parser.tokens at index parser.current_index
        
        If token.type is equal to OPERATOR and token.value is equal to ";":
            Set parser.current_index to parser.current_index plus 1
            Break
        
        Set parser.current_index to parser.current_index plus 1
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index

Note: Performance and Memory Management
Process called "get_parser_performance_metrics" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Get performance metrics from parser
    Return parser.performance_metrics

Process called "get_parser_memory_usage" that takes parser as Parser returns Dictionary[String, Integer]:
    Note: Get memory usage from parser
    Return parser.memory_usage

Process called "clear_parser_cache" that takes parser as Parser:
    Note: Clear the parser cache to free memory
    Set parser.parsing_cache to dictionary containing

Process called "optimize_parser_memory" that takes parser as Parser:
    Note: Optimize parser memory usage
    If length of parser.parsing_cache is greater than 1000:
        Set parser.parsing_cache to dictionary containing
    
    If length of parser.ast_node_pool is greater than 1000:
        Set parser.ast_node_pool to list containing
    If that_token is not None and that_token.normalized_value is equal to "that":
        Consume_token with parser as parser
        
        Let takes_token be consume_token with parser as parser
        If takes_token is None or takes_token.normalized_value is not equal to "takes":
            Return dictionary containing "success" as false and "error" as "Expected 'takes' keyword"
        
        Let params_result be parse_enhanced_parameter_list with parser as parser
        If params_result.success is false:
            Return params_result
        
        Set parameters to params_result.parameters
        
        Let returns_token be peek_token with parser as parser
        If returns_token is not None and returns_token.normalized_value is equal to "returns":
            Consume_token with parser as parser
            
            Let return_type_result be parse_enhanced_type_expression with parser as parser
            If return_type_result.success is false:
                Return return_type_result
            
            Set return_type to return_type_result.type_expression
    
    Let body_result be parse_enhanced_statement_block with parser as parser
    If body_result.success is false:
        Return body_result
    
    Let declaration be create_process_declaration_node with name as name_result.identifier and parameters as parameters and return_type as return_type and body as body_result.statements and location as get_token_location with token as process_token
    
    Return dictionary containing:
        "success" as true
        "statement" as declaration
        "next_index" as body_result.next_index

Note: Control Flow Parsing
Process called "parse_if_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an if statement (If condition then body otherwise body)
    Let if_token be consume_token with parser as parser
    
    If if_token is None or if_token.normalized_value is not equal to "if":
        Return dictionary containing "success" as false and "error" as "Expected 'if' keyword"
    
    Let condition_result be parse_expression with parser as parser
    If condition_result.success is false:
        Return condition_result
    
    Let then_token be peek_token with parser as parser
    If then_token is None or then_token.normalized_value is not equal to "then":
        Return dictionary containing "success" as false and "error" as "Expected 'then' keyword"
    
    Consume_token with parser as parser
    
    Let then_body_result be parse_statement_block with parser as parser
    If then_body_result.success is false:
        Return then_body_result
    
    Let else_body be list containing
    
    Let otherwise_token be peek_token with parser as parser
    If otherwise_token is not None and otherwise_token.normalized_value is equal to "otherwise":
        Consume_token with parser as parser
        
        Let else_body_result be parse_statement_block with parser as parser
        If else_body_result.success is false:
            Return else_body_result
        
        Set else_body to else_body_result.statements
    
    Let statement be create_if_statement_node with condition as condition_result.expression and then_body as then_body_result.statements and else_body as else_body and location as get_token_location with token as if_token
    
    Return dictionary containing:
        "success" as true
        "statement" as statement
        "next_index" as (then_body_result.next_index if length of else_body is equal to 0 else else_body_result.next_index)

Note: Utility Parsing Functions
Process called "parse_identifier" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an identifier
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is equal to IDENTIFIER or token.type is equal to QUALIFIED_IDENTIFIER or token.type is equal to MULTI_WORD_CONSTRUCT:
        Consume_token with parser as parser
        Return dictionary containing:
            "success" as true
            "identifier" as token.value
            "next_index" as parser.current_index
    Otherwise:
        Return dictionary containing "success" as false and "error" as "Expected identifier, got: " plus token.type

Process called "parse_parameter_list" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a parameter list
    Let parameters be list containing
    
    Let current_token be peek_token with parser as parser
    If current_token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If current_token.normalized_value is equal to "that" or current_token.normalized_value is equal to "returns":
        Return dictionary containing:
            "success" as true
            "parameters" as parameters
            "next_index" as parser.current_index
    
    While true:
        Let param_result be parse_parameter with parser as parser
        If param_result.success is false:
            Return param_result
        
        Add param_result.parameter to parameters
        
        Let comma_token be peek_token with parser as parser
        If comma_token is None or comma_token.value is not equal to ",":
            Break
        
        Consume_token with parser as parser
    
    Return dictionary containing:
        "success" as true
        "parameters" as parameters
        "next_index" as parser.current_index

Process called "parse_parameter" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a single parameter
    Let name_result be parse_identifier with parser as parser
    If name_result.success is false:
        Return name_result
    
    Let type_annotation be None
    Let default_value be None
    
    Let as_token be peek_token with parser as parser
    If as_token is not None and as_token.normalized_value is equal to "as":
        Consume_token with parser as parser
        
        Let type_result be parse_type_expression with parser as parser
        If type_result.success is false:
            Return type_result
        
        Set type_annotation to type_result.type_expression
    
    Let be_token be peek_token with parser as parser
    If be_token is not None and be_token.normalized_value is equal to "be":
        Consume_token with parser as parser
        
        Let default_result be parse_expression with parser as parser
        If default_result.success is false:
            Return default_result
        
        Set default_value to default_result.expression
    
    Let parameter be Parameter with:
        name as name_result.identifier
        type_annotation as type_annotation
        default_value as default_value
        is_variadic as false
    
    Return dictionary containing:
        "success" as true
        "parameter" as parameter
        "next_index" as parser.current_index

Process called "parse_type_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a type expression
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is equal to IDENTIFIER or token.type is equal to MULTI_WORD_CONSTRUCT:
        Consume_token with parser as parser
        
        Let type_expr be SimpleType with name as token.normalized_value
        
        Return dictionary containing:
            "success" as true
            "type_expression" as type_expr
            "next_index" as parser.current_index
    Otherwise:
        Return dictionary containing "success" as false and "error" as "Expected type expression, got: " plus token.type

Process called "parse_statement_block" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a block of statements
    Let statements be list containing
    Let start_index be parser.current_index
    
    While parser.current_index is less than length of parser.tokens:
        Let current_token be peek_token with parser as parser
        
        If current_token is None:
            Break
        
        If current_token.type is equal to DEDENT:
            Consume_token with parser as parser
            Break
        
        If current_token.type is equal to NEWLINE:
            Consume_token with parser as parser
            Continue
        
        Let statement_result be parse_statement with parser as parser
        
        If statement_result.success is false:
            If parser.error_count is greater than or equal to parser.max_errors:
                Break
            
            Let recovery_result be handle_parse_error with parser as parser and error as statement_result.error
            If recovery_result.success is false:
                Break
            
            Set parser.current_index to recovery_result.next_index
            Continue
        
        Add statement_result.statement to statements
        Set parser.current_index to statement_result.next_index
    
    Return dictionary containing:
        "success" as true
        "statements" as statements
        "next_index" as parser.current_index

Note: Error Handling
Process called "handle_parse_error" that takes parser as Parser and error as String returns Dictionary[String, Any]:
    Note: Handle parsing errors with recovery strategies
    Set parser.error_count to parser.error_count plus 1
    
    If not parser.recovery_enabled:
        Return dictionary containing "success" as false and "error" as error
    
    Let current_token be peek_token with parser as parser
    If current_token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    Let expected_tokens be list containing "let", "be", "if", "then", "otherwise", "process", "called", "that", "takes", "returns", "(", ")", "[", "]", "{", "}", ":", ","
    
    Let strategy be select_recovery_strategy with error_token as current_token and expected_tokens as expected_tokens and remaining_tokens as (parser.tokens from index parser.current_index)
    
    Let recovery_result be apply_recovery_strategy with strategy as strategy and tokens as (parser.tokens from index parser.current_index) and start_index as 0
    
    If recovery_result.success is false:
        Return dictionary containing "success" as false and "error" as "Recovery failed"
    
    Let quality_assessment be assess_recovery_quality with result as recovery_result
    
    Note: Report recovery information
    Let recovery_message be "Parser recovery applied: " plus quality_assessment.recommendation
    Report with engine as parser.diagnostic_engine and error as ParseError with message as recovery_message and location as recovery_result.context.error_location
    
    Return dictionary containing:
        "success" as true
        "next_index" as parser.current_index plus recovery_result.context.tokens_consumed

Note: Precedence Climbing Algorithm (Consolidated)
Process called "parse_expression_with_precedence" that takes parser as Parser and min_precedence as Integer returns Dictionary[String, Any]:
    Note: Parse expression using precedence climbing algorithm
    Let left_expr be parse_primary_expression with parser as parser
    
    If left_expr.success is false:
        Return left_expr
    
    Let left_node be left_expr.expression
    
    While parser.current_index is less than length of parser.tokens:
        Let current_token be peek_token with parser as parser
        
        If current_token is None:
            Break
        
        If not is_operator_token with token as current_token:
            Break
        
        Let operator be current_token.normalized_value
        Let precedence be get_operator_precedence with operator as operator
        
        If precedence is less than min_precedence:
            Break
        
        Let associativity be get_operator_associativity with operator as operator
        
        If associativity is equal to "right":
            Set precedence to precedence minus 1
        
        Consume_token with parser as parser
        
        Let right_result be parse_expression_with_precedence with parser as parser and min_precedence as precedence
        
        If right_result.success is false:
            Return right_result
        
        Let right_node be right_result.expression
        
        Let binary_op be create_binary_operation_node with left as left_node and operator as operator and right as right_node and location as get_token_location with token as current_token
        
        Set left_node to binary_op
    
    Return dictionary containing:
        "success" as true
        "expression" as left_node
        "next_index" as parser.current_index

Process called "parse_primary_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse primary expressions (literals, identifiers, parenthesized expressions)
    Let current_token be peek_token with parser as parser
    
    If current_token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    Match current_token.type:
        When STRING_LITERAL, INTEGER_LITERAL, FLOAT_LITERAL, BOOLEAN_LITERAL:
            Consume_token with parser as parser
            Let literal_node be create_literal_node with value as current_token.value and literal_type as get_literal_type with token as current_token and location as get_token_location with token as current_token
            Return dictionary containing:
                "success" as true
                "expression" as literal_node
                "next_index" as parser.current_index
        
        When IDENTIFIER, QUALIFIED_IDENTIFIER:
            Consume_token with parser as parser
            Let identifier_node be create_identifier_node with name as current_token.value and location as get_token_location with token as current_token
            Return dictionary containing:
                "success" as true
                "expression" as identifier_node
                "next_index" as parser.current_index
        
        When MULTI_WORD_CONSTRUCT:
            Note: Handle special multi-word constructs like "get ... from ..."
            If current_token.value is equal to "get":
                Let get_from_result be parse_get_from_expression with parser as parser
                Return get_from_result
            Otherwise:
                Consume_token with parser as parser
                Let construct_node be create_identifier_node with name as current_token.value and location as get_token_location with token as current_token
                Return dictionary containing:
                    "success" as true
                    "expression" as construct_node
                    "next_index" as parser.current_index
        
        When OPERATOR:
            If current_token.value is equal to "(":
                Consume_token with parser as parser
                Let parenthesized_result be parse_parenthesized_expression with parser as parser
                Return parenthesized_result
            Otherwise if current_token.value is equal to "[":
                Consume_token with parser as parser
                Let list_result be parse_list_literal with parser as parser
                Return list_result
            Otherwise if current_token.value is equal to "{":
                Consume_token with parser as parser
                Let dict_result be parse_dictionary_literal with parser as parser
                Return dict_result
            Otherwise:
                Return dictionary containing "success" as false and "error" as "Unexpected operator: " plus current_token.value
        
        Otherwise:
            Return dictionary containing "success" as false and "error" as "Unexpected token type: " plus current_token.type

Process called "parse_parenthesized_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse parenthesized expressions
    Let expr_result be parse_expression_with_precedence with parser as parser and min_precedence as 0
    
    If expr_result.success is false:
        Return expr_result
    
    Let closing_token be peek_token with parser as parser
    
    If closing_token is None:
        Return dictionary containing "success" as false and "error" as "Missing closing parenthesis"
    
    If closing_token.value is not equal to ")":
        Return dictionary containing "success" as false and "error" as "Expected closing parenthesis, got: " plus closing_token.value
    
    Consume_token with parser as parser
    
    Let parenthesized_node be ParenthesizedExpression with expression as expr_result.expression
    
    Return dictionary containing:
        "success" as true
        "expression" as parenthesized_node
        "next_index" as parser.current_index

Process called "parse_list_literal" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse list literals
    Let elements be list containing
    
    While parser.current_index is less than length of parser.tokens:
        Let current_token be peek_token with parser as parser
        
        If current_token is None:
            Return dictionary containing "success" as false and "error" as "Missing closing bracket"
        
        If current_token.value is equal to "]":
            Consume_token with parser as parser
            Let list_node be ListLiteral with elements as elements
            Return dictionary containing:
                "success" as true
                "expression" as list_node
                "next_index" as parser.current_index
        
        If length of elements is greater than 0:
            If current_token.value is not equal to ",":
                Return dictionary containing "success" as false and "error" as "Expected comma or closing bracket"
            Consume_token with parser as parser
        
        Let element_result be parse_expression_with_precedence with parser as parser and min_precedence as 0
        
        If element_result.success is false:
            Return element_result
        
        Add element_result.expression to elements
        Set parser.current_index to element_result.next_index
    
    Return dictionary containing "success" as false and "error" as "Missing closing bracket"

Process called "parse_dictionary_literal" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse dictionary literals
    Let keys be list containing
    Let values be list containing
    
    While parser.current_index is less than length of parser.tokens:
        Let current_token be peek_token with parser as parser
        
        If current_token is None:
            Return dictionary containing "success" as false and "error" as "Missing closing brace"
        
        If current_token.value is equal to "}":
            Consume_token with parser as parser
            Let dict_node be DictionaryLiteral with keys as keys and values as values
            Return dictionary containing:
                "success" as true
                "expression" as dict_node
                "next_index" as parser.current_index
        
        If length of keys is greater than 0:
            If current_token.value is not equal to ",":
                Return dictionary containing "success" as false and "error" as "Expected comma or closing brace"
            Consume_token with parser as parser
        
        Let key_result be parse_expression_with_precedence with parser as parser and min_precedence as 0
        
        If key_result.success is false:
            Return key_result
        
        Set parser.current_index to key_result.next_index
        
        Let colon_token be peek_token with parser as parser
        If colon_token is None:
            Return dictionary containing "success" as false and "error" as "Missing colon in dictionary literal"
        
        If colon_token.value is not equal to ":":
            Return dictionary containing "success" as false and "error" as "Expected colon in dictionary literal"
        
        Consume_token with parser as parser
        
        Let value_result be parse_expression_with_precedence with parser as parser and min_precedence as 0
        
        If value_result.success is false:
            Return value_result
        
        Add key_result.expression to keys
        Add value_result.expression to values
        Set parser.current_index to value_result.next_index
    
    Return dictionary containing "success" as false and "error" as "Missing closing brace"

Note: Precedence and Grammar Utilities (Consolidated)
Process called "get_operator_precedence" that takes operator as String returns Integer:
    Note: Get the precedence level of an operator
    Let definitions be load_definitions
    Let precedence_levels be definitions.operator_precedence
    
    If operator is in precedence_levels:
        Return precedence_levels at key operator
    Otherwise:
        Return -1

Process called "get_operator_associativity" that takes operator as String returns String:
    Note: Get the associativity of an operator
    Let definitions be load_definitions
    Let associativity be definitions.operator_associativity
    
    If operator is in associativity:
        Return associativity at key operator
    Otherwise:
        Return "left"

Process called "get_operator_type" that takes operator as String returns String:
    Note: Get the type classification of an operator
    Let definitions be load_definitions
    Let operator_types be definitions.operator_types
    
    If operator is in operator_types:
        Return operator_types at key operator
    Otherwise:
        Return "unknown"

Process called "is_operator" that takes construct as String returns Boolean:
    Note: Check if a multi-word construct is an operator in operator context
    Let definitions be load_definitions
    Let multi_word_constructs be definitions.multi_word_constructs
    
    Return construct is in multi_word_constructs

Process called "is_keyword" that takes construct as String returns Boolean:
    Note: Check if a multi-word construct is a keyword in keyword context
    Let definitions be load_definitions
    Let multi_word_constructs be definitions.multi_word_constructs
    
    Return construct is in multi_word_constructs

Process called "interpret_multi_word_construct" that takes construct as String and context as String returns String:
    Note: Interpret a multi-word construct based on context
    If context is equal to "operator_context":
        If is_operator with construct as construct:
            Return "operator"
        Otherwise:
            Return "identifier"
    Otherwise if context is equal to "keyword_context":
        If is_keyword with construct as construct:
            Return "keyword"
        Otherwise:
            Return "identifier"
    Otherwise:
        Return "identifier"

Process called "get_keyword_category" that takes keyword as String returns String:
    Note: Get the category of a keyword
    Let definitions be load_definitions
    Let keyword_categories be definitions.keyword_categories
    
    For each category in keyword_categories:
        If keyword is in (keyword_categories at key category):
            Return category
    Return "unknown"

Process called "is_control_flow_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a control flow keyword
    Return get_keyword_category with keyword as keyword is equal to "control_flow"

Process called "is_declaration_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a declaration keyword
    Return get_keyword_category with keyword as keyword is equal to "declaration"

Process called "is_function_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a function-related keyword
    Return get_keyword_category with keyword as keyword is equal to "function"

Process called "is_exception_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is an exception handling keyword
    Return get_keyword_category with keyword as keyword is equal to "exception"

Process called "is_boolean_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a boolean keyword
    Return get_keyword_category with keyword as keyword is equal to "boolean"

Process called "is_type_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a type keyword
    Return get_keyword_category with keyword as keyword is equal to "type"

Process called "is_special_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword is a special value keyword
    Return get_keyword_category with keyword as keyword is equal to "special"

Process called "has_higher_precedence" that takes op1 as String and op2 as String returns Boolean:
    Note: Check if op1 has higher precedence than op2
    Let prec1 be get_operator_precedence with operator as op1
    Let prec2 be get_operator_precedence with operator as op2
    Return prec1 is greater than prec2

Process called "has_equal_precedence" that takes op1 as String and op2 as String returns Boolean:
    Note: Check if op1 and op2 have equal precedence
    Let prec1 be get_operator_precedence with operator as op1
    Let prec2 be get_operator_precedence with operator as op2
    Return prec1 is equal to prec2

Process called "should_associate_left" that takes operator as String returns Boolean:
    Note: Check if an operator should associate left
    Let associativity be get_operator_associativity with operator as operator
    Return associativity is equal to "left"

Process called "should_associate_right" that takes operator as String returns Boolean:
    Note: Check if an operator should associate right
    Let associativity be get_operator_associativity with operator as operator
    Return associativity is equal to "right"

Process called "validate_operator_usage" that takes operator as String and left_type as String and right_type as String returns Dictionary[String, Any]:
    Note: STRICT VALIDATION: Mathematical symbols must only be used for mathematical operations
    Let errors be list containing
    Let warnings be list containing
    
    Let op_type be get_operator_type with operator as operator
    
    Note: ENFORCEMENT: Mathematical and mathematical_comparison operators have strict requirements
    If op_type is equal to "mathematical":
        If left_type is not equal to "number" and left_type is not equal to "integer" and left_type is not equal to "float":
            Add "Mathematical operators (+, -, *, /, %) can ONLY be used with numeric types" to errors
        If right_type is not equal to "number" and right_type is not equal to "integer" and right_type is not equal to "float":
            Add "Mathematical operators (+, -, *, /, %) can ONLY be used with numeric types" to errors
    Otherwise if op_type is equal to "mathematical_comparison":
        If left_type is not equal to "number" and left_type is not equal to "integer" and left_type is not equal to "float":
            Add "Mathematical comparison operators (<, >, <=, >=, !=) can ONLY be used with numeric types" to errors
        If right_type is not equal to "number" and right_type is not equal to "integer" and right_type is not equal to "float":
            Add "Mathematical comparison operators (<, >, <=, >=, !=) can ONLY be used with numeric types" to errors
    Otherwise if op_type is equal to "comparison":
        If left_type is not equal to right_type:
            Add "General comparison operators work best with operands of the same type" to warnings
    Otherwise if op_type is equal to "bitwise":
        If left_type is not equal to "integer" or right_type is not equal to "integer":
            Add "Bitwise operators expect integer operands" to warnings
    Otherwise if op_type is equal to "logical":
        If left_type is not equal to "boolean" or right_type is not equal to "boolean":
            Add "Logical operators expect boolean operands" to warnings
    
    Return dictionary containing "errors" as errors and "warnings" as warnings

Process called "validate_keyword_context" that takes keyword as String and context as String returns Boolean:
    Note: Validate if a keyword is appropriate in the given context
    Let category be get_keyword_category with keyword as keyword
    
    If context is equal to "statement_start":
        Return is_control_flow_keyword with keyword as keyword or
               is_declaration_keyword with keyword as keyword
    Otherwise if context is equal to "expression":
        Return is_boolean_keyword with keyword as keyword or
               is_special_keyword with keyword as keyword
    Otherwise if context is equal to "type_annotation":
        Return is_type_keyword with keyword as keyword
    Otherwise:
        Return true

Note: Expression Optimization (Consolidated)
Process called "optimize_expression" that takes expression as Expression returns Expression:
    Note: Optimize expression by simplifying constant expressions
    Match expression:
        When BinaryOperation with left as left and operator as operator and right as right:
            Let optimized_left be optimize_expression with expression as left
            Let optimized_right be optimize_expression with expression as right
            
            If is_literal_node with node as optimized_left and is_literal_node with node as optimized_right:
                Let result be evaluate_constant_expression with left as optimized_left and operator as operator and right as optimized_right
                If result is not None:
                    Return result
            
            Return BinaryOperation with left as optimized_left and operator as operator and right as optimized_right
        
        Otherwise:
            Return expression

Process called "evaluate_constant_expression" that takes left as Expression and operator as String and right as Expression returns Optional[Expression]:
    Note: Evaluate constant expressions at parse time
    Match left:
        When Literal with value as left_value and literal_type as left_type:
            Match right:
                When Literal with value as right_value and literal_type as right_type:
                    If left_type is equal to "integer" and right_type is equal to "integer":
                        Return evaluate_integer_operation with left as left_value and operator as operator and right as right_value
                    Otherwise if left_type is equal to "float" or right_type is equal to "float":
                        Return evaluate_float_operation with left as left_value and operator as operator and right as right_value
                    Otherwise if left_type is equal to "string" and right_type is equal to "string":
                        Return evaluate_string_operation with left as left_value and operator as operator and right as right_value
                    Otherwise:
                        Return None
                Otherwise:
                    Return None
        Otherwise:
            Return None

Process called "evaluate_integer_operation" that takes left as Integer and operator as String and right as Integer returns Optional[Expression]:
    Note: Evaluate integer operations
    Match operator:
        When "plus":
            Return Literal with value as (left plus right) and literal_type as "integer"
        When "minus":
            Return Literal with value as (left minus right) and literal_type as "integer"
        When "multiplied by":
            Return Literal with value as (left multiplied by right) and literal_type as "integer"
        When "divided by":
            If right is equal to 0:
                Return None
            Return Literal with value as (left divided by right) and literal_type as "integer"
        Otherwise:
            Return None

Process called "evaluate_float_operation" that takes left as Float and operator as String and right as Float returns Optional[Expression]:
    Note: Evaluate float operations
    Match operator:
        When "plus":
            Return Literal with value as (left plus right) and literal_type as "float"
        When "minus":
            Return Literal with value as (left minus right) and literal_type as "float"
        When "multiplied by":
            Return Literal with value as (left multiplied by right) and literal_type as "float"
        When "divided by":
            If right is equal to 0.0:
                Return None
            Return Literal with value as (left divided by right) and literal_type as "float"
        Otherwise:
            Return None

Process called "evaluate_string_operation" that takes left as String and operator as String and right as String returns Optional[Expression]:
    Note: Evaluate string operations
    Match operator:
        When "plus":
            Return Literal with value as (left plus right) and literal_type as "string"
        Otherwise:
            Return None

Note: Parser Utility Functions
Process called "peek_token" that takes parser as Parser returns Optional[Token]:
    Note: Peek at the current token without consuming it
    If parser.current_index is greater than or equal to length of parser.tokens:
        Return None
    Otherwise:
        Return parser.tokens at index parser.current_index

Process called "consume_token" that takes parser as Parser returns Optional[Token]:
    Note: Consume the current token and advance
    Let token be peek_token with parser as parser
    If token is not None:
        Set parser.current_index to parser.current_index plus 1
    Return token

Process called "expect_token" that takes parser as Parser and expected_type as String returns Dictionary[String, Any]:
    Note: Expect and consume a token of a specific type
    Let token be peek_token with parser as parser
    
    If token is None:
        Return dictionary containing "success" as false and "error" as "Unexpected end of tokens"
    
    If token.type is not equal to expected_type:
        Return dictionary containing "success" as false and "error" as "Expected " plus expected_type plus ", got " plus token.type
    
    Consume_token with parser as parser
    
    Return dictionary containing:
        "success" as true
        "token" as token
        "next_index" as parser.current_index

Note: Additional Statement Parsers
Process called "parse_import_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an import statement
    Let import_token be consume_token with parser as parser
    
    Let module_result be parse_identifier with parser as parser
    If module_result.success is false:
        Return module_result
    
    Let alias be None
    Let as_token be peek_token with parser as parser
    If as_token is not None and as_token.normalized_value is equal to "as":
        Consume_token with parser as parser
        
        Let alias_result be parse_identifier with parser as parser
        If alias_result.success is false:
            Return alias_result
        
        Set alias to alias_result.identifier
    
    Let statement be ImportStatement with module as module_result.identifier and alias as alias
    
    Return dictionary containing:
        "success" as true
        "statement" as statement
        "next_index" as (alias_result.next_index if alias is not None else module_result.next_index)

Process called "parse_export_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse an export statement
    Let export_token be consume_token with parser as parser
    
    Let items be list containing
    
    While true:
        Let item_result be parse_identifier with parser as parser
        If item_result.success is false:
            Return item_result
        
        Let alias be None
        Let as_token be peek_token with parser as parser
        If as_token is not None and as_token.normalized_value is equal to "as":
            Consume_token with parser as parser
            
            Let alias_result be parse_identifier with parser as parser
            If alias_result.success is false:
                Return alias_result
            
            Set alias to alias_result.identifier
        
        Let export_item be ExportItem with name as item_result.identifier and alias as alias
        Add export_item to items
        
        Let comma_token be peek_token with parser as parser
        If comma_token is None or comma_token.value is not equal to ",":
            Break
        
        Consume_token with parser as parser
    
    Let statement be ExportStatement with items as items
    
    Return dictionary containing:
        "success" as true
        "statement" as statement
        "next_index" as parser.current_index

Process called "parse_comment_statement" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse a comment statement
    Let comment_token be consume_token with parser as parser
    
    Let statement be CommentStatement with content as comment_token.value and is_multiline as comment_token.value contains ":End Note"
    
    Return dictionary containing:
        "success" as true
        "statement" as statement
        "next_index" as parser.current_index

Note: Parse Error Type for Diagnostics
Type ParseError is Dictionary with:
    message as String
    location as SourceLocation

Note: Error Recovery Strategies (Consolidated)
Type RecoveryStrategy is:
    | SkipToDelimiter with delimiter as String
    | SkipToKeyword with keywords as List[String]
    | SkipToNewline
    | SkipToStatementEnd
    | InsertMissingToken with token_type as String and value as String
    | DeleteUnexpectedToken
    | ReplaceToken with old_value as String and new_value as String

Type RecoveryContext is Dictionary with:
    error_location as SourceLocation
    error_message as String
    recovery_strategy as RecoveryStrategy
    confidence as Float
    tokens_consumed as Integer
    tokens_inserted as Integer

Type RecoveryResult is Dictionary with:
    success as Boolean
    recovered_tokens as List[Token]
    remaining_tokens as List[Token]
    context as RecoveryContext
    error_count as Integer

Process called "select_recovery_strategy" that takes parser as Parser and error_token as Token and expected_tokens as List[String] returns RecoveryStrategy:
    Note: Select the best recovery strategy based on error context
    Let error_type be error_token.type
    Let error_value be error_token.normalized_value
    
    Note: Handle unexpected characters
    If error_type is equal to ERROR:
        Return SkipToNewline
    
    Note: Handle missing delimiters
    If "(" is in expected_tokens:
        Return SkipToDelimiter with delimiter as ")"
    Otherwise if "[" is in expected_tokens:
        Return SkipToDelimiter with delimiter as "]"
    Otherwise if "{" is in expected_tokens:
        Return SkipToDelimiter with delimiter as "}"
    Otherwise if ":" is in expected_tokens:
        Return InsertMissingToken with token_type as "COLON" and value as ":"
    Otherwise if "," is in expected_tokens:
        Return InsertMissingToken with token_type as "COMMA" and value as ","
    
    Note: Handle missing keywords
    If "let" is in expected_tokens:
        Return InsertMissingToken with token_type as "KEYWORD" and value as "let"
    Otherwise if "be" is in expected_tokens:
        Return InsertMissingToken with token_type as "KEYWORD" and value as "be"
    Otherwise if "if" is in expected_tokens:
        Return InsertMissingToken with token_type as "KEYWORD" and value as "if"
    Otherwise if "otherwise" is in expected_tokens:
        Return InsertMissingToken with token_type as "KEYWORD" and value as "otherwise"
    
    Note: Handle statement-level errors
    If is_statement_start_keyword with keyword as error_value:
        Return SkipToStatementEnd
    Otherwise:
        Return SkipToNewline

Process called "apply_recovery_strategy" that takes parser as Parser and strategy as RecoveryStrategy returns RecoveryResult:
    Note: Apply a recovery strategy to tokens
    Let recovered_tokens be list containing
    Let remaining_tokens be list containing
    Let tokens_consumed be 0
    let tokens_inserted be 0
    
    Match strategy:
        When SkipToDelimiter with delimiter as delimiter:
            Let result be skip_to_delimiter with parser as parser and delimiter as delimiter
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When SkipToKeyword with keywords as keywords:
            Let result be skip_to_keyword with parser as parser and keywords as keywords
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When SkipToNewline:
            Let result be skip_to_newline with parser as parser
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When SkipToStatementEnd:
            Let result be skip_to_statement_end with parser as parser
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When InsertMissingToken with token_type as token_type and value as value:
            Let result be insert_missing_token with parser as parser and token_type as token_type and value as value
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When DeleteUnexpectedToken:
            Let result be delete_unexpected_token with parser as parser
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
        
        When ReplaceToken with old_value as old_value and new_value as new_value:
            Let result be replace_token with parser as parser and old_value as old_value and new_value as new_value
            Set recovered_tokens to result.recovered_tokens
            Set remaining_tokens to result.remaining_tokens
            Set tokens_consumed to result.tokens_consumed
            Set tokens_inserted to result.tokens_inserted
    
    Let context be RecoveryContext with:
        error_location as get_token_location with token as (parser.tokens at index parser.current_index)
        error_message as "Applied recovery strategy"
        recovery_strategy as strategy
        confidence as calculate_recovery_confidence with strategy as strategy and tokens_consumed as tokens_consumed
        tokens_consumed as tokens_consumed
        tokens_inserted as tokens_inserted
    
    Return RecoveryResult with:
        success as true
        recovered_tokens as recovered_tokens
        remaining_tokens as remaining_tokens
        context as context
        error_count as 1

Note: Recovery Strategy Implementations
Process called "skip_to_delimiter" that takes parser as Parser and delimiter as String returns Dictionary[String, Any]:
    Note: Skip tokens until a specific delimiter is found
    Let recovered_tokens be list containing
    Let tokens_consumed be 0
    Let tokens_inserted be 0
    Let start_index be parser.current_index
    
    For i from start_index to length of parser.tokens minus 1:
        Let token be parser.tokens at index i
        Set tokens_consumed to tokens_consumed plus 1
        
        If token.value is equal to delimiter:
            Set remaining_tokens to (parser.tokens from index (i plus 1))
            Break
        Otherwise if token.type is equal to NEWLINE:
            Set remaining_tokens to (parser.tokens from index i)
            Break
        Otherwise:
            Add token to recovered_tokens
    
    If i is equal to length of parser.tokens:
        Set remaining_tokens to list containing
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "skip_to_keyword" that takes parser as Parser and keywords as List[String] returns Dictionary[String, Any]:
    Note: Skip tokens until a specific keyword is found
    Let recovered_tokens be list containing
    Let tokens_consumed be 0
    Let tokens_inserted be 0
    Let start_index be parser.current_index
    
    For i from start_index to length of parser.tokens minus 1:
        Let token be parser.tokens at index i
        Set tokens_consumed to tokens_consumed plus 1
        
        If token.normalized_value is in keywords:
            Set remaining_tokens to (parser.tokens from index i)
            Break
        Otherwise if token.type is equal to NEWLINE:
            Set remaining_tokens to (parser.tokens from index i)
            Break
        Otherwise:
            Add token to recovered_tokens
    
    If i is equal to length of parser.tokens:
        Set remaining_tokens to list containing
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "skip_to_newline" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip tokens until a newline is found
    Let recovered_tokens be list containing
    Let tokens_consumed be 0
    Let tokens_inserted be 0
    Let start_index be parser.current_index
    
    For i from start_index to length of parser.tokens minus 1:
        Let token be parser.tokens at index i
        Set tokens_consumed to tokens_consumed plus 1
        
        If token.type is equal to NEWLINE:
            Set remaining_tokens to (parser.tokens from index (i plus 1))
            Break
        Otherwise:
            Add token to recovered_tokens
    
    If i is equal to length of parser.tokens:
        Set remaining_tokens to list containing
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "skip_to_statement_end" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Skip tokens until the end of a statement
    Let recovered_tokens be list containing
    Let tokens_consumed be 0
    Let tokens_inserted be 0
    Let start_index be parser.current_index
    Let statement_keywords be list containing "let", "if", "process", "import", "export", "try", "for", "while"
    
    For i from start_index to length of parser.tokens minus 1:
        Let token be parser.tokens at index i
        Set tokens_consumed to tokens_consumed plus 1
        
        If token.type is equal to NEWLINE:
            Set remaining_tokens to (parser.tokens from index (i plus 1))
            Break
        Otherwise if token.normalized_value is in statement_keywords and i is greater than start_index:
            Set remaining_tokens to (parser.tokens from index i)
            Break
        Otherwise:
            Add token to recovered_tokens
    
    If i is equal to length of parser.tokens:
        Set remaining_tokens to list containing
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "insert_missing_token" that takes parser as Parser and token_type as String and value as String returns Dictionary[String, Any]:
    Note: Insert a missing token at the current position
    Let recovered_tokens be list containing
    Let tokens_consumed be 0
    Let tokens_inserted be 1
    
    Let missing_token be create_token with type as token_type and value as value and line as (parser.tokens at index parser.current_index).line and column as (parser.tokens at index parser.current_index).column and file as (parser.tokens at index parser.current_index).file_path
    
    Add missing_token to recovered_tokens
    
    Set remaining_tokens to (parser.tokens from index parser.current_index)
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "delete_unexpected_token" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Delete an unexpected token
    Let recovered_tokens be list containing
    Let tokens_consumed be 1
    Let tokens_inserted be 0
    
    Set remaining_tokens to (parser.tokens from index (parser.current_index plus 1))
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Process called "replace_token" that takes parser as Parser and old_value as String and new_value as String returns Dictionary[String, Any]:
    Note: Replace a token with a corrected value
    Let recovered_tokens be list containing
    Let tokens_consumed be 1
    Let tokens_inserted be 1
    
    Let original_token be parser.tokens at index parser.current_index
    Let corrected_token be create_token with type as original_token.type and value as new_value and line as original_token.line and column as original_token.column and file as original_token.file_path
    
    Add corrected_token to recovered_tokens
    Set remaining_tokens to (parser.tokens from index (parser.current_index plus 1))
    
    Return dictionary containing:
        "recovered_tokens" as recovered_tokens
        "remaining_tokens" as remaining_tokens
        "tokens_consumed" as tokens_consumed
        "tokens_inserted" as tokens_inserted

Note: Recovery Quality Assessment
Process called "calculate_recovery_confidence" that takes strategy as RecoveryStrategy and tokens_consumed as Integer returns Float:
    Note: Calculate confidence score for a recovery strategy
    Let base_confidence be 0.0
    
    Match strategy:
        When SkipToDelimiter with delimiter as delimiter:
            Set base_confidence to 0.8
        When SkipToKeyword with keywords as keywords:
            Set base_confidence to 0.7
        When SkipToNewline:
            Set base_confidence to 0.6
        When SkipToStatementEnd:
            Set base_confidence to 0.7
        When InsertMissingToken with token_type as token_type and value as value:
            Set base_confidence to 0.9
        When DeleteUnexpectedToken:
            Set base_confidence to 0.5
        When ReplaceToken with old_value as old_value and new_value as new_value:
            Set base_confidence to 0.8
    
    Note: Penalize for consuming too many tokens
    If tokens_consumed is greater than 10:
        Set base_confidence to base_confidence minus 0.2
    Otherwise if tokens_consumed is greater than 5:
        Set base_confidence to base_confidence minus 0.1
    
    Return maximum with a as base_confidence and b as 0.0

Process called "assess_recovery_quality" that takes result as RecoveryResult returns Dictionary[String, Any]:
    Note: Assess the quality of a recovery result
    Let quality_score be 0.0
    Let issues be list containing
    
    Note: Base score from confidence
    Set quality_score to result.context.confidence
    
    Note: Penalize for consuming too many tokens
    If result.context.tokens_consumed is greater than 15:
        Set quality_score to quality_score minus 0.3
        Add "Recovery consumed too many tokens" to issues
    
    Note: Penalize for inserting too many tokens
    If result.context.tokens_inserted is greater than 3:
        Set quality_score to quality_score minus 0.2
        Add "Recovery inserted too many tokens" to issues
    
    Note: Bonus for successful recovery
    If result.success:
        Set quality_score to quality_score plus 0.1
    
    Return dictionary containing:
        "quality_score" as quality_score
        "issues" as issues
        "recommendation" as get_recovery_recommendation with score as quality_score

Process called "get_recovery_recommendation" that takes score as Float returns String:
    Note: Get recommendation based on recovery quality score
    If score is greater than or equal to 0.8:
        Return "High confidence recovery - continue parsing"
    Otherwise if score is greater than or equal to 0.6:
        Return "Moderate confidence recovery - continue with caution"
    Otherwise if score is greater than or equal to 0.4:
        Return "Low confidence recovery - consider manual review"
    Otherwise:
        Return "Poor recovery - manual intervention recommended"

Note: Additional Utility Functions
Process called "is_operator_token" that takes token as Token returns Boolean:
    Note: Check if a token is an operator
    Return token.type is equal to OPERATOR or token.type is equal to MULTI_WORD_CONSTRUCT

Process called "get_literal_type" that takes token as Token returns String:
    Note: Get the type of a literal token
    Match token.type:
        When STRING_LITERAL:
            Return "string"
        When INTEGER_LITERAL:
            Return "integer"
        When FLOAT_LITERAL:
            Return "float"
        When BOOLEAN_LITERAL:
            Return "boolean"
        Otherwise:
            Return "unknown"

Process called "is_statement_start_keyword" that takes keyword as String returns Boolean:
    Note: Check if a keyword can start a statement
    Let statement_starters be list containing "let", "if", "process", "import", "export", "try", "for", "while", "break", "continue", "return", "throw"
    Return keyword is in statement_starters

Process called "create_token" that takes type as String and value as String and line as Integer and column as Integer and file as String returns Token:
    Note: Create a token for recovery purposes
    Return Token with:
        type as type
        value as value
        normalized_value as normalize_identifier with text as value
        line as line
        column as column
        file as file
        metadata as dictionary containing

Process called "parse_get_from_expression" that takes parser as Parser returns Dictionary[String, Any]:
    Note: Parse natural language module access: "get function_name from module_name"
    Note: Expected pattern: get [function_name] from [module_name]
    
    Note: Consume "get" token
    Consume_token with parser as parser
    
    Note: Parse function name (can be multi-word)
    Let function_name_parts be list containing
    Let current_token be peek_token with parser as parser
    
    Note: Collect function name parts until we hit "from"
    While current_token is not None and current_token.value is not equal to "from":
        If current_token.type is equal to "IDENTIFIER" or current_token.type is equal to "MULTI_WORD_CONSTRUCT":
            Add current_token.value to function_name_parts
            Consume_token with parser as parser
            Set current_token to peek_token with parser as parser
        Otherwise:
            Return dictionary containing "success" as false and "error" as "Expected function name after 'get'"
    
    If length of function_name_parts is equal to 0:
        Return dictionary containing "success" as false and "error" as "Missing function name in 'get ... from' expression"
    
    Note: Expect "from" keyword
    If current_token is None or current_token.value is not equal to "from":
        Return dictionary containing "success" as false and "error" as "Expected 'from' after function name"
    
    Note: Consume "from" token
    Consume_token with parser as parser
    
    Note: Parse module name
    Set current_token to peek_token with parser as parser
    If current_token is None or current_token.type is not equal to "IDENTIFIER":
        Return dictionary containing "success" as false and "error" as "Expected module name after 'from'"
    
    Let module_name be current_token.value
    Consume_token with parser as parser
    
    Note: Join function name parts with underscores for multi-word functions
    Let function_name be join function_name_parts with separator as " "
    
    Note: Create a member access node equivalent to "module.function"
    Let module_node be create_identifier_node with name as module_name and location as get_current_location with parser as parser
    Let member_node be create_member_access_node with 
        object as module_node and 
        member as function_name and 
        location as get_current_location with parser as parser
    
    Return dictionary containing:
        "success" as true
        "expression" as member_node
        "next_index" as parser.current_index

Process called "normalize_identifier" that takes text as String returns String:
    Note: Normalize identifier names for consistency
    Return text

Process called "is_letter" that takes char as String returns Boolean:
    Note: Check if character is a letter
    If char is greater than or equal to "A" and char is less than or equal to "Z":
        Return true
    If char is greater than or equal to "a" and char is less than or equal to "z":
        Return true
    Return false