Note: Metal Backend for Apple GPU Acceleration  
Note: Targets Apple Silicon (M1/M2/M3) and macOS/iOS GPUs
Note: Generates Metal Shading Language (MSL) for optimal Apple performance

Import "collections" as Collections
Import "os" as OS

Note: Metal-Specific Types and Configuration

Type called "MetalDeviceType":
    | IntegratedGPU
    | DiscreteGPU
    | ExternalGPU

Type called "MetalFeatureSet":
    | iOS_GPUFamily1_v1
    | iOS_GPUFamily2_v1  
    | iOS_GPUFamily3_v1
    | iOS_GPUFamily4_v1
    | iOS_GPUFamily5_v1
    | macOS_GPUFamily1_v1
    | macOS_GPUFamily2_v1
    | Common_1
    | Common_2
    | Common_3

Type called "MetalDeviceInfo":
    device_type as MetalDeviceType
    feature_set as MetalFeatureSet
    max_threads_per_threadgroup as Integer
    threadgroup_memory_length as Integer
    max_buffer_length as Integer
    supports_simd_groups as Boolean
    supports_imageblocks as Boolean
    supports_tile_shaders as Boolean
    supports_argument_buffers as Boolean
    apple_silicon_version as String  Note: "M1", "M2", "M3", etc.

Type called "MetalKernel":
    name as String
    parameters as List[String]
    msl_code as String
    metal_lib_binary as String
    threadgroup_memory_size as Integer
    threads_per_threadgroup as List[Integer]
    threadgroups_per_grid as List[Integer]
    kernel_arguments as List[String]
    uses_simd_groups as Boolean

Type called "MetalComputePipelineState":
    kernel as MetalKernel
    thread_execution_width as Integer
    max_total_threads_per_threadgroup as Integer
    threadgroup_memory_length as Integer
    static_threadgroup_memory_length as Integer

Note: Metal Device Detection and Management

Process called "metal_detect_devices" that takes no_parameters returns List[GpuDevice]:
    Note: Detect all available Metal devices
    Let devices be empty list
    
    Note: Check if running on macOS/iOS
    Let current_platform be OS.get_platform with no parameters
    If current_platform is not "darwin":
        Return devices  Note: Metal only available on Apple platforms
    
    Let metal_devices be get_metal_devices with no parameters
    For each device in metal_devices:
        Let gpu_device be create_gpu_device_from_metal with device
        Add gpu_device to devices
    
    Return devices

Process called "get_metal_devices" that takes no_parameters returns List[Dictionary]:
    Note: Get all available Metal devices
    Return host_call_metal_get_devices with no parameters

Process called "create_gpu_device_from_metal" that takes device_info as Dictionary returns GpuDevice:
    Note: Convert Metal device info to generic GPU device
    Let device_name be device_info["name"]
    Let memory_size be device_info["recommended_max_working_set_size"]
    
    Return GpuDevice with:
        device_id as device_info["device_id"]
        name as device_name
        backend_type as Metal
        capabilities as GpuCapabilities with:
            compute_capability as device_info["feature_set"]
            max_threads_per_block as device_info["max_threads_per_threadgroup"]
            max_shared_memory as device_info["threadgroup_memory_length"]
            max_registers_per_thread as 0  Note: Metal doesn't expose register count
            memory_bandwidth as estimate_metal_memory_bandwidth with device_info
            supports_double_precision as false  Note: Most Apple GPUs don't support double precision
            supports_tensor_cores as device_info["apple_silicon_version"] is not ""  Note: Apple Silicon has specialized ML units
            warp_size as device_info["simd_group_size"]
        memory_total as memory_size
        memory_available as memory_size  Note: Assume all available initially
        is_available as true

Note: Metal Kernel Compilation

Process called "metal_compile_kernel" that takes runa_ast as Dictionary and target as GpuCompilationTarget and hints as List[ParallelizationHint] returns GpuCompilationResult:
    Note: Compile Runa AST to Metal kernel
    Let result be empty dictionary
    Set result["success"] to false
    Set result["error_messages"] to empty list
    Set result["warnings"] to empty list
    
    Note: Validate Metal compilation target
    Let validation_errors be validate_metal_target with target
    If length of validation_errors > 0:
        Set result["error_messages"] to validation_errors
        Return result as GpuCompilationResult
    
    Note: Generate Metal kernels from parallelization hints
    Let kernels be empty list
    For each hint in hints:
        Let kernel be generate_metal_kernel_from_hint with runa_ast and hint and target
        If kernel is not null:
            Add kernel to kernels
    
    If length of kernels equals 0:
        Add "No Metal kernels generated from parallelization hints" to result["error_messages"]
        Return result as GpuCompilationResult
    
    Note: Compile Metal shading language code
    Let compilation_success be compile_metal_shaders with kernels and target
    If not compilation_success:
        Add "Metal shader compilation failed" to result["error_messages"]
        Return result as GpuCompilationResult
    
    Note: Create Metal library binary
    Let metal_library be create_metal_library with kernels and target
    
    Set result["success"] to true
    Set result["module"] to create_gpu_module_from_metal with kernels
    Set result["binary_code"] to metal_library
    Set result["performance_estimates"] to estimate_metal_performance with kernels and target.device
    
    Return result as GpuCompilationResult

Process called "generate_metal_kernel_from_hint" that takes runa_ast as Dictionary and hint as ParallelizationHint and target as GpuCompilationTarget returns MetalKernel:
    Note: Generate Metal kernel from parallelization hint
    Let kernel_name be "runa_kernel_" joined with hint.loop_start joined with "_" joined with hint.loop_end
    
    Note: Calculate threadgroup configuration for Apple Silicon optimization
    Let threadgroup_config be calculate_metal_threadgroup_config with hint and target.device
    
    Note: Generate kernel parameters
    Let parameters be extract_metal_kernel_parameters with runa_ast and hint
    
    Note: Generate Metal Shading Language kernel code
    Let msl_code be generate_metal_kernel_code with runa_ast and hint and threadgroup_config
    
    Note: Calculate threadgroup memory usage
    Let threadgroup_memory_usage be calculate_metal_threadgroup_memory_usage with hint and threadgroup_config
    
    Note: Check if SIMD groups should be used for optimization
    Let uses_simd be should_use_simd_groups with hint and target.device
    
    Return MetalKernel with:
        name as kernel_name
        parameters as parameters
        msl_code as msl_code
        metal_lib_binary as ""  Note: Generated during compilation
        threadgroup_memory_size as threadgroup_memory_usage
        threads_per_threadgroup as threadgroup_config["threads_per_threadgroup"]
        threadgroups_per_grid as threadgroup_config["threadgroups_per_grid"]
        kernel_arguments as parameters
        uses_simd_groups as uses_simd

Process called "calculate_metal_threadgroup_config" that takes hint as ParallelizationHint and device as GpuDevice returns Dictionary:
    Note: Calculate optimal Metal threadgroup configuration for Apple Silicon
    Let total_threads be hint.loop_end - hint.loop_start
    Let max_threadgroup_size be device.capabilities.max_threads_per_block
    
    Note: Apple Silicon performs best with specific threadgroup sizes
    Let optimal_threadgroup_size be optimize_metal_threadgroup_size with hint and device
    
    Note: Calculate threadgroups needed
    Let threadgroups_needed be (total_threads + optimal_threadgroup_size - 1) / optimal_threadgroup_size
    
    Note: Optimize for memory access patterns on Apple unified memory
    Match hint.memory_access_pattern:
        Case "sequential":
            Note: 1D layout optimal for sequential access
            Return Dictionary with:
                "threads_per_threadgroup" as [optimal_threadgroup_size, 1, 1]
                "threadgroups_per_grid" as [threadgroups_needed, 1, 1]
                
        Case "strided":
            Note: 2D layout for better cache utilization on Apple Silicon
            Let threadgroup_x be minimum of optimal_threadgroup_size and 32
            Let threadgroup_y be optimal_threadgroup_size / threadgroup_x
            Let threadgroups_x be (total_threads + threadgroup_x - 1) / threadgroup_x
            Let threadgroups_y be threadgroup_y
            
            Return Dictionary with:
                "threads_per_threadgroup" as [threadgroup_x, threadgroup_y, 1]
                "threadgroups_per_grid" as [threadgroups_x, threadgroups_y, 1]
                
        Case "random":
            Note: Balanced 2D layout for unpredictable access
            Let balanced_size be square_root of optimal_threadgroup_size
            Return Dictionary with:
                "threads_per_threadgroup" as [balanced_size, balanced_size, 1]
                "threadgroups_per_grid" as [threadgroups_needed / balanced_size, balanced_size, 1]
                
        Otherwise:
            Note: Default 1D layout
            Return Dictionary with:
                "threads_per_threadgroup" as [optimal_threadgroup_size, 1, 1]
                "threadgroups_per_grid" as [threadgroups_needed, 1, 1]

Process called "optimize_metal_threadgroup_size" that takes hint as ParallelizationHint and device as GpuDevice returns Integer:
    Note: Find optimal threadgroup size for Apple Silicon
    Let max_size be device.capabilities.max_threads_per_block
    
    Note: Apple Silicon performs well with these threadgroup sizes
    Let apple_silicon_optimal_sizes be [32, 64, 128, 256, 512]
    Let best_size be 128  Note: Good default for Apple Silicon
    
    Note: Check if device supports SIMD groups (Apple Silicon feature)
    If device.capabilities.supports_tensor_cores:  Note: Using this as proxy for Apple Silicon
        Note: Apple Silicon SIMD groups are 32 threads
        Let simd_size be 32
        For each size in apple_silicon_optimal_sizes:
            If size <= max_size and (size % simd_size) equals 0:
                Set best_size to size
                Break  Note: First SIMD-aligned size is usually good
    
    Return best_size

Process called "generate_metal_kernel_code" that takes runa_ast as Dictionary and hint as ParallelizationHint and config as Dictionary returns String:
    Note: Generate Metal Shading Language kernel code from Runa AST
    Let kernel_code be "#include <metal_stdlib>\n"
    Set kernel_code to kernel_code joined with "using namespace metal;\n\n"
    
    Note: Generate kernel function signature
    Set kernel_code to kernel_code joined with "kernel void runa_kernel("
    
    Note: Generate kernel parameters with Metal buffer attributes
    Let param_declarations be generate_metal_parameters with runa_ast and hint
    Set kernel_code to kernel_code joined with param_declarations
    
    Note: Add Metal-specific thread position parameters
    Set kernel_code to kernel_code joined with ",\n                                    uint3 gid [[thread_position_in_grid]],\n"
    Set kernel_code to kernel_code joined with "                                    uint3 tid [[thread_position_in_threadgroup]],\n"
    Set kernel_code to kernel_code joined with "                                    uint3 bid [[threadgroup_position_in_grid]]) {\n"
    
    Note: Generate thread indexing for 1D or 2D configurations
    Let threads_per_threadgroup be config["threads_per_threadgroup"]
    If length of threads_per_threadgroup > 1 and threads_per_threadgroup[1] > 1:
        Note: 2D configuration
        Set kernel_code to kernel_code joined with "    uint index = gid.x + gid.y * " joined with threads_per_threadgroup[0] joined with ";\n"
    Otherwise:
        Note: 1D configuration
        Set kernel_code to kernel_code joined with "    uint index = gid.x;\n"
    
    Note: Generate bounds checking
    Let total_work_items be hint.loop_end - hint.loop_start
    Set kernel_code to kernel_code joined with "    if (index >= " joined with total_work_items joined with ") return;\n"
    Set kernel_code to kernel_code joined with "    \n"
    
    Note: Generate loop body translation
    Let loop_body be translate_runa_loop_to_metal with runa_ast and hint
    Set kernel_code to kernel_code joined with loop_body
    
    Set kernel_code to kernel_code joined with "}\n"
    
    Return kernel_code

Process called "generate_metal_parameters" that takes runa_ast as Dictionary and hint as ParallelizationHint returns String:
    Note: Generate Metal kernel parameter declarations with buffer attributes
    Let params be empty list
    
    Note: Extract variable references from the loop
    Let loop_variables be extract_loop_variables with runa_ast and hint
    Let buffer_index be 0
    
    For each variable in loop_variables:
        Let param_type be infer_metal_type with variable
        Let address_space be infer_metal_address_space with variable
        Let param_declaration be address_space joined with " " joined with param_type joined with "* " joined with variable["name"] joined with " [[buffer(" joined with buffer_index joined with ")]]"
        Add param_declaration to params
        Set buffer_index to buffer_index + 1
    
    Return join_list with params and ",\n                                    "

Process called "translate_runa_loop_to_metal" that takes runa_ast as Dictionary and hint as ParallelizationHint returns String:
    Note: Translate Runa loop body to Metal Shading Language
    Let metal_body be "    // Translated from Runa loop\n"
    
    Note: Get the loop AST node
    Let loop_node be find_loop_node with runa_ast and hint.loop_start
    If loop_node is null:
        Return "    // Error: Could not find loop node\n"
    
    Note: Translate loop body statements
    Let body_statements be loop_node["body"]
    For each statement in body_statements:
        Let metal_statement be translate_runa_statement_to_metal with statement
        Set metal_body to metal_body joined with "    " joined with metal_statement joined with "\n"
    
    Return metal_body

Process called "translate_runa_statement_to_metal" that takes statement as Dictionary returns String:
    Note: Translate individual Runa statement to Metal Shading Language
    Match statement["type"]:
        Case "assignment":
            Let variable be statement["variable"]
            Let expression be statement["expression"]
            Let metal_expr be translate_runa_expression_to_metal with expression
            Return variable joined with " = " joined with metal_expr joined with ";"
            
        Case "function_call":
            Let function_name be statement["function"]
            Let metal_function be map_runa_function_to_metal with function_name
            Let args be statement["arguments"]
            Let metal_args be empty list
            For each arg in args:
                Let metal_arg be translate_runa_expression_to_metal with arg
                Add metal_arg to metal_args
            Return metal_function joined with "(" joined with join_list with metal_args and ", " joined with ");"
            
        Case "if_statement":
            Let condition be statement["condition"]
            Let metal_condition be translate_runa_expression_to_metal with condition
            Let if_body be statement["body"]
            Let metal_body be translate_runa_statement_to_metal with if_body
            Return "if (" joined with metal_condition joined with ") {\n" joined with metal_body joined with "\n}"
            
        Otherwise:
            Return "/* Unsupported statement type: " joined with statement["type"] joined with " */"

Process called "translate_runa_expression_to_metal" that takes expression as Dictionary returns String:
    Note: Translate Runa expression to Metal Shading Language
    Match expression["type"]:
        Case "variable":
            Return expression["name"]
            
        Case "literal":
            Return expression["value"]
            
        Case "binary_operation":
            Let left be translate_runa_expression_to_metal with expression["left"]
            Let right be translate_runa_expression_to_metal with expression["right"]
            Let operator be expression["operator"]
            Return "(" joined with left joined with " " joined with operator joined with " " joined with right joined with ")"
            
        Case "array_access":
            Let array be translate_runa_expression_to_metal with expression["array"]
            Let index be translate_runa_expression_to_metal with expression["index"]
            Return array joined with "[" joined with index joined with "]"
            
        Case "math_function":
            Let function_name be expression["function"]
            Let metal_function be map_math_function_to_metal with function_name
            Let arg be translate_runa_expression_to_metal with expression["argument"]
            Return metal_function joined with "(" joined with arg joined with ")"
            
        Otherwise:
            Return "/* Unsupported expression: " joined with expression["type"] joined with " */"

Process called "map_runa_function_to_metal" that takes function_name as String returns String:
    Note: Map Runa function names to Metal Shading Language equivalents
    Match function_name:
        Case "sqrt":
            Return "sqrt"
        Case "sin":
            Return "sin"
        Case "cos":
            Return "cos"
        Case "exp":
            Return "exp"
        Case "log":
            Return "log"
        Case "abs":
            Return "abs"
        Case "min":
            Return "min"
        Case "max":
            Return "max"
        Case "pow":
            Return "pow"
        Otherwise:
            Return function_name  Note: Assume direct mapping

Process called "map_math_function_to_metal" that takes function_name as String returns String:
    Note: Map Runa math functions to Metal math functions
    Match function_name:
        Case "square_root":
            Return "sqrt"
        Case "natural_log":
            Return "log"
        Case "power":
            Return "pow"
        Case "absolute_value":
            Return "abs"
        Case "fast_inverse_sqrt":
            Return "rsqrt"  Note: Metal-specific fast reciprocal square root
        Otherwise:
            Return function_name

Note: Metal Library Compilation

Process called "compile_metal_shaders" that takes kernels as List[MetalKernel] and target as GpuCompilationTarget returns Boolean:
    Note: Compile Metal shading language code to Metal library
    For each kernel in kernels:
        Let compilation_success be compile_single_metal_kernel with kernel and target
        If not compilation_success:
            Return false
    Return true

Process called "compile_single_metal_kernel" that takes kernel as MetalKernel and target as GpuCompilationTarget returns Boolean:
    Note: Compile individual Metal kernel
    Let build_options be generate_metal_build_options with target
    Return host_call_metal_compile_kernel with kernel.msl_code and build_options

Process called "generate_metal_build_options" that takes target as GpuCompilationTarget returns String:
    Note: Generate Metal compiler build options for Apple Silicon optimization
    Let options be empty list
    
    Note: Optimization level
    Match target.optimization_level:
        Case 0:
            Add "-gline-tables-only" to options  Note: Debug info
        Case 1:
            Add "-O1" to options  Note: Basic optimization
        Case 2:
            Add "-O2" to options  Note: Standard optimization
        Case 3:
            Add "-O3" to options  Note: Aggressive optimization
            Add "-ffast-math" to options  Note: Fast math for Apple Silicon
        Otherwise:
            Add "-O2" to options  Note: Default
    
    Note: Apple Silicon specific optimizations
    If target.device.capabilities.supports_tensor_cores:  Note: Apple Silicon proxy
        Add "-fapple-silicon-optimizations" to options  Note: Hypothetical Apple Silicon flag
    
    Note: Enable SIMD group optimizations if supported
    Add "-fsimd-group-optimize" to options
    
    Return join_list with options and " "

Process called "create_metal_library" that takes kernels as List[MetalKernel] and target as GpuCompilationTarget returns String:
    Note: Create Metal library binary from compiled kernels
    Let combined_msl be ""
    
    Note: Combine all kernel MSL code
    For each kernel in kernels:
        Set combined_msl to combined_msl joined with kernel.msl_code joined with "\n"
    
    Note: Compile to Metal library binary
    Let build_options be generate_metal_build_options with target
    Return host_call_metal_create_library with combined_msl and build_options

Note: Performance Estimation and Optimization

Process called "estimate_metal_performance" that takes kernels as List[MetalKernel] and device as GpuDevice returns Dictionary[String, Integer]:
    Note: Estimate Metal kernel performance on Apple Silicon
    Let estimates be empty dictionary
    
    Note: Calculate threadgroup efficiency for each kernel
    For each kernel in kernels:
        Let threadgroup_size be kernel.threads_per_threadgroup[0]
        Let efficiency be estimate_metal_threadgroup_efficiency with threadgroup_size and device
        Set estimates[kernel.name joined with "_efficiency"] to efficiency
    
    Note: Estimate Apple Silicon specific metrics
    If device.capabilities.supports_tensor_cores:  Note: Apple Silicon proxy
        Let neural_engine_utilization be estimate_neural_engine_utilization with kernels
        Set estimates["neural_engine_utilization"] to neural_engine_utilization
    
    Note: Estimate unified memory bandwidth utilization
    Let memory_ops be count_memory_operations_metal with kernels
    Let bandwidth_utilization be estimate_unified_memory_bandwidth with memory_ops and device
    Set estimates["unified_memory_bandwidth_utilization"] to bandwidth_utilization
    
    Return estimates

Process called "estimate_metal_threadgroup_efficiency" that takes threadgroup_size as Integer and device as GpuDevice returns Integer:
    Note: Estimate threadgroup efficiency for Apple Silicon
    Let max_threadgroup_size be device.capabilities.max_threads_per_block
    
    Note: Apple Silicon prefers SIMD-aligned sizes
    Let simd_size be device.capabilities.warp_size  Note: 32 for Apple Silicon
    Let simd_efficiency be 100
    If (threadgroup_size % simd_size) is not 0:
        Set simd_efficiency to 80  Note: Penalize non-SIMD-aligned sizes
    
    Note: Calculate basic utilization
    Let utilization be (threadgroup_size * 100) / max_threadgroup_size
    
    Note: Combine SIMD and utilization efficiency
    Let combined_efficiency be (simd_efficiency * utilization) / 100
    
    If combined_efficiency > 100:
        Return 100
    Otherwise:
        Return combined_efficiency

Process called "estimate_neural_engine_utilization" that takes kernels as List[MetalKernel] returns Integer:
    Note: Estimate Apple Neural Engine utilization for ML workloads
    Let ml_operations be 0
    
    For each kernel in kernels:
        Let kernel_ml_ops be count_ml_operations_in_kernel with kernel
        Set ml_operations to ml_operations + kernel_ml_ops
    
    Note: Rough estimate based on operation count
    If ml_operations > 1000:
        Return 90  Note: High utilization
    Otherwise if ml_operations > 100:
        Return 60  Note: Medium utilization
    Otherwise:
        Return 20  Note: Low utilization

Process called "estimate_unified_memory_bandwidth" that takes memory_ops as Integer and device as GpuDevice returns Integer:
    Note: Estimate unified memory bandwidth utilization on Apple Silicon
    Let theoretical_bandwidth be device.capabilities.memory_bandwidth
    Let estimated_usage be (memory_ops * 4) / theoretical_bandwidth * 100  Note: Assume 4-byte operations
    
    Note: Apple Silicon unified memory is very efficient
    Set estimated_usage to estimated_usage * 1.2  Note: Boost for unified memory efficiency
    
    If estimated_usage > 100:
        Return 100
    Otherwise:
        Return estimated_usage

Note: Utility Functions

Process called "should_use_simd_groups" that takes hint as ParallelizationHint and device as GpuDevice returns Boolean:
    Note: Determine if SIMD groups should be used for optimization
    Note: Apple Silicon benefits from SIMD groups for certain patterns
    If not device.capabilities.supports_tensor_cores:  Note: Not Apple Silicon
        Return false
    
    Note: Use SIMD groups for sequential access patterns
    If hint.memory_access_pattern equals "sequential":
        Return true
    
    Note: Use SIMD groups for large work groups
    If hint.suggested_block_size >= 64:
        Return true
    
    Return false

Process called "infer_metal_type" that takes variable as Dictionary returns String:
    Note: Infer Metal Shading Language type from Runa variable
    Match variable["type"]:
        Case "Integer":
            Return "int"
        Case "Float":
            Return "float"
        Case "Double":
            Return "float"  Note: Most Apple GPUs don't support double precision
        Case "Boolean":
            Return "bool"
        Case "Vector2":
            Return "float2"
        Case "Vector3":
            Return "float3"
        Case "Vector4":
            Return "float4"
        Case "Array":
            Let element_type be infer_metal_type with variable["element_type"]
            Return element_type
        Otherwise:
            Return "float"  Note: Default to float

Process called "infer_metal_address_space" that takes variable as Dictionary returns String:
    Note: Infer Metal address space from variable usage
    If variable["is_input_buffer"]:
        Return "device"
    Otherwise if variable["is_threadgroup_shared"]:
        Return "threadgroup"
    Otherwise if variable["is_constant"]:
        Return "constant"
    Otherwise:
        Return "device"  Note: Default to device memory

Process called "calculate_metal_threadgroup_memory_usage" that takes hint as ParallelizationHint and config as Dictionary returns Integer:
    Note: Calculate threadgroup memory usage for Metal kernel
    Let threadgroup_size be config["threads_per_threadgroup"][0]
    
    Note: Apple Silicon has efficient threadgroup memory
    Match hint.memory_access_pattern:
        Case "sequential":
            Return threadgroup_size * 4  Note: 4 bytes per thread for sequential optimization
        Case "strided":
            Return threadgroup_size * 8  Note: More memory for stride optimization
        Otherwise:
            Return threadgroup_size * 4

Process called "count_memory_operations_metal" that takes kernels as List[MetalKernel] returns Integer:
    Note: Count memory operations in Metal kernels
    Let total_ops be 0
    For each kernel in kernels:
        Let kernel_ops be count_metal_kernel_memory_ops with kernel
        Set total_ops to total_ops + kernel_ops
    Return total_ops

Process called "count_metal_kernel_memory_ops" that takes kernel as MetalKernel returns Integer:
    Note: Count memory operations in single Metal kernel
    Let msl_code be kernel.msl_code
    Let load_count be count_occurrences with msl_code and "["  Note: Array accesses
    Let store_count be count_occurrences with msl_code and "]"
    Return load_count + store_count

Process called "count_ml_operations_in_kernel" that takes kernel as MetalKernel returns Integer:
    Note: Count machine learning operations in Metal kernel
    Let msl_code be kernel.msl_code
    Let ml_functions be ["dot", "matrix_multiply", "convolution", "activation"]
    Let ml_ops be 0
    
    For each ml_function in ml_functions:
        Let count be count_occurrences with msl_code and ml_function
        Set ml_ops to ml_ops + count
    
    Return ml_ops

Process called "estimate_metal_memory_bandwidth" that takes device_info as Dictionary returns Integer:
    Note: Estimate memory bandwidth for Apple devices
    Let apple_silicon_version be device_info["apple_silicon_version"]
    
    Match apple_silicon_version:
        Case "M1":
            Return 68  Note: M1 unified memory bandwidth in GB/s
        Case "M1 Pro":
            Return 200  Note: M1 Pro bandwidth
        Case "M1 Max":
            Return 400  Note: M1 Max bandwidth  
        Case "M2":
            Return 100  Note: M2 bandwidth
        Case "M2 Pro":
            Return 200  Note: M2 Pro bandwidth
        Case "M2 Max":
            Return 400  Note: M2 Max bandwidth
        Case "M3":
            Return 100  Note: M3 bandwidth (estimated)
        Otherwise:
            Return 50   Note: Conservative estimate for other Apple devices

Note: Validation

Process called "validate_metal_target" that takes target as GpuCompilationTarget returns List[String]:
    Note: Validate Metal compilation target
    Let errors be empty list
    
    If target.backend_type is not Metal:
        Add "Target backend is not Metal" to errors
    
    Note: Check if running on Apple platform
    Let current_platform be OS.get_platform with no parameters
    If current_platform is not "darwin":
        Add "Metal only available on macOS/iOS platforms" to errors
    
    If target.device.memory_available <= 0:
        Add "Device has no available memory" to errors
    
    Return errors

Note: Metal Backend Initialization

Process called "initialize_metal_backend" that takes configuration as Dictionary returns Boolean:
    Note: Initialize Metal backend
    Let current_platform be OS.get_platform with no parameters
    If current_platform is not "darwin":
        Return false  Note: Metal only available on Apple platforms
    
    Let metal_available be check_metal_availability with no parameters
    If not metal_available:
        Return false
    
    Return host_call_metal_runtime_init with configuration

Process called "shutdown_metal_backend" that takes no_parameters returns Boolean:
    Note: Shutdown Metal backend
    Return host_call_metal_runtime_shutdown with no parameters

Process called "check_metal_availability" that takes no_parameters returns Boolean:
    Note: Check if Metal is available on the system
    Return host_call_check_metal_availability with no parameters

Process called "create_gpu_module_from_metal" that takes kernels as List[MetalKernel] returns GpuModule:
    Note: Convert Metal kernels to generic GPU module
    Let kernel_functions be empty list
    
    For each metal_kernel in kernels:
        Let kernel_function be KernelFunction with:
            name as metal_kernel.name
            parameters as metal_kernel.parameters
            body as metal_kernel.msl_code
            shared_memory_size as metal_kernel.threadgroup_memory_size
            register_count as 0  Note: Metal doesn't expose register usage
            thread_count as metal_kernel.threads_per_threadgroup[0]
            block_dimensions as metal_kernel.threads_per_threadgroup
            grid_dimensions as metal_kernel.threadgroups_per_grid
        Add kernel_function to kernel_functions
    
    Return GpuModule with:
        kernels as kernel_functions
        global_memory_allocations as empty list
        constant_memory_data as empty dictionary
        backend_specific_code as join_kernels_msl with kernels
        compilation_flags as empty list

Process called "join_kernels_msl" that takes kernels as List[MetalKernel] returns String:
    Note: Combine all Metal kernel MSL code
    Let combined_msl be ""
    For each kernel in kernels:
        Set combined_msl to combined_msl joined with kernel.msl_code joined with "\n"
    Return combined_msl

Process called "square_root" that takes value as Integer returns Integer:
    Note: Calculate integer square root for threadgroup optimization
    Let guess be value / 2
    While (guess * guess) > value:
        Set guess to (guess + value / guess) / 2
    Return guess

Note: Host Interface Functions (implemented by runtime)

Process called "host_call_metal_get_devices" that takes no_parameters returns List[Dictionary]:
    Note: Host-provided Metal device detection
    Return empty list  Note: Will be implemented by host environment

Process called "host_call_metal_compile_kernel" that takes msl_code as String and build_options as String returns Boolean:
    Note: Host-provided Metal kernel compilation
    Return false  Note: Will be implemented by host environment

Process called "host_call_metal_create_library" that takes msl_code as String and build_options as String returns String:
    Note: Host-provided Metal library creation
    Return ""  Note: Will be implemented by host environment

Process called "host_call_metal_runtime_init" that takes configuration as Dictionary returns Boolean:
    Note: Host-provided Metal runtime initialization
    Return false  Note: Will be implemented by host environment

Process called "host_call_metal_runtime_shutdown" that takes no_parameters returns Boolean:
    Note: Host-provided Metal runtime shutdown
    Return false  Note: Will be implemented by host environment

Process called "host_call_check_metal_availability" that takes no_parameters returns Boolean:
    Note: Host-provided Metal availability check
    Return false  Note: Will be implemented by host environment

Note: Missing Utility Functions Implementation (Shared with CUDA/OpenCL backends)

Process called "extract_loop_variables" that takes runa_ast as Dictionary and hint as ParallelizationHint returns List[Dictionary]:
    Note: Extract variables used in loop for kernel parameter generation
    Let variables be empty list
    
    Note: Find loop node in AST
    Let loop_node be find_loop_node with runa_ast and hint.loop_start
    If loop_node is null:
        Return variables
    
    Note: Extract variable references from loop body
    Let variable_names be extract_variable_names_from_node with loop_node
    For each var_name in variable_names:
        Let variable_info be Dictionary with:
            "name" as var_name
            "type" as "Float"  Note: Default type
            "is_input_buffer" as true
            "is_threadgroup_shared" as false
            "is_constant" as false
        Add variable_info to variables
    
    Return variables

Process called "find_loop_node" that takes runa_ast as Dictionary and loop_start as Integer returns Dictionary:
    Note: Find loop node in AST by start position
    If runa_ast contains "type" and runa_ast["type"] equals "loop":
        If runa_ast contains "start_position" and runa_ast["start_position"] equals loop_start:
            Return runa_ast
    
    Note: Search child nodes recursively
    If runa_ast contains "children":
        For each child in runa_ast["children"]:
            Let found_node be find_loop_node with child and loop_start
            If found_node is not null:
                Return found_node
    
    Return null

Process called "extract_variable_names_from_node" that takes node as Dictionary returns List[String]:
    Note: Extract all variable names referenced in AST node
    Let variable_names be empty list
    
    If node contains "type":
        Match node["type"]:
            Case "variable":
                If node contains "name":
                    Add node["name"] to variable_names
            Case "array_access":
                If node contains "array":
                    Let array_vars be extract_variable_names_from_node with node["array"]
                    Add all items from array_vars to variable_names
                If node contains "index":
                    Let index_vars be extract_variable_names_from_node with node["index"]
                    Add all items from index_vars to variable_names
            Otherwise:
                Note: Handle other node types
                If node contains "children":
                    For each child in node["children"]:
                        Let child_vars be extract_variable_names_from_node with child
                        Add all items from child_vars to variable_names
    
    Return variable_names

Process called "join_list" that takes items as List[String] and separator as String returns String:
    Note: Join list of strings with separator
    If length of items equals 0:
        Return ""
    
    Let result be items[0]
    Let index be 1
    While index < length of items:
        Set result to result joined with separator joined with items[index]
        Set index to index + 1
    
    Return result

Process called "minimum" that takes a as Integer and b as Integer returns Integer:
    Note: Return minimum of two integers
    If a < b:
        Return a
    Otherwise:
        Return b