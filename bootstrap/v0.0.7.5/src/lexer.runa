# Lexer module for Runa self-hosted compiler v0.0.7.5
# Transliterated from lexer.c in v0.0.7

Import "types" as Types

# Helper process to check if a character is whitespace
Process called "is_whitespace" that takes ch as String returns Boolean:
    If ch is equal to " " or ch is equal to "\t" or ch is equal to "\n" or ch is equal to "\r":
        Return true
    Otherwise:
        Return false
End Process

# Helper process to check if a character is a digit
Process called "is_digit" that takes ch as String returns Boolean:
    Let ascii_value be ascii_value_of(ch)
    If ascii_value is greater than or equal to 48 and ascii_value is less than or equal to 57:
        Return true
    Otherwise:
        Return false
End Process

# Helper process to check if a character is alphabetic
Process called "is_alpha" that takes ch as String returns Boolean:
    Let ascii_value be ascii_value_of(ch)
    If ascii_value is greater than or equal to 65 and ascii_value is less than or equal to 90:
        Return true
    Otherwise if ascii_value is greater than or equal to 97 and ascii_value is less than or equal to 122:
        Return true
    Otherwise:
        Return false
End Process

# Helper process to check if a character is alphanumeric or underscore
Process called "is_word_char" that takes ch as String returns Boolean:
    If is_alpha(ch) or is_digit(ch) or ch is equal to "_":
        Return true
    Otherwise:
        Return false
End Process

# Advance the lexer position by one character
Process called "lexer_advance" that takes lexer as Types.Lexer returns Nothing:
    If lexer.current_char is equal to "\n":
        Set lexer.line to lexer.line plus 1
        Set lexer.column to 0
    End If

    Set lexer.position to lexer.position plus 1
    Set lexer.column to lexer.column plus 1

    Let source_length be length_of(lexer.source)
    If lexer.position is greater than or equal to source_length:
        Set lexer.current_char to ""
    Otherwise:
        Set lexer.current_char to substring_of(lexer.source, lexer.position, 1)
    End If
End Process

# Skip whitespace characters
Process called "lexer_skip_whitespace" that takes lexer as Types.Lexer returns Nothing:
    While lexer.current_char is not equal to "" and is_whitespace(lexer.current_char):
        lexer_advance(lexer)
    End While
End Process

# Create a new token
Process called "token_create" that takes token_type as Types.TokenType and value as String and line as Integer and column as Integer returns Types.Token:
    Let token be a value of type Types.Token with
        token_type as token_type,
        value as value,
        line as line,
        column as column
    Return token
End Process

# Read a string literal from the source
Process called "lexer_read_string_literal" that takes lexer as Types.Lexer returns String:
    lexer_advance(lexer) # Skip opening quote
    Let start_pos be lexer.position

    While lexer.current_char is not equal to "" and lexer.current_char is not equal to "\"":
        lexer_advance(lexer)
    End While

    If lexer.current_char is equal to "\"":
        Let length be lexer.position minus start_pos
        Let string be substring_of(lexer.source, start_pos, length)
        lexer_advance(lexer) # Skip closing quote
        Return string
    Otherwise:
        Return "" # Unterminated string
    End If
End Process

# Read a word (identifier or keyword) from the source
Process called "lexer_read_word" that takes lexer as Types.Lexer returns String:
    Let start_pos be lexer.position

    While lexer.current_char is not equal to "" and is_word_char(lexer.current_char):
        lexer_advance(lexer)
    End While

    Let length be lexer.position minus start_pos
    Let word be substring_of(lexer.source, start_pos, length)
    Return word
End Process

# Read an integer literal from the source
Process called "lexer_read_integer" that takes lexer as Types.Lexer returns String:
    Let start_pos be lexer.position

    While lexer.current_char is not equal to "" and is_digit(lexer.current_char):
        lexer_advance(lexer)
    End While

    Let length be lexer.position minus start_pos
    Let integer be substring_of(lexer.source, start_pos, length)
    Return integer
End Process

# Classify a word as either a keyword or identifier
Process called "classify_word" that takes word as String returns Integer:
    If word is equal to "Process":
        Return Types.TOKEN_PROCESS
    Otherwise if word is equal to "called":
        Return Types.TOKEN_CALLED
    Otherwise if word is equal to "returns":
        Return Types.TOKEN_RETURNS
    Otherwise if word is equal to "Integer":
        Return Types.TOKEN_INTEGER_TYPE
    Otherwise if word is equal to "Return":
        Return Types.TOKEN_RETURN
    Otherwise if word is equal to "End":
        Return Types.TOKEN_END
    Otherwise if word is equal to "Let":
        Return Types.TOKEN_LET
    Otherwise if word is equal to "be":
        Return Types.TOKEN_BE
    Otherwise if word is equal to "Set":
        Return Types.TOKEN_SET
    Otherwise if word is equal to "to":
        Return Types.TOKEN_TO
    Otherwise if word is equal to "plus":
        Return Types.TOKEN_PLUS
    Otherwise if word is equal to "minus":
        Return Types.TOKEN_MINUS
    Otherwise if word is equal to "If":
        Return Types.TOKEN_IF
    Otherwise if word is equal to "Otherwise":
        Return Types.TOKEN_OTHERWISE
    Otherwise if word is equal to "While":
        Return Types.TOKEN_WHILE
    Otherwise if word is equal to "is":
        Return Types.TOKEN_IS
    Otherwise if word is equal to "equal":
        Return Types.TOKEN_EQUAL
    Otherwise if word is equal to "less":
        Return Types.TOKEN_LESS
    Otherwise if word is equal to "greater":
        Return Types.TOKEN_GREATER
    Otherwise if word is equal to "than":
        Return Types.TOKEN_THAN
    Otherwise if word is equal to "or":
        Return Types.TOKEN_OR
    Otherwise if word is equal to "that":
        Return Types.TOKEN_THAT
    Otherwise if word is equal to "takes":
        Return Types.TOKEN_TAKES
    Otherwise if word is equal to "as":
        Return Types.TOKEN_AS
    Otherwise if word is equal to "multiplied":
        Return Types.TOKEN_MULTIPLIED
    Otherwise if word is equal to "divided":
        Return Types.TOKEN_DIVIDED
    Otherwise if word is equal to "by":
        Return Types.TOKEN_BY
    Otherwise if word is equal to "Print":
        Return Types.TOKEN_PRINT
    Otherwise if word is equal to "Type":
        Return Types.TOKEN_TYPE
    Otherwise if word is equal to "read_file":
        Return Types.TOKEN_READ_FILE
    Otherwise if word is equal to "write_file":
        Return Types.TOKEN_WRITE_FILE
    Otherwise if word is equal to "Import":
        Return Types.TOKEN_IMPORT
    Otherwise:
        Return Types.TOKEN_IDENTIFIER
    End If
End Process

# Create a new lexer instance
Process called "lexer_create" that takes source as String returns Types.Lexer:
    Let first_char be ""
    If length_of(source) is greater than 0:
        Set first_char to substring_of(source, 0, 1)
    End If

    Let lexer be a value of type Types.Lexer with
        source as source,
        position as 0,
        line as 1,
        column as 1,
        current_char as first_char
    Return lexer
End Process

# Get the next token from the lexer
Process called "lexer_next_token" that takes lexer as Types.Lexer returns Types.Token:
    While lexer.current_char is not equal to "":
        Let line be lexer.line
        Let column be lexer.column

        If is_whitespace(lexer.current_char):
            lexer_skip_whitespace(lexer)
            # Continue to next iteration
        Otherwise if lexer.current_char is equal to "\"":
            Let string_value be lexer_read_string_literal(lexer)
            If string_value is equal to "":
                # Unterminated string - create error token
                Let error_token_type be a value of type Types.TokenType with value as Types.TOKEN_ERROR
                Return token_create(error_token_type, "Unterminated string", line, column)
            Otherwise:
                Let string_token_type be a value of type Types.TokenType with value as Types.TOKEN_STRING_LITERAL
                Return token_create(string_token_type, string_value, line, column)
            End If
        Otherwise if is_digit(lexer.current_char):
            Let integer_value be lexer_read_integer(lexer)
            Let integer_token_type be a value of type Types.TokenType with value as Types.TOKEN_INTEGER
            Return token_create(integer_token_type, integer_value, line, column)
        Otherwise if is_alpha(lexer.current_char):
            Let word be lexer_read_word(lexer)
            Let word_type be classify_word(word)
            Let word_token_type be a value of type Types.TokenType with value as word_type
            Return token_create(word_token_type, word, line, column)
        Otherwise if lexer.current_char is equal to ":":
            lexer_advance(lexer)
            Let colon_token_type be a value of type Types.TokenType with value as Types.TOKEN_COLON
            Return token_create(colon_token_type, ":", line, column)
        Otherwise if lexer.current_char is equal to "(":
            lexer_advance(lexer)
            Let lparen_token_type be a value of type Types.TokenType with value as Types.TOKEN_LPAREN
            Return token_create(lparen_token_type, "(", line, column)
        Otherwise if lexer.current_char is equal to ")":
            lexer_advance(lexer)
            Let rparen_token_type be a value of type Types.TokenType with value as Types.TOKEN_RPAREN
            Return token_create(rparen_token_type, ")", line, column)
        Otherwise if lexer.current_char is equal to ".":
            lexer_advance(lexer)
            Let dot_token_type be a value of type Types.TokenType with value as Types.TOKEN_DOT
            Return token_create(dot_token_type, ".", line, column)
        Otherwise if lexer.current_char is equal to ",":
            lexer_advance(lexer)
            Let comma_token_type be a value of type Types.TokenType with value as Types.TOKEN_COMMA
            Return token_create(comma_token_type, ",", line, column)
        Otherwise:
            # Unexpected character - create error token
            lexer_advance(lexer)
            Let error_token_type be a value of type Types.TokenType with value as Types.TOKEN_ERROR
            Return token_create(error_token_type, "Unexpected character", line, column)
        End If
    End While

    # End of file reached
    Let eof_token_type be a value of type Types.TokenType with value as Types.TOKEN_EOF
    Return token_create(eof_token_type, "", lexer.line, lexer.column)
End Process