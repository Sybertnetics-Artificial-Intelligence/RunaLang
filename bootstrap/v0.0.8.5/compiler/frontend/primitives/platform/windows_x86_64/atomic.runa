Note:
Copyright 2025 Sybertnetics Artificial Intelligence Solutions

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
:End Note

Note:
WINDOWS x86_64 ATOMIC OPERATIONS

Platform-specific atomic operations using x86_64 inline assembly.

x86_64 Memory Model: Total Store Order (TSO)
- Loads are NOT reordered with loads
- Stores are NOT reordered with stores
- Loads are NOT reordered with earlier stores
- Stores MAY be reordered with earlier loads
- Result: Most operations are naturally acquire/release on x86_64

Atomic Instructions:
- MOV (aligned 64-bit): Naturally atomic
- LOCK prefix: Makes instruction atomic (XADD, CMPXCHG, etc.)
- MFENCE: Full memory barrier
- LFENCE: Load fence (acquire barrier)
- SFENCE: Store fence (release barrier)

Important: x86_64 TSO means we rarely need explicit fences!
:End Note

Import "compiler/frontend/primitives/core/memory_core.runa" as Memory

Note: ============================================================================
Note: Atomic Load Operations
Note: ============================================================================

Process called "atomic_load_relaxed" takes ptr as Integer returns Integer:
    Note: x86_64: regular aligned load is naturally atomic
    Note: No fence needed for relaxed ordering
    Let result be 0
    Inline Assembly:
        // Load ptr address into RDI
        mov rdi, [rbp-8]

        // Atomic load (naturally atomic on x86_64 for aligned 64-bit)
        mov rax, [rdi]

        // Store result
        mov [rbp-16], rax
    End Assembly
    Return result
End Process

Process called "atomic_load_acquire" takes ptr as Integer returns Integer:
    Note: x86_64 TSO: all loads are naturally acquire (no reordering after)
    Note: No fence needed!
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rax, [rdi]
        mov [rbp-16], rax
    End Assembly
    Return result
End Process

Process called "atomic_load_seq_cst" takes ptr as Integer returns Integer:
    Note: x86_64: use MFENCE after load for seq_cst
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rax, [rdi]
        mfence  // Full memory barrier for seq_cst
        mov [rbp-16], rax
    End Assembly
    Return result
End Process

Note: ============================================================================
Note: Atomic Store Operations
Note: ============================================================================

Process called "atomic_store_relaxed" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: regular aligned store is naturally atomic
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // value
        mov [rdi], rax     // Atomic store
    End Assembly
    Return 0
End Process

Process called "atomic_store_release" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64 TSO: all stores are naturally release (no reordering before)
    Note: No fence needed!
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rax, [rbp-16]
        mov [rdi], rax
    End Assembly
    Return 0
End Process

Process called "atomic_store_seq_cst" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: use XCHG for seq_cst store (XCHG has implicit LOCK)
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // value
        xchg [rdi], rax    // Atomic exchange (seq_cst)
    End Assembly
    Return 0
End Process

Note: ============================================================================
Note: Atomic Fetch-and-Modify Operations
Note: ============================================================================

Process called "atomic_fetch_add" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: LOCK XADD atomically adds and returns old value
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // value
        lock xadd [rdi], rax  // Atomic fetch-add
        mov [rbp-24], rax     // OLD value is in RAX
    End Assembly
    Return result
End Process

Process called "atomic_fetch_sub" takes ptr as Integer, value as Integer returns Integer:
    Note: Subtract is add with negated value
    Note: x86_64: NEG + LOCK XADD
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // value
        neg rax            // Negate for subtraction
        lock xadd [rdi], rax
        mov [rbp-24], rax
    End Assembly
    Return result
End Process

Process called "atomic_fetch_and" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: Use CAS loop (no LOCK AND that returns old value)
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rcx, [rbp-16]  // value (mask)

        // Load current value
        mov rax, [rdi]

        .retry:
        mov rdx, rax       // Copy current to RDX
        and rdx, rcx       // Apply AND operation
        lock cmpxchg [rdi], rdx  // Try to swap
        jne .retry         // Retry if failed

        // RAX contains old value
        mov [rbp-24], rax
    End Assembly
    Return result
End Process

Process called "atomic_fetch_or" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: Use CAS loop
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rcx, [rbp-16]
        mov rax, [rdi]

        .retry:
        mov rdx, rax
        or rdx, rcx
        lock cmpxchg [rdi], rdx
        jne .retry

        mov [rbp-24], rax
    End Assembly
    Return result
End Process

Process called "atomic_fetch_xor" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: Use CAS loop
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rcx, [rbp-16]
        mov rax, [rdi]

        .retry:
        mov rdx, rax
        xor rdx, rcx
        lock cmpxchg [rdi], rdx
        jne .retry

        mov [rbp-24], rax
    End Assembly
    Return result
End Process

Note: ============================================================================
Note: Compare-and-Swap Operations
Note: ============================================================================

Process called "atomic_compare_and_swap" takes ptr as Integer, expected as Integer, desired as Integer returns Integer:
    Note: x86_64: LOCK CMPXCHG (strong CAS, never spuriously fails)
    Note: Compares RAX with [ptr], if equal sets [ptr]=desired
    Note: Returns 1 if successful, 0 if failed
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // expected
        mov rcx, [rbp-24]  // desired

        lock cmpxchg [rdi], rcx

        // Set result based on ZF (zero flag)
        sete al            // AL = 1 if ZF=1 (success), 0 otherwise
        movzx rax, al      // Zero-extend to 64-bit
        mov [rbp-32], rax
    End Assembly
    Return result
End Process

Process called "atomic_compare_and_swap_weak" takes ptr as Integer, expected as Integer, desired as Integer returns Integer:
    Note: x86_64: Same as strong CAS (CMPXCHG never spuriously fails)
    Note: Weak CAS is for LL/SC architectures, not needed on x86_64
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]
        mov rax, [rbp-16]
        mov rcx, [rbp-24]

        lock cmpxchg [rdi], rcx

        sete al
        movzx rax, al
        mov [rbp-32], rax
    End Assembly
    Return result
End Process

Process called "atomic_exchange" takes ptr as Integer, value as Integer returns Integer:
    Note: x86_64: XCHG (implicit LOCK prefix)
    Note: Returns old value
    Let result be 0
    Inline Assembly:
        mov rdi, [rbp-8]   // ptr
        mov rax, [rbp-16]  // value

        xchg [rdi], rax    // Atomic exchange (implicit lock)

        mov [rbp-24], rax  // OLD value
    End Assembly
    Return result
End Process

Note: ============================================================================
Note: Memory Barrier Operations
Note: ============================================================================

Process called "memory_barrier_acquire" returns Integer:
    Note: x86_64 TSO: compiler barrier only (no CPU fence needed)
    Note: Use empty asm with memory clobber to prevent compiler reordering
    Inline Assembly:
        // Compiler barrier (prevents compiler reordering)
        // CPU doesn't need fence due to TSO
    End Assembly
    Return 0
End Process

Process called "memory_barrier_release" returns Integer:
    Note: x86_64 TSO: compiler barrier only
    Inline Assembly:
        // Compiler barrier
    End Assembly
    Return 0
End Process

Process called "memory_barrier_full" returns Integer:
    Note: x86_64: MFENCE for full barrier
    Inline Assembly:
        mfence  // Full memory fence
    End Assembly
    Return 0
End Process

Process called "memory_barrier_seq_cst" returns Integer:
    Note: x86_64: MFENCE for sequentially consistent ordering
    Inline Assembly:
        mfence
    End Assembly
    Return 0
End Process
